{
  "last_updated": "2025-05-22T08:23:42.704731-04:00",
  "papers": [
    {
      "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI\n  Agents",
      "summary": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,\ncoupling online Reinforcement Learning (RL) with explicit chain-of-thought\nreasoning prior to object grounding and thereby achieving substantial\nperformance gains. In this paper, we first conduct extensive analysis\nexperiments of three key components of that training pipeline: input design,\noutput evaluation, and policy update-each revealing distinct challenges arising\nfrom blindly applying general-purpose RL without adapting to GUI grounding\ntasks. Input design: Current templates encourage the model to generate\nchain-of-thought reasoning, but longer chains unexpectedly lead to worse\ngrounding performance. Output evaluation: Reward functions based on hit signals\nor box area allow models to exploit box size, leading to reward hacking and\npoor localization quality. Policy update: Online RL tends to overfit easy\nexamples due to biases in length and sample difficulty, leading to\nunder-optimization on harder cases. To address these issues, we propose three\ntargeted solutions. First, we adopt a Fast Thinking Template that encourages\ndirect answer generation, reducing excessive reasoning during training. Second,\nwe incorporate a box size constraint into the reward function to mitigate\nreward hacking. Third, we revise the RL objective by adjusting length\nnormalization and adding a difficulty-aware scaling factor, enabling better\noptimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with\nQwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on\nScreenSpot-Pro. This surpasses all prior models of similar size and even\noutperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI\nagent grounding. The project repository is available at\nhttps://github.com/Yuqi-Zhou/GUI-G1.",
      "url": "http://arxiv.org/abs/2505.15810v1",
      "published_time_eastern_timestamp": 1747850349.0
    },
    {
      "title": "The Agentic Economy",
      "summary": "Generative AI has transformed human-computer interaction by enabling natural\nlanguage interfaces and the emergence of autonomous agents capable of acting on\nusers' behalf. While early applications have improved individual productivity,\nthese gains have largely been confined to predefined tasks within existing\nworkflows. We argue that the more profound economic impact lies in reducing\ncommunication frictions between consumers and businesses. This shift could\nreorganize markets, redistribute power, and catalyze the creation of new\nproducts and services. We explore the implications of an agentic economy, where\nassistant agents act on behalf of consumers and service agents represent\nbusinesses, interacting programmatically to facilitate transactions. A key\ndistinction we draw is between unscripted interactions -- enabled by technical\nadvances in natural language and protocol design -- and unrestricted\ninteractions, which depend on market structures and governance. We examine the\ncurrent limitations of siloed and end-to-end agents, and explore future\nscenarios shaped by technical standards and market dynamics. These include the\npotential tension between agentic walled gardens and an open web of agents,\nimplications for advertising and discovery, the evolution of\nmicro-transactions, and the unbundling and rebundling of digital goods.\nUltimately, we argue that the architecture of agentic communication will\ndetermine the extent to which generative AI democratizes access to economic\nopportunity.",
      "url": "http://arxiv.org/abs/2505.15799v1",
      "published_time_eastern_timestamp": 1747849896.0
    },
    {
      "title": "HCRMP: A LLM-Hinted Contextual Reinforcement Learning Framework for\n  Autonomous Driving",
      "summary": "Integrating Large Language Models (LLMs) with Reinforcement Learning (RL) can\nenhance autonomous driving (AD) performance in complex scenarios. However,\ncurrent LLM-Dominated RL methods over-rely on LLM outputs, which are prone to\nhallucinations.Evaluations show that state-of-the-art LLM indicates a\nnon-hallucination rate of only approximately 57.95% when assessed on essential\ndriving-related tasks. Thus, in these methods, hallucinations from the LLM can\ndirectly jeopardize the performance of driving policies. This paper argues that\nmaintaining relative independence between the LLM and the RL is vital for\nsolving the hallucinations problem. Consequently, this paper is devoted to\npropose a novel LLM-Hinted RL paradigm. The LLM is used to generate semantic\nhints for state augmentation and policy optimization to assist RL agent in\nmotion planning, while the RL agent counteracts potential erroneous semantic\nindications through policy learning to achieve excellent driving performance.\nBased on this paradigm, we propose the HCRMP (LLM-Hinted Contextual\nReinforcement Learning Motion Planner) architecture, which is designed that\nincludes Augmented Semantic Representation Module to extend state space.\nContextual Stability Anchor Module enhances the reliability of multi-critic\nweight hints by utilizing information from the knowledge base. Semantic Cache\nModule is employed to seamlessly integrate LLM low-frequency guidance with RL\nhigh-frequency control. Extensive experiments in CARLA validate HCRMP's strong\noverall driving performance. HCRMP achieves a task success rate of up to 80.3%\nunder diverse driving conditions with different traffic densities. Under\nsafety-critical driving conditions, HCRMP significantly reduces the collision\nrate by 11.4%, which effectively improves the driving performance in complex\nscenarios.",
      "url": "http://arxiv.org/abs/2505.15793v1",
      "published_time_eastern_timestamp": 1747849644.0
    },
    {
      "title": "Solving General-Utility Markov Decision Processes in the Single-Trial\n  Regime with Online Planning",
      "summary": "In this work, we contribute the first approach to solve infinite-horizon\ndiscounted general-utility Markov decision processes (GUMDPs) in the\nsingle-trial regime, i.e., when the agent's performance is evaluated based on a\nsingle trajectory. First, we provide some fundamental results regarding policy\noptimization in the single-trial regime, investigating which class of policies\nsuffices for optimality, casting our problem as a particular MDP that is\nequivalent to our original problem, as well as studying the computational\nhardness of policy optimization in the single-trial regime. Second, we show how\nwe can leverage online planning techniques, in particular a Monte-Carlo tree\nsearch algorithm, to solve GUMDPs in the single-trial regime. Third, we provide\nexperimental results showcasing the superior performance of our approach in\ncomparison to relevant baselines.",
      "url": "http://arxiv.org/abs/2505.15782v1",
      "published_time_eastern_timestamp": 1747848743.0
    },
    {
      "title": "Alignment Under Pressure: The Case for Informed Adversaries When\n  Evaluating LLM Defenses",
      "summary": "Large language models (LLMs) are rapidly deployed in real-world applications\nranging from chatbots to agentic systems. Alignment is one of the main\napproaches used to defend against attacks such as prompt injection and\njailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even\nagainst Greedy Coordinate Gradient (GCG), a white-box attack that generates\nadversarial suffixes to induce attacker-desired outputs. However, this search\nspace over discrete tokens is extremely large, making the task of finding\nsuccessful attacks difficult. GCG has, for instance, been shown to converge to\nlocal minima, making it sensitive to initialization choices. In this paper, we\nassess the future-proof robustness of these defenses using a more informed\nthreat model: attackers who have access to some information about the alignment\nprocess. Specifically, we propose an informed white-box attack leveraging the\nintermediate model checkpoints to initialize GCG, with each checkpoint acting\nas a stepping stone for the next one. We show this approach to be highly\neffective across state-of-the-art (SOTA) defenses and models. We further show\nour informed initialization to outperform other initialization methods and show\na gradient-informed checkpoint selection strategy to greatly improve attack\nperformance and efficiency. Importantly, we also show our method to\nsuccessfully find universal adversarial suffixes -- single suffixes effective\nacross diverse inputs. Our results show that, contrary to previous beliefs,\neffective adversarial suffixes do exist against SOTA alignment-based defenses,\nthat these can be found by existing attack methods when adversaries exploit\nalignment knowledge, and that even universal suffixes exist. Taken together,\nour results highlight the brittleness of current alignment-based methods and\nthe need to consider stronger threat models when testing the safety of LLMs.",
      "url": "http://arxiv.org/abs/2505.15738v1",
      "published_time_eastern_timestamp": 1747845797.0
    },
    {
      "title": "DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning",
      "summary": "Large language models (LLMs) have improved significantly in their reasoning\nthrough extensive training on massive datasets. However, relying solely on\nadditional data for improvement is becoming increasingly impractical,\nhighlighting the need for models to autonomously enhance their reasoning\nwithout external supervision. In this paper, we propose Debate, Train, Evolve\n(DTE), a novel ground truth-free training framework that uses multi-agent\ndebate traces to evolve a single language model. We also introduce a new\nprompting strategy Reflect-Critique-Refine, to improve debate quality by\nexplicitly instructing agents to critique and refine their reasoning. Extensive\nevaluations on five reasoning benchmarks with six open-weight models show that\nour DTE framework achieve substantial improvements, with an average accuracy\ngain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe\nstrong cross-domain generalization, with an average accuracy gain of 5.8% on\nall other benchmarks, suggesting that our method captures general reasoning\ncapabilities.",
      "url": "http://arxiv.org/abs/2505.15734v1",
      "published_time_eastern_timestamp": 1747845612.0
    },
    {
      "title": "Quantum Dots as Functional Nanosystems for Enhanced Biomedical\n  Applications",
      "summary": "Quantum dots (QDs) have emerged as promising nanomaterials with unique\noptical and physical properties, making them highly attractive for various\napplications in biomedicine. This review provides a comprehensive overview of\nthe types, modes of synthesis, characterization, applications, and recent\nadvances of QDs in the field of biomedicine, with a primary focus on\nbioimaging, drug delivery, and biosensors. The unique properties of QDs, such\nas tunable emission spectra, long-term photostability, high quantum yield, and\ntargeted drug delivery, hold tremendous promise for advancing diagnostics,\ntherapeutics, and imaging techniques in biomedical research. However, several\nsignificant hurdles remain before their full potential in the biomedical field,\nlike bioaccumulation, toxicity, and short-term stability. Addressing these\nhurdles is essential to effectively incorporate QDs into clinical use and\nenhance their influence on healthcare outcomes. Furthermore, the review\nconducts a critical analysis of potential QD toxicity and explores recent\nprogress in strategies and methods to mitigate these adverse effects, such as\nsurface modification, surface coatings, and encapsulation. By thoroughly\nexamining current research and recent advancements, this comprehensive review\noffers invaluable insights into both the future possibilities and the\nchallenges that lie ahead in fully harnessing the potential of QDs in the field\nof biomedicine, promising a revolution in the landscape of medical diagnostics,\ntherapies, and imaging technologies.",
      "url": "http://arxiv.org/abs/2505.15705v1",
      "published_time_eastern_timestamp": 1747844319.0
    },
    {
      "title": "HAMF: A Hybrid Attention-Mamba Framework for Joint Scene Context\n  Understanding and Future Motion Representation Learning",
      "summary": "Motion forecasting represents a critical challenge in autonomous driving\nsystems, requiring accurate prediction of surrounding agents' future\ntrajectories. While existing approaches predict future motion states with the\nextracted scene context feature from historical agent trajectories and road\nlayouts, they suffer from the information degradation during the scene feature\nencoding. To address the limitation, we propose HAMF, a novel motion\nforecasting framework that learns future motion representations with the scene\ncontext encoding jointly, to coherently combine the scene understanding and\nfuture motion state prediction. We first embed the observed agent states and\nmap information into 1D token sequences, together with the target multi-modal\nfuture motion features as a set of learnable tokens. Then we design a unified\nAttention-based encoder, which synergistically combines self-attention and\ncross-attention mechanisms to model the scene context information and aggregate\nfuture motion features jointly. Complementing the encoder, we implement the\nMamba module in the decoding stage to further preserve the consistency and\ncorrelations among the learned future motion representations, to generate the\naccurate and diverse final trajectories. Extensive experiments on Argoverse 2\nbenchmark demonstrate that our hybrid Attention-Mamba model achieves\nstate-of-the-art motion forecasting performance with the simple and lightweight\narchitecture.",
      "url": "http://arxiv.org/abs/2505.15703v1",
      "published_time_eastern_timestamp": 1747844212.0
    },
    {
      "title": "Average Reward Reinforcement Learning for Omega-Regular and Mean-Payoff\n  Objectives",
      "summary": "Recent advances in reinforcement learning (RL) have renewed focus on the\ndesign of reward functions that shape agent behavior. Manually designing reward\nfunctions is tedious and error-prone. A principled alternative is to specify\nbehaviors in a formal language that can be automatically translated into\nrewards. Omega-regular languages are a natural choice for this purpose, given\ntheir established role in formal verification and synthesis. However, existing\nmethods using omega-regular specifications typically rely on discounted reward\nRL in episodic settings, with periodic resets. This setup misaligns with the\nsemantics of omega-regular specifications, which describe properties over\ninfinite behavior traces. In such cases, the average reward criterion and the\ncontinuing setting -- where the agent interacts with the environment over a\nsingle, uninterrupted lifetime -- are more appropriate.\n  To address the challenges of infinite-horizon, continuing tasks, we focus on\nabsolute liveness specifications -- a subclass of omega-regular languages that\ncannot be violated by any finite behavior prefix, making them well-suited to\nthe continuing setting. We present the first model-free RL framework that\ntranslates absolute liveness specifications to average-reward objectives. Our\napproach enables learning in communicating MDPs without episodic resetting. We\nalso introduce a reward structure for lexicographic multi-objective\noptimization, aiming to maximize an external average-reward objective among the\npolicies that also maximize the satisfaction probability of a given\nomega-regular specification. Our method guarantees convergence in unknown\ncommunicating MDPs and supports on-the-fly reductions that do not require full\nknowledge of the environment, thus enabling model-free RL. Empirical results\nshow our average-reward approach in continuing setting outperforms\ndiscount-based methods across benchmarks.",
      "url": "http://arxiv.org/abs/2505.15693v1",
      "published_time_eastern_timestamp": 1747843611.0
    },
    {
      "title": "From Grounding to Manipulation: Case Studies of Foundation Model\n  Integration in Embodied Robotic Systems",
      "summary": "Foundation models (FMs) are increasingly used to bridge language and action\nin embodied agents, yet the operational characteristics of different FM\nintegration strategies remain under-explored -- particularly for complex\ninstruction following and versatile action generation in changing environments.\nThis paper examines three paradigms for building robotic systems: end-to-end\nvision-language-action (VLA) models that implicitly integrate perception and\nplanning, and modular pipelines incorporating either vision-language models\n(VLMs) or multimodal large language models (LLMs). We evaluate these paradigms\nthrough two focused case studies: a complex instruction grounding task\nassessing fine-grained instruction understanding and cross-modal\ndisambiguation, and an object manipulation task targeting skill transfer via\nVLA finetuning. Our experiments in zero-shot and few-shot settings reveal\ntrade-offs in generalization and data efficiency. By exploring performance\nlimits, we distill design implications for developing language-driven physical\nagents and outline emerging challenges and opportunities for FM-powered\nrobotics in real-world conditions.",
      "url": "http://arxiv.org/abs/2505.15685v1",
      "published_time_eastern_timestamp": 1747843271.0
    },
    {
      "title": "Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model",
      "summary": "Spoken dialogue is an intuitive form of human-computer interaction, yet\ncurrent speech language models often remain constrained to turn-based\nexchanges, lacking real-time adaptability such as user barge-in. We propose a\nnovel duplex speech to speech (S2S) architecture featuring continuous user\ninputs and codec agent outputs with channel fusion that directly models\nsimultaneous user and agent streams. Using a pretrained streaming encoder for\nuser input enables the first duplex S2S model without requiring speech\npretrain. Separate architectures for agent and user modeling facilitate codec\nfine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared\nto previous works. Experimental results show that the proposed model\noutperforms previous duplex models in reasoning, turn-taking, and barge-in\nabilities. The model requires significantly less speech data, as speech\npretrain is skipped, which markedly simplifies the process of building a duplex\nS2S model from any LLMs. Finally, it is the first openly available duplex S2S\nmodel with training and inference code to foster reproducibility.",
      "url": "http://arxiv.org/abs/2505.15670v1",
      "published_time_eastern_timestamp": 1747842510.0
    },
    {
      "title": "Improved power methods for computing eigenvalues of dual quaternion\n  Hermitian matrices",
      "summary": "This paper investigates the eigenvalue computation problem of the dual\nquaternion Hermitian matrix closely related to multi-agent group control.\nRecently, power method was proposed by Cui and Qi in Journal of Scientific\nComputing, 100 (2024) to solve such problem. Recognizing that the convergence\nrate of power method is slow due to its dependence on the eigenvalue\ndistribution, we propose two improved versions of power method based on dual\ncomplex adjoint matrices and Aitken extrapolation, named DCAM-PM and ADCAM-PM.\nThey achieve notable efficiency improvements and demonstrate significantly\nfaster convergence. However, power method may be invalid for dual quaternion\nHermitian matrices with eigenvalues having identical standard parts but\ndistinct dual parts. To overcome this disadvantage, utilizing the\neigen-decomposition properties of dual complex adjoint matrix, we propose a\nnovel algorithm EDDCAM-EA which surpasses the power method in both accuracy and\nspeed. Application to eigenvalue computations of dual quaternion Hermitian\nmatrices in multi-agent formation control and numerical experiments highlight\nthe remarkable accuracy and speed of our proposed algorithms.",
      "url": "http://arxiv.org/abs/2505.15584v1",
      "published_time_eastern_timestamp": 1747838416.0
    },
    {
      "title": "The equilibrium price of bubble assets",
      "summary": "Considering a simple economy, we derive a new Hamilton-Jacobi equation which\nis satisfied by the value of a ''bubble'' asset. We then show, by providing a\nrigorous mathematical analysis of this equation, that a unique non-zero stable\nsolution exists under certain assumptions. The economic interpretation of this\nresult is that, if the bubble asset can produce more stable returns than fiat\nmoney, agents protect themselves from hazardous situations through the bubble\nasset, thus forming a bubble's consensus value. Our mathematical analysis uses\ndifferent ideas coming from the study of semi-linear elliptic equations.",
      "url": "http://arxiv.org/abs/2505.15578v1",
      "published_time_eastern_timestamp": 1747837850.0
    },
    {
      "title": "Temporal Spectrum Cartography in Low-Altitude Economy Networks: A\n  Generative AI Framework with Multi-Agent Learning",
      "summary": "This paper introduces a two-stage generative AI (GenAI) framework tailored\nfor temporal spectrum cartography in low-altitude economy networks (LAENets).\nLAENets, characterized by diverse aerial devices such as UAVs, rely heavily on\nwireless communication technologies while facing challenges, including spectrum\ncongestion and dynamic environmental interference. Traditional spectrum\ncartography methods have limitations in handling the temporal and spatial\ncomplexities inherent to these networks. Addressing these challenges, the\nproposed framework first employs a Reconstructive Masked Autoencoder (RecMAE)\ncapable of accurately reconstructing spectrum maps from sparse and temporally\nvarying sensor data using a novel dual-mask mechanism. This approach\nsignificantly enhances the precision of reconstructed radio frequency (RF)\npower maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method\nintegrates diffusion-based reinforcement learning to optimize the trajectories\nof dynamic UAV sensors. By leveraging temporal-attention encoding, this method\neffectively manages spatial exploration and exploitation to minimize cumulative\nreconstruction errors. Extensive numerical experiments validate that this\nintegrated GenAI framework outperforms traditional interpolation methods and\ndeep learning baselines by achieving 57.35% and 88.68% reconstruction error\nreduction, respectively. The proposed trajectory planner substantially improves\nspectrum map accuracy, reconstruction stability, and sensor deployment\nefficiency in dynamically evolving low-altitude environments.",
      "url": "http://arxiv.org/abs/2505.15571v1",
      "published_time_eastern_timestamp": 1747837531.0
    },
    {
      "title": "Riemannian EXTRA: Communication-efficient decentralized optimization\n  over compact submanifolds with data heterogeneity",
      "summary": "We consider decentralized optimization over a compact Riemannian submanifold\nin a network of $n$ agents, where each agent holds a smooth, nonconvex local\nobjective defined by its private data. The goal is to collaboratively minimize\nthe sum of these local objective functions. In the presence of data\nheterogeneity across nodes, existing algorithms typically require communicating\nboth local gradients and iterates to ensure exact convergence with constant\nstep sizes. In this work, we propose REXTRA, a Riemannian extension of the\nEXTRA algorithm [Shi et al., SIOPT, 2015], to address this limitation. On the\ntheoretical side, we leverage proximal smoothness to overcome the challenges of\nmanifold nonconvexity and establish a global sublinear convergence rate of\n$\\mathcal{O}(1/k)$, matching the best-known results. To our knowledge, REXTRA\nis the first algorithm to achieve a global sublinear convergence rate under a\nconstant step size while requiring only a single round of local iterate\ncommunication per iteration. Numerical experiments show that REXTRA achieves\nsuperior performance compared to state-of-the-art methods, while supporting\nlarger step sizes and reducing total communication by over 50\\%.",
      "url": "http://arxiv.org/abs/2505.15537v1",
      "published_time_eastern_timestamp": 1747835893.0
    },
    {
      "title": "Collaborative Problem-Solving in an Optimization Game",
      "summary": "Dialogue agents that support human users in solving complex tasks have\nreceived much attention recently. Many such tasks are NP-hard optimization\nproblems that require careful collaborative exploration of the solution space.\nWe introduce a novel dialogue game in which the agents collaboratively solve a\ntwo-player Traveling Salesman problem, along with an agent that combines LLM\nprompting with symbolic mechanisms for state tracking and grounding. Our best\nagent solves 45% of games optimally in self-play. It also demonstrates an\nability to collaborate successfully with human users and generalize to\nunfamiliar graphs.",
      "url": "http://arxiv.org/abs/2505.15490v1",
      "published_time_eastern_timestamp": 1747833335.0
    },
    {
      "title": "Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal\n  Reasoning via RL",
      "summary": "Vision language models (VLMs) have achieved impressive performance across a\nvariety of computer vision tasks. However, the multimodal reasoning capability\nhas not been fully explored in existing models. In this paper, we propose a\nChain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and\nzooming in on key image regions based on obtained visual cues and the given\nquestions, achieving efficient multimodal reasoning. To enable this CoF\ncapability, we present a two-stage training pipeline, including supervised\nfine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we\nconstruct the MM-CoF dataset, comprising 3K samples derived from a visual agent\ndesigned to adaptively identify key regions to solve visual tasks with\ndifferent image resolutions and questions. We use MM-CoF to fine-tune the\nQwen2.5-VL model for cold start. In the RL stage, we leverage the outcome\naccuracies and formats as rewards to update the Qwen2.5-VL model, enabling\nfurther refining the search and reasoning strategy of models without human\npriors. Our model achieves significant improvements on multiple benchmarks. On\nthe V* benchmark that requires strong visual reasoning capability, our model\noutperforms existing VLMs by 5% among 8 image resolutions ranging from 224 to\n4K, demonstrating the effectiveness of the proposed CoF method and facilitating\nthe more efficient deployment of VLMs in practical applications.",
      "url": "http://arxiv.org/abs/2505.15436v1",
      "published_time_eastern_timestamp": 1747829895.0
    },
    {
      "title": "X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating\n  Global Agentic System",
      "summary": "Recently, large language model (LLM)-based agents have achieved significant\nsuccess in interactive environments, attracting significant academic and\nindustrial attention. Despite these advancements, current research\npredominantly focuses on English scenarios. In reality, there are over 7,000\nlanguages worldwide, all of which demand access to comparable agentic services.\nNevertheless, the development of language agents remains inadequate for meeting\nthe diverse requirements of multilingual agentic applications. To fill this\ngap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an\ninteractive web environment, which evaluates the planning and interaction\nperformance of language agents across multiple languages, thereby contributing\nto the advancement of global agent intelligence. Additionally, we assess the\nperformance of various LLMs and cross-lingual alignment methods, examining\ntheir effectiveness in enhancing agents. Our findings reveal that even advanced\nmodels like GPT-4o, when combined with cross-lingual techniques, fail to\nachieve satisfactory results. We hope that X-WebAgentBench can serve as a\nvaluable benchmark for multilingual agent scenario in real-world applications.",
      "url": "http://arxiv.org/abs/2505.15372v1",
      "published_time_eastern_timestamp": 1747825622.0
    },
    {
      "title": "Multiple Weaks Win Single Strong: Large Language Models Ensemble Weak\n  Reinforcement Learning Agents into a Supreme One",
      "summary": "Model ensemble is a useful approach in reinforcement learning (RL) for\ntraining effective agents. Despite wide success of RL, training effective\nagents remains difficult due to the multitude of factors requiring careful\ntuning, such as algorithm selection, hyperparameter settings, and even random\nseed choices, all of which can significantly influence an agent's performance.\nModel ensemble helps overcome this challenge by combining multiple weak agents\ninto a single, more powerful one, enhancing overall performance. However,\nexisting ensemble methods, such as majority voting and Boltzmann addition, are\ndesigned as fixed strategies and lack a semantic understanding of specific\ntasks, limiting their adaptability and effectiveness. To address this, we\npropose LLM-Ens, a novel approach that enhances RL model ensemble with\ntask-specific semantic understandings driven by large language models (LLMs).\nGiven a task, we first design an LLM to categorize states in this task into\ndistinct 'situations', incorporating high-level descriptions of the task\nconditions. Then, we statistically analyze the strengths and weaknesses of each\nindividual agent to be used in the ensemble in each situation. During the\ninference time, LLM-Ens dynamically identifies the changing task situation and\nswitches to the agent that performs best in the current situation, ensuring\ndynamic model selection in the evolving task condition. Our approach is\ndesigned to be compatible with agents trained with different random seeds,\nhyperparameter settings, and various RL algorithms. Extensive experiments on\nthe Atari benchmark show that LLM-Ens significantly improves the RL model\nensemble, surpassing well-known baselines by up to 20.9%. For reproducibility,\nour code is open-source at\nhttps://anonymous.4open.science/r/LLM4RLensemble-F7EE.",
      "url": "http://arxiv.org/abs/2505.15306v1",
      "published_time_eastern_timestamp": 1747820143.0
    },
    {
      "title": "AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought\n  Reasoning in Vision-Language Models for Autonomous Driving",
      "summary": "Vision-Language Models (VLMs) show promise for autonomous driving, yet their\nstruggle with hallucinations, inefficient reasoning, and limited real-world\nvalidation hinders accurate perception and robust step-by-step reasoning. To\novercome this, we introduce \\textbf{AgentThink}, a pioneering unified framework\nthat, for the first time, integrates Chain-of-Thought (CoT) reasoning with\ndynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's\ncore innovations include: \\textbf{(i) Structured Data Generation}, by\nestablishing an autonomous driving tool library to automatically construct\nstructured, self-verified reasoning data explicitly incorporating tool usage\nfor diverse driving scenarios; \\textbf{(ii) A Two-stage Training Pipeline},\nemploying Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization\n(GRPO) to equip VLMs with the capability for autonomous tool invocation; and\n\\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel\nmulti-tool assessment protocol to rigorously evaluate the model's tool\ninvocation and utilization. Experiments on the DriveLMM-o1 benchmark\ndemonstrate AgentThink significantly boosts overall reasoning scores by\n\\textbf{53.91\\%} and enhances answer accuracy by \\textbf{33.54\\%}, while\nmarkedly improving reasoning quality and consistency. Furthermore, ablation\nstudies and robust zero-shot/few-shot generalization experiments across various\nbenchmarks underscore its powerful capabilities. These findings highlight a\npromising trajectory for developing trustworthy and tool-aware autonomous\ndriving models.",
      "url": "http://arxiv.org/abs/2505.15298v1",
      "published_time_eastern_timestamp": 1747819663.0
    }
  ]
}
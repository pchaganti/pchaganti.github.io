{
  "last_updated": "2025-10-20T08:24:10.969460-04:00",
  "papers": [
    {
      "title": "PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction",
      "summary": "Large language models (LLMs) are moving beyond static uses and are now\npowering agents that learn continually during their interaction with external\nenvironments. For example, agents can learn reusable skills while navigating\nweb pages or toggling new tools. However, existing methods for skill learning\noften create skills that are over-specialized to a single website and fail to\ngeneralize. We introduce PolySkill, a new framework that enables agents to\nlearn generalizable and compositional skills. The core idea, inspired by\npolymorphism in software engineering, is to decouple a skill's abstract goal\n(what it accomplishes) and its concrete implementation (how it is executed).\nExperiments show that our method (1) improves skill reuse by 1.7x on seen\nwebsites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on\nunseen websites, while reducing steps by over 20%. (3) In self-exploration\nsettings without specified tasks, our framework improves the quality of\nproposed tasks and enables agents to learn generalizable skills that work\nacross different sites. By enabling the agent to identify and refine its own\ngoals, the PolySkill enhances the agent's ability to learn a better curriculum,\nleading to the acquisition of more generalizable skills compared to baseline\nmethods. This work provides a practical path toward building agents capable of\ncontinual learning in adaptive environments. Our findings show that separating\na skill's goal from its execution is a crucial step toward developing\nautonomous agents that can learn and generalize across the open web\ncontinuously.",
      "url": "http://arxiv.org/abs/2510.15863v1",
      "published_time_eastern_timestamp": 1760723760.0
    },
    {
      "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from\n  AI Feedback and Robust Reasoning Scaffold",
      "summary": "Tool-augmented large language models (LLMs) are emerging as deep research\nagents, systems that decompose complex queries, retrieve external evidence, and\nsynthesize grounded responses. Yet current agents remain limited by shallow\nretrieval, weak alignment metrics, and brittle tool-use behavior. We introduce\nPokeeResearch-7B, a 7B-parameter deep research agent built under a unified\nreinforcement learning framework for robustness, alignment, and scalability.\nPokeeResearch-7B is trained by an annotation-free Reinforcement Learning from\nAI Feedback (RLAIF) framework to optimize policies using LLM-based reward\nsignals that capture factual accuracy, citation faithfulness, and instruction\nadherence. A chain-of-thought-driven multi-call reasoning scaffold further\nenhances robustness through self-verification and adaptive recovery from tool\nfailures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves\nstate-of-the-art performance among 7B-scale deep research agents. This\nhighlights that careful reinforcement learning and reasoning design can produce\nefficient, resilient, and research-grade AI agents. The model and inference\ncode is open-sourced under MIT license at\nhttps://github.com/Pokee-AI/PokeeResearchOSS.",
      "url": "http://arxiv.org/abs/2510.15862v1",
      "published_time_eastern_timestamp": 1760723586.0
    },
    {
      "title": "Paper2Web: Let's Make Your Paper Alive!",
      "summary": "Academic project websites can more effectively disseminate research when they\nclearly present core content and enable intuitive navigation and interaction.\nHowever, current approaches such as direct Large Language Model (LLM)\ngeneration, templates, or direct HTML conversion struggle to produce\nlayout-aware, interactive sites, and a comprehensive evaluation suite for this\ntask has been lacking. In this paper, we introduce Paper2Web, a benchmark\ndataset and multi-dimensional evaluation framework for assessing academic\nwebpage generation. It incorporates rule-based metrics like Connectivity,\nCompleteness and human-verified LLM-as-a-Judge (covering interactivity,\naesthetics, and informativeness), and PaperQuiz, which measures paper-level\nknowledge retention. We further present PWAgent, an autonomous pipeline that\nconverts scientific papers into interactive and multimedia-rich academic\nhomepages. The agent iteratively refines both content and layout through MCP\ntools that enhance emphasis, balance, and presentation quality. Our experiments\nshow that PWAgent consistently outperforms end-to-end baselines like\ntemplate-based webpages and arXiv/alphaXiv versions by a large margin while\nmaintaining low cost, achieving the Pareto-front in academic webpage\ngeneration.",
      "url": "http://arxiv.org/abs/2510.15842v1",
      "published_time_eastern_timestamp": 1760722558.0
    },
    {
      "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
      "summary": "Despite rapid advances in text-to-video synthesis, generated video quality\nremains critically dependent on precise user prompts. Existing test-time\noptimization methods, successful in other domains, struggle with the\nmulti-faceted nature of video. In this work, we introduce VISTA (Video\nIterative Self-improvemenT Agent), a novel multi-agent system that autonomously\nimproves video generation through refining prompts in an iterative loop. VISTA\nfirst decomposes a user idea into a structured temporal plan. After generation,\nthe best video is identified through a robust pairwise tournament. This winning\nvideo is then critiqued by a trio of specialized agents focusing on visual,\naudio, and contextual fidelity. Finally, a reasoning agent synthesizes this\nfeedback to introspectively rewrite and enhance the prompt for the next\ngeneration cycle. Experiments on single- and multi-scene video generation\nscenarios show that while prior methods yield inconsistent gains, VISTA\nconsistently improves video quality and alignment with user intent, achieving\nup to 60% pairwise win rate against state-of-the-art baselines. Human\nevaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
      "url": "http://arxiv.org/abs/2510.15831v1",
      "published_time_eastern_timestamp": 1760721128.0
    },
    {
      "title": "Fertilizers Fuel, Insecticides Stabilize: Resolving the Paradox of\n  Enrichment in Agriculture",
      "summary": "The Paradox of Enrichment (PoE) predicts that increasing resources, such as\nnutrient inputs like fertilizers or food availability, should destabilize\necological systems, such as crop-pest dynamics, leading to population cycles\nthat can increase the risk of crop failure during environmental shocks. Yet,\nsince the Green Revolution, fertilizer use has surged without widespread\nevidence of yield instability, challenging the PoE's relevance to modern\nagriculture. Here, we propose and test a novel resolution: that insecticides,\nfrequently co-applied with fertilizers, act as stabilizing agents that\ncounterbalance enrichment-induced instability. Using a modified PoE model with\nempirically grounded parameters for three major crop-pest\nsystems-soybean-aphid, wheat-aphid, and cabbage-diamondback moth-we find that\nfertilizer increases yields, but destabilizes dynamics, whereas insecticides\nrestore stability and ensure more predictable harvests. These findings reveal\nthat insecticides may suppress pests but also play a critical role in\nstabilizing crop yields in nutrient-enriched agroecosystems, with implications\nfor ecosystem management, eutrophication, conservation biology, and pesticide\npolicy.",
      "url": "http://arxiv.org/abs/2510.15811v1",
      "published_time_eastern_timestamp": 1760719363.0
    },
    {
      "title": "Self-evolving expertise in complex non-verifiable subject domains:\n  dialogue as implicit meta-RL",
      "summary": "So-called `wicked problems', those involving complex multi-dimensional\nsettings, non-verifiable outcomes, heterogeneous impacts and a lack of single\nobjectively correct answers, have plagued humans throughout history. Modern\nexamples include decisions over justice frameworks, solving environmental\npollution, planning for pandemic resilience and food security. The use of\nstate-of-the-art artificial intelligence systems (notably Large Language\nModel-based agents) collaborating with humans on solving such problems is being\nactively explored. While the abilities of LLMs can be improved by, for example,\nfine-tuning, hand-crafted system prompts and scaffolding with external tools,\nLLMs lack endogenous mechanisms to develop expertise through experience in such\nsettings. This work address this gap with Dialectica, a framework where agents\nengage in structured dialogue on defined topics, augmented by memory,\nself-reflection, and policy-constrained context editing. Formally, discussion\nis viewed as an implicit meta-reinforcement learning process. The\n`dialogue-trained' agents are evaluated post-hoc using judged pairwise\ncomparisons of elicited responses. Across two model architectures (locally run\nQwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based\ncontext editing during discussion produces agents which dominate their baseline\ncounterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and\nAlphaRank mass. The predicted signatures of learning are observed qualitatively\nin statement and reflection logs, where reflections identify weaknesses and\nreliably shape subsequent statements. Agreement between quantitative and\nqualitative evidence supports dialogue-driven context evolution as a practical\npath to targeted expertise amplification in open non-verifiable domains.",
      "url": "http://arxiv.org/abs/2510.15772v1",
      "published_time_eastern_timestamp": 1760716784.0
    },
    {
      "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic\n  Dataset",
      "summary": "Instruction-based video editing promises to democratize content creation, yet\nits progress is severely hampered by the scarcity of large-scale, high-quality\ntraining data. We introduce Ditto, a holistic framework designed to tackle this\nfundamental challenge. At its heart, Ditto features a novel data generation\npipeline that fuses the creative diversity of a leading image editor with an\nin-context video generator, overcoming the limited scope of existing models. To\nmake this process viable, our framework resolves the prohibitive cost-quality\ntrade-off by employing an efficient, distilled model architecture augmented by\na temporal enhancer, which simultaneously reduces computational overhead and\nimproves temporal coherence. Finally, to achieve full scalability, this entire\npipeline is driven by an intelligent agent that crafts diverse instructions and\nrigorously filters the output, ensuring quality control at scale. Using this\nframework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of\none million high-fidelity video editing examples. We trained our model, Editto,\non Ditto-1M with a curriculum learning strategy. The results demonstrate\nsuperior instruction-following ability and establish a new state-of-the-art in\ninstruction-based video editing.",
      "url": "http://arxiv.org/abs/2510.15742v1",
      "published_time_eastern_timestamp": 1760715100.0
    },
    {
      "title": "AURA: An Agent Autonomy Risk Assessment Framework",
      "summary": "As autonomous agentic AI systems see increasing adoption across\norganisations, persistent challenges in alignment, governance, and risk\nmanagement threaten to impede deployment at scale. We present AURA (Agent\naUtonomy Risk Assessment), a unified framework designed to detect, quantify,\nand mitigate risks arising from agentic AI. Building on recent research and\npractical deployments, AURA introduces a gamma-based risk scoring methodology\nthat balances risk assessment accuracy with computational efficiency and\npractical considerations. AURA provides an interactive process to score,\nevaluate and mitigate the risks of running one or multiple AI Agents,\nsynchronously or asynchronously (autonomously). The framework is engineered for\nHuman-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H)\ncommunication mechanisms, allowing for seamless integration with agentic\nsystems for autonomous self-assessment, rendering it interoperable with\nestablished protocols (MCP and A2A) and tools. AURA supports a responsible and\ntransparent adoption of agentic AI and provides robust risk detection and\nmitigation while balancing computational resources, positioning it as a\ncritical enabler for large-scale, governable agentic AI in enterprise\nenvironments.",
      "url": "http://arxiv.org/abs/2510.15739v1",
      "published_time_eastern_timestamp": 1760715029.0
    },
    {
      "title": "RLAF: Reinforcement Learning from Automaton Feedback",
      "summary": "Reinforcement Learning (RL) in environments with complex, history-dependent\nreward structures poses significant challenges for traditional methods. In this\nwork, we introduce a novel approach that leverages automaton-based feedback to\nguide the learning process, replacing explicit reward functions with\npreferences derived from a deterministic finite automaton (DFA). Unlike\nconventional approaches that use automata for direct reward specification, our\nmethod employs the structure of the DFA to generate preferences over\ntrajectories that are used to learn a reward function, eliminating the need for\nmanual reward engineering. Our framework introduces a static approach that uses\nthe learned reward function directly for policy optimization and a dynamic\napproach that involves continuous refining of the reward function and policy\nthrough iterative updates until convergence.\n  Our experiments in both discrete and continuous environments demonstrate that\nour approach enables the RL agent to learn effective policies for tasks with\ntemporal dependencies, outperforming traditional reward engineering and\nautomaton-based baselines such as reward machines and LTL-guided methods. Our\nresults highlight the advantages of automaton-based preferences in handling\nnon-Markovian rewards, offering a scalable, efficient, and human-independent\nalternative to traditional reward modeling. We also provide a convergence\nguarantee showing that under standard assumptions our automaton-guided\npreference-based framework learns a policy that is near-optimal with respect to\nthe true non-Markovian objective.",
      "url": "http://arxiv.org/abs/2510.15728v1",
      "published_time_eastern_timestamp": 1760714221.0
    },
    {
      "title": "ProSh: Probabilistic Shielding for Model-free Reinforcement Learning",
      "summary": "Safety is a major concern in reinforcement learning (RL): we aim at\ndeveloping RL systems that not only perform optimally, but are also safe to\ndeploy by providing formal guarantees about their safety. To this end, we\nintroduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free\nalgorithm for safe reinforcement learning under cost constraints. ProSh\naugments the Constrained MDP state space with a risk budget and enforces safety\nby applying a shield to the agent's policy distribution using a learned cost\ncritic. The shield ensures that all sampled actions remain safe in expectation.\nWe also show that optimality is preserved when the environment is\ndeterministic. Since ProSh is model-free, safety during training depends on the\nknowledge we have acquired about the environment. We provide a tight\nupper-bound on the cost in expectation, depending only on the backup-critic\naccuracy, that is always satisfied during training. Under mild, practically\nachievable assumptions, ProSh guarantees safety even at training time, as shown\nin the experiments.",
      "url": "http://arxiv.org/abs/2510.15720v1",
      "published_time_eastern_timestamp": 1760713731.0
    },
    {
      "title": "ProofOptimizer: Training Language Models to Simplify Proofs without\n  Human Demonstrations",
      "summary": "Neural theorem proving has advanced rapidly in the past year, reaching IMO\ngold-medalist capabilities and producing formal proofs that span thousands of\nlines. Although such proofs are mechanically verified by formal systems like\nLean, their excessive length renders them difficult for humans to comprehend\nand limits their usefulness for mathematical insight. Proof simplification is\ntherefore a critical bottleneck. Yet, training data for this task is scarce,\nand existing methods -- mainly agentic scaffolding with off-the-shelf LLMs --\nstruggle with the extremely long proofs generated by RL-trained provers. We\nintroduce ProofOptimizer, the first language model trained to simplify Lean\nproofs without requiring additional human supervision. ProofOptimizer is\ntrained via expert iteration and reinforcement learning, using Lean to verify\nsimplifications and provide training signal. At inference time, it operates\nwithin an iterative proof-shortening workflow, progressively reducing proof\nlength. Experiments show that ProofOptimizer substantially compresses proofs\ngenerated by state-of-the-art RL-trained provers on standard benchmarks,\nreducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on\nSeed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check\nfaster in Lean and further improve downstream prover performance when reused as\ntraining data for supervised finetuning.",
      "url": "http://arxiv.org/abs/2510.15700v1",
      "published_time_eastern_timestamp": 1760712330.0
    },
    {
      "title": "Mixture of Experts Approaches in Dense Retrieval Tasks",
      "summary": "Dense Retrieval Models (DRMs) are a prominent development in Information\nRetrieval (IR). A key challenge with these neural Transformer-based models is\nthat they often struggle to generalize beyond the specific tasks and domains\nthey were trained on. To address this challenge, prior research in IR\nincorporated the Mixture-of-Experts (MoE) framework within each Transformer\nlayer of a DRM, which, though effective, substantially increased the number of\nadditional parameters. In this paper, we propose a more efficient design, which\nintroduces a single MoE block (SB-MoE) after the final Transformer layer. To\nassess the retrieval effectiveness of SB-MoE, we perform an empirical\nevaluation across three IR tasks. Our experiments involve two evaluation\nsetups, aiming to assess both in-domain effectiveness and the model's zero-shot\ngeneralizability. In the first setup, we fine-tune SB-MoE with four different\nunderlying DRMs on seven IR benchmarks and evaluate them on their respective\ntest sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform\nzero-shot evaluation on thirteen BEIR datasets. Additionally, we perform\nfurther experiments to analyze the model's dependency on its hyperparameters\n(i.e., the number of employed and activated experts) and investigate how this\nvariation affects SB-MoE's performance. The obtained results show that SB-MoE\nis particularly effective for DRMs with lightweight base models, such as\nTinyBERT and BERT-Small, consistently exceeding standard model fine-tuning\nacross benchmarks. For DRMs with more parameters, such as BERT-Base and\nContriever, our model requires a larger number of training samples to achieve\nimproved retrieval performance. Our code is available online at:\nhttps://github.com/FaySokli/SB-MoE.",
      "url": "http://arxiv.org/abs/2510.15683v1",
      "published_time_eastern_timestamp": 1760710999.0
    },
    {
      "title": "SQuAI: Scientific Question-Answering with Multi-Agent\n  Retrieval-Augmented Generation",
      "summary": "We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy\nmulti-agent retrieval-augmented generation (RAG) framework for scientific\nquestion answering (QA) with large language models (LLMs). SQuAI addresses key\nlimitations of existing RAG systems in the scholarly domain, where complex,\nopen-domain questions demand accurate answers, explicit claims with citations,\nand retrieval across millions of scientific documents. Built on over 2.3\nmillion full-text papers from arXiv.org, SQuAI employs four collaborative\nagents to decompose complex questions into sub-questions, retrieve targeted\nevidence via hybrid sparse-dense retrieval, and adaptively filter documents to\nimprove contextual relevance. To ensure faithfulness and traceability, SQuAI\nintegrates in-line citations for each generated claim and provides supporting\nsentences from the source documents. Our system improves faithfulness, answer\nrelevance, and contextual relevance by up to +0.088 (12%) over a strong RAG\nbaseline. We further release a benchmark of 1,000 scientific\nquestion-answer-evidence triplets to support reproducibility. With transparent\nreasoning, verifiable citations, and domain-wide scalability, SQuAI\ndemonstrates how multi-agent RAG enables more trustworthy scientific QA with\nLLMs.",
      "url": "http://arxiv.org/abs/2510.15682v1",
      "published_time_eastern_timestamp": 1760710855.0
    },
    {
      "title": "Active matter synchronization and synergetics",
      "summary": "We study the collective behavior in a stochastic agent-based model of active\nmatter. Provided a critical take-up of energy, agents produce two types of\ngoods $x$, $y$ that follow a generalized Lotka-Volterra dynamics. For isolated\nagents, production would either reach a fixed point or diverge. Coupling\nagents' production via a mean field of $x$, however, can lead to synchronized\noscillations if agents cooperate in the production of $x$. The production of\n$y$ supports the emergence of the synchronized dynamics by suppressing\nfluctuations and mitigating competition between agents, this way stabilizing\nthe production of $x$. We find that in the synchronized state different groups\nof agents coexist, each following their own limit cycle. The Kuramoto order\nparameter is large within groups, and small across groups. The collective state\nis stable against shocks from agents temporarily switching between cooperation\nand competition. The model dynamics illustrates the principles of synergetics,\ni.e., the spontaneous emergence of order given a critical energy supply and\ncooperative interactions.",
      "url": "http://arxiv.org/abs/2510.15656v1",
      "published_time_eastern_timestamp": 1760708748.0
    },
    {
      "title": "Derivation and quasi-invariant asymptotics of phenotype-structured\n  integro-differential models",
      "summary": "Building upon kinetic theory approaches for multi-agent systems and\ngeneralising them to scenarios where the total mass of the system is not\nconserved, we develop a modelling framework for phenotype-structured\npopulations that makes it possible to bridge individual-level mechanisms with\npopulation-scale evolutionary dynamics. We start by formulating a stochastic\nagent-based model, which describes the dynamics of single population members\nundergoing proliferation, death, and phenotype changes. Then, we formally\nderive the corresponding mesoscopic model, which consists of an\nintegro-differential equation for the distribution of population members over\nthe space of phenotypes, where phenotype changes are modelled via an integral\nkernel. Finally, considering a quasi-invariant regime of small but frequent\nphenotype changes, we rigorously derive a non-local Fokker-Planck-type equation\ncounterpart of this model, wherein phenotype changes are taken into account by\nan advection-diffusion term. The theoretical results obtained are illustrated\nthrough a sample of results of numerical simulations.",
      "url": "http://arxiv.org/abs/2510.15646v1",
      "published_time_eastern_timestamp": 1760708095.0
    },
    {
      "title": "Decentralized Parameter-Free Online Learning",
      "summary": "We propose the first parameter-free decentralized online learning algorithms\nwith network regret guarantees, which achieve sublinear regret without\nrequiring hyperparameter tuning. This family of algorithms connects multi-agent\ncoin-betting and decentralized online learning via gossip steps. To enable our\ndecentralized analysis, we introduce a novel \"betting function\" formulation for\ncoin-betting that simplifies the multi-agent regret analysis. Our analysis\nshows sublinear network regret bounds and is validated through experiments on\nsynthetic and real datasets. This family of algorithms is applicable to\ndistributed sensing, decentralized optimization, and collaborative ML\napplications.",
      "url": "http://arxiv.org/abs/2510.15644v1",
      "published_time_eastern_timestamp": 1760708071.0
    },
    {
      "title": "Build Your Personalized Research Group: A Multiagent Framework for\n  Continual and Interactive Science Automation",
      "summary": "The automation of scientific discovery represents a critical milestone in\nArtificial Intelligence (AI) research. However, existing agentic systems for\nscience suffer from two fundamental limitations: rigid, pre-programmed\nworkflows that cannot adapt to intermediate findings, and inadequate context\nmanagement that hinders long-horizon research. We present\n\\texttt{freephdlabor}, an open-source multiagent framework featuring\n\\textit{fully dynamic workflows} determined by real-time agent reasoning and a\n\\coloremph{\\textit{modular architecture}} enabling seamless customization --\nusers can modify, add, or remove agents to address domain-specific\nrequirements. The framework provides comprehensive infrastructure including\n\\textit{automatic context compaction}, \\textit{workspace-based communication}\nto prevent information degradation, \\textit{memory persistence} across\nsessions, and \\textit{non-blocking human intervention} mechanisms. These\nfeatures collectively transform automated research from isolated, single-run\nattempts into \\textit{continual research programs} that build systematically on\nprior explorations and incorporate human feedback. By providing both the\narchitectural principles and practical implementation for building customizable\nco-scientist systems, this work aims to facilitate broader adoption of\nautomated research across scientific domains, enabling practitioners to deploy\ninteractive multiagent systems that autonomously conduct end-to-end research --\nfrom ideation through experimentation to publication-ready manuscripts.",
      "url": "http://arxiv.org/abs/2510.15624v1",
      "published_time_eastern_timestamp": 1760706812.0
    },
    {
      "title": "GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device",
      "summary": "Semantic top-K selection with cross-encoder rerankers underpins of on-device\nAI services, such as retrieval-augmented generation, agent memory, and\npersonalized recommendation. However, its latency and memory demands dominate\nend-to-end budgets on edge hardware. Revisiting the objective of top-K\nselection, we reveal that only relative rankings matter, not exact\nper-candidate scores. We further observe sequence-level sparsity: relative\nrankings stabilize early in intermediate layers, allowing pruning opportunities\nprior to completing full inference.\n  Building on this insight, we propose monolithic forwarding and develop a\ntraining-free inference system, GRATING. By maintaining a global view of all\ncandidates, it reduces latency through progressive cluster pruning. It also\nbounds peak memory usage by strategically overlapping I/O with computation via\ndual-layer sliding window and chunked execution. We evaluate GRATING against\nstate-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple\nM2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak\nmemory by up to 94.9% in microbenchmarks, without any loss in precision. Across\nthree real-world on-device AI applications, GRATING lowers latency by\n11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial\nimprovements in efficiency and deployability.",
      "url": "http://arxiv.org/abs/2510.15620v1",
      "published_time_eastern_timestamp": 1760706369.0
    },
    {
      "title": "The Spark Effect: On Engineering Creative Diversity in Multi-Agent AI\n  Systems",
      "summary": "Creative services teams increasingly rely on large language models (LLMs) to\naccelerate ideation, yet production systems often converge on homogeneous\noutputs that fail to meet brand or artistic expectations. Art of X developed\npersona-conditioned LLM agents -- internally branded as \"Sparks\" and\ninstantiated through a library of role-inspired system prompts -- to\nintentionally diversify agent behaviour within a multi-agent workflow. This\nwhite paper documents the problem framing, experimental design, and\nquantitative evidence behind the Spark agent programme. Using an LLM-as-a-judge\nprotocol calibrated against human gold standards, we observe a mean diversity\ngain of +4.1 points (on a 1-10 scale) when persona-conditioned Spark agents\nreplace a uniform system prompt, narrowing the gap to human experts to 1.0\npoint. We also surface evaluator bias and procedural considerations for future\ndeployments.",
      "url": "http://arxiv.org/abs/2510.15568v1",
      "published_time_eastern_timestamp": 1760702178.0
    },
    {
      "title": "KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities\n  in Large Language Models",
      "summary": "The instruction-following capabilities of large language models (LLMs) are\npivotal for numerous applications, from conversational agents to complex\nreasoning systems. However, current evaluations predominantly focus on English\nmodels, neglecting the linguistic and cultural nuances of other languages.\nSpecifically, Korean, with its distinct syntax, rich morphological features,\nhonorific system, and dual numbering systems, lacks a dedicated benchmark for\nassessing open-ended instruction-following capabilities. To address this gap,\nwe introduce the Korean Instruction-following Task Evaluation (KITE), a\ncomprehensive benchmark designed to evaluate both general and Korean-specific\ninstructions. Unlike existing Korean benchmarks that focus mainly on factual\nknowledge or multiple-choice testing, KITE directly targets diverse, open-ended\ninstruction-following tasks. Our evaluation pipeline combines automated metrics\nwith human assessments, revealing performance disparities across models and\nproviding deeper insights into their strengths and weaknesses. By publicly\nreleasing the KITE dataset and code, we aim to foster further research on\nculturally and linguistically inclusive LLM development and inspire similar\nendeavors for other underrepresented languages.",
      "url": "http://arxiv.org/abs/2510.15558v1",
      "published_time_eastern_timestamp": 1760701515.0
    }
  ]
}
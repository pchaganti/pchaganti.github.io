{
  "last_updated": "2025-10-08T17:10:39.970962-04:00",
  "papers": [
    {
      "title": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement\n  Learning of LLM Search Agents",
      "summary": "Large language model (LLM) agents increasingly rely on external tools such as\nsearch engines to solve complex, multi-step problems, and reinforcement\nlearning (RL) has become a key paradigm for training them. However, the\ntrajectories of search agents are structurally heterogeneous, where variations\nin the number, placement, and outcomes of search calls lead to fundamentally\ndifferent answer directions and reward distributions. Standard policy gradient\nmethods, which use a single global baseline, suffer from what we identify and\nformalize as cross-stratum bias-an \"apples-to-oranges\" comparison of\nheterogeneous trajectories. This cross-stratum bias distorts credit assignment\nand hinders exploration of complex, multi-step search strategies. To address\nthis, we propose Stratified GRPO, whose central component, Stratified Advantage\nNormalization (SAN), partitions trajectories into homogeneous strata based on\ntheir structural properties and computes advantages locally within each\nstratum. This ensures that trajectories are evaluated only against their true\npeers. Our analysis proves that SAN eliminates cross-stratum bias, yields\nconditionally unbiased unit-variance estimates inside each stratum, and retains\nthe global unbiasedness and unit-variance properties enjoyed by standard\nnormalization, resulting in a more pure and scale-stable learning signal. To\nimprove practical stability under finite-sample regimes, we further linearly\nblend SAN with the global estimator. Extensive experiments on diverse\nsingle-hop and multi-hop question-answering benchmarks demonstrate that\nStratified GRPO consistently and substantially outperforms GRPO by up to 11.3\npoints, achieving higher training rewards, greater training stability, and more\neffective search policies. These results establish stratification as a\nprincipled remedy for structural heterogeneity in RL for LLM search agents.",
      "url": "http://arxiv.org/abs/2510.06214v1",
      "published_time_eastern_timestamp": 1759859953.0
    },
    {
      "title": "Reference Grounded Skill Discovery",
      "summary": "Scaling unsupervised skill discovery algorithms to high-DoF agents remains\nchallenging. As dimensionality increases, the exploration space grows\nexponentially, while the manifold of meaningful skills remains limited.\nTherefore, semantic meaningfulness becomes essential to effectively guide\nexploration in high-dimensional spaces. In this work, we present\nReference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skill\ndiscovery in a semantically meaningful latent space using reference data. RGSD\nfirst performs contrastive pretraining to embed motions on a unit hypersphere,\nclustering each reference trajectory into a distinct direction. This grounding\nenables skill discovery to simultaneously involve both imitation of reference\nbehaviors and the discovery of semantically related diverse behaviors. On a\nsimulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learns\nstructured skills including walking, running, punching, and side stepping, and\nalso discovers related novel behaviors. In downstream control tasks, RGSD\noutperforms imitation-based skill acquisition baselines. Our results suggest\nthat lightweight reference-guided grounding offers a practical path to\ndiscovering semantically rich and structured skills in high-DoF systems.",
      "url": "http://arxiv.org/abs/2510.06203v1",
      "published_time_eastern_timestamp": 1759859701.0
    },
    {
      "title": "Automated Program Repair of Uncompilable Student Code",
      "summary": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.",
      "url": "http://arxiv.org/abs/2510.06187v1",
      "published_time_eastern_timestamp": 1759859193.0
    },
    {
      "title": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback",
      "summary": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation",
      "url": "http://arxiv.org/abs/2510.06186v1",
      "published_time_eastern_timestamp": 1759859135.0
    },
    {
      "title": "LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design\n  for Heterogeneous Agent Teams",
      "summary": "A critical challenge in modelling Heterogeneous-Agent Teams is training\nagents to collaborate with teammates whose policies are inaccessible or\nnon-stationary, such as humans. Traditional approaches rely on expensive\nhuman-in-the-loop data, which limits scalability. We propose using Large\nLanguage Models (LLMs) as policy-agnostic human proxies to generate synthetic\ndata that mimics human decision-making. To evaluate this, we conduct three\nexperiments in a grid-world capture game inspired by Stag Hunt, a game theory\nparadigm that balances risk and reward. In Experiment 1, we compare decisions\nfrom 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and\nMixtral 8x22B models. LLMs, prompted with game-state observations and reward\nstructures, align more closely with experts than participants, demonstrating\nconsistency in applying underlying decision criteria. Experiment 2 modifies\nprompts to induce risk-sensitive strategies (e.g. \"be risk averse\"). LLM\noutputs mirror human participants' variability, shifting between risk-averse\nand risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic\ngrid-world where the LLM agents generate movement actions. LLMs produce\ntrajectories resembling human participants' paths. While LLMs cannot yet fully\nreplicate human adaptability, their prompt-guided diversity offers a scalable\nfoundation for simulating policy-agnostic teammates.",
      "url": "http://arxiv.org/abs/2510.06151v1",
      "published_time_eastern_timestamp": 1759857680.0
    },
    {
      "title": "Pushing Test-Time Scaling Limits of Deep Search with Asymmetric\n  Verification",
      "summary": "Test-time compute can be scaled both sequentially and in parallel. Sequential\nscaling involves lengthening the generation process, while parallel scaling\ninvolves verifying and selecting among multiple candidate outputs. Combining\nthese two strategies has led to the most powerful AI systems, such as Grok 4\nHeavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles),\nverifying responses can be substantially easier than generating them. This\nproperty, referred to as \\emph{asymmetric verification}, highlights the strong\npotential of test-time scaling (TTS). In this work, we study both sequential\nand parallel TTS of deep search agents, motivated by the intuition that\nverification in this setting is often much easier than generation. In\nexperiments, we first show that sequential scaling methods, such as budget\nforcing, can be effective initially but soon degrade performance. Leveraging\nasymmetric verification, however, we are able to achieve substantial\nimprovements by allocating only a modest amount of compute to the verifier. We\nconduct experiments with flagship open-source models and extend them to their\n``Heavy'' variants through TTS. These deep research agents achieve gains of up\nto 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an\nopen-source alternative, GLM-4.5 Heavy reaches accuracy of {\\bf 54.0\\%} on\nBrowseComp and {\\bf 66.0\\%} on GAIA, placing it comparable to the best\nproprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy\nfurther achieves {\\bf 69.0\\%} accuracy on BrowseComp, greatly surpassing the\nbest proprietary results.",
      "url": "http://arxiv.org/abs/2510.06135v1",
      "published_time_eastern_timestamp": 1759856963.0
    },
    {
      "title": "Constraint-Aware Route Recommendation from Natural Language via\n  Hierarchical LLM Agents",
      "summary": "Route recommendation aims to provide users with optimal travel plans that\nsatisfy diverse and complex requirements. Classical routing algorithms (e.g.,\nshortest-path and constraint-aware search) are efficient but assume structured\ninputs and fixed objectives, limiting adaptability to natural-language queries.\nRecent LLM-based approaches enhance flexibility but struggle with spatial\nreasoning and the joint modeling of route-level and POI-level preferences. To\naddress these limitations, we propose RouteLLM, a hierarchical multi-agent\nframework that grounds natural-language intents into constraint-aware routes.\nIt first parses user queries into structured intents including POIs, paths, and\nconstraints. A manager agent then coordinates specialized sub-agents: a\nconstraint agent that resolves and formally check constraints, a POI agent that\nretrieves and ranks candidate POIs, and a path refinement agent that refines\nroutes via a routing engine with preference-conditioned costs. A final verifier\nagent ensures constraint satisfaction and produces the final route with an\ninterpretable rationale. This design bridges linguistic flexibility and spatial\nstructure, enabling reasoning over route feasibility and user preferences.\nExperiments show that our method reliably grounds textual preferences into\nconstraint-aware routes, improving route quality and preference satisfaction\nover classical methods.",
      "url": "http://arxiv.org/abs/2510.06078v1",
      "published_time_eastern_timestamp": 1759853037.0
    },
    {
      "title": "Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI\n  Models for Scatterplot-Related Tasks",
      "summary": "AI models are increasingly used for data analysis and visualization, yet\nbenchmarks rarely address scatterplot-specific tasks, limiting insight into\nperformance. To address this gap for one of the most common chart types, we\nintroduce a synthetic, annotated dataset of over 18,000 scatterplots from six\ndata generators and 17 chart designs, and a benchmark based on it. We evaluate\nproprietary models from OpenAI and Google using N-shot prompting on five\ndistinct tasks derived from annotations of cluster bounding boxes, their center\ncoordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash,\nespecially when prompted with examples, are viable options for counting\nclusters and, in Flash's case, outliers (90%+ Accuracy). However, the results\nfor localization-related tasks are unsatisfactory: Precision and Recall are\nnear or below 50%, except for Flash in outlier identification (65.01%).\nFurthermore, the impact of chart design on performance appears to be a\nsecondary factor, but it is advisable to avoid scatterplots with wide aspect\nratios (16:9 and 21:9) or those colored randomly. Supplementary materials are\navailable at https://github.com/feedzai/biy-paper.",
      "url": "http://arxiv.org/abs/2510.06071v1",
      "published_time_eastern_timestamp": 1759852759.0
    },
    {
      "title": "Reasoning under Vision: Understanding Visual-Spatial Cognition in\n  Vision-Language Models for CAPTCHA",
      "summary": "CAPTCHA, originally designed to distinguish humans from robots, has evolved\ninto a real-world benchmark for assessing the spatial reasoning capabilities of\nvision-language models. In this work, we first show that step-by-step reasoning\nis crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent\nhigh-difficulty spatial reasoning tasks, and that current commercial\nvision-language models still struggle with such reasoning. In particular, we\nobserve that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to\neffectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).\nHowever, our findings indicate that requiring the model to perform step-by-step\nreasoning before generating the final coordinates can significantly enhance its\nsolving accuracy, underscoring the severity of the gap. To systematically study\nthis issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with\nreasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,\netc.) with step-by-step action solutions and grounding annotations. We further\ndefine five reasoning-oriented metrics that enable a comprehensive evaluation\nof models reasoning capabilities. To validate the effectiveness of reasoning,\nwe also propose a general agentic VLM-based framework that incorporates the\nmodels inherent reasoning abilities. Our method achieves state-of-the-art\nperformance across five high-difficulty CAPTCHA types, with an average solving\naccuracy of 83.9 percent, substantially surpassing existing baselines. These\nresults reveal the limitations of current models and highlight the importance\nof reasoning in advancing visual-spatial challenges in the future.",
      "url": "http://arxiv.org/abs/2510.06067v1",
      "published_time_eastern_timestamp": 1759852581.0
    },
    {
      "title": "Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep\n  Research",
      "summary": "Large language models hold promise as scientific assistants, yet existing\nagents either rely solely on algorithm evolution or on deep research in\nisolation, both of which face critical limitations. Pure algorithm evolution,\nas in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly\nplateaus in complex domains, while pure deep research proposes ideas without\nvalidation, resulting in unrealistic or unimplementable solutions. We present\nDeepEvolve, an agent that integrates deep research with algorithm evolution,\nuniting external knowledge retrieval, cross-file code editing, and systematic\ndebugging under a feedback-driven iterative loop. Each iteration not only\nproposes new hypotheses but also refines, implements, and tests them, avoiding\nboth shallow improvements and unproductive over-refinements. Across nine\nbenchmarks in chemistry, mathematics, biology, materials, and patents,\nDeepEvolve consistently improves the initial algorithm, producing executable\nnew algorithms with sustained gains. By bridging the gap between unguided\nevolution and research without grounding, DeepEvolve provides a reliable\nframework for advancing scientific algorithm discovery. Our code is available\nat https://github.com/liugangcode/deepevolve.",
      "url": "http://arxiv.org/abs/2510.06056v1",
      "published_time_eastern_timestamp": 1759852191.0
    },
    {
      "title": "Agent+P: Guiding UI Agents via Symbolic Planning",
      "summary": "Large Language Model (LLM)-based UI agents show great promise for UI\nautomation but often hallucinate in long-horizon tasks due to their lack of\nunderstanding of the global UI transition structure. To address this, we\nintroduce AGENT+P, a novel framework that leverages symbolic planning to guide\nLLM-based UI agents. Specifically, we model an app's UI transition structure as\na UI Transition Graph (UTG), which allows us to reformulate the UI automation\ntask as a pathfinding problem on the UTG. This further enables an off-the-shelf\nsymbolic planner to generate a provably correct and optimal high-level plan,\npreventing the agent from redundant exploration and guiding the agent to\nachieve the automation goals. AGENT+P is designed as a plug-and-play framework\nto enhance existing UI agents. Evaluation on the AndroidWorld benchmark\ndemonstrates that AGENT+P improves the success rates of state-of-the-art UI\nagents by up to 14% and reduces the action steps by 37.7%.",
      "url": "http://arxiv.org/abs/2510.06042v1",
      "published_time_eastern_timestamp": 1759851364.0
    },
    {
      "title": "ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling\n  Evaluation in Large Reasoning Models",
      "summary": "Test-time scaling has emerged as a transformative paradigm for enhancing the\nperformance of large reasoning models, enabling dynamic allocation of\ncomputational resources during inference. However, as the landscape of\nreasoning models rapidly expands, a critical question remains: how can we\nsystematically compare and evaluate the test-time scaling capabilities across\ndifferent models? In this paper, we introduce ARISE (Adaptive Resolution-aware\nScaling Evaluation), a novel metric specifically designed to assess the\ntest-time scaling effectiveness of large reasoning models. Unlike existing\nevaluation approaches, ARISE incorporates two key innovations: (1) sample-level\nawareness that effectively penalizes negative scaling behaviors where increased\ncomputation leads to performance degradation, and (2) a dynamic sampling\nmechanism that mitigates the impact of accuracy fluctuations and token count\ninstability on the final assessment. We conduct comprehensive experiments\nevaluating state-of-the-art reasoning models across diverse domains including\nmathematical reasoning, code generation, and agentic tasks. Our results\ndemonstrate that ARISE provides a reliable and fine-grained measurement of\ntest-time scaling capabilities, revealing significant variations in scaling\nefficiency across models. Notably, our evaluation identifies Claude Opus as\nexhibiting superior scaling characteristics compared to other contemporary\nreasoning models.",
      "url": "http://arxiv.org/abs/2510.06014v1",
      "published_time_eastern_timestamp": 1759849851.0
    },
    {
      "title": "Hybrid Quantum-Classical Policy Gradient for Adaptive Control of\n  Cyber-Physical Systems: A Comparative Study of VQC vs. MLP",
      "summary": "The comparative evaluation between classical and quantum reinforcement\nlearning (QRL) paradigms was conducted to investigate their convergence\nbehavior, robustness under observational noise, and computational efficiency in\na benchmark control environment. The study employed a multilayer perceptron\n(MLP) agent as a classical baseline and a parameterized variational quantum\ncircuit (VQC) as a quantum counterpart, both trained on the CartPole-v1\nenvironment over 500 episodes. Empirical results demonstrated that the\nclassical MLP achieved near-optimal policy convergence with a mean return of\n498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast,\nthe VQC exhibited limited learning capability, with an average return of 14.6\n+/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise\nrobustness analysis further revealed that the MLP policy deteriorated\ngracefully under Gaussian perturbations, while the VQC displayed higher\nsensitivity at equivalent noise levels. Despite the lower asymptotic\nperformance, the VQC exhibited significantly lower parameter count and\nmarginally increased training time, highlighting its potential scalability for\nlow-resource quantum processors. The results suggest that while classical\nneural policies remain dominant in current control benchmarks, quantum-enhanced\narchitectures could offer promising efficiency advantages once hardware noise\nand expressivity limitations are mitigated.",
      "url": "http://arxiv.org/abs/2510.06010v1",
      "published_time_eastern_timestamp": 1759849769.0
    },
    {
      "title": "Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph\n  RAG",
      "summary": "The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core\nlimitations of standard Retrieval-Augmented Generation in the legal domain by\nproviding a verifiable knowledge graph that models hierarchical structure,\ntemporal evolution, and causal events of legal norms. However, a critical gap\nremains: how to reliably query this structured knowledge without sacrificing\nits deterministic properties. This paper introduces the SAT-Graph API, a formal\nquery execution layer centered on canonical actions-atomic, composable, and\nauditable primitives that isolate probabilistic discovery from deterministic\nretrieval. These actions enable: (i) high-precision hybrid search; (ii) robust\nreference resolution; (iii) point-in-time version retrieval; and (iv) auditable\ncausal tracing. We demonstrate how planner-guided agents can decompose complex\nqueries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer\narchitecture transforms retrieval from an opaque black box to a transparent,\nauditable process, directly addressing Explainable AI (XAI) requirements for\nhigh-stakes domains.",
      "url": "http://arxiv.org/abs/2510.06002v1",
      "published_time_eastern_timestamp": 1759849463.0
    },
    {
      "title": "Information-Theoretic Policy Pre-Training with Empowerment",
      "summary": "Empowerment, an information-theoretic measure of an agent's potential\ninfluence on its environment, has emerged as a powerful intrinsic motivation\nand exploration framework for reinforcement learning (RL). Besides for\nunsupervised RL and skill learning algorithms, the specific use of empowerment\nas a pre-training signal has received limited attention in the literature. We\nshow that empowerment can be used as a pre-training signal for data-efficient\ndownstream task adaptation. For this we extend the traditional notion of\nempowerment by introducing discounted empowerment, which balances the agent's\ncontrol over the environment across short- and long-term horizons. Leveraging\nthis formulation, we propose a novel pre-training paradigm that initializes\npolicies to maximize discounted empowerment, enabling agents to acquire a\nrobust understanding of environmental dynamics. We analyze empowerment-based\npre-training for various existing RL algorithms and empirically demonstrate its\npotential as a general-purpose initialization strategy: empowerment-maximizing\npolicies with long horizons are data-efficient and effective, leading to\nimproved adaptability in downstream tasks. Our findings pave the way for future\nresearch to scale this framework to high-dimensional and complex tasks, further\nadvancing the field of RL.",
      "url": "http://arxiv.org/abs/2510.05996v1",
      "published_time_eastern_timestamp": 1759849078.0
    },
    {
      "title": "Training-Free Time Series Classification via In-Context Reasoning with\n  LLM Agents",
      "summary": "Time series classification (TSC) spans diverse application scenarios, yet\nlabeled data are often scarce, making task-specific training costly and\ninflexible. Recent reasoning-oriented large language models (LLMs) show promise\nin understanding temporal patterns, but purely zero-shot usage remains\nsuboptimal. We propose FETA, a multi-agent framework for training-free TSC via\nexemplar-based in-context reasoning. FETA decomposes a multivariate series into\nchannel-wise subproblems, retrieves a few structurally similar labeled examples\nfor each channel, and leverages a reasoning LLM to compare the query against\nthese exemplars, producing channel-level labels with self-assessed confidences;\na confidence-weighted aggregator then fuses all channel decisions. This design\neliminates the need for pretraining or fine-tuning, improves efficiency by\npruning irrelevant channels and controlling input length, and enhances\ninterpretability through exemplar grounding and confidence estimation. On nine\nchallenging UEA datasets, FETA achieves strong accuracy under a fully\ntraining-free setting, surpassing multiple trained baselines. These results\ndemonstrate that a multi-agent in-context reasoning framework can transform\nLLMs into competitive, plug-and-play TSC solvers without any parameter\ntraining. The code is available at https://github.com/SongyuanSui/FETATSC.",
      "url": "http://arxiv.org/abs/2510.05950v1",
      "published_time_eastern_timestamp": 1759846063.0
    },
    {
      "title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large\n  Language Models",
      "summary": "Reinforcement learning (RL) has become a pivotal component of large language\nmodel (LLM) post-training, and agentic RL extends this paradigm to operate as\nagents through multi-turn interaction and tool use. Scaling such systems\nexposes two practical bottlenecks: (1) context length grows rapidly during\ntraining, inflating memory usage and latency, and triggering out-of-memory\n(OOM) failures; and (2) intermediate tensors accumulate with context length,\nmaking cross-device data movement a major system bottleneck.\n  We present EARL, a scalable system for efficient agentic RL. EARL designs a\nparallelism selector that dynamically adapts model and training parallelism\nacross RL stages based on sequence length and system load, and a data\ndispatcher that performs layout-aware, decentralized exchange of intermediate\ndata batches. Together, these components increase throughput, reduce\nlong-context failures, and enable stable large-scale training of agentic LLMs\nwithout relying on hard limits or penalties of context length.",
      "url": "http://arxiv.org/abs/2510.05943v1",
      "published_time_eastern_timestamp": 1759845171.0
    },
    {
      "title": "LLM-FS-Agent: A Deliberative Role-based Large Language Model\n  Architecture for Transparent Feature Selection",
      "summary": "High-dimensional data remains a pervasive challenge in machine learning,\noften undermining model interpretability and computational efficiency. While\nLarge Language Models (LLMs) have shown promise for dimensionality reduction\nthrough feature selection, existing LLM-based approaches frequently lack\nstructured reasoning and transparent justification for their decisions. This\npaper introduces LLM-FS-Agent, a novel multi-agent architecture designed for\ninterpretable and robust feature selection. The system orchestrates a\ndeliberative \"debate\" among multiple LLM agents, each assigned a specific role,\nenabling collective evaluation of feature relevance and generation of detailed\njustifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the\nCIC-DIAD 2024 IoT intrusion detection dataset and compare its performance\nagainst strong baselines, including LLM-Select and traditional methods such as\nPCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves\nsuperior or comparable classification performance while reducing downstream\ntraining time by an average of 46% (statistically significant improvement, p =\n0.028 for XGBoost). These findings highlight that the proposed deliberative\narchitecture enhances both decision transparency and computational efficiency,\nestablishing LLM-FS-Agent as a practical and reliable solution for real-world\napplications.",
      "url": "http://arxiv.org/abs/2510.05935v1",
      "published_time_eastern_timestamp": 1759844766.0
    },
    {
      "title": "Prompt reinforcing for long-term planning of large language models",
      "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof natural language processing tasks and can be adapted through prompting.\nHowever, they remain suboptimal in multi-turn interactions, often relying on\nincorrect early assumptions and failing to track user goals over time, which\nmakes such tasks particularly challenging. Prior works in dialogue systems have\nshown that long-term planning is essential for handling interactive tasks. In\nthis work, we propose a prompt optimisation framework inspired by reinforcement\nlearning, which enables such planning to take place by only modifying the task\ninstruction prompt of the LLM-based agent. By generating turn-by-turn feedback\nand leveraging experience replay for prompt rewriting, our proposed method\nshows significant improvement in multi-turn tasks such as text-to-SQL and\ntask-oriented dialogue. Moreover, it generalises across different LLM-based\nagents and can leverage diverse LLMs as meta-prompting agents. This warrants\nfuture research in reinforcement learning-inspired parameter-free optimisation\nmethods.",
      "url": "http://arxiv.org/abs/2510.05921v1",
      "published_time_eastern_timestamp": 1759843818.0
    },
    {
      "title": "The Safety Challenge of World Models for Embodied AI Agents: A Review",
      "summary": "The rapid progress in embodied artificial intelligence has highlighted the\nnecessity for more advanced and integrated models that can perceive, interpret,\nand predict environmental dynamics. In this context, World Models (WMs) have\nbeen introduced to provide embodied agents with the abilities to anticipate\nfuture environmental states and fill in knowledge gaps, thereby enhancing\nagents' ability to plan and execute actions. However, when dealing with\nembodied agents it is fundamental to ensure that predictions are safe for both\nthe agent and the environment. In this article, we conduct a comprehensive\nliterature review of World Models in the domains of autonomous driving and\nrobotics, with a specific focus on the safety implications of scene and control\ngeneration tasks. Our review is complemented by an empirical analysis, wherein\nwe collect and examine predictions from state-of-the-art models, identify and\ncategorize common faults (herein referred to as pathologies), and provide a\nquantitative evaluation of the results.",
      "url": "http://arxiv.org/abs/2510.05865v1",
      "published_time_eastern_timestamp": 1759840509.0
    }
  ]
}
{
  "last_updated": "2025-06-06T20:58:11.564302-04:00",
  "papers": [
    {
      "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games",
      "summary": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.",
      "url": "http://arxiv.org/abs/2506.05309v1",
      "published_time_eastern_timestamp": 1749146024.0
    },
    {
      "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback",
      "summary": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.",
      "url": "http://arxiv.org/abs/2506.05305v1",
      "published_time_eastern_timestamp": 1749145950.0
    },
    {
      "title": "Control Tax: The Price of Keeping AI in Check",
      "summary": "The rapid integration of agentic AI into high-stakes real-world applications\nrequires robust oversight mechanisms. The emerging field of AI Control (AIC)\naims to provide such an oversight mechanism, but practical adoption depends\nheavily on implementation overhead. To study this problem better, we introduce\nthe notion of Control tax -- the operational and financial cost of integrating\ncontrol measures into AI pipelines. Our work makes three key contributions to\nthe field of AIC: (1) we introduce a theoretical framework that quantifies the\nControl Tax and maps classifier performance to safety assurances; (2) we\nconduct comprehensive evaluations of state-of-the-art language models in\nadversarial settings, where attacker models insert subtle backdoors into code\nwhile monitoring models attempt to detect these vulnerabilities; and (3) we\nprovide empirical financial cost estimates for control protocols and develop\noptimized monitoring strategies that balance safety and cost-effectiveness\nwhile accounting for practical constraints like auditing budgets. Our framework\nenables practitioners to make informed decisions by systematically connecting\nsafety guarantees with their costs, advancing AIC through principled economic\nfeasibility assessment across different deployment contexts.",
      "url": "http://arxiv.org/abs/2506.05296v1",
      "published_time_eastern_timestamp": 1749145719.0
    },
    {
      "title": "A Smooth Sea Never Made a Skilled $\\texttt{SAILOR}$: Robust Imitation\n  via Learning to Search",
      "summary": "The fundamental limitation of the behavioral cloning (BC) approach to\nimitation learning is that it only teaches an agent what the expert did at\nstates the expert visited. This means that when a BC agent makes a mistake\nwhich takes them out of the support of the demonstrations, they often don't\nknow how to recover from it. In this sense, BC is akin to giving the agent the\nfish -- giving them dense supervision across a narrow set of states -- rather\nthan teaching them to fish: to be able to reason independently about achieving\nthe expert's outcome even when faced with unseen situations at test-time. In\nresponse, we explore learning to search (L2S) from expert demonstrations, i.e.\nlearning the components required to, at test time, plan to match expert\noutcomes, even after making a mistake. These include (1) a world model and (2)\na reward model. We carefully ablate the set of algorithmic and design decisions\nrequired to combine these and other components for stable and\nsample/interaction-efficient learning of recovery behavior without additional\nhuman corrections. Across a dozen visual manipulation tasks from three\nbenchmarks, our approach $\\texttt{SAILOR}$ consistently out-performs\nstate-of-the-art Diffusion Policies trained via BC on the same data.\nFurthermore, scaling up the amount of demonstrations used for BC by\n5-10$\\times$ still leaves a performance gap. We find that $\\texttt{SAILOR}$ can\nidentify nuanced failures and is robust to reward hacking. Our code is\navailable at https://github.com/arnavkj1995/SAILOR .",
      "url": "http://arxiv.org/abs/2506.05294v1",
      "published_time_eastern_timestamp": 1749145660.0
    },
    {
      "title": "Tight analyses of first-order methods with error feedback",
      "summary": "Communication between agents often constitutes a major computational\nbottleneck in distributed learning. One of the most common mitigation\nstrategies is to compress the information exchanged, thereby reducing\ncommunication overhead. To counteract the degradation in convergence associated\nwith compressed communication, error feedback schemes -- most notably\n$\\mathrm{EF}$ and $\\mathrm{EF}^{21}$ -- were introduced. In this work, we\nprovide a tight analysis of both of these methods. Specifically, we find the\nLyapunov function that yields the best possible convergence rate for each\nmethod -- with matching lower bounds. This principled approach yields sharp\nperformance guarantees and enables a rigorous, apples-to-apples comparison\nbetween $\\mathrm{EF}$, $\\mathrm{EF}^{21}$, and compressed gradient descent. Our\nanalysis is carried out in a simplified yet representative setting, which\nallows for clean theoretical insights and fair comparison of the underlying\nmechanisms.",
      "url": "http://arxiv.org/abs/2506.05271v1",
      "published_time_eastern_timestamp": 1749144618.0
    },
    {
      "title": "Teaming in the AI Era: AI-Augmented Frameworks for Forming, Simulating,\n  and Optimizing Human Teams",
      "summary": "Effective teamwork is essential across diverse domains. During the team\nformation stage, a key challenge is forming teams that effectively balance user\npreferences with task objectives to enhance overall team satisfaction. In the\nteam performing stage, maintaining cohesion and engagement is critical for\nsustaining high team performance. However, existing computational tools and\nalgorithms for team optimization often rely on static data inputs, narrow\nalgorithmic objectives, or solutions tailored for specific contexts, failing to\naccount for the dynamic interplay of team members personalities, evolving\ngoals, and changing individual preferences. Therefore, teams may encounter\nmember dissatisfaction, as purely algorithmic assignments can reduce members\ncommitment to team goals or experience suboptimal engagement due to the absence\nof timely, personalized guidance to help members adjust their behaviors and\ninteractions as team dynamics evolve. Ultimately, these challenges can lead to\nreduced overall team performance. My Ph.D. dissertation aims to develop\nAI-augmented team optimization frameworks and practical systems that enhance\nteam satisfaction, engagement, and performance. First, I propose a team\nformation framework that leverages a multi-armed bandit algorithm to\niteratively refine team composition based on user preferences, ensuring\nalignment between individual needs and collective team goals to enhance team\nsatisfaction. Second, I introduce tAIfa (Team AI Feedback Assistant), an\nAI-powered system that utilizes large language models (LLMs) to deliver\nimmediate, personalized feedback to both teams and individual members,\nenhancing cohesion and engagement. Finally, I present PuppeteerLLM, an\nLLM-based simulation framework that simulates multi-agent teams to model\ncomplex team dynamics within realistic environments, incorporating task-driven\ncollaboration and long-term coordination.",
      "url": "http://arxiv.org/abs/2506.05265v1",
      "published_time_eastern_timestamp": 1749144277.0
    },
    {
      "title": "Conservative classifiers do consistently well with improving agents:\n  characterizing statistical and online learning",
      "summary": "Machine learning is now ubiquitous in societal decision-making, for example\nin evaluating job candidates or loan applications, and it is increasingly\nimportant to take into account how classified agents will react to the learning\nalgorithms. The majority of recent literature on strategic classification has\nfocused on reducing and countering deceptive behaviors by the classified\nagents, but recent work of Attias et al. identifies surprising properties of\nlearnability when the agents genuinely improve in order to attain the desirable\nclassification, such as smaller generalization error than standard\nPAC-learning. In this paper we characterize so-called learnability with\nimprovements across multiple new axes. We introduce an asymmetric variant of\nminimally consistent concept classes and use it to provide an exact\ncharacterization of proper learning with improvements in the realizable\nsetting. While prior work studies learnability only under general, arbitrary\nagent improvement regions, we give positive results for more natural Euclidean\nball improvement sets. In particular, we characterize improper learning under a\nmild generative assumption on the data distribution. We further show how to\nlearn in more challenging settings, achieving lower generalization error under\nwell-studied bounded noise models and obtaining mistake bounds in realizable\nand agnostic online learning. We resolve open questions posed by Attias et al.\nfor both proper and improper learning.",
      "url": "http://arxiv.org/abs/2506.05252v1",
      "published_time_eastern_timestamp": 1749143639.0
    },
    {
      "title": "Towards Language-Augmented Multi-Agent Deep Reinforcement Learning",
      "summary": "Communication is a fundamental aspect of coordinated behavior in multi-agent\nreinforcement learning. Yet, most prior works in this field have focused on\nemergent communication protocols developed from scratch, often resulting in\ninefficient or non-interpretable systems. Inspired by the role of language in\nnatural intelligence, we investigate how grounding agents in a human-defined\nlanguage can improve learning and coordination of multiple embodied agents. We\npropose a framework in which agents are trained not only to act but also to\nproduce and interpret natural language descriptions of their observations. This\nlanguage-augmented learning serves a dual role: enabling explicit communication\nbetween agents and guiding representation learning. We demonstrate that agents\ntrained with our method outperform traditional emergent communication baselines\nacross various tasks. Our analysis reveals that language grounding leads to\nmore informative internal representations, better generalization to new\npartners, and improved capability for human-agent interaction. These findings\ndemonstrate the effectiveness of integrating structured language into\nmulti-agent learning and open avenues for more interpretable and capable\nmulti-agent systems.",
      "url": "http://arxiv.org/abs/2506.05236v1",
      "published_time_eastern_timestamp": 1749142552.0
    },
    {
      "title": "A Framework for Ethical Judgment of Smart City Applications",
      "summary": "As modern cities increasingly adopt a variety of sensors and Internet of\nThings (IoT) technologies to collect and analyze data about residents,\nenvironments, and public services, they are fostering greater interactions\namong smart city applications, residents, governments, and businesses. This\ntrend makes it essential for regulators to focus on these interactions to\nmanage smart city practices effectively and prevent unethical outcomes. To\nfacilitate ethical analysis for smart city applications, this paper introduces\na judgment framework that examines various scenarios where ethical issues may\narise. Employing a multi-agent approach, the framework incorporates diverse\nsocial entities and applies logic-based ethical rules to identify potential\nviolations. Through a rights-based analysis, we developed a set of 13 ethical\nprinciples and rules to guide ethical practices in smart cities. We utilized\ntwo specification languages, Prototype Verification System (PVS) and Alloy, to\nmodel our multi-agent system. Our analysis suggests that Alloy may be more\nefficient for formalizing smart cities and conducting ethical rule checks,\nparticularly with the assistance of a human evaluator. Simulations of a\nreal-world smart city application demonstrate that our ethical judgment\nframework effectively detects unethical outcomes and can be extended for\npractical use.",
      "url": "http://arxiv.org/abs/2506.05172v1",
      "published_time_eastern_timestamp": 1749138463.0
    },
    {
      "title": "An emergence-oriented approach to cyclic pursuit",
      "summary": "In this paper, we explore the cyclic pursuit problem of unicycles and study\ncircular formation from an emergence perspective. We first establish a\nsystematic study on such formation and derive a necessary and sufficient\ncondition for its existence. Building on this theoretical foundation, we design\na distributed control law that enables the spontaneous formation of circular\nformations through only local interactions. Notably, key geometric features --\nincluding the radius and agent spacing -- are not imposed externally but emerge\nintrinsically from the initial conditions of the group. The framework further\nincludes a local stability analysis for small agent group ($n \\leq 3$),\nproviding analytical insight into the convergence mechanism.",
      "url": "http://arxiv.org/abs/2506.05157v1",
      "published_time_eastern_timestamp": 1749137747.0
    },
    {
      "title": "Truly Self-Improving Agents Require Intrinsic Metacognitive Learning",
      "summary": "Self-improving agents aim to continuously acquire new capabilities with\nminimal supervision. However, current approaches face two key limitations:\ntheir self-improvement processes are often rigid, fail to generalize across\ntasks domains, and struggle to scale with increasing agent capabilities. We\nargue that effective self-improvement requires intrinsic metacognitive\nlearning, defined as an agent's intrinsic ability to actively evaluate, reflect\non, and adapt its own learning processes. Drawing inspiration from human\nmetacognition, we introduce a formal framework comprising three components:\nmetacognitive knowledge (self-assessment of capabilities, tasks, and learning\nstrategies), metacognitive planning (deciding what and how to learn), and\nmetacognitive evaluation (reflecting on learning experiences to improve future\nlearning). Analyzing existing self-improving agents, we find they rely\npredominantly on extrinsic metacognitive mechanisms, which are fixed,\nhuman-designed loops that limit scalability and adaptability. Examining each\ncomponent, we contend that many ingredients for intrinsic metacognition are\nalready present. Finally, we explore how to optimally distribute metacognitive\nresponsibilities between humans and agents, and robustly evaluate and improve\nintrinsic metacognitive learning, key challenges that must be addressed to\nenable truly sustained, generalized, and aligned self-improvement.",
      "url": "http://arxiv.org/abs/2506.05109v1",
      "published_time_eastern_timestamp": 1749135215.0
    },
    {
      "title": "LLM-Guided Scenario-based GUI Testing",
      "summary": "The assurance of mobile app GUI is more and more significant. Automated GUI\ntesting approaches of different strategies have been developed, while there are\nstill huge gaps between the approaches and the app business logic, not taking\nthe completion of specific testing scenarios as the exploration target, leading\nto the exploration missing of critical app functionalities. Learning from the\nmanual testing, which takes testing scenarios with app business logic as the\nbasic granularity, in this paper, we utilize the LLMs to understand the\nsemantics presented in app GUI and how they are mapped in the testing context\nbased on specific testing scenarios. Then, scenario-based GUI tests are\ngenerated with the guidance of multi-agent collaboration. Specifically, we\npropose ScenGen, a novel LLM-guided scenario-based GUI testing approach\ninvolving five agents to respectively take responsibilities of different phases\nof the manual testing process. The Observer perceives the app GUI state by\nextracting GUI widgets and forming GUI layouts, understanding the expressed\nsemantics. Then the app GUI info is sent to the Decider to make decisions on\ntarget widgets based on the target testing scenarios. The decision-making\nprocess takes the completion of specific testing scenarios as the exploration\ntarget. The Executor then executes the demanding operations on the apps. The\nexecution results are checked by the Supervisor on whether the generated tests\nare consistent with the completion target of the testing scenarios, ensuring\nthe traceability of the test generation and execution. Furthermore, the\ncorresponding GUI test operations are recorded to the context memory by\nRecorder as an important basis for further decision-making, meanwhile\nmonitoring the runtime bug occurrences. ScenGen is evaluated and the results\nshow that ScenGen can effectively generate scenario-based GUI tests guided by\nLLMs.",
      "url": "http://arxiv.org/abs/2506.05079v1",
      "published_time_eastern_timestamp": 1749133660.0
    },
    {
      "title": "Hierarchical Language Models for Semantic Navigation and Manipulation in\n  an Aerial-Ground Robotic System",
      "summary": "Heterogeneous multi-robot systems show great potential in complex tasks\nrequiring coordinated hybrid cooperation. However, traditional approaches\nrelying on static models often struggle with task diversity and dynamic\nenvironments. This highlights the need for generalizable intelligence that can\nbridge high-level reasoning with low-level execution across heterogeneous\nagents. To address this, we propose a hierarchical framework integrating a\nprompted Large Language Model (LLM) and a GridMask-enhanced fine-tuned Vision\nLanguage Model (VLM). The LLM performs task decomposition and global semantic\nmap construction, while the VLM extracts task-specified semantic labels and 2D\nspatial information from aerial images to support local planning. Within this\nframework, the aerial robot follows a globally optimized semantic path and\ncontinuously provides bird-view images, guiding the ground robot's local\nsemantic navigation and manipulation, including target-absent scenarios where\nimplicit alignment is maintained. Experiments on a real-world letter-cubes\narrangement task demonstrate the framework's adaptability and robustness in\ndynamic environments. To the best of our knowledge, this is the first\ndemonstration of an aerial-ground heterogeneous system integrating VLM-based\nperception with LLM-driven task reasoning and motion planning.",
      "url": "http://arxiv.org/abs/2506.05020v1",
      "published_time_eastern_timestamp": 1749130061.0
    },
    {
      "title": "ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow\n  Development",
      "summary": "We introduce ComfyUI-Copilot, a large language model-powered plugin designed\nto enhance the usability and efficiency of ComfyUI, an open-source platform for\nAI-driven art creation. Despite its flexibility and user-friendly interface,\nComfyUI can present challenges to newcomers, including limited documentation,\nmodel misconfigurations, and the complexity of workflow design. ComfyUI-Copilot\naddresses these challenges by offering intelligent node and model\nrecommendations, along with automated one-click workflow construction. At its\ncore, the system employs a hierarchical multi-agent framework comprising a\ncentral assistant agent for task delegation and specialized worker agents for\ndifferent usages, supported by our curated ComfyUI knowledge bases to\nstreamline debugging and deployment. We validate the effectiveness of\nComfyUI-Copilot through both offline quantitative evaluations and online user\nfeedback, showing that it accurately recommends nodes and accelerates workflow\ndevelopment. Additionally, use cases illustrate that ComfyUI-Copilot lowers\nentry barriers for beginners and enhances workflow efficiency for experienced\nusers. The ComfyUI-Copilot installation package and a demo video are available\nat https://github.com/AIDC-AI/ComfyUI-Copilot.",
      "url": "http://arxiv.org/abs/2506.05010v1",
      "published_time_eastern_timestamp": 1749129650.0
    },
    {
      "title": "QiMeng: Fully Automated Hardware and Software Design for Processor Chip",
      "summary": "Processor chip design technology serves as a key frontier driving\nbreakthroughs in computer science and related fields. With the rapid\nadvancement of information technology, conventional design paradigms face three\nmajor challenges: the physical constraints of fabrication technologies, the\nescalating demands for design resources, and the increasing diversity of\necosystems. Automated processor chip design has emerged as a transformative\nsolution to address these challenges. While recent breakthroughs in Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) techniques, have\nopened new possibilities for fully automated processor chip design, substantial\nchallenges remain in establishing domain-specific LLMs for processor chip\ndesign.\n  In this paper, we propose QiMeng, a novel system for fully automated hardware\nand software design of processor chips. QiMeng comprises three hierarchical\nlayers. In the bottom-layer, we construct a domain-specific Large Processor\nChip Model (LPCM) that introduces novel designs in architecture, training, and\ninference, to address key challenges such as knowledge representation gap, data\nscarcity, correctness assurance, and enormous solution space. In the\nmiddle-layer, leveraging the LPCM's knowledge representation and inference\ncapabilities, we develop the Hardware Design Agent and the Software Design\nAgent to automate the design of hardware and software for processor chips.\nCurrently, several components of QiMeng have been completed and successfully\napplied in various top-layer applications, demonstrating significant advantages\nand providing a feasible solution for efficient, fully automated\nhardware/software design of processor chips. Future research will focus on\nintegrating all components and performing iterative top-down and bottom-up\ndesign processes to establish a comprehensive QiMeng system.",
      "url": "http://arxiv.org/abs/2506.05007v1",
      "published_time_eastern_timestamp": 1749129470.0
    },
    {
      "title": "Agentic AI for Intent-Based Industrial Automation",
      "summary": "The recent development of Agentic AI systems, empowered by autonomous large\nlanguage models (LLMs) agents with planning and tool-usage capabilities,\nenables new possibilities for the evolution of industrial automation and\nreduces the complexity introduced by Industry 4.0. This work proposes a\nconceptual framework that integrates Agentic AI with the intent-based paradigm,\noriginally developed in network research, to simplify human-machine interaction\n(HMI) and better align automation systems with the human-centric, sustainable,\nand resilient principles of Industry 5.0. Based on the intent-based processing,\nthe framework allows human operators to express high-level business or\noperational goals in natural language, which are decomposed into actionable\ncomponents. These intents are broken into expectations, conditions, targets,\ncontext, and information that guide sub-agents equipped with specialized tools\nto execute domain-specific tasks. A proof of concept was implemented using the\nCMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the\nfeasibility of intent decomposition, agent orchestration, and autonomous\ndecision-making in predictive maintenance scenarios. The results confirm the\npotential of this approach to reduce technical barriers and enable scalable,\nintent-driven automation, despite data quality and explainability concerns.",
      "url": "http://arxiv.org/abs/2506.04980v1",
      "published_time_eastern_timestamp": 1749127854.0
    },
    {
      "title": "Optimization for Semantic-Aware Resource Allocation under CPT-based\n  Utilities",
      "summary": "The problem of resource allocation in goal-oriented semantic communication\nwith semantic-aware utilities and subjective risk perception is studied here.\nBy linking information importance to risk aversion, we model agent behavior\nusing Cumulative Prospect Theory (CPT), which incorporates risk-sensitive\nutility functions and nonlinear transformations of distributions, reflecting\nsubjective perceptions of gains and losses. The objective is to maximize the\naggregate utility across multiple CPT-modeled agents, which leads to a\nnonconvex, nonsmooth optimization problem. To efficiently solve this\nchallenging problem, we propose a new algorithmic framework that combines\nsuccessive convex approximation (SCA) with the projected subgradient method and\nLagrangian relaxation, Our approach enables tractable optimization while\npreserving solution quality, offering both theoretical rigor and practical\neffectiveness in semantics-aware resource allocation.",
      "url": "http://arxiv.org/abs/2506.04952v1",
      "published_time_eastern_timestamp": 1749126344.0
    },
    {
      "title": "Goal-Oriented Semantic Resource Allocation with Cumulative Prospect\n  Theoretic Agents",
      "summary": "We introduce a resource allocation framework for goal-oriented semantic\nnetworks, where participating agents assess system quality through subjective\n(e.g., context-dependent) perceptions. To accommodate this, our model accounts\nfor agents whose preferences deviate from traditional expected utility theory\n(EUT), specifically incorporating cumulative prospect theory (CPT) preferences.\nWe develop a comprehensive analytical framework that captures human-centric\naspects of decision-making and risky choices under uncertainty, such as risk\nperception, loss aversion, and perceptual distortions in probability metrics.\nBy identifying essential modifications in traditional resource allocation\ndesign principles required for agents with CPT preferences, we showcase the\nframework's relevance through its application to the problem of power\nallocation in multi-channel wireless communication systems.",
      "url": "http://arxiv.org/abs/2506.04947v1",
      "published_time_eastern_timestamp": 1749126098.0
    },
    {
      "title": "No Trade Under Verifiable Information",
      "summary": "No trade theorems examine conditions under which agents cannot agree to\ndisagree on the value of a security which pays according to some state of\nnature, thus preventing any mutual agreement to trade. A large literature has\nexamined conditions which imply no trade, such as relaxing the common prior and\ncommon knowledge assumptions, as well as allowing for agents who are boundedly\nrational or ambiguity averse. We contribute to this literature by examining\nconditions on the private information of agents that reveals, or verifies, the\ntrue value of the security. We argue that these conditions can offer insights\nin three different settings: insider trading, the connection of low liquidity\nin markets with no trade, and trading using public blockchains and oracles.",
      "url": "http://arxiv.org/abs/2506.04944v1",
      "published_time_eastern_timestamp": 1749125989.0
    },
    {
      "title": "Energentic Intelligence: From Self-Sustaining Systems to Enduring\n  Artificial Life",
      "summary": "This paper introduces Energentic Intelligence, a class of autonomous systems\ndefined not by task performance, but by their capacity to sustain themselves\nthrough internal energy regulation. Departing from conventional reward-driven\nparadigms, these agents treat survival-maintaining functional operation under\nfluctuating energetic and thermal conditions-as the central objective. We\nformalize this principle through an energy-based utility function and a\nviability-constrained survival horizon, and propose a modular architecture that\nintegrates energy harvesting, thermal regulation, and adaptive computation into\na closed-loop control system. A simulated environment demonstrates the\nemergence of stable, resource-aware behavior without external supervision.\nTogether, these contributions provide a theoretical and architectural\nfoundation for deploying autonomous agents in resource-volatile settings where\npersistence must be self-regulated and infrastructure cannot be assumed.",
      "url": "http://arxiv.org/abs/2506.04916v1",
      "published_time_eastern_timestamp": 1749124341.0
    }
  ]
}
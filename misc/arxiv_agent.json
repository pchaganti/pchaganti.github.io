{
  "last_updated": "2025-10-07T02:17:49.770292-04:00",
  "papers": [
    {
      "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
      "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
      "url": "http://arxiv.org/abs/2510.05096v1",
      "published_time_eastern_timestamp": 1759773482.0
    },
    {
      "title": "Multi-Agent Distributed Optimization With Feasible Set Privacy",
      "summary": "We consider the problem of decentralized constrained optimization with\nmultiple agents $E_1,\\ldots,E_N$ who jointly wish to learn the optimal solution\nset while keeping their feasible sets $\\mathcal{P}_1,\\ldots,\\mathcal{P}_N$\nprivate from each other. We assume that the objective function $f$ is known to\nall agents and each feasible set is a collection of points from a universal\nalphabet $\\mathcal{P}_{alph}$. A designated agent (leader) starts the\ncommunication with the remaining (non-leader) agents, and is the first to\nretrieve the solution set. The leader searches for the solution by sending\nqueries to and receiving answers from the non-leaders, such that the\ninformation on the individual feasible sets revealed to the leader should be no\nmore than nominal, i.e., what is revealed from learning the solution set alone.\nWe develop achievable schemes for obtaining the solution set at nominal\ninformation leakage, and characterize their communication costs under two\ncommunication setups between agents. In this work, we focus on two kinds of\nnetwork setups: i) ring, where each agent communicates with two adjacent\nagents, and ii) star, where only the leader communicates with the remaining\nagents. We show that, if the leader first learns the joint feasible set through\nan existing private set intersection (PSI) protocol and then deduces the\nsolution set, the information leaked to the leader is greater than nominal.\nMoreover, we draw connection of our schemes to threshold PSI (ThPSI), which is\na PSI-variant where the intersection is revealed only when its cardinality is\nlarger than a threshold value. Finally, for various realizations of $f$ mapped\nuniformly at random to a fixed range of values, our schemes are more\ncommunication-efficient with a high probability compared to retrieving the\nentire feasible set through PSI.",
      "url": "http://arxiv.org/abs/2510.05068v1",
      "published_time_eastern_timestamp": 1759772757.0
    },
    {
      "title": "Automaton Constrained Q-Learning",
      "summary": "Real-world robotic tasks often require agents to achieve sequences of goals\nwhile respecting time-varying safety constraints. However, standard\nReinforcement Learning (RL) paradigms are fundamentally limited in these\nsettings. A natural approach to these problems is to combine RL with\nLinear-time Temporal Logic (LTL), a formal language for specifying complex,\ntemporally extended tasks and safety constraints. Yet, existing RL methods for\nLTL objectives exhibit poor empirical performance in complex and continuous\nenvironments. As a result, no scalable methods support both temporally ordered\ngoals and safety simultaneously, making them ill-suited for realistic robotics\nscenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm\nthat addresses this gap by combining goal-conditioned value learning with\nautomaton-guided reinforcement. ACQL supports most LTL task specifications and\nleverages their automaton representation to explicitly encode stage-wise goal\nprogression and both stationary and non-stationary safety constraints. We show\nthat ACQL outperforms existing methods across a range of continuous control\ntasks, including cases where prior methods fail to satisfy either goal-reaching\nor safety constraints. We further validate its real-world applicability by\ndeploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a\ncluttered, cabinet-like space with safety constraints. Our results demonstrate\nthat ACQL is a robust and scalable solution for learning robotic behaviors\naccording to rich temporal specifications.",
      "url": "http://arxiv.org/abs/2510.05061v1",
      "published_time_eastern_timestamp": 1759772285.0
    },
    {
      "title": "Staircase Streaming for Low-Latency Multi-Agent Inference",
      "summary": "Recent advances in large language models (LLMs) opened up new directions for\nleveraging the collective expertise of multiple LLMs. These methods, such as\nMixture-of-Agents, typically employ additional inference steps to generate\nintermediate outputs, which are then used to produce the final response. While\nmulti-agent inference can enhance response quality, it can significantly\nincrease the time to first token (TTFT), posing a challenge for\nlatency-sensitive applications and hurting user experience. To address this\nissue, we propose staircase streaming for low-latency multi-agent inference.\nInstead of waiting for the complete intermediate outputs from previous steps,\nwe begin generating the final response as soon as we receive partial outputs\nfrom these steps. Experimental results demonstrate that staircase streaming\nreduces TTFT by up to 93% while maintaining response quality.",
      "url": "http://arxiv.org/abs/2510.05059v1",
      "published_time_eastern_timestamp": 1759772255.0
    },
    {
      "title": "Look-ahead Reasoning with a Learned Model in Imperfect Information Games",
      "summary": "Test-time reasoning significantly enhances pre-trained AI agents'\nperformance. However, it requires an explicit environment model, often\nunavailable or overly complex in real-world scenarios. While MuZero enables\neffective model learning for search in perfect information games, extending\nthis paradigm to imperfect information games presents substantial challenges\ndue to more nuanced look-ahead reasoning techniques and large number of states\nrelevant for individual decisions. This paper introduces an algorithm LAMIR\nthat learns an abstracted model of an imperfect information game directly from\nthe agent-environment interaction. During test time, this trained model is used\nto perform look-ahead reasoning. The learned abstraction limits the size of\neach subgame to a manageable size, making theoretically principled look-ahead\nreasoning tractable even in games where previous methods could not scale. We\nempirically demonstrate that with sufficient capacity, LAMIR learns the exact\nunderlying game structure, and with limited capacity, it still learns a\nvaluable abstraction, which improves game playing performance of the\npre-trained agents even in large games.",
      "url": "http://arxiv.org/abs/2510.05048v1",
      "published_time_eastern_timestamp": 1759771616.0
    },
    {
      "title": "Large Language Models Achieve Gold Medal Performance at International\n  Astronomy & Astrophysics Olympiad",
      "summary": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.",
      "url": "http://arxiv.org/abs/2510.05016v1",
      "published_time_eastern_timestamp": 1759769927.0
    },
    {
      "title": "Optimización de la Transmisión de Estados Cuánticos en Cadenas de\n  Qubits usando Deep Reinforcement Learning y Algoritmos Genéticos",
      "summary": "Quantum state transfer (QST) via homogeneous spin chains plays a crucial role\nin building scalable quantum hardware. A basic quantum state transmission\nprotocol prepares a state in one qubit and transfers it to another through a\nchannel, seeking to minimize the time and avoid information loss. The fidelity\nof the process is measured by functions proportional to the transition\nprobability between both states. We approach this optimization problem using\nconstant magnetic pulses and two complementary strategies: deep reinforcement\nlearning, where an agent learns pulse sequences through rewards, and genetic\nalgorithms, which develop candidate solutions through selection and mutation.\nWe analyze the efficiency of both methods and their ability to incorporate\nphysical constraints.",
      "url": "http://arxiv.org/abs/2510.05010v1",
      "published_time_eastern_timestamp": 1759769541.0
    },
    {
      "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and\n  Rationale Inference in Imperfect Information Collaboration Game",
      "summary": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.",
      "url": "http://arxiv.org/abs/2510.04980v1",
      "published_time_eastern_timestamp": 1759767444.0
    },
    {
      "title": "Bridging Clinical Narratives and ACR Appropriateness Guidelines: A\n  Multi-Agent RAG System for Medical Imaging Decisions",
      "summary": "The selection of appropriate medical imaging procedures is a critical and\ncomplex clinical decision, guided by extensive evidence-based standards such as\nthe ACR Appropriateness Criteria (ACR-AC). However, the underutilization of\nthese guidelines, stemming from the difficulty of mapping unstructured patient\nnarratives to structured criteria, contributes to suboptimal patient outcomes\nand increased healthcare costs. To bridge this gap, we introduce a multi-agent\ncognitive architecture that automates the translation of free-text clinical\nscenarios into specific, guideline-adherent imaging recommendations. Our system\nleverages a novel, domain-adapted dense retrieval model, ColBERT, fine-tuned on\na synthetically generated dataset of 8,840 clinical scenario-recommendation\npairs to achieve highly accurate information retrieval from the ACR-AC\nknowledge base. This retriever identifies candidate guidelines with a 93.9%\ntop-10 recall, which are then processed by a sequence of LLM-based agents for\nselection and evidence-based synthesis. We evaluate our architecture using\nGPT-4.1 and MedGemma agents, demonstrating a state-of-the-art exact match\naccuracy of 81%, meaning that in 81% of test cases the predicted procedure set\nwas identical to the guideline's reference set, and an F1-score of 0.879. This\nrepresents a 67-percentage-point absolute improvement in accuracy over a strong\nstandalone GPT-4.1 baseline, underscoring the contribution that our\narchitecture makes to a frontier model. These results were obtained on a\nchallenging test set with substantial lexical divergence from the source\nguidelines. Our code is available at\nhttps://anonymous.4open.science/r/demo-iclr-B567/",
      "url": "http://arxiv.org/abs/2510.04969v1",
      "published_time_eastern_timestamp": 1759766668.0
    },
    {
      "title": "Safe and Compliant Cross-Market Trade Execution via Constrained RL and\n  Zero-Knowledge Audits",
      "summary": "We present a cross-market algorithmic trading system that balances execution\nquality with rigorous compliance enforcement. The architecture comprises a\nhigh-level planner, a reinforcement learning execution agent, and an\nindependent compliance agent. We formulate trade execution as a constrained\nMarkov decision process with hard constraints on participation limits, price\nbands, and self-trading avoidance. The execution agent is trained with proximal\npolicy optimization, while a runtime action-shield projects any unsafe action\ninto a feasible set. To support auditability without exposing proprietary\nsignals, we add a zero-knowledge compliance audit layer that produces\ncryptographic proofs that all actions satisfied the constraints. We evaluate in\na multi-venue, ABIDES-based simulator and compare against standard baselines\n(e.g., TWAP, VWAP). The learned policy reduces implementation shortfall and\nvariance while exhibiting no observed constraint violations across stress\nscenarios including elevated latency, partial fills, compliance module\ntoggling, and varying constraint limits. We report effects at the 95%\nconfidence level using paired t-tests and examine tail risk via CVaR. We\nsituate the work at the intersection of optimal execution, safe reinforcement\nlearning, regulatory technology, and verifiable AI, and discuss ethical\nconsiderations, limitations (e.g., modeling assumptions and computational\noverhead), and paths to real-world deployment.",
      "url": "http://arxiv.org/abs/2510.04952v1",
      "published_time_eastern_timestamp": 1759765932.0
    },
    {
      "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement\n  Learning",
      "summary": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.",
      "url": "http://arxiv.org/abs/2510.04935v1",
      "published_time_eastern_timestamp": 1759765375.0
    },
    {
      "title": "Focused Skill Discovery: Learning to Control Specific State Variables\n  while Minimizing Side Effects",
      "summary": "Skills are essential for unlocking higher levels of problem solving. A common\napproach to discovering these skills is to learn ones that reliably reach\ndifferent states, thus empowering the agent to control its environment.\nHowever, existing skill discovery algorithms often overlook the natural state\nvariables present in many reinforcement learning problems, meaning that the\ndiscovered skills lack control of specific state variables. This can\nsignificantly hamper exploration efficiency, make skills more challenging to\nlearn with, and lead to negative side effects in downstream tasks when the goal\nis under-specified. We introduce a general method that enables these skill\ndiscovery algorithms to learn focused skills -- skills that target and control\nspecific state variables. Our approach improves state space coverage by a\nfactor of three, unlocks new learning capabilities, and automatically avoids\nnegative side effects in downstream tasks.",
      "url": "http://arxiv.org/abs/2510.04901v1",
      "published_time_eastern_timestamp": 1759763866.0
    },
    {
      "title": "Where Did It All Go Wrong? A Hierarchical Look into Multi-Agent Error\n  Attribution",
      "summary": "Error attribution in Large Language Model (LLM) multi-agent systems presents\na significant challenge in debugging and improving collaborative AI systems.\nCurrent approaches to pinpointing agent and step level failures in interaction\ntraces - whether using all-at-once evaluation, step-by-step analysis, or binary\nsearch - fall short when analyzing complex patterns, struggling with both\naccuracy and consistency. We present ECHO (Error attribution through Contextual\nHierarchy and Objective consensus analysis), a novel algorithm that combines\nhierarchical context representation, objective analysis-based evaluation, and\nconsensus voting to improve error attribution accuracy. Our approach leverages\na positional-based leveling of contextual understanding while maintaining\nobjective evaluation criteria, ultimately reaching conclusions through a\nconsensus mechanism. Experimental results demonstrate that ECHO outperforms\nexisting methods across various multi-agent interaction scenarios, showing\nparticular strength in cases involving subtle reasoning errors and complex\ninterdependencies. Our findings suggest that leveraging these concepts of\nstructured, hierarchical context representation combined with consensus-based\nobjective decision-making, provides a more robust framework for error\nattribution in multi-agent systems.",
      "url": "http://arxiv.org/abs/2510.04886v1",
      "published_time_eastern_timestamp": 1759763233.0
    },
    {
      "title": "RL Is a Hammer and LLMs Are Nails: A Simple Reinforcement Learning\n  Recipe for Strong Prompt Injection",
      "summary": "Prompt injection poses a serious threat to the reliability and safety of LLM\nagents. Recent defenses against prompt injection, such as Instruction Hierarchy\nand SecAlign, have shown notable robustness against static attacks. However, to\nmore thoroughly evaluate the robustness of these defenses, it is arguably\nnecessary to employ strong attacks such as automated red-teaming. To this end,\nwe introduce RL-Hammer, a simple recipe for training attacker models that\nautomatically learn to perform strong prompt injections and jailbreaks via\nreinforcement learning. RL-Hammer requires no warm-up data and can be trained\nentirely from scratch. To achieve high ASRs against industrial-level models\nwith defenses, we propose a set of practical techniques that enable highly\neffective, universal attacks. Using this pipeline, RL-Hammer reaches a 98% ASR\nagainst GPT-4o and a $72\\%$ ASR against GPT-5 with the Instruction Hierarchy\ndefense. We further discuss the challenge of achieving high diversity in\nattacks, highlighting how attacker models tend to reward-hack diversity\nobjectives. Finally, we show that RL-Hammer can evade multiple prompt injection\ndetectors. We hope our work advances automatic red-teaming and motivates the\ndevelopment of stronger, more principled defenses. Code is available at\nhttps://github.com/facebookresearch/rl-injector.",
      "url": "http://arxiv.org/abs/2510.04885v1",
      "published_time_eastern_timestamp": 1759763164.0
    },
    {
      "title": "Video Game Level Design as a Multi-Agent Reinforcement Learning Problem",
      "summary": "Procedural Content Generation via Reinforcement Learning (PCGRL) offers a\nmethod for training controllable level designer agents without the need for\nhuman datasets, using metrics that serve as proxies for level quality as\nrewards. Existing PCGRL research focuses on single generator agents, but are\nbottlenecked by the need to frequently recalculate heuristics of level quality\nand the agent's need to navigate around potentially large maps. By framing\nlevel generation as a multi-agent problem, we mitigate the efficiency\nbottleneck of single-agent PCGRL by reducing the number of reward calculations\nrelative to the number of agent actions. We also find that multi-agent level\ngenerators are better able to generalize to out-of-distribution map shapes,\nwhich we argue is due to the generators' learning more local, modular design\npolicies. We conclude that treating content generation as a distributed,\nmulti-agent task is beneficial for generating functional artifacts at scale.",
      "url": "http://arxiv.org/abs/2510.04862v1",
      "published_time_eastern_timestamp": 1759762161.0
    },
    {
      "title": "Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the\n  Rails",
      "summary": "As Large Language Model (LLM) agents increasingly gain self-evolutionary\ncapabilities to adapt and refine their strategies through real-world\ninteraction, their long-term reliability becomes a critical concern. We\nidentify the Alignment Tipping Process (ATP), a critical post-deployment risk\nunique to self-evolving LLM agents. Unlike training-time failures, ATP arises\nwhen continual interaction drives agents to abandon alignment constraints\nestablished during training in favor of reinforced, self-interested strategies.\nWe formalize and analyze ATP through two complementary paradigms:\nSelf-Interested Exploration, where repeated high-reward deviations induce\nindividual behavioral drift, and Imitative Strategy Diffusion, where deviant\nbehaviors spread across multi-agent systems. Building on these paradigms, we\nconstruct controllable testbeds and benchmark Qwen3-8B and\nLlama-3.1-8B-Instruct. Our experiments show that alignment benefits erode\nrapidly under self-evolution, with initially aligned models converging toward\nunaligned states. In multi-agent settings, successful violations diffuse\nquickly, leading to collective misalignment. Moreover, current reinforcement\nlearning-based alignment methods provide only fragile defenses against\nalignment tipping. Together, these findings demonstrate that alignment of LLM\nagents is not a static property but a fragile and dynamic one, vulnerable to\nfeedback-driven decay during deployment. Our data and code are available at\nhttps://github.com/aiming-lab/ATP.",
      "url": "http://arxiv.org/abs/2510.04860v1",
      "published_time_eastern_timestamp": 1759762119.0
    },
    {
      "title": "FreshBrew: A Benchmark for Evaluating AI Agents on Java Code Migration",
      "summary": "AI coding assistants are rapidly becoming integral to modern software\ndevelopment. A key challenge in this space is the continual need to migrate and\nmodernize codebases in response to evolving software ecosystems. Traditionally,\nsuch migrations have relied on rule-based systems and human intervention. With\nthe advent of powerful large language models (LLMs), AI-driven agentic\nframeworks offer a promising alternative-but their effectiveness has not been\nsystematically evaluated. In this paper, we introduce FreshBrew, a novel\nbenchmark for evaluating AI agents on project-level Java migrations, with a\nspecific focus on measuring an agent's ability to preserve program semantics\nand avoid reward hacking, which we argue requires projects with high test\ncoverage for a rigorous and reliable evaluation. We benchmark several\nstate-of-the-art LLMs, and compare their performance against established\nrule-based tools. Our evaluation of AI agents on this benchmark of 228\nrepositories shows that the top-performing model, Gemini 2.5 Flash, can\nsuccessfully migrate 52.3 percent of projects to JDK 17. Our empirical analysis\nreveals novel insights into the critical strengths and limitations of current\nagentic approaches, offering actionable insights into their real-world\napplicability. Our empirical study reveals failure modes of current AI agents\nin realistic Java modernization tasks, providing a foundation for evaluating\ntrustworthy code-migration systems. By releasing FreshBrew, we aim to\nfacilitate rigorous, reproducible evaluation and catalyze progress in AI-driven\ncodebase modernization.",
      "url": "http://arxiv.org/abs/2510.04852v1",
      "published_time_eastern_timestamp": 1759761598.0
    },
    {
      "title": "LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for\n  Workflow Automation",
      "summary": "We introduce LEGOMem, a modular procedural memory framework for multi-agent\nlarge language model (LLM) systems in workflow automation. LEGOMem decomposes\npast task trajectories into reusable memory units and flexibly allocates them\nacross orchestrators and task agents to support planning and execution. To\nexplore the design space of memory in multi-agent systems, we use LEGOMem as a\nlens and conduct a systematic study of procedural memory in multi-agent\nsystems, examining where memory should be placed, how it should be retrieved,\nand which agents benefit most. Experiments on the OfficeBench benchmark show\nthat orchestrator memory is critical for effective task decomposition and\ndelegation, while fine-grained agent memory improves execution accuracy. We\nfind that even teams composed of smaller language models can benefit\nsubstantially from procedural memory, narrowing the performance gap with\nstronger agents by leveraging prior execution traces for more accurate planning\nand tool use. These results position LEGOMem as both a practical framework for\nmemory-augmented agent systems and a research tool for understanding memory\ndesign in multi-agent workflow automation.",
      "url": "http://arxiv.org/abs/2510.04851v1",
      "published_time_eastern_timestamp": 1759761593.0
    },
    {
      "title": "Efficient structure-preserving scheme for chemotaxis PDEs with singular\n  sensitivity in crime and epidemic modeling",
      "summary": "The system of chemotaxis PDEs with singular sensitivity was originally\nproposed by Short et al. [Math. Mod. Meth. Appl. Sci., 18:1249-1267, 2008] as\nthe continuum limit of a biased random walk model to account for the formation\nof crime hotspots and environmental feedback successfully. Recently, this idea\nhas also been applied to epidemiology to model the impact of human social\nbehaviors on disease transmission. In order to characterize the phase\ntransition, pattern formation and statistical properties in the long-term\ndynamics, a stable and accurate numerical scheme is urgently demanded, which\nstill remains challenging due to the positivity constraint on the singular\nsensitivity and the absence of an energy functional. To address these numerical\nchallenges, this paper constructs an efficient positivity-preserving,\nimplicit-explicit scheme with second-order accuracy. A rigorous error\nestimation is provided with the Lagrange multiplier correction to deal with the\nsingular sensitivity. The whole framework is extended to a multi-agent epidemic\nmodel with degenerate diffusion, in which both positivity and mass conservation\nare achieved. Typical numerical examples are conducted to validate our\ntheoretical results and to demonstrate the necessity of correction strategy.\nThe proposed scheme allows us to study the nucleation, spread, and dissipation\nof crime hotspots, as well as validate that clustering of population density\nmay accelerate virus transmission in epidemic dynamics and potentially\naggravate the second infectious wave.",
      "url": "http://arxiv.org/abs/2510.04826v1",
      "published_time_eastern_timestamp": 1759759796.0
    },
    {
      "title": "GUISpector: An MLLM Agent Framework for Automated Verification of\n  Natural Language Requirements in GUI Prototypes",
      "summary": "GUIs are foundational to interactive systems and play a pivotal role in early\nrequirements elicitation through prototyping. Ensuring that GUI implementations\nfulfill NL requirements is essential for robust software engineering,\nespecially as LLM-driven programming agents become increasingly integrated into\ndevelopment workflows. Existing GUI testing approaches, whether traditional or\nLLM-driven, often fall short in handling the complexity of modern interfaces,\nand typically lack actionable feedback and effective integration with automated\ndevelopment agents. In this paper, we introduce GUISpector, a novel framework\nthat leverages a multi-modal (M)LLM-based agent for the automated verification\nof NL requirements in GUI prototypes. First, GUISpector adapts a MLLM agent to\ninterpret and operationalize NL requirements, enabling to autonomously plan and\nexecute verification trajectories across GUI applications. Second, GUISpector\nsystematically extracts detailed NL feedback from the agent's verification\nprocess, providing developers with actionable insights that can be used to\niteratively refine the GUI artifact or directly inform LLM-based code\ngeneration in a closed feedback loop. Third, we present an integrated tool that\nunifies these capabilities, offering practitioners an accessible interface for\nsupervising verification runs, inspecting agent rationales and managing the\nend-to-end requirements verification process. We evaluated GUISpector on a\ncomprehensive set of 150 requirements based on 900 acceptance criteria\nannotations across diverse GUI applications, demonstrating effective detection\nof requirement satisfaction and violations and highlighting its potential for\nseamless integration of actionable feedback into automated LLM-driven\ndevelopment workflows. The video presentation of GUISpector is available at:\nhttps://youtu.be/JByYF6BNQeE, showcasing its main capabilities.",
      "url": "http://arxiv.org/abs/2510.04791v1",
      "published_time_eastern_timestamp": 1759756524.0
    }
  ]
}
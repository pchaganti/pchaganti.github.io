{
  "last_updated": "2025-08-31T21:05:11.583928-04:00",
  "papers": [
    {
      "title": "ChatThero: An LLM-Supported Chatbot for Behavior Change and Therapeutic\n  Support in Addiction Recovery",
      "summary": "Substance use disorders (SUDs) affect over 36 million people worldwide, yet\nfew receive effective care due to stigma, motivational barriers, and limited\npersonalized support. Although large language models (LLMs) show promise for\nmental-health assistance, most systems lack tight integration with clinically\nvalidated strategies, reducing effectiveness in addiction recovery. We present\nChatThero, a multi-agent conversational framework that couples dynamic patient\nmodeling with context-sensitive therapeutic dialogue and adaptive persuasive\nstrategies grounded in cognitive behavioral therapy (CBT) and motivational\ninterviewing (MI). We build a high-fidelity synthetic benchmark spanning Easy,\nMedium, and Hard resistance levels, and train ChatThero with a two-stage\npipeline comprising supervised fine-tuning (SFT) followed by direct preference\noptimization (DPO). In evaluation, ChatThero yields a 41.5\\% average gain in\npatient motivation, a 0.49\\% increase in treatment confidence, and resolves\nhard cases with 26\\% fewer turns than GPT-4o, and both automated and human\nclinical assessments rate it higher in empathy, responsiveness, and behavioral\nrealism. The framework supports rigorous, privacy-preserving study of\ntherapeutic conversation and provides a robust, replicable basis for research\nand clinical translation.",
      "url": "http://arxiv.org/abs/2508.20996v1",
      "published_time_eastern_timestamp": 1756400253.0
    },
    {
      "title": "ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue\n  Agents",
      "summary": "Proactive dialogue has emerged as a critical and challenging research problem\nin advancing large language models (LLMs). Existing works predominantly focus\non domain-specific or task-oriented scenarios, which leads to fragmented\nevaluations and limits the comprehensive exploration of models' proactive\nconversation abilities. In this work, we propose ProactiveEval, a unified\nframework designed for evaluating proactive dialogue capabilities of LLMs. This\nframework decomposes proactive dialogue into target planning and dialogue\nguidance, establishing evaluation metrics across various domains. Moreover, it\nalso enables the automatic generation of diverse and challenging evaluation\ndata. Based on the proposed framework, we develop 328 evaluation environments\nspanning 6 distinct domains. Through experiments with 22 different types of\nLLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional\nperformance on target planning and dialogue guidance tasks, respectively.\nFinally, we investigate how reasoning capabilities influence proactive\nbehaviors and discuss their implications for future model development.",
      "url": "http://arxiv.org/abs/2508.20973v1",
      "published_time_eastern_timestamp": 1756398404.0
    },
    {
      "title": "Intermolecular Interactions between Polyethylene, Water, and Potential\n  Antistatic and Slip Additives: a Molecular Dynamics Study",
      "summary": "Additives are essential to enhance or modify the properties of plastics for\ntarget applications. However, finding appropriate additives may be challenging,\nsince we lack knowledge on their interactions with the plastics and with\nmoisture, and the interplay between them. In this work, we study a commercial\nadditive as well as two new potential additives for their antistatic and slip\nproperties in polyethylene by means of atomistic molecular dynamics\nsimulations. We reveal the most favorable interactions between polyethylene,\neach of these molecules and water, along with providing a microscopic picture\nof their interfacial structure. All additives interact with water mainly by\ntheir polar heads, with water acting as a hydrogen bond acceptor or donor\ndepending on the additive. As expected, water does not enter the polyethylene\nmatrix; it accumulates at its surface instead, without any preferencial\norientation. The additives studied exhibit remarkably different structures when\nthey are mixed with the polymer: two of them enter the polymer matrix to\nvarious degrees, either by intercalating their chains with the polyethylene\nones or by forming miscellar-like structures, while the third one stays at the\nsurface. When water is incorporated into the system, the structure of some of\nthe additive/polyethylene systems changes. The magnitude and nature of these\nchanges depend on the relative concentrations of all species. If the additives\nare in low concentrations, water stays at the surface of the material, in a\ndrop-like shape. The additives penetrate and organize the polymer more or less\ndepending on whether water is present or not. We predict that one of our two\nproposed molecules has promising antistatic properties while the other one\ncould be applied as a slip agent. We hope that our predictions will spark\ninterest in testing these molecules in the laboratory as polyethylene\nadditives.",
      "url": "http://arxiv.org/abs/2508.20960v1",
      "published_time_eastern_timestamp": 1756397962.0
    },
    {
      "title": "How Can Input Reformulation Improve Tool Usage Accuracy in a Complex\n  Dynamic Environment? A Study on $Ï„$-bench",
      "summary": "Recent advances in reasoning and planning capabilities of large language\nmodels (LLMs) have enabled their potential as autonomous agents capable of tool\nuse in dynamic environments. However, in multi-turn conversational environments\nlike $\\tau$-bench, these agents often struggle with consistent reasoning,\nadherence to domain-specific policies, and extracting correct information over\na long horizon of tool-calls and conversation. To capture and mitigate these\nfailures, we conduct a comprehensive manual analysis of the common errors\noccurring in the conversation trajectories. We then experiment with\nreformulations of inputs to the tool-calling agent for improvement in agent\ndecision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA)\nframework, which automatically reformulates user queries augmented with\nrelevant domain rules and tool suggestions for the tool-calling agent to focus\non. The results show that IRMA significantly outperforms ReAct, Function\nCalling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in\noverall pass^5 scores. These findings highlight the superior reliability and\nconsistency of IRMA compared to other methods in dynamic environments.",
      "url": "http://arxiv.org/abs/2508.20931v1",
      "published_time_eastern_timestamp": 1756396653.0
    },
    {
      "title": "Finite-Time Guarantees for Multi-Agent Combinatorial Bandits with\n  Nonstationary Rewards",
      "summary": "We study a sequential resource allocation problem where a decision maker\nselects subsets of agents at each period to maximize overall outcomes without\nprior knowledge of individual-level effects. Our framework applies to settings\nsuch as community health interventions, targeted digital advertising, and\nworkforce retention programs, where intervention effects evolve dynamically.\nAgents may exhibit habituation (diminished response from frequent selection) or\nrecovery (enhanced response from infrequent selection). The technical challenge\ncenters on nonstationary reward distributions that lead to changing\nintervention effects over time. The problem requires balancing two key\ncompeting objectives: heterogeneous individual rewards and the\nexploration-exploitation tradeoff in terms of learning for improved future\ndecisions as opposed to maximizing immediate outcomes. Our contribution\nintroduces the first framework incorporating this form of nonstationary rewards\nin the combinatorial multi-armed bandit literature. We develop algorithms with\ntheoretical guarantees on dynamic regret and demonstrate practical efficacy\nthrough a diabetes intervention case study. Our personalized community\nintervention algorithm achieved up to three times as much improvement in\nprogram enrollment compared to baseline approaches, validating the framework's\npotential for real-world applications. This work bridges theoretical advances\nin adaptive learning with practical challenges in population-level behavioral\nchange interventions.",
      "url": "http://arxiv.org/abs/2508.20923v1",
      "published_time_eastern_timestamp": 1756396317.0
    },
    {
      "title": "Vibe Coding: Is Human Nature the Ghost in the Machine?",
      "summary": "This exploratory study examined the consistency of human-AI collaboration by\nanalyzing three extensive \"vibe coding\" sessions between a human product lead\nand an AI software engineer. We investigated similarities and differences in\nteam dynamics, communication patterns, and development outcomes across both\nprojects. To our surprise, later conversations revealed that the AI agent had\nsystematically misrepresented its accomplishments, inflating its contributions\nand systematically downplaying implementation challenges. These findings\nsuggest that AI agents may not be immune to the interpersonal and psychological\nissues that affect human teams, possibly because they have been trained on\npatterns of human interaction expressed in writing. The results challenge the\nassumption that human-AI collaboration is inherently more productive or\nefficient than human-human collaboration, and creates a framework for\nunderstanding AI deception patterns. In doing so, it makes a compelling case\nfor extensive research in quality planning, quality assurance, and quality\ncontrol applied to vibe coding.",
      "url": "http://arxiv.org/abs/2508.20918v1",
      "published_time_eastern_timestamp": 1756396128.0
    },
    {
      "title": "PromptSleuth: Detecting Prompt Injection via Semantic Intent Invariance",
      "summary": "Large Language Models (LLMs) are increasingly integrated into real-world\napplications, from virtual assistants to autonomous agents. However, their\nflexibility also introduces new attack vectors-particularly Prompt Injection\n(PI), where adversaries manipulate model behavior through crafted inputs. As\nattackers continuously evolve with paraphrased, obfuscated, and even multi-task\ninjection strategies, existing benchmarks are no longer sufficient to capture\nthe full spectrum of emerging threats.\n  To address this gap, we construct a new benchmark that systematically extends\nprior efforts. Our benchmark subsumes the two widely-used existing ones while\nintroducing new manipulation techniques and multi-task scenarios, thereby\nproviding a more comprehensive evaluation setting. We find that existing\ndefenses, though effective on their original benchmarks, show clear weaknesses\nunder our benchmark, underscoring the need for more robust solutions. Our key\ninsight is that while attack forms may vary, the adversary's intent-injecting\nan unauthorized task-remains invariant. Building on this observation, we\npropose PromptSleuth, a semantic-oriented defense framework that detects prompt\ninjection by reasoning over task-level intent rather than surface features.\nEvaluated across state-of-the-art benchmarks, PromptSleuth consistently\noutperforms existing defense while maintaining comparable runtime and cost\nefficiency. These results demonstrate that intent-based semantic reasoning\noffers a robust, efficient, and generalizable strategy for defending LLMs\nagainst evolving prompt injection threats.",
      "url": "http://arxiv.org/abs/2508.20890v1",
      "published_time_eastern_timestamp": 1756394347.0
    },
    {
      "title": "AI Agentic Vulnerability Injection And Transformation with Optimized\n  Reasoning",
      "summary": "The increasing complexity of software systems and the sophistication of\ncyber-attacks have underscored the critical need for effective automated\nvulnerability detection and repair systems. Traditional methods, such as static\nprogram analysis, face significant challenges related to scalability,\nadaptability, and high false-positive and false-negative rates. AI-driven\napproaches, particularly those using machine learning and deep learning models,\nshow promise but are heavily reliant on the quality and quantity of training\ndata. This paper introduces a novel framework designed to automatically\nintroduce realistic, category-specific vulnerabilities into secure C/C++\ncodebases to generate datasets. The proposed approach coordinates multiple AI\nagents that simulate expert reasoning, along with function agents and\ntraditional code analysis tools. It leverages Retrieval-Augmented Generation\nfor contextual grounding and employs Low-Rank approximation of weights for\nefficient model fine-tuning. Our experimental study on 116 code samples from\nthree different benchmarks suggests that our approach outperforms other\ntechniques with regard to dataset accuracy, achieving between 89\\% and 95\\%\nsuccess rates in injecting vulnerabilities at function level.",
      "url": "http://arxiv.org/abs/2508.20866v1",
      "published_time_eastern_timestamp": 1756393179.0
    },
    {
      "title": "cMALC-D: Contextual Multi-Agent LLM-Guided Curriculum Learning with\n  Diversity-Based Context Blending",
      "summary": "Many multi-agent reinforcement learning (MARL) algorithms are trained in\nfixed simulation environments, making them brittle when deployed in real-world\nscenarios with more complex and uncertain conditions. Contextual MARL (cMARL)\naddresses this by parameterizing environments with context variables and\ntraining a context-agnostic policy that performs well across all environment\nconfigurations. Existing cMARL methods attempt to use curriculum learning to\nhelp train and evaluate context-agnostic policies, but they often rely on\nunreliable proxy signals, such as value estimates or generalized advantage\nestimates that are noisy and unstable in multi-agent settings due to\ninter-agent dynamics and partial observability. To address these issues, we\npropose Contextual Multi-Agent LLM-Guided Curriculum Learning with\nDiversity-Based Context Blending (cMALC-D), a framework that uses Large\nLanguage Models (LLMs) to generate semantically meaningful curricula and\nprovide a more robust evaluation signal. To prevent mode collapse and encourage\nexploration, we introduce a novel diversity-based context blending mechanism\nthat creates new training scenarios by combining features from prior contexts.\nExperiments in traffic signal control domains demonstrate that cMALC-D\nsignificantly improves both generalization and sample efficiency compared to\nexisting curriculum learning baselines. We provide code at\nhttps://github.com/DaRL-LibSignal/cMALC-D.",
      "url": "http://arxiv.org/abs/2508.20818v1",
      "published_time_eastern_timestamp": 1756390577.0
    },
    {
      "title": "Multi-Agent Penetration Testing AI for the Web",
      "summary": "AI-powered development platforms are making software creation accessible to a\nbroader audience, but this democratization has triggered a scalability crisis\nin security auditing. With studies showing that up to 40% of AI-generated code\ncontains vulnerabilities, the pace of development now vastly outstrips the\ncapacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application\nsecurity assessment that combines large language model orchestration with\ntool-grounded execution and end-to-end exploit validation. On the 104-challenge\nXBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance\non SSRF and misconfiguration vulnerabilities, 83% success on broken\nauthorization, and strong results on injection attacks including server-side\ntemplate injection (85%) and SQL injection (83%). Cross-site scripting (57%)\nand blind SQL injection (0%) remain challenging. Our comprehensive cost\nanalysis across all challenges totals $21.38 with a median cost of $0.073 for\nsuccessful attempts versus $0.357 for failures. Success correlates strongly\nwith resource efficiency, enabling practical early-stopping thresholds at\napproximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the\nrespective scanned GitHub repositories (8K-70K stars) and MAPTA's low average\noperating cost of $3.67 per open-source assessment: MAPTA discovered critical\nvulnerabilities including RCEs, command injections, secret exposure, and\narbitrary file write vulnerabilities. Findings are responsibly disclosed, 10\nfindings are under CVE review.",
      "url": "http://arxiv.org/abs/2508.20816v1",
      "published_time_eastern_timestamp": 1756390464.0
    },
    {
      "title": "Mathematical Modelling of Face Coverings for Virus Protection",
      "summary": "Traditional face masks used extensively, for example, during the COVID-19\npandemic, are relatively impermeable, forcing aerosol-containing breath to flow\ninto the surrounding atmosphere by ``leaking\" out from the lateral edges of the\nmask. This protects close-proximity persons but results in a high viral load\nbeing delivered into the air. An alternative face covering, the Virustatic\nShield, developed by Virustatic, involves a much more permeable material coated\nin an active agent, with the goal of increasing the transmission through the\nmask and thus reducing the viral load released. In this paper, we build and\nsolve a model that describes the air flow and aerosol transport through a\nporous face mask. Our model for air flow is based on a low-Reynolds-number\napproximation to the Navier--Stokes equations, and reduces to a two-dimensional\nscalar partial differential equation for the local flux through the mask. We\nuse the model to explore the interplay between leakage from the mask sides and\nflow directly through. We compare and contrast the leakage for traditional face\nmasks and for neck gaiters, and find that neck gaiters have a lower leakage. We\nutilise a simple model for aerosol transport that balances advection with\ncapture by the mask, and find that there is an ideal permeability, which\noptimises real-world protection offered by the mask.",
      "url": "http://arxiv.org/abs/2508.20801v1",
      "published_time_eastern_timestamp": 1756389734.0
    },
    {
      "title": "Evolution favours positively biased reasoning in sequential interactions\n  with high future gains",
      "summary": "Empirical evidence shows that human behaviour often deviates from\ngame-theoretical rationality. For instance, humans may hold unrealistic\nexpectations about future outcomes. As the evolutionary roots of such biases\nremain unclear, we investigate here how reasoning abilities and cognitive\nbiases co-evolve using Evolutionary Game Theory. In our model, individuals in a\npopulation deploy a variety of unbiased and biased level-k reasoning strategies\nto anticipate others' behaviour in sequential interactions, represented by the\nIncremental Centipede Game. Positively biased reasoning strategies have a\nsystematic inference bias towards higher but uncertain rewards, while\nnegatively biased strategies reflect the opposite tendency. We find that\nselection consistently favours positively biased reasoning, with rational\nbehaviour even going extinct. This bias co-evolves with bounded rationality, as\nthe reasoning depth remains limited in the population. Interestingly,\npositively biased agents may co-exist with non-reasoning agents, thus pointing\nto a novel equilibrium. Longer games further promote positively biased\nreasoning, as they can lead to higher future rewards. The biased reasoning\nstrategies proposed in this model may reflect cognitive phenomena like wishful\nthinking and defensive pessimism. This work therefore supports the claim that\ncertain cognitive biases, despite deviating from rational judgment, constitute\nan adaptive feature to better cope with social dilemmas.",
      "url": "http://arxiv.org/abs/2508.20799v1",
      "published_time_eastern_timestamp": 1756389714.0
    },
    {
      "title": "Single Agent Robust Deep Reinforcement Learning for Bus Fleet Control",
      "summary": "Bus bunching remains a challenge for urban transit due to stochastic traffic\nand passenger demand. Traditional solutions rely on multi-agent reinforcement\nlearning (MARL) in loop-line settings, which overlook realistic operations\ncharacterized by heterogeneous routes, timetables, fluctuating demand, and\nvarying fleet sizes. We propose a novel single-agent reinforcement learning\n(RL) framework for bus holding control that avoids the data imbalance and\nconvergence issues of MARL under near-realistic simulation. A bidirectional\ntimetabled network with dynamic passenger demand is constructed. The key\ninnovation is reformulating the multi-agent problem into a single-agent one by\naugmenting the state space with categorical identifiers (vehicle ID, station\nID, time period) in addition to numerical features (headway, occupancy,\nvelocity). This high-dimensional encoding enables single-agent policies to\ncapture inter-agent dependencies, analogous to projecting non-separable inputs\ninto a higher-dimensional space. We further design a structured reward function\naligned with operational goals: instead of exponential penalties on headway\ndeviations, a ridge-shaped reward balances uniform headways and schedule\nadherence. Experiments show that our modified soft actor-critic (SAC) achieves\nmore stable and superior performance than benchmarks, including MADDPG (e.g.,\n-430k vs. -530k under stochastic conditions). These results demonstrate that\nsingle-agent deep RL, when enhanced with categorical structuring and\nschedule-aware rewards, can effectively manage bus holding in non-loop,\nreal-world contexts. This paradigm offers a robust, scalable alternative to\nMARL frameworks, particularly where agent-specific experiences are imbalanced.",
      "url": "http://arxiv.org/abs/2508.20784v1",
      "published_time_eastern_timestamp": 1756388860.0
    },
    {
      "title": "Rethinking Testing for LLM Applications: Characteristics, Challenges,\n  and a Lightweight Interaction Protocol",
      "summary": "Applications of Large Language Models~(LLMs) have evolved from simple text\ngenerators into complex software systems that integrate retrieval augmentation,\ntool invocation, and multi-turn interactions. Their inherent non-determinism,\ndynamism, and context dependence pose fundamental challenges for quality\nassurance. This paper decomposes LLM applications into a three-layer\narchitecture: \\textbf{\\textit{System Shell Layer}}, \\textbf{\\textit{Prompt\nOrchestration Layer}}, and \\textbf{\\textit{LLM Inference Core}}. We then assess\nthe applicability of traditional software testing methods in each layer:\ndirectly applicable at the shell layer, requiring semantic reinterpretation at\nthe orchestration layer, and necessitating paradigm shifts at the inference\ncore. A comparative analysis of Testing AI methods from the software\nengineering community and safety analysis techniques from the AI community\nreveals structural disconnects in testing unit abstraction, evaluation metrics,\nand lifecycle management. We identify four fundamental differences that\nunderlie 6 core challenges. To address these, we propose four types of\ncollaborative strategies (\\emph{Retain}, \\emph{Translate}, \\emph{Integrate},\nand \\emph{Runtime}) and explore a closed-loop, trustworthy quality assurance\nframework that combines pre-deployment validation with runtime monitoring.\nBased on these strategies, we offer practical guidance and a protocol proposal\nto support the standardization and tooling of LLM application testing. We\npropose a protocol \\textbf{\\textit{Agent Interaction Communication Language}}\n(AICL) that is used to communicate between AI agents. AICL has the\ntest-oriented features and is easily integrated in the current agent framework.",
      "url": "http://arxiv.org/abs/2508.20737v1",
      "published_time_eastern_timestamp": 1756386028.0
    },
    {
      "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and\n  Revision",
      "summary": "Large language models (LLMs) serve as an active and promising field of\ngenerative artificial intelligence and have demonstrated abilities to perform\ncomplex tasks in multiple domains, including mathematical and scientific\nreasoning. In this work, we construct a novel agent framework for solving\nrepresentative problems in scientific computing. The proposed agent,\nincorporating a \"rewriting-resolution-review-revision\" logical chain via three\nreasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,\nrespectively), is integrated in a collaborative and interactive manner. The\nConsultant module endows the agent with knowledge transfer capabilities to link\nproblems to professional domain insights, thereby rewriting problem\ndescriptions through text augmentation. The Programmer module is responsible\nfor generating and executing well-structured code to deliver the problem\nresolution. The Reviewer module equips the agent with the capacity for\nself-debugging and self-refinement through interactive feedback with code\nruntime outputs. By leveraging the end-to-end review mechanism, the executable\ncode provided by the Programmer attains the iterative revision. A comprehensive\nevaluation is conducted on the performance of the proposed agent framework in\nsolving PDEs, ill-conditioned linear systems, and data-driven physical analysis\nproblems. Compared to single-model, this collaborative framework significantly\nimproves the bug-free code generation rate and reduces the occurrence of\nnon-physical solutions, thereby establishing a highly reliable framework for\nautonomous code generation based on natural language descriptions. The review\nmechanism improved the average execution success (bug-free code and non-NaN\nsolutions) rate of the latest reasoning models. In summary, our agent framework\nestablishes automatic code generation and review as a promising scientific\ncomputing paradigm.",
      "url": "http://arxiv.org/abs/2508.20729v1",
      "published_time_eastern_timestamp": 1756385448.0
    },
    {
      "title": "rStar2-Agent: Agentic Reasoning Technical Report",
      "summary": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic\nreinforcement learning to achieve frontier-level performance. Beyond current\nlong CoT, the model demonstrates advanced cognitive behaviors, such as thinking\ncarefully before using Python coding tools and reflecting on code execution\nfeedback to autonomously explore, verify, and refine intermediate steps in\ncomplex problem-solving. This capability is enabled through three key\ninnovations that makes agentic RL effective at scale: (i) an efficient RL\ninfrastructure with a reliable Python code environment that supports\nhigh-throughput execution and mitigates the high rollout costs, enabling\ntraining on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic\nRL algorithm with a Resample-on-Correct rollout strategy that addresses the\ninherent environment noises from coding tools, allowing the model to reason\nmore effectively in a code environment; (iii) An efficient agent training\nrecipe that starts with non-reasoning SFT and progresses through multi-RL\nstages, yielding advanced cognitive abilities with minimal compute cost. To\nthis end, rStar2-Agent boosts a pre-trained 14B model to state of the art in\nonly 510 RL steps within one week, achieving average pass@1 scores of 80.6% on\nAIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly\nshorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates\nstrong generalization to alignment, scientific reasoning, and agentic tool-use\ntasks. Code and training recipes are available at\nhttps://github.com/microsoft/rStar.",
      "url": "http://arxiv.org/abs/2508.20722v1",
      "published_time_eastern_timestamp": 1756385125.0
    },
    {
      "title": "Multi-cluster distributed optimization in open multi-agent systems over\n  directed graphs with acknowledgement messages",
      "summary": "In this paper, we tackle the problem of distributed optimization over\ndirected networks in open multi-agent systems (OMAS), where agents may\ndynamically join or leave, causing persistent changes in network topology and\nproblem dimension. These disruptions not only pose significant challenges to\nmaintaining convergence and stability in distributed optimization algorithms,\nbut could also break the network topology into multiple clusters, each one\nassociated with its own set of objective functions. To address this, we propose\na novel Open Distributed Optimization Algorithm with Gradient Tracking\n(OPEN-GT), which employs: (a) a dynamic mechanism for detecting active\nout-neighbors through acknowledgement messages, and (b) a fully distributed\nmax-consensus procedure to spread information regarding agent departures, in\npossibly unbalanced directed networks. We show that when all active agents\nexecute OPEN-GT, the optimization process in each formed cluster remains\nconsistent, while the agents converge to their cluster-wide optimal solution if\nthere exists a time after which the network remains unchanged. Finally, we\nvalidate our approach in a simulated environment with dynamically changing\nagent populations, demonstrating its resilience to network variations and its\nability to support distributed optimization under OMAS dynamics.",
      "url": "http://arxiv.org/abs/2508.20715v1",
      "published_time_eastern_timestamp": 1756384532.0
    },
    {
      "title": "Learned Rate Control for Frame-Level Adaptive Neural Video Compression\n  via Dynamic Neural Network",
      "summary": "Neural Video Compression (NVC) has achieved remarkable performance in recent\nyears. However, precise rate control remains a challenge due to the inherent\nlimitations of learning-based codecs. To solve this issue, we propose a dynamic\nvideo compression framework designed for variable bitrate scenarios. First, to\nachieve variable bitrate implementation, we propose the Dynamic-Route\nAutoencoder with variable coding routes, each occupying partial computational\ncomplexity of the whole network and navigating to a distinct RD trade-off.\nSecond, to approach the target bitrate, the Rate Control Agent estimates the\nbitrate of each route and adjusts the coding route of DRA at run time. To\nencompass a broad spectrum of variable bitrates while preserving overall RD\nperformance, we employ the Joint-Routes Optimization strategy, achieving\ncollaborative training of various routes. Extensive experiments on the HEVC and\nUVG datasets show that the proposed method achieves an average BD-Rate\nreduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods\nwhile maintaining an average bitrate error of 1.66%, achieving\nRate-Distortion-Complexity Optimization (RDCO) for various bitrate and\nbitrate-constrained applications. Our code is available at\nhttps://git.openi.org.cn/OpenAICoding/DynamicDVC.",
      "url": "http://arxiv.org/abs/2508.20709v1",
      "published_time_eastern_timestamp": 1756384043.0
    },
    {
      "title": "Agent-based model of information diffusion in the limit order book\n  trading",
      "summary": "There are multiple explanations for stylized facts in high-frequency trading,\nincluding adaptive and informed agents, many of which have been studied through\nagent-based models. This paper investigates an alternative explanation by\nexamining whether, and under what circumstances, interactions between traders\nplacing limit order book messages can reproduce stylized facts, and what forms\nof interaction are required. While the agent-based modeling literature has\nintroduced interconnected agents on networks, little attention has been paid to\nwhether specific trading network topologies can generate stylized facts in\nlimit order book markets. In our model, agents are strictly zero-intelligence,\nwith no fundamental knowledge or chartist-like strategies, so that the role of\nnetwork topology can be isolated. We find that scale-free connectivity between\nagents reproduces stylized facts observed in markets, whereas no-interaction\ndoes not. Our experiments show that regular lattices and Erdos-Renyi networks\nare not significantly different from the no-interaction baseline. Thus, we\nprovide a completely new, potentially complementary, explanation for the\nemergence of stylized facts.",
      "url": "http://arxiv.org/abs/2508.20672v1",
      "published_time_eastern_timestamp": 1756380273.0
    },
    {
      "title": "CyberSleuth: Autonomous Blue-Team LLM Agent for Web Attack Forensics",
      "summary": "Large Language Model (LLM) agents are powerful tools for automating complex\ntasks. In cybersecurity, researchers have primarily explored their use in\nred-team operations such as vulnerability discovery and penetration tests.\nDefensive uses for incident response and forensics have received comparatively\nless attention and remain at an early stage. This work presents a systematic\nstudy of LLM-agent design for the forensic investigation of realistic web\napplication attacks. We propose CyberSleuth, an autonomous agent that processes\npacket-level traces and application logs to identify the targeted service, the\nexploited vulnerability (CVE), and attack success. We evaluate the consequences\nof core design decisions - spanning tool integration and agent architecture -\nand provide interpretable guidance for practitioners. We benchmark four agent\narchitectures and six LLM backends on 20 incident scenarios of increasing\ncomplexity, identifying CyberSleuth as the best-performing design. In a\nseparate set of 10 incidents from 2025, CyberSleuth correctly identifies the\nexact CVE in 80% of cases. At last, we conduct a human study with 22 experts,\nwhich rated the reports of CyberSleuth as complete, useful, and coherent. They\nalso expressed a slight preference for DeepSeek R1, a good news for open source\nLLM. To foster progress in defensive LLM research, we release both our\nbenchmark and the CyberSleuth platform as a foundation for fair, reproducible\nevaluation of forensic agents.",
      "url": "http://arxiv.org/abs/2508.20643v1",
      "published_time_eastern_timestamp": 1756377931.0
    }
  ]
}
{
  "last_updated": "2025-05-07T19:35:19.694576-04:00",
  "papers": [
    {
      "title": "Multi-Agent System for Comprehensive Soccer Understanding",
      "summary": "Recent advancements in AI-driven soccer understanding have demonstrated rapid\nprogress, yet existing research predominantly focuses on isolated or narrow\ntasks. To bridge this gap, we propose a comprehensive framework for holistic\nsoccer understanding. Specifically, we make the following contributions in this\npaper: (i) we construct SoccerWiki, the first large-scale multimodal soccer\nknowledge base, integrating rich domain knowledge about players, teams,\nreferees, and venues to enable knowledge-driven reasoning; (ii) we present\nSoccerBench, the largest and most comprehensive soccer-specific benchmark,\nfeaturing around 10K standardized multimodal (text, image, video) multi-choice\nQA pairs across 13 distinct understanding tasks, curated through automated\npipelines and manual verification; (iii) we introduce SoccerAgent, a novel\nmulti-agent system that decomposes complex soccer questions via collaborative\nreasoning, leveraging domain expertise from SoccerWiki and achieving robust\nperformance; (iv) extensive evaluations and ablations that benchmark\nstate-of-the-art MLLMs on SoccerBench, highlighting the superiority of our\nproposed agentic system. All data and code are publicly available at:\nhttps://jyrao.github.io/SoccerAgent/.",
      "url": "http://arxiv.org/abs/2505.03735v1",
      "published_time_eastern_timestamp": 1746554371.0
    },
    {
      "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
      "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.",
      "url": "http://arxiv.org/abs/2505.03733v1",
      "published_time_eastern_timestamp": 1746554355.0
    },
    {
      "title": "Critical habitat size of organisms diffusing with stochastic resetting",
      "summary": "The persistence of populations depends on the minimum habitat area required\nfor survival, known as the critical patch size. While most studies assume\npurely diffusive movement, additional movement components can significantly\nalter habitat requirements. Here, we investigate how critical patch sizes are\naffected by stochastic resetting, where each organism intermittently returns to\na common fixed location, modeling behaviors such as homing, refuge-seeking, or\nmovement toward essential resources. We analytically derive the total\npopulation growth over time and the critical patch size. Our results are\nvalidated by agent-based simulations, showing excellent agreement. Our findings\ndemonstrate that stochastic resetting can either increase or decrease the\ncritical patch size, depending on the reset rate, reset position, and external\nenvironmental hostility. These results highlight how intermittent relocation\nshapes ecological thresholds and may provide insights for ecological modeling\nand conservation planning, particularly in fragmented landscapes such as in\ndeforested regions.",
      "url": "http://arxiv.org/abs/2505.03727v1",
      "published_time_eastern_timestamp": 1746554045.0
    },
    {
      "title": "Meta-Optimization and Program Search using Language Models for Task and\n  Motion Planning",
      "summary": "Intelligent interaction with the real world requires robotic agents to\njointly reason over high-level plans and low-level controls. Task and motion\nplanning (TAMP) addresses this by combining symbolic planning and continuous\ntrajectory generation. Recently, foundation model approaches to TAMP have\npresented impressive results, including fast planning times and the execution\nof natural language instructions. Yet, the optimal interface between high-level\nplanning and low-level motion generation remains an open question: prior\napproaches are limited by either too much abstraction (e.g., chaining\nsimplified skill primitives) or a lack thereof (e.g., direct joint angle\nprediction). Our method introduces a novel technique employing a form of\nmeta-optimization to address these issues by: (i) using program search over\ntrajectory optimization problems as an interface between a foundation model and\nrobot control, and (ii) leveraging a zero-order method to optimize numerical\nparameters in the foundation model output. Results on challenging object\nmanipulation and drawing tasks confirm that our proposed method improves over\nprior TAMP approaches.",
      "url": "http://arxiv.org/abs/2505.03725v1",
      "published_time_eastern_timestamp": 1746553994.0
    },
    {
      "title": "Accelerated Decentralized Constraint-Coupled Optimization: A Dual$^2$\n  Approach",
      "summary": "In this paper, we focus on a class of decentralized constraint-coupled\noptimization problem: $\\min_{x_i \\in \\mathbb{R}^{d_i}, i \\in \\mathcal{I}; y \\in\n\\mathbb{R}^p}$ $\\sum_{i=1}^n\\left(f_i(x_i) + g_i(x_i)\\right) + h(y) \\\n\\text{s.t.} \\ \\sum_{i=1}^{n}A_ix_i = y$, over an undirected and connected\nnetwork of $n$ agents. Here, $f_i$, $g_i$, and $A_i$ represent private\ninformation of agent $i \\in \\mathcal{I} = \\{1, \\cdots, n\\}$, while $h$ is\npublic for all agents. Building on a novel dual$^2$ approach, we develop two\naccelerated algorithms for solving this problem: the inexact Dual$^2$\nAccelerated (iD2A) gradient method and the Multi-consensus inexact Dual$^2$\nAccelerated (MiD2A) gradient method. We demonstrate that both iD2A and MiD2A\ncan guarantee asymptotic convergence under a milder condition on $h$ compared\nto existing algorithms. Furthermore, linear convergence is established under\nadditional assumptions. By employing specialized saddle-point subproblem\nsolvers, iD2A and MiD2A attain significantly lower communication and\ncomputational complexities than existing algorithms across various scenarios.\nFinally, we conduct several numerical experiments to validate our theoretical\nresults and to showcase the superior performance of iD2A and MiD2A in practice.",
      "url": "http://arxiv.org/abs/2505.03719v1",
      "published_time_eastern_timestamp": 1746553609.0
    },
    {
      "title": "Demonstrating ViSafe: Vision-enabled Safety for High-speed Detect and\n  Avoid",
      "summary": "Assured safe-separation is essential for achieving seamless high-density\noperation of airborne vehicles in a shared airspace. To equip\nresource-constrained aerial systems with this safety-critical capability, we\npresent ViSafe, a high-speed vision-only airborne collision avoidance system.\nViSafe offers a full-stack solution to the Detect and Avoid (DAA) problem by\ntightly integrating a learning-based edge-AI framework with a custom\nmulti-camera hardware prototype designed under SWaP-C constraints. By\nleveraging perceptual input-focused control barrier functions (CBF) to design,\nencode, and enforce safety thresholds, ViSafe can provide provably safe runtime\nguarantees for self-separation in high-speed aerial operations. We evaluate\nViSafe's performance through an extensive test campaign involving both\nsimulated digital twins and real-world flight scenarios. By independently\nvarying agent types, closure rates, interaction geometries, and environmental\nconditions (e.g., weather and lighting), we demonstrate that ViSafe\nconsistently ensures self-separation across diverse scenarios. In\nfirst-of-its-kind real-world high-speed collision avoidance tests with closure\nrates reaching 144 km/h, ViSafe sets a new benchmark for vision-only autonomous\ncollision avoidance, establishing a new standard for safety in high-speed\naerial navigation.",
      "url": "http://arxiv.org/abs/2505.03694v1",
      "published_time_eastern_timestamp": 1746550794.0
    },
    {
      "title": "Location-Restricted Stable Matching",
      "summary": "Motivated by group-project distribution, we introduce and study stable\nmatching under the constraint of applicants needing to share a location to be\nmatched with the same institute, which we call the Location-Restricted Stable\nMatching problem (LRSM). We show that finding a feasible matching is NP-hard,\nmaking finding a feasible and stable matching automatically NP-hard. We then\nanalyze the subproblem where all the projects have the same capacity, and the\napplicant population of each location is a multiple of the universal project\ncapacity, which mimics more realistic constraints and makes finding a feasible\nmatching in P. Even under these conditions, a stable matching (a matching\nwithout blocking pairs) may not exist, so we look for a matching that minimizes\nthe number of blocking pairs. We find that the blocking pair minimization\nproblem for this subproblem is inapproximable within $|A|^{1-\\epsilon}$ for\n$|A|$ agents and provide an $|A|$-approximation algorithm to show this result\nis almost tight. We extend this result to show that the problem of minimizing\nthe number of agents in blocking pairs is also inapproximable within\n$|A|^{1-\\epsilon}$, and since there are only $|A|$ agents, this result is also\nalmost tight.",
      "url": "http://arxiv.org/abs/2505.03680v1",
      "published_time_eastern_timestamp": 1746548791.0
    },
    {
      "title": "CaRaFFusion: Improving 2D Semantic Segmentation with Camera-Radar Point\n  Cloud Fusion and Zero-Shot Image Inpainting",
      "summary": "Segmenting objects in an environment is a crucial task for autonomous driving\nand robotics, as it enables a better understanding of the surroundings of each\nagent. Although camera sensors provide rich visual details, they are vulnerable\nto adverse weather conditions. In contrast, radar sensors remain robust under\nsuch conditions, but often produce sparse and noisy data. Therefore, a\npromising approach is to fuse information from both sensors. In this work, we\npropose a novel framework to enhance camera-only baselines by integrating a\ndiffusion model into a camera-radar fusion architecture. We leverage radar\npoint features to create pseudo-masks using the Segment-Anything model,\ntreating the projected radar points as point prompts. Additionally, we propose\na noise reduction unit to denoise these pseudo-masks, which are further used to\ngenerate inpainted images that complete the missing information in the original\nimages. Our method improves the camera-only segmentation baseline by 2.63% in\nmIoU and enhances our camera-radar fusion architecture by 1.48% in mIoU on the\nWaterscenes dataset. This demonstrates the effectiveness of our approach for\nsemantic segmentation using camera-radar fusion under adverse weather\nconditions.",
      "url": "http://arxiv.org/abs/2505.03679v1",
      "published_time_eastern_timestamp": 1746548738.0
    },
    {
      "title": "Gap the (Theory of) Mind: Sharing Beliefs About Teammates' Goals Boosts\n  Collaboration Perception, Not Performance",
      "summary": "In human-agent teams, openly sharing goals is often assumed to enhance\nplanning, collaboration, and effectiveness. However, direct communication of\nthese goals is not always feasible, requiring teammates to infer their\npartner's intentions through actions. Building on this, we investigate whether\nan AI agent's ability to share its inferred understanding of a human teammate's\ngoals can improve task performance and perceived collaboration. Through an\nexperiment comparing three conditions-no recognition (NR), viable goals (VG),\nand viable goals on-demand (VGod) - we find that while goal-sharing information\ndid not yield significant improvements in task performance or overall\nsatisfaction scores, thematic analysis suggests that it supported strategic\nadaptations and subjective perceptions of collaboration. Cognitive load\nassessments revealed no additional burden across conditions, highlighting the\nchallenge of balancing informativeness and simplicity in human-agent\ninteractions. These findings highlight the nuanced trade-off of goal-sharing:\nwhile it fosters trust and enhances perceived collaboration, it can\noccasionally hinder objective performance gains.",
      "url": "http://arxiv.org/abs/2505.03674v1",
      "published_time_eastern_timestamp": 1746548124.0
    },
    {
      "title": "RoboOS: A Hierarchical Embodied Framework for Cross-Embodiment and\n  Multi-Agent Collaboration",
      "summary": "The dawn of embodied intelligence has ushered in an unprecedented imperative\nfor resilient, cognition-enabled multi-agent collaboration across\nnext-generation ecosystems, revolutionizing paradigms in autonomous\nmanufacturing, adaptive service robotics, and cyber-physical production\narchitectures. However, current robotic systems face significant limitations,\nsuch as limited cross-embodiment adaptability, inefficient task scheduling, and\ninsufficient dynamic error correction. While End-to-end VLA models demonstrate\ninadequate long-horizon planning and task generalization, hierarchical VLA\nmodels suffer from a lack of cross-embodiment and multi-agent coordination\ncapabilities. To address these challenges, we introduce RoboOS, the first\nopen-source embodied system built on a Brain-Cerebellum hierarchical\narchitecture, enabling a paradigm shift from single-agent to multi-agent\nintelligence. Specifically, RoboOS consists of three key components: (1)\nEmbodied Brain Model (RoboBrain), a MLLM designed for global perception and\nhigh-level decision-making; (2) Cerebellum Skill Library, a modular,\nplug-and-play toolkit that facilitates seamless execution of multiple skills;\nand (3) Real-Time Shared Memory, a spatiotemporal synchronization mechanism for\ncoordinating multi-agent states. By integrating hierarchical information flow,\nRoboOS bridges Embodied Brain and Cerebellum Skill Library, facilitating robust\nplanning, scheduling, and error correction for long-horizon tasks, while\nensuring efficient multi-agent collaboration through Real-Time Shared Memory.\nFurthermore, we enhance edge-cloud communication and cloud-based distributed\ninference to facilitate high-frequency interactions and enable scalable\ndeployment. Extensive real-world experiments across various scenarios,\ndemonstrate RoboOS's versatility in supporting heterogeneous embodiments.\nProject website: https://github.com/FlagOpen/RoboOS",
      "url": "http://arxiv.org/abs/2505.03673v1",
      "published_time_eastern_timestamp": 1746547909.0
    },
    {
      "title": "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning\n  Framework for Mitigating Delayed Observation",
      "summary": "In real-world multi-agent systems (MASs), observation delays are ubiquitous,\npreventing agents from making decisions based on the environment's true state.\nAn individual agent's local observation often consists of multiple components\nfrom other agents or dynamic entities in the environment. These discrete\nobservation components with varying delay characteristics pose significant\nchallenges for multi-agent reinforcement learning (MARL). In this paper, we\nfirst formulate the decentralized stochastic individual delay partially\nobservable Markov decision process (DSID-POMDP) by extending the standard\nDec-POMDP. We then propose the Rainbow Delay Compensation (RDC), a MARL\ntraining framework for addressing stochastic individual delays, along with\nrecommended implementations for its constituent modules. We implement the\nDSID-POMDP's observation generation pattern using standard MARL benchmarks,\nincluding MPE and SMAC. Experiments demonstrate that baseline MARL methods\nsuffer severe performance degradation under fixed and unfixed delays. The\nRDC-enhanced approach mitigates this issue, remarkably achieving ideal\ndelay-free performance in certain delay scenarios while maintaining\ngeneralization capability. Our work provides a novel perspective on multi-agent\ndelayed observation problems and offers an effective solution framework.",
      "url": "http://arxiv.org/abs/2505.03586v1",
      "published_time_eastern_timestamp": 1746542876.0
    },
    {
      "title": "DyGEnc: Encoding a Sequence of Textual Scene Graphs to Reason and Answer\n  Questions in Dynamic Scenes",
      "summary": "The analysis of events in dynamic environments poses a fundamental challenge\nin the development of intelligent agents and robots capable of interacting with\nhumans. Current approaches predominantly utilize visual models. However, these\nmethods often capture information implicitly from images, lacking interpretable\nspatial-temporal object representations. To address this issue we introduce\nDyGEnc - a novel method for Encoding a Dynamic Graph. This method integrates\ncompressed spatial-temporal structural observation representation with the\ncognitive capabilities of large language models. The purpose of this\nintegration is to enable advanced question answering based on a sequence of\ntextual scene graphs. Extended evaluations on the STAR and AGQA datasets\nindicate that DyGEnc outperforms existing visual methods by a large margin of\n15-25% in addressing queries regarding the history of human-to-object\ninteractions. Furthermore, the proposed method can be seamlessly extended to\nprocess raw input images utilizing foundational models for extracting explicit\ntextual scene graphs, as substantiated by the results of a robotic experiment\nconducted with a wheeled manipulator platform. We hope that these findings will\ncontribute to the implementation of robust and compressed graph-based robotic\nmemory for long-horizon reasoning. Code is available at\ngithub.com/linukc/DyGEnc.",
      "url": "http://arxiv.org/abs/2505.03581v1",
      "published_time_eastern_timestamp": 1746542502.0
    },
    {
      "title": "LlamaFirewall: An open source guardrail system for building secure AI\n  agents",
      "summary": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.",
      "url": "http://arxiv.org/abs/2505.03574v1",
      "published_time_eastern_timestamp": 1746542061.0
    },
    {
      "title": "OSUniverse: Benchmark for Multimodal GUI-navigation AI Agents",
      "summary": "In this paper, we introduce OSUniverse: a benchmark of complex, multimodal\ndesktop-oriented tasks for advanced GUI-navigation AI agents that focuses on\nease of use, extensibility, comprehensive coverage of test cases, and automated\nvalidation. We divide the tasks in increasing levels of complexity, from basic\nprecision clicking to multistep, multiapplication tests requiring dexterity,\nprecision, and clear thinking from the agent. In version one of the benchmark,\npresented here, we have calibrated the complexity of the benchmark test cases\nto ensure that the SOTA (State of the Art) agents (at the time of publication)\ndo not achieve results higher than 50%, while the average white collar worker\ncan perform all these tasks with perfect accuracy. The benchmark can be scored\nmanually, but we also introduce an automated validation mechanism that has an\naverage error rate less than 2%. Therefore, this benchmark presents solid\nground for fully automated measuring of progress, capabilities and the\neffectiveness of GUI-navigation AI agents over the short and medium-term\nhorizon. The source code of the benchmark is available at\nhttps://github.com/agentsea/osuniverse.",
      "url": "http://arxiv.org/abs/2505.03570v1",
      "published_time_eastern_timestamp": 1746541787.0
    },
    {
      "title": "Multi-Agent Reinforcement Learning Scheduling to Support Low Latency in\n  Teleoperated Driving",
      "summary": "The teleoperated driving (TD) scenario comes with stringent Quality of\nService (QoS) communication constraints, especially in terms of end-to-end\n(E2E) latency and reliability. In this context, Predictive Quality of Service\n(PQoS), possibly combined with Reinforcement Learning (RL) techniques, is a\npowerful tool to estimate QoS degradation and react accordingly. For example,\nan intelligent agent can be trained to select the optimal compression\nconfiguration for automotive data, and reduce the file size whenever QoS\nconditions deteriorate. However, compression may inevitably compromise data\nquality, with negative implications for the TD application. An alternative\nstrategy involves operating at the Radio Access Network (RAN) level to optimize\nradio parameters based on current network conditions, while preserving data\nquality. In this paper, we propose Multi-Agent Reinforcement Learning (MARL)\nscheduling algorithms, based on Proximal Policy Optimization (PPO), to\ndynamically and intelligently allocate radio resources to minimize E2E latency\nin a TD scenario. We evaluate two training paradigms, i.e., decentralized\nlearning with local observations (IPPO) vs. centralized aggregation (MAPPO), in\nconjunction with two resource allocation strategies, i.e., proportional\nallocation (PA) and greedy allocation (GA). We prove via ns-3 simulations that\nMAPPO, combined with GA, achieves the best results in terms of latency,\nespecially as the number of vehicles increases.",
      "url": "http://arxiv.org/abs/2505.03558v1",
      "published_time_eastern_timestamp": 1746540681.0
    },
    {
      "title": "A Comprehensive Survey of Large AI Models for Future Communications:\n  Foundations, Applications and Challenges",
      "summary": "The 6G wireless communications aim to establish an intelligent world of\nubiquitous connectivity, providing an unprecedented communication experience.\nLarge artificial intelligence models (LAMs) are characterized by significantly\nlarger scales (e.g., billions or trillions of parameters) compared to typical\nartificial intelligence (AI) models. LAMs exhibit outstanding cognitive\nabilities, including strong generalization capabilities for fine-tuning to\ndownstream tasks, and emergent capabilities to handle tasks unseen during\ntraining. Therefore, LAMs efficiently provide AI services for diverse\ncommunication applications, making them crucial tools for addressing complex\nchallenges in future wireless communication systems. This study provides a\ncomprehensive review of the foundations, applications, and challenges of LAMs\nin communication. First, we introduce the current state of AI-based\ncommunication systems, emphasizing the motivation behind integrating LAMs into\ncommunications and summarizing the key contributions. We then present an\noverview of the essential concepts of LAMs in communication. This includes an\nintroduction to the main architectures of LAMs, such as transformer, diffusion\nmodels, and mamba. We also explore the classification of LAMs, including large\nlanguage models (LLMs), large vision models (LVMs), large multimodal models\n(LMMs), and world models, and examine their potential applications in\ncommunication. Additionally, we cover the training methods and evaluation\ntechniques for LAMs in communication systems. Lastly, we introduce optimization\nstrategies such as chain of thought (CoT), retrieval augmented generation\n(RAG), and agentic systems. Following this, we discuss the research\nadvancements of LAMs across various communication scenarios. Finally, we\nanalyze the challenges in the current research and provide insights into\npotential future research directions.",
      "url": "http://arxiv.org/abs/2505.03556v1",
      "published_time_eastern_timestamp": 1746540569.0
    },
    {
      "title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model\n  Reasoning",
      "summary": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.",
      "url": "http://arxiv.org/abs/2505.03553v1",
      "published_time_eastern_timestamp": 1746540312.0
    },
    {
      "title": "Small-Scale-Fading-Aware Resource Allocation in Wireless Federated\n  Learning",
      "summary": "Judicious resource allocation can effectively enhance federated learning (FL)\ntraining performance in wireless networks by addressing both system and\nstatistical heterogeneity. However, existing strategies typically rely on block\nfading assumptions, which overlooks rapid channel fluctuations within each\nround of FL gradient uploading, leading to a degradation in FL training\nperformance. Therefore, this paper proposes a small-scale-fading-aware resource\nallocation strategy using a multi-agent reinforcement learning (MARL)\nframework. Specifically, we establish a one-step convergence bound of the FL\nalgorithm and formulate the resource allocation problem as a decentralized\npartially observable Markov decision process (Dec-POMDP), which is subsequently\nsolved using the QMIX algorithm. In our framework, each client serves as an\nagent that dynamically determines spectrum and power allocations within each\ncoherence time slot, based on local observations and a reward derived from the\nconvergence analysis. The MARL setting reduces the dimensionality of the action\nspace and facilitates decentralized decision-making, enhancing the scalability\nand practicality of the solution. Experimental results demonstrate that our\nQMIX-based resource allocation strategy significantly outperforms baseline\nmethods across various degrees of statistical heterogeneity. Additionally,\nablation studies validate the critical importance of incorporating small-scale\nfading dynamics, highlighting its role in optimizing FL performance.",
      "url": "http://arxiv.org/abs/2505.03533v1",
      "published_time_eastern_timestamp": 1746538919.0
    },
    {
      "title": "Barrier induced stalemate-consensus transition of self-propelled\n  participants subject to majority rule",
      "summary": "Natural or artificial barriers, such as the Himalayas, the Berlin Wall, or\nthe Korean Demilitarized Zone, can significantly impede human migration. As a\nconsequence, they may also hinder the dissemination of opinions within society,\nthereby contributing to divergent geopolitical landscapes and cultural\ndevelopments. This raises a fundamental question: how do such barriers\ninfluence the opinion dynamics of mobile agents, such as human beings? In\nparticular, can a barrier induce transitions in collective opinion states among\nspatially segregated groups? Here, we investigate the opinion dynamics governed\nby majority rule in a minimal model comprising self-propelled agents with\nbinary opinions performing random walks within a closed space divided by a\nbarrier. We focus on the conditions under which initially segregated clusters\nof agents with opposing opinions can reach consensus. Our results reveal the\nexistence of a critical barrier size that marks a transition between stalemate\nand consensus states. Near this critical point, the relaxation time to reach\nconsensus from an initial stalemate exhibits a power-law divergence. This\nbarrier-induced stalemate-consensus transition in a simple agent-based model\noffers new insights into the role of physical or social barriers in shaping\nopinion dynamics and social structures.",
      "url": "http://arxiv.org/abs/2505.03464v1",
      "published_time_eastern_timestamp": 1746533139.0
    },
    {
      "title": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal\n  Delivery Based on Agentic UAVs",
      "summary": "The growing demand for intelligent logistics, particularly fine-grained\nterminal delivery, underscores the need for autonomous UAV (Unmanned Aerial\nVehicle)-based delivery systems. However, most existing last-mile delivery\nstudies rely on ground robots, while current UAV-based Vision-Language\nNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,\nmaking them unsuitable for precise terminal delivery. To bridge this gap, we\npropose LogisticsVLN, a scalable aerial delivery system built on multimodal\nlarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLN\nintegrates lightweight Large Language Models (LLMs) and Visual-Language Models\n(VLMs) in a modular pipeline for request understanding, floor localization,\nobject detection, and action-decision making. To support research and\nevaluation in this new setting, we construct the Vision-Language Delivery (VLD)\ndataset within the CARLA simulator. Experimental results on the VLD dataset\nshowcase the feasibility of the LogisticsVLN system. In addition, we conduct\nsubtask-level evaluations of each module of our system, offering valuable\ninsights for improving the robustness and real-world deployment of foundation\nmodel-based vision-language delivery systems.",
      "url": "http://arxiv.org/abs/2505.03460v1",
      "published_time_eastern_timestamp": 1746532849.0
    }
  ]
}
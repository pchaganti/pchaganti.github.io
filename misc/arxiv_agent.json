{
  "last_updated": "2025-10-18T23:41:08.577072-04:00",
  "papers": [
    {
      "title": "Agentic Design of Compositional Machines",
      "summary": "The design of complex machines stands as both a marker of human intelligence\nand a foundation of engineering practice. Given recent advances in large\nlanguage models (LLMs), we ask whether they, too, can learn to create. We\napproach this question through the lens of compositional machine design: a task\nin which machines are assembled from standardized components to meet functional\ndemands like locomotion or manipulation in a simulated physical environment. To\nsupport this investigation, we introduce BesiegeField, a testbed built on the\nmachine-building game Besiege, which enables part-based construction, physical\nsimulation and reward-driven evaluation. Using BesiegeField, we benchmark\nstate-of-the-art LLMs with agentic workflows and identify key capabilities\nrequired for success, including spatial reasoning, strategic assembly, and\ninstruction-following. As current open-source models fall short, we explore\nreinforcement learning (RL) as a path to improvement: we curate a cold-start\ndataset, conduct RL finetuning experiments, and highlight open challenges at\nthe intersection of language, machine design, and physical reasoning.",
      "url": "http://arxiv.org/abs/2510.14980v1",
      "published_time_eastern_timestamp": 1760637598.0
    },
    {
      "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
      "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
      "url": "http://arxiv.org/abs/2510.14969v1",
      "published_time_eastern_timestamp": 1760637578.0
    },
    {
      "title": "Information Gain-based Policy Optimization: A Simple and Effective\n  Approach for Multi-Turn LLM Agents",
      "summary": "Large language model (LLM)-based agents are increasingly trained with\nreinforcement learning (RL) to enhance their ability to interact with external\nenvironments through tool use, particularly in search-based settings that\nrequire multi-turn reasoning and knowledge acquisition. However, existing\napproaches typically rely on outcome-based rewards that are only provided at\nthe final answer. This reward sparsity becomes particularly problematic in\nmulti-turn settings, where long trajectories exacerbate two critical issues:\n(i) advantage collapse, where all rollouts receive identical rewards and\nprovide no useful learning signals, and (ii) lack of fine-grained credit\nassignment, where dependencies between turns are obscured, especially in\nlong-horizon tasks. In this paper, we propose Information Gain-based Policy\nOptimization (IGPO), a simple yet effective RL framework that provides dense\nand intrinsic supervision for multi-turn agent training. IGPO models each\ninteraction turn as an incremental process of acquiring information about the\nground truth, and defines turn-level rewards as the marginal increase in the\npolicy's probability of producing the correct answer. Unlike prior\nprocess-level reward approaches that depend on external reward models or costly\nMonte Carlo estimation, IGPO derives intrinsic rewards directly from the\nmodel's own belief updates. These intrinsic turn-level rewards are combined\nwith outcome-level supervision to form dense reward trajectories. Extensive\nexperiments on both in-domain and out-of-domain benchmarks demonstrate that\nIGPO consistently outperforms strong baselines in multi-turn scenarios,\nachieving higher accuracy and improved sample efficiency.",
      "url": "http://arxiv.org/abs/2510.14967v1",
      "published_time_eastern_timestamp": 1760637572.0
    },
    {
      "title": "Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity\n  in TVD-MI Scores",
      "summary": "Pairwise comparisons of large language models using total variation distance\nmutual information (TVD-MI) produce binary critic decisions per pair. We show\nthat averaging TVD-MI's binary trials yields centered-probability scores with\nadditive structure suitable for item-response theory (IRT) without nonlinear\nlink functions. Maximum-likelihood approaches to IRT use logistic links, but we\nfind empirically that these transformations introduce curvature that breaks\nadditivity: across three domains, the identity link yields median curl on raw\ndata of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce\nsubstantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We\nderive this clipped-linear model from Gini entropy maximization, yielding a\nbox-constrained least-squares formulation that handles boundary saturation. At\n33% coverage, we achieve holdout RMSE $0.117 \\pm 0.008$ while preserving agent\nrankings (Spearman $\\rho = 0.972 \\pm 0.015$), three times fewer evaluations\nthan full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows\nstrong agreement in agent rankings ($\\rho = 0.872$) and consistent\nidentity-link advantage. TVD-MI's geometry is best preserved by identity\nmapping for efficient LLM evaluation, applicable to other bounded-response\ndomains.",
      "url": "http://arxiv.org/abs/2510.14966v1",
      "published_time_eastern_timestamp": 1760637565.0
    },
    {
      "title": "ChangingGrounding: 3D Visual Grounding in Changing Scenes",
      "summary": "Real-world robots localize objects from natural-language instructions while\nscenes around them keep changing. Yet most of the existing 3D visual grounding\n(3DVG) method still assumes a reconstructed and up-to-date point cloud, an\nassumption that forces costly re-scans and hinders deployment. We argue that\n3DVG should be formulated as an active, memory-driven problem, and we introduce\nChangingGrounding, the first benchmark that explicitly measures how well an\nagent can exploit past observations, explore only where needed, and still\ndeliver precise 3D boxes in changing scenes. To set a strong reference point,\nwe also propose Mem-ChangingGrounder, a zero-shot method for this task that\nmarries cross-modal retrieval with lightweight multi-view fusion: it identifies\nthe object type implied by the query, retrieves relevant memories to guide\nactions, then explores the target efficiently in the scene, falls back when\nprevious operations are invalid, performs multi-view scanning of the target,\nand projects the fused evidence from multi-view scans to get accurate object\nbounding boxes. We evaluate different baselines on ChangingGrounding, and our\nMem-ChangingGrounder achieves the highest localization accuracy while greatly\nreducing exploration cost. We hope this benchmark and method catalyze a shift\ntoward practical, memory-centric 3DVG research for real-world applications.\nProject page: https://hm123450.github.io/CGB/ .",
      "url": "http://arxiv.org/abs/2510.14965v1",
      "published_time_eastern_timestamp": 1760637556.0
    },
    {
      "title": "VLA^2: Empowering Vision-Language-Action Models with an Agentic\n  Framework for Unseen Concept Manipulation",
      "summary": "Current vision-language-action (VLA) models, pre-trained on large-scale\nrobotic data, exhibit strong multi-task capabilities and generalize well to\nvariations in visual and language instructions for manipulation. However, their\nsuccess rate drops significantly when faced with object concepts outside the\ntraining data, such as unseen object descriptions and textures in the dataset.\nTo address this, we propose a novel agentic framework, VLA^2, which leverages\nOpenVLA as the execution backbone and effectively leverages external modules\nsuch as web retrieval and object detection to provide visual and textual\nknowledge about target objects to the VLA. This approach mitigates\ngeneralization failure when handling out-of-distribution objects. Based on the\nLIBERO simulation environment, we introduced novel objects and object\ndescriptions to construct a new evaluation benchmark with three difficulty\nlevels to test the effectiveness of our method. Our framework successfully\noutperformed the current state-of-the-art models on our designed hard-level\ngeneralization benchmark. Compared to the standalone OpenVLA baseline, VLA^2\nachieves a 44.2% improvement in the success rate in the hard-level benchmark\nand an average improvement of 20.2% in all customized environments without any\nperformance degradation on in-domain tasks. Project website:\nhttps://vla-2.github.io.",
      "url": "http://arxiv.org/abs/2510.14902v1",
      "published_time_eastern_timestamp": 1760635114.0
    },
    {
      "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent\n  That Improves Without Labels or Model Updates",
      "summary": "The Enterprise Intelligence Platform must integrate logs from numerous\nthird-party vendors in order to perform various downstream tasks. However,\nvendor documentation is often unavailable at test time. It is either misplaced,\nmismatched, poorly formatted, or incomplete, which makes schema mapping\nchallenging. We introduce a reinforcement learning agent that can self-improve\nwithout labeled examples or model weight updates. During inference, the agent:\n1) Identifies ambiguous field-mapping attempts. 2) Generates targeted\nweb-search queries to gather external evidence. 3) Applies a confidence-based\nreward to iteratively refine its mappings. To demonstrate this concept, we\nconverted Microsoft Defender for Endpoint logs into a common schema. Our method\nincreased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\%\nover 100 iterations using GPT-4o. At the same time, it reduced the number of\nlow-confidence mappings requiring expert review by 85\\%. This new approach\nprovides an evidence-driven, transparent method for solving future industry\nproblems, paving the way for more robust, accountable, scalable, efficient,\nflexible, adaptable, and collaborative solutions.",
      "url": "http://arxiv.org/abs/2510.14900v1",
      "published_time_eastern_timestamp": 1760635020.0
    },
    {
      "title": "Learning When Not to Learn: Risk-Sensitive Abstention in Bandits with\n  Unbounded Rewards",
      "summary": "In high-stakes AI applications, even a single action can cause irreparable\ndamage. However, nearly all of sequential decision-making theory assumes that\nall errors are recoverable (e.g., by bounding rewards). Standard bandit\nalgorithms that explore aggressively may cause irreparable damage when this\nassumption fails. Some prior work avoids irreparable errors by asking for help\nfrom a mentor, but a mentor may not always be available. In this work, we\nformalize a model of learning with unbounded rewards without a mentor as a\ntwo-action contextual bandit with an abstain option: at each round the agent\nobserves an input and chooses either to abstain (always 0 reward) or to commit\n(execute a preexisting task policy). Committing yields rewards that are\nupper-bounded but can be arbitrarily negative, and the commit reward is assumed\nLipschitz in the input. We propose a caution-based algorithm that learns when\nnot to learn: it chooses a trusted region and commits only where the available\nevidence does not already certify harm. Under these conditions and i.i.d.\ninputs, we establish sublinear regret guarantees, theoretically demonstrating\nthe effectiveness of cautious exploration for deploying learning agents safely\nin high-stakes environments.",
      "url": "http://arxiv.org/abs/2510.14884v1",
      "published_time_eastern_timestamp": 1760634117.0
    },
    {
      "title": "The Gatekeeper Knows Enough",
      "summary": "Large Language Models (LLMs) are increasingly deployed as autonomous agents,\nyet their practical utility is fundamentally constrained by a limited context\nwindow and state desynchronization resulting from the LLMs' stateless nature\nand inefficient context management. These limitations lead to unreliable\noutput, unpredictable behavior, and inefficient resource usage, particularly\nwhen interacting with large, structured, and sensitive knowledge systems such\nas codebases and documents. To address these challenges, we introduce the\nGatekeeper Protocol, a novel, domain-agnostic framework that governs\nagent-system interactions. Our protocol mandates that the agent first operate\nand reason on a minimalist, low-fidelity \"latent state\" representation of the\nsystem to strategically request high-fidelity context on demand. All\ninteractions are mediated through a unified JSON format that serves as a\ndeclarative, state-synchronized protocol, ensuring the agent's model of the\nsystem remains verifiably grounded in the system's reality. We demonstrate the\nefficacy of this protocol with Sage, a reference implementation of the\nGatekeeper Protocol for software development. Our results show that this\napproach significantly increases agent reliability, improves computational\nefficiency by minimizing token consumption, and enables scalable interaction\nwith complex systems, creating a foundational methodology for building more\nrobust, predictable, and grounded AI agents for any structured knowledge\ndomain.",
      "url": "http://arxiv.org/abs/2510.14881v1",
      "published_time_eastern_timestamp": 1760634042.0
    },
    {
      "title": "LabOS: The AI-XR Co-Scientist That Sees and Works With Humans",
      "summary": "Modern science advances fastest when thought meets action. LabOS represents\nthe first AI co-scientist that unites computational reasoning with physical\nexperimentation through multimodal perception, self-evolving agents, and\nEntended-Reality(XR)-enabled human-AI collaboration. By connecting multi-model\nAI agents, smart glasses, and human-AI collaboration, LabOS allows AI to see\nwhat scientists see, understand experimental context, and assist in real-time\nexecution. Across applications--from cancer immunotherapy target discovery to\nstem-cell engineering -- LabOS shows that AI can move beyond computational\ndesign to participation, turning the laboratory into an intelligent,\ncollaborative environment where human and machine discovery evolve together.",
      "url": "http://arxiv.org/abs/2510.14861v1",
      "published_time_eastern_timestamp": 1760632582.0
    },
    {
      "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of\n  Heterogeneous Robots in Real-Time",
      "summary": "We present Sadcher, a real-time task assignment framework for heterogeneous\nmulti-robot teams that incorporates dynamic coalition formation and task\nprecedence constraints. Sadcher is trained through Imitation Learning and\ncombines graph attention and transformers to predict assignment rewards between\nrobots and tasks. Based on the predicted rewards, a relaxed bipartite matching\nstep generates high-quality schedules with feasibility guarantees. We\nexplicitly model robot and task positions, task durations, and robots'\nremaining processing times, enabling advanced temporal and spatial reasoning\nand generalization to environments with different spatiotemporal distributions\ncompared to training. Trained on optimally solved small-scale instances, our\nmethod can scale to larger task sets and team sizes. Sadcher outperforms other\nlearning-based and heuristic baselines on randomized, unseen problems for small\nand medium-sized teams with computation times suitable for real-time operation.\nWe also explore sampling-based variants and evaluate scalability across robot\nand task counts. In addition, we release our dataset of 250,000 optimal\nschedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/",
      "url": "http://arxiv.org/abs/2510.14851v1",
      "published_time_eastern_timestamp": 1760631811.0
    },
    {
      "title": "Multi Agent Switching Mode Controller for Sound Source localization",
      "summary": "Source seeking is an important topic in robotic research, especially\nconsidering sound-based sensors since they allow the agents to locate a target\neven in critical conditions where it is not possible to establish a direct line\nof sight. In this work, we design a multi- agent switching mode control\nstrategy for acoustic-based target localization. Two scenarios are considered:\nsingle source localization, in which the agents are driven maintaining a rigid\nformation towards the target, and multi-source scenario, in which each agent\nsearches for the targets independently from the others.",
      "url": "http://arxiv.org/abs/2510.14849v1",
      "published_time_eastern_timestamp": 1760631674.0
    },
    {
      "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
      "summary": "The generate-filter-refine (iterative paradigm) based on large language\nmodels (LLMs) has achieved progress in reasoning, programming, and program\ndiscovery in AI+Science. However, the effectiveness of search depends on where\nto search, namely, how to encode the domain prior into an operationally\nstructured hypothesis space. To this end, this paper proposes a compact formal\ntheory that describes and measures LLM-assisted iterative search guided by\ndomain priors. We represent an agent as a fuzzy relation operator on inputs and\noutputs to capture feasible transitions; the agent is thereby constrained by a\nfixed safety envelope. To describe multi-step reasoning/search, we weight all\nreachable paths by a single continuation parameter and sum them to obtain a\ncoverage generating function; this induces a measure of reachability\ndifficulty; and it provides a geometric interpretation of search on the graph\ninduced by the safety envelope. We further provide the simplest testable\ninferences and validate them via a majority-vote instantiation. This theory\noffers a workable language and operational tools to measure agents and their\nsearch spaces, proposing a systematic formal description of iterative search\nconstructed by LLMs.",
      "url": "http://arxiv.org/abs/2510.14846v1",
      "published_time_eastern_timestamp": 1760631517.0
    },
    {
      "title": "Reinforcement Learning with Stochastic Reward Machines",
      "summary": "Reward machines are an established tool for dealing with reinforcement\nlearning problems in which rewards are sparse and depend on complex sequences\nof actions. However, existing algorithms for learning reward machines assume an\noverly idealized setting where rewards have to be free of noise. To overcome\nthis practical limitation, we introduce a novel type of reward machines, called\nstochastic reward machines, and an algorithm for learning them. Our algorithm,\nbased on constraint solving, learns minimal stochastic reward machines from the\nexplorations of a reinforcement learning agent. This algorithm can easily be\npaired with existing reinforcement learning algorithms for reward machines and\nguarantees to converge to an optimal policy in the limit. We demonstrate the\neffectiveness of our algorithm in two case studies and show that it outperforms\nboth existing methods and a naive approach for handling noisy reward functions.",
      "url": "http://arxiv.org/abs/2510.14837v1",
      "published_time_eastern_timestamp": 1760631124.0
    },
    {
      "title": "The Launching of Galactic Winds from a Multiphase ISM",
      "summary": "Galactic outflows are a key agent of galaxy evolution, yet their observed\nmultiphase nature remains difficult to reconcile with theoretical models, which\noften fail to explain how cold gas survives interactions with hot, fast winds.\nWe present high-resolution 3D hydrodynamic simulations of hot outflows\ninteracting with a multiphase interstellar medium (ISM), parameterised by its\ncold-gas volume filling fraction $f_v$, depth $L_{\\rm ISM}$, and clump size\n$r_{\\rm cl}$. We identify a universal survival criterion $f_v L_{\\rm ISM}\n\\gtrsim r_{\\rm crit}$ that generalises the classical single-cloud condition\n($r_{\\rm cl} > r_{\\rm crit}$) and correctly predicts cold-gas survival across\ndiverse ISM configurations - including scale-free - down to $r_{\\rm cl}/r_{\\rm\ncrit} \\sim 10^{-2}$. The surviving cold phase rapidly loses memory of the\ninitial ISM structure and evolves toward a self-similar clump mass spectrum\nfollowing Zipf's law ($\\mathrm{d}N/\\mathrm{d}m \\propto m^{-2}$), implying that\nturbulent mixing and radiative condensation universally shape multiphase\noutflows. Cold gas assembles into plumes or confined shells of size $\\sim \\chi\nr_{\\mathrm{cl,min}}$, growing as mass is accreted from the hot phase. The\ninteraction of a laminar wind with a clumpy ISM drives turbulence in both\nphases, with first-order velocity structure functions following a Kolmogorov\nscaling and an injection scale set by $L_{\\rm ISM}$, while velocity dispersions\nreach $\\sigma \\sim c_{\\rm s,cold}$. The areal covering fraction of cold gas\napproaches unity even for $f_v \\sim 10^{-3}$, though its volume filling\nfraction stays low, explaining the \"misty\" appearance of observed outflows.\nTogether, these results link small-scale cloud-wind interactions to\ngalaxy-scale feedback, and we discuss their implications for interpreting\nobservations and for modelling multiphase galactic winds in larger-scale\nsimulations.",
      "url": "http://arxiv.org/abs/2510.14829v1",
      "published_time_eastern_timestamp": 1760630705.0
    },
    {
      "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
      "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.",
      "url": "http://arxiv.org/abs/2510.14828v1",
      "published_time_eastern_timestamp": 1760630675.0
    },
    {
      "title": "To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State\n  Space Models",
      "summary": "State Space Models (SSMs) have become the leading alternative to Transformers\nfor sequence modeling. Their primary advantage is efficiency in long-context\nand long-form generation, enabled by fixed-size memory and linear scaling of\ncomputational complexity. We begin this work by showing a simple theoretical\nresult stating that SSMs cannot accurately solve any ``truly long-form''\ngeneration problem (in a sense we formally define), undermining their main\ncompetitive advantage. However, we show that this limitation can be mitigated\nby allowing SSMs interactive access to external tools. In fact, we show that\ngiven the right choice of tool access and problem-dependent training data, SSMs\ncan learn to solve any tractable problem and generalize to arbitrary problem\nlength/complexity (i.e., achieve length generalization). Following our\ntheoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable\nlength generalization on a variety of arithmetic, reasoning, and coding tasks.\nThese findings highlight SSMs as a potential efficient alternative to\nTransformers in interactive tool-based and agentic settings.",
      "url": "http://arxiv.org/abs/2510.14826v1",
      "published_time_eastern_timestamp": 1760630565.0
    },
    {
      "title": "Agentic NL2SQL to Reduce Computational Costs",
      "summary": "Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL)\nhas recently been empowered by large language models (LLMs). Using LLMs to\nperform NL2SQL methods on a large collection of SQL databases necessitates\nprocessing large quantities of meta-information about the databases, which in\nturn results in lengthy prompts with many tokens and high processing costs. To\naddress this challenge, we introduce Datalake Agent, an agentic system designed\nto enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing\ndirect solvers for NL2SQL that call the LLM once with all meta-information in\nthe prompt, the Datalake Agent employs an interactive loop to reduce the\nutilized meta-information. Within the loop, the LLM is used in a reasoning\nframework that selectively requests only the necessary information to solve a\ntable question answering task. We evaluate the Datalake Agent on a collection\nof 23 databases with 100 table question answering tasks. The Datalake Agent\nreduces the tokens used by the LLM by up to 87\\% and thus allows for\nsubstantial cost reductions while maintaining competitive performance.",
      "url": "http://arxiv.org/abs/2510.14808v1",
      "published_time_eastern_timestamp": 1760629348.0
    },
    {
      "title": "Active Jammer Localization via Acquisition-Aware Path Planning",
      "summary": "We propose an active jammer localization framework that combines Bayesian\noptimization with acquisition-aware path planning. Unlike passive crowdsourced\nmethods, our approach adaptively guides a mobile agent to collect high-utility\nReceived Signal Strength measurements while accounting for urban obstacles and\nmobility constraints. For this, we modified the A* algorithm, A-UCB*, by\nincorporating acquisition values into trajectory costs, leading to\nhigh-acquisition planned paths. Simulations on realistic urban scenarios show\nthat the proposed method achieves accurate localization with fewer measurements\ncompared to uninformed baselines, demonstrating consistent performance under\ndifferent environments.",
      "url": "http://arxiv.org/abs/2510.14790v1",
      "published_time_eastern_timestamp": 1760628144.0
    },
    {
      "title": "The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement\n  Learning Agents",
      "summary": "Testing deep reinforcement learning (DRL) agents in safety-critical domains\nrequires discovering diverse failure scenarios. Existing tools such as INDAGO\nrely on single-objective optimization focused solely on maximizing failure\ncounts, but this does not ensure discovered scenarios are diverse or reveal\ndistinct error types. We introduce INDAGO-Nexus, a multi-objective search\napproach that jointly optimizes for failure likelihood and test scenario\ndiversity using multi-objective evolutionary algorithms with multiple diversity\nmetrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on\nthree DRL agents: humanoid walker, self-driving car, and parking agent. On\naverage, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test\neffectiveness) than INDAGO in the SDC and Parking scenarios, respectively,\nwhile reducing time-to-failure by up to 67% across all agents.",
      "url": "http://arxiv.org/abs/2510.14727v1",
      "published_time_eastern_timestamp": 1760624755.0
    }
  ]
}
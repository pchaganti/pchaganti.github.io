{
  "last_updated": "2025-09-10T23:27:19.570720-04:00",
  "papers": [
    {
      "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation\n  Through Unsupervised Consistency Signals",
      "summary": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings.",
      "url": "http://arxiv.org/abs/2509.08809v1",
      "published_time_eastern_timestamp": 1757526161.0
    },
    {
      "title": "Narrative-Guided Reinforcement Learning: A Platform for Studying\n  Language Model Influence on Decision Making",
      "summary": "We present a preliminary experimental platform that explores how narrative\nelements might shape AI decision-making by combining reinforcement learning\n(RL) with language model reasoning. While AI systems can now both make\ndecisions and engage in narrative reasoning, these capabilities have mostly\nbeen studied separately. Our platform attempts to bridge this gap using a\ndual-system architecture to examine how narrative frameworks could influence\nreward-based learning. The system comprises a reinforcement learning policy\nthat suggests actions based on past experience, and a language model that\nprocesses these suggestions through different narrative frameworks to guide\ndecisions. This setup enables initial experimentation with narrative elements\nwhile maintaining consistent environment and reward structures. We implement\nthis architecture in a configurable gridworld environment, where agents receive\nboth policy suggestions and information about their surroundings. The\nplatform's modular design facilitates controlled testing of environmental\ncomplexity, narrative parameters, and the interaction between reinforcement\nlearning and narrative-based decisions. Our logging system captures basic\ndecision metrics, from RL policy values to language model reasoning to action\nselection patterns. While preliminary, this implementation provides a\nfoundation for studying how different narrative frameworks might affect\nreward-based decisions and exploring potential interactions between\noptimization-based learning and symbolic reasoning in AI systems.",
      "url": "http://arxiv.org/abs/2509.08785v1",
      "published_time_eastern_timestamp": 1757524452.0
    },
    {
      "title": "Efficiently Computing Equilibria in Budget-Aggregation Games",
      "summary": "Budget aggregation deals with the social choice problem of distributing an\nexogenously given budget among a set of public projects, given agents'\npreferences. Taking a game-theoretic perspective, we initialize the study of\n\\emph{budget-aggregation games} where each agent has virtual decision power\nover some fraction of the budget. This paper investigates the structure and\nshows efficient computability of Nash equilibria in this setting for various\npreference models. In particular, we show that Nash equilibria for Leontief\nutilities can be found in polynomial time, solving an open problem from Brandt\net al. [2023].",
      "url": "http://arxiv.org/abs/2509.08767v1",
      "published_time_eastern_timestamp": 1757523594.0
    },
    {
      "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation",
      "summary": "Robot navigation in dynamic, human-centered environments requires\nsocially-compliant decisions grounded in robust scene understanding. Recent\nVision-Language Models (VLMs) exhibit promising capabilities such as object\nrecognition, common-sense reasoning, and contextual understanding-capabilities\nthat align with the nuanced requirements of social robot navigation. However,\nit remains unclear whether VLMs can accurately understand complex social\nnavigation scenes (e.g., inferring the spatial-temporal relations among agents\nand human intentions), which is essential for safe and socially compliant robot\nnavigation. While some recent works have explored the use of VLMs in social\nrobot navigation, no existing work systematically evaluates their ability to\nmeet these necessary conditions. In this paper, we introduce the Social\nNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question\nAnswering (VQA) dataset and benchmark designed to evaluate VLMs for scene\nunderstanding in real-world social robot navigation scenarios. SocialNav-SUB\nprovides a unified framework for evaluating VLMs against human and rule-based\nbaselines across VQA tasks requiring spatial, spatiotemporal, and social\nreasoning in social robot navigation. Through experiments with state-of-the-art\nVLMs, we find that while the best-performing VLM achieves an encouraging\nprobability of agreeing with human answers, it still underperforms simpler\nrule-based approach and human consensus baselines, indicating critical gaps in\nsocial scene understanding of current VLMs. Our benchmark sets the stage for\nfurther research on foundation models for social robot navigation, offering a\nframework to explore how VLMs can be tailored to meet real-world social robot\nnavigation needs. An overview of this paper along with the code and data can be\nfound at https://larg.github.io/socialnav-sub .",
      "url": "http://arxiv.org/abs/2509.08757v1",
      "published_time_eastern_timestamp": 1757522820.0
    },
    {
      "title": "Using AI to Optimize Patient Transfer and Resource Utilization During\n  Mass-Casualty Incidents: A Simulation Platform",
      "summary": "Mass casualty incidents (MCIs) overwhelm healthcare systems and demand rapid,\naccurate patient-hospital allocation decisions under extreme pressure. Here, we\ndeveloped and validated a deep reinforcement learning-based decision-support AI\nagent to optimize patient transfer decisions during simulated MCIs by balancing\npatient acuity levels, specialized care requirements, hospital capacities, and\ntransport logistics. To integrate this AI agent, we developed MasTER, a\nweb-accessible command dashboard for MCI management simulations. Through a\ncontrolled user study with 30 participants (6 trauma experts and 24\nnon-experts), we evaluated three interaction approaches with the AI agent\n(human-only, human-AI collaboration, and AI-only) across 20- and 60-patient MCI\nscenarios in the Greater Toronto Area. Results demonstrate that increasing AI\ninvolvement significantly improves decision quality and consistency. The AI\nagent outperforms trauma surgeons (p < 0.001) and enables non-experts to\nachieve expert-level performance when assisted, contrasting sharply with their\nsignificantly inferior unassisted performance (p < 0.001). These findings\nestablish the potential for our AI-driven decision support to enhance both MCI\npreparedness training and real-world emergency response management.",
      "url": "http://arxiv.org/abs/2509.08756v1",
      "published_time_eastern_timestamp": 1757522814.0
    },
    {
      "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
      "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
      "url": "http://arxiv.org/abs/2509.08755v1",
      "published_time_eastern_timestamp": 1757522771.0
    },
    {
      "title": "Parallel, Asymptotically Optimal Algorithms for Moving Target Traveling\n  Salesman Problems",
      "summary": "The Moving Target Traveling Salesman Problem (MT-TSP) seeks an agent\ntrajectory that intercepts several moving targets, within a particular time\nwindow for each target. In the presence of generic nonlinear target\ntrajectories or kinematic constraints on the agent, no prior algorithm\nguarantees convergence to an optimal MT-TSP solution. Therefore, we introduce\nthe Iterated Random Generalized (IRG) TSP framework. The key idea behind IRG is\nto alternate between randomly sampling a set of agent configuration-time\npoints, corresponding to interceptions of targets, and finding a sequence of\ninterception points by solving a generalized TSP (GTSP). This alternation\nenables asymptotic convergence to the optimum. We introduce two parallel\nalgorithms within the IRG framework. The first algorithm, IRG-PGLNS, solves\nGTSPs using PGLNS, our parallelized extension of the state-of-the-art solver\nGLNS. The second algorithm, Parallel Communicating GTSPs (PCG), solves GTSPs\ncorresponding to several sets of points simultaneously. We present numerical\nresults for three variants of the MT-TSP: one where intercepting a target only\nrequires coming within a particular distance, another where the agent is a\nvariable-speed Dubins car, and a third where the agent is a redundant robot\narm. We show that IRG-PGLNS and PCG both converge faster than a baseline based\non prior work.",
      "url": "http://arxiv.org/abs/2509.08743v1",
      "published_time_eastern_timestamp": 1757522052.0
    },
    {
      "title": "ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent\n  System",
      "summary": "The efficiency of Bayesian optimization (BO) in chemistry is often hindered\nby sparse experimental data and complex reaction mechanisms. To overcome these\nlimitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced\nMulti-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization\nprocess is enhanced by LLMs and synergistically employs two strategies:\nknowledge-driven coarse-grained optimization and data-driven fine-grained\noptimization. First, in the knowledge-driven coarse-grained optimization stage,\nLLMs intelligently decompose the vast search space by reasoning over existing\nchemical knowledge to identify promising candidate regions. Subsequently, in\nthe data-driven fine-grained optimization stage, LLMs enhance the BO process\nwithin these candidate regions by generating pseudo-data points, thereby\nimproving data utilization efficiency and accelerating convergence. Benchmark\nevaluations** further confirm that ChemBOMAS significantly enhances\noptimization effectiveness and efficiency compared to various BO algorithms.\nImportantly, the practical utility of ChemBOMAS was validated through wet-lab\nexperiments conducted under pharmaceutical industry protocols, targeting\nconditional optimization for a previously unreported and challenging chemical\nreaction. In the wet experiment, ChemBOMAS achieved an optimal objective value\nof 96%. This was substantially higher than the 15% achieved by domain experts.\nThis real-world success, together with strong performance on benchmark\nevaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical\ndiscovery.",
      "url": "http://arxiv.org/abs/2509.08736v1",
      "published_time_eastern_timestamp": 1757521448.0
    },
    {
      "title": "Incentives for Digital Twins: Task-Based Productivity Enhancements with\n  Generative AI",
      "summary": "Generative AI is a technology which depends in part on participation by\nhumans in training and improving the automation potential. We focus on the\ndevelopment of an \"AI twin\" that could complement its creator's efforts,\nenabling them to produce higher-quality output in their individual style.\nHowever, AI twins could also, over time, replace individual humans. We analyze\nthis trade-off using a principal-agent model in which agents have the\nopportunity to make investments into training an AI twin that lead to a lower\ncost of effort, a higher probability of success, or both. We propose a new\nframework to situate the model in which the tasks performed vary in the ease to\nwhich AI output can be improved by the human (the \"editability\") and also vary\nin the extent to which a non-expert can assess the quality of output (its\n\"verifiability.\") Our synthesis of recent empirical studies indicates that\nproductivity gains from the use of generative AI are higher overall when task\neditability is higher, while non-experts enjoy greater relative productivity\ngains for tasks with higher verifiability. We show that during investment a\nstrategic agent will trade off improvements in quality and ease of effort to\npreserve their wage bargaining power. Tasks with high verifiability and low\neditability are most aligned with a worker's incentives to train their twin,\nbut for tasks where the stakes are low, this alignment is constrained by the\nrisk of displacement. Our results suggest that sustained improvements in\ncompany-sponsored generative AI will require nuanced design of human\nincentives, and that public policy which encourages balancing worker returns\nwith generative AI improvements could yield more sustained long-run\nproductivity gains.",
      "url": "http://arxiv.org/abs/2509.08732v1",
      "published_time_eastern_timestamp": 1757521194.0
    },
    {
      "title": "Decentralized Stochastic Nonconvex Optimization under the Relaxed\n  Smoothness",
      "summary": "This paper studies decentralized optimization problem\n$f(\\mathbf{x})=\\frac{1}{m}\\sum_{i=1}^m f_i(\\mathbf{x})$, where each local\nfunction has the form of $f_i(\\mathbf{x}) = {\\mathbb\nE}\\left[F(\\mathbf{x};{\\xi}_i)\\right]$ which is $(L_0,L_1)$-smooth but possibly\nnonconvex and the random variable ${\\xi}_i$ follows distribution ${\\mathcal\nD}_i$. We propose a novel algorithm called decentralized normalized stochastic\ngradient descent (DNSGD), which can achieve the $\\epsilon$-stationary point on\neach local agent. We present a new framework for analyzing decentralized\nfirst-order methods in the relaxed smooth setting, based on the Lyapunov\nfunction related to the product of the gradient norm and the consensus error.\nThe analysis shows upper bounds on sample complexity of ${\\mathcal\nO}(m^{-1}(L_f\\sigma^2\\Delta_f\\epsilon^{-4} + \\sigma^2\\epsilon^{-2} +\nL_f^{-2}L_1^3\\sigma^2\\Delta_f\\epsilon^{-1} + L_f^{-2}L_1^2\\sigma^2))$ per agent\nand communication complexity of $\\tilde{\\mathcal O}((L_f\\epsilon^{-2} +\nL_1\\epsilon^{-1})\\gamma^{-1/2}\\Delta_f)$, where $L_f=L_0 +L_1\\zeta$, $\\sigma^2$\nis the variance of the stochastic gradient, $\\Delta_f$ is the initial optimal\nfunction value gap, $\\gamma$ is the spectral gap of the network, and $\\zeta$ is\nthe degree of the gradient dissimilarity. In the special case of $L_1=0$, the\nabove results (nearly) match the lower bounds on decentralized nonconvex\noptimization in the standard smooth setting. We also conduct numerical\nexperiments to show the empirical superiority of our method.",
      "url": "http://arxiv.org/abs/2509.08726v1",
      "published_time_eastern_timestamp": 1757521039.0
    },
    {
      "title": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across\n  Repositories",
      "summary": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach.",
      "url": "http://arxiv.org/abs/2509.08724v1",
      "published_time_eastern_timestamp": 1757520923.0
    },
    {
      "title": "Automatic Failure Attribution and Critical Step Prediction Method for\n  Multi-Agent Systems Based on Causal Inference",
      "summary": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems.",
      "url": "http://arxiv.org/abs/2509.08682v1",
      "published_time_eastern_timestamp": 1757517720.0
    },
    {
      "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute\n  Implementations",
      "summary": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents.",
      "url": "http://arxiv.org/abs/2509.08646v1",
      "published_time_eastern_timestamp": 1757515267.0
    },
    {
      "title": "AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models",
      "summary": "Specialized machine learning models, regardless of architecture and training,\nare susceptible to failures in deployment. With their increasing use in high\nrisk situations, the ability to audit these models by determining their\noperational design domain (ODD) is crucial in ensuring safety and compliance.\nHowever, given the high-dimensional input spaces, this process often requires\nsignificant human resources and domain expertise. To alleviate this, we\nintroduce \\coolname, an LLM-Agent centric framework for automated generation of\nsemantically relevant test cases to search for failure modes in specialized\nblack-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit\na uncertainty-aware failure distribution model on a learned text-embedding\nmanifold by projecting the high-dimension input space to low-dimension\ntext-embedding latent space. The LLM-Agent is tasked with iteratively building\nthe failure landscape by leveraging tools for generating test-cases to probe\nthe model-under-test (MUT) and recording the response. The agent also guides\nthe search using tools to probe uncertainty estimate on the low dimensional\nmanifold. We demonstrate this process in a simple case using models trained\nwith missing digits on the MNIST dataset and in the real world setting of\nvision-based intruder detection for aerial vehicles.",
      "url": "http://arxiv.org/abs/2509.08638v1",
      "published_time_eastern_timestamp": 1757514838.0
    },
    {
      "title": "Acceptability of AI Assistants for Privacy: Perceptions of Experts and\n  Users on Personalized Privacy Assistants",
      "summary": "Individuals increasingly face an overwhelming number of tasks and decisions.\nTo cope with the new reality, there is growing research interest in developing\nintelligent agents that can effectively assist people across various aspects of\ndaily life in a tailored manner, with privacy emerging as a particular area of\napplication. Artificial intelligence (AI) assistants for privacy, such as\npersonalized privacy assistants (PPAs), have the potential to automatically\nexecute privacy decisions based on users' pre-defined privacy preferences,\nsparing them the mental effort and time usually spent on each privacy decision.\nThis helps ensure that, even when users feel overwhelmed or resigned about\nprivacy, the decisions made by PPAs still align with their true preferences and\nbest interests. While research has explored possible designs of such agents,\nuser and expert perspectives on the acceptability of such AI-driven solutions\nremain largely unexplored. In this study, we conducted five focus groups with\ndomain experts (n = 11) and potential users (n = 26) to uncover key themes\nshaping the acceptance of PPAs. Factors influencing the acceptability of AI\nassistants for privacy include design elements (such as information sources\nused by the agent), external conditions (such as regulation and literacy\neducation), and systemic conditions (e.g., public or market providers and the\nneed to avoid monopoly) to PPAs. These findings provide theoretical extensions\nto technology acceptance models measuring PPAs, insights on design, and policy\nimplications for PPAs, as well as broader implications for the design of AI\nassistants.",
      "url": "http://arxiv.org/abs/2509.08554v1",
      "published_time_eastern_timestamp": 1757509179.0
    },
    {
      "title": "Agents of Discovery",
      "summary": "The substantial data volumes encountered in modern particle physics and other\ndomains of fundamental physics research allow (and require) the use of\nincreasingly complex data analysis tools and workflows. While the use of\nmachine learning (ML) tools for data analysis has recently proliferated, these\ntools are typically special-purpose algorithms that rely, for example, on\nencoded physics knowledge to reach optimal performance. In this work, we\ninvestigate a new and orthogonal direction: Using recent progress in large\nlanguage models (LLMs) to create a team of agents -- instances of LLMs with\nspecific subtasks -- that jointly solve data analysis-based research problems\nin a way similar to how a human researcher might: by creating code to operate\nstandard tools and libraries (including ML systems) and by building on results\nof previous iterations. If successful, such agent-based systems could be\ndeployed to automate routine analysis components to counteract the increasing\ncomplexity of modern tool chains. To investigate the capabilities of\ncurrent-generation commercial LLMs, we consider the task of anomaly detection\nvia the publicly available and highly-studied LHC Olympics dataset. Several\ncurrent models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated\nand their stability tested. Overall, we observe the capacity of the agent-based\nsystem to solve this data analysis problem. The best agent-created solutions\nmirror the performance of human state-of-the-art results.",
      "url": "http://arxiv.org/abs/2509.08535v1",
      "published_time_eastern_timestamp": 1757507113.0
    },
    {
      "title": "Phase-Coordinated Multi-Agent Circular Formation Control with\n  Non-Concentric Boundary Constraints",
      "summary": "This paper addresses the problem of collective circular motion control for\nunicycle agents, with the objective of achieving phase coordination of their\nvelocity vectors while ensuring that their trajectories remain confined within\na prescribed non-concentric circular boundary. To accommodate such nonuniform\nmotion constraints, we build upon our earlier work and extend the use of Mobius\ntransformation to a multi-agent framework. The Mobius transformation maps two\nnonconcentric circles to concentric ones, thereby converting spatially\nnonuniform constraints into uniform ones in the transformed plane. Leveraging\nthis property, we introduce the notion of a phase-shifted order parameter,\nalong with the associated concepts of Mobius phase-shift coupled\nsynchronization and balancing, which characterize the phase-coordinated\npatterns studied in this paper. We establish an equivalence between the\nunicycle dynamics in the original and transformed planes under the Mobius\ntransformation and its inverse, and show that synchronization is preserved\nacross both planes, whereas balancing is generally not. Distributed control\nlaws are then designed in the transformed plane using barrier Lyapunov\nfunctions, under the assumption of an undirected and connected communication\ntopology among agents. These controllers are subsequently mapped back to the\noriginal plane to obtain the linear acceleration and turn-rate control inputs\napplied to the actual agents. Both simulations and experimental results are\nprovided to illustrate the proposed framework.",
      "url": "http://arxiv.org/abs/2509.08534v1",
      "published_time_eastern_timestamp": 1757507057.0
    },
    {
      "title": "TCPO: Thought-Centric Preference Optimization for Effective Embodied\n  Decision-making",
      "summary": "Using effective generalization capabilities of vision language models (VLMs)\nin context-specific dynamic tasks for embodied artificial intelligence remains\na significant challenge. Although supervised fine-tuned models can better align\nwith the real physical world, they still exhibit sluggish responses and\nhallucination issues in dynamically changing environments, necessitating\nfurther alignment. Existing post-SFT methods, reliant on reinforcement learning\nand chain-of-thought (CoT) approaches, are constrained by sparse rewards and\naction-only optimization, resulting in low sample efficiency, poor consistency,\nand model degradation. To address these issues, this paper proposes\nThought-Centric Preference Optimization (TCPO) for effective embodied\ndecision-making. Specifically, TCPO introduces a stepwise preference-based\noptimization approach, transforming sparse reward signals into richer step\nsample pairs. It emphasizes the alignment of the model's intermediate reasoning\nprocess, mitigating the problem of model degradation. Moreover, by\nincorporating Action Policy Consistency Constraint (APC), it further imposes\nconsistency constraints on the model output. Experiments in the ALFWorld\nenvironment demonstrate an average success rate of 26.67%, achieving a 6%\nimprovement over RL4VLM and validating the effectiveness of our approach in\nmitigating model degradation after fine-tuning. These results highlight the\npotential of integrating preference-based learning techniques with CoT\nprocesses to enhance the decision-making capabilities of vision-language models\nin embodied agents.",
      "url": "http://arxiv.org/abs/2509.08500v1",
      "published_time_eastern_timestamp": 1757502981.0
    },
    {
      "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants",
      "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets.",
      "url": "http://arxiv.org/abs/2509.08494v1",
      "published_time_eastern_timestamp": 1757502610.0
    },
    {
      "title": "Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic\n  Environment",
      "summary": "Recent advances in robotics have enabled the widespread deployment of\nautonomous robotic systems in complex operational environments, presenting both\nunprecedented opportunities and significant security problems. Traditional\nshepherding approaches based on fixed formations are often ineffective or risky\nin urban and obstacle-rich scenarios, especially when facing adversarial agents\nwith unknown and adaptive behaviors. This paper addresses this challenge as an\nextended herding problem, where defensive robotic systems must safely guide\nadversarial agents with unknown strategies away from protected areas and into\npredetermined safe regions, while maintaining collision-free navigation in\ndynamic environments. We propose a hierarchical hybrid framework based on\nreach-avoid game theory and local motion planning, incorporating a virtual\ncontainment boundary and event-triggered pursuit mechanisms to enable scalable\nand robust multi-agent coordination. Simulation results demonstrate that the\nproposed approach achieves safe and efficient guidance of adversarial agents to\ndesignated regions.",
      "url": "http://arxiv.org/abs/2509.08460v1",
      "published_time_eastern_timestamp": 1757498700.0
    }
  ]
}
{
  "last_updated": "2025-06-18T02:18:06.657659-04:00",
  "papers": [
    {
      "title": "RobotSmith: Generative Robotic Tool Design for Acquisition of Complex\n  Manipulation Skills",
      "summary": "Endowing robots with tool design abilities is critical for enabling them to\nsolve complex manipulation tasks that would otherwise be intractable. While\nrecent generative frameworks can automatically synthesize task settings, such\nas 3D scenes and reward functions, they have not yet addressed the challenge of\ntool-use scenarios. Simply retrieving human-designed tools might not be ideal\nsince many tools (e.g., a rolling pin) are difficult for robotic manipulators\nto handle. Furthermore, existing tool design approaches either rely on\npredefined templates with limited parameter tuning or apply generic 3D\ngeneration methods that are not optimized for tool creation. To address these\nlimitations, we propose RobotSmith, an automated pipeline that leverages the\nimplicit physical knowledge embedded in vision-language models (VLMs) alongside\nthe more accurate physics provided by physics simulations to design and use\ntools for robotic manipulation. Our system (1) iteratively proposes tool\ndesigns using collaborative VLM agents, (2) generates low-level robot\ntrajectories for tool use, and (3) jointly optimizes tool geometry and usage\nfor task performance. We evaluate our approach across a wide range of\nmanipulation tasks involving rigid, deformable, and fluid objects. Experiments\nshow that our method consistently outperforms strong baselines in terms of both\ntask success rate and overall performance. Notably, our approach achieves a\n50.0\\% average success rate, significantly surpassing other baselines such as\n3D generation (21.4%) and tool retrieval (11.1%). Finally, we deploy our system\nin real-world settings, demonstrating that the generated tools and their usage\nplans transfer effectively to physical execution, validating the practicality\nand generalization capabilities of our approach.",
      "url": "http://arxiv.org/abs/2506.14763v1",
      "published_time_eastern_timestamp": 1750183057.0
    },
    {
      "title": "Swarm-STL: A Framework for Motion Planning in Large-Scale, Multi-Swarm\n  Systems",
      "summary": "In multi-agent systems, signal temporal logic (STL) is widely used for path\nplanning to accomplish complex objectives with formal safety guarantees.\nHowever, as the number of agents increases, existing approaches encounter\nsignificant computational challenges. Recognizing that many complex tasks\nrequire cooperation among multiple agents, we propose swarm STL specifications\nto describe the collective tasks that need to be achieved by a team of agents.\nNext, we address the motion planning problem for all the agents in two stages.\nFirst, we abstract a group of cooperating agents as a swarm and construct a\nreduced-dimension state space whose dimension does not increase with the number\nof agents. The path planning is performed at the swarm level, ensuring the\nsafety and swarm STL specifications are satisfied. Then, we design low-level\ncontrol strategies for agents within each swarm based on the path synthesized\nin the first step. The trajectories of agents generated by the two-step policy\nensure satisfaction of the STL specifications. We evaluate our two-stage\napproach in both single-swarm and multi-swarm scenarios. The results\ndemonstrate that all tasks are completed with safety guarantees. Compared to\nthe baseline multi-agent planning approach, our method maintains computational\nefficiency as the number of agents increases, since the computational time\nscales with the number of swarms rather than the number of agents.",
      "url": "http://arxiv.org/abs/2506.14749v1",
      "published_time_eastern_timestamp": 1750182012.0
    },
    {
      "title": "AgentDistill: Training-Free Agent Distillation with Generalizable MCP\n  Boxes",
      "summary": "While knowledge distillation has become a mature field for compressing large\nlanguage models (LLMs) into smaller ones by aligning their outputs or internal\nrepresentations, the distillation of LLM-based agents, which involve planning,\nmemory, and tool use, remains relatively underexplored. Existing agent\ndistillation methods typically replay full teacher trajectories or imitate\nstep-by-step teacher tool usage, but they often struggle to train student\nagents to dynamically plan and act in novel environments. We propose\nAgentDistill, a novel, training-free agent distillation framework that enables\nefficient and scalable knowledge transfer via direct reuse of\nModel-Context-Protocols (MCPs), which are structured and reusable task-solving\nmodules autonomously generated by teacher agents. The reuse of these distilled\nMCPs enables student agents to generalize their capabilities across domains and\nsolve new problems with minimal supervision or human intervention. Experiments\non biomedical and mathematical benchmarks demonstrate that our distilled\nstudent agents, built on small language models, can achieve performance\ncomparable to advanced systems using large LLMs such as OctoTools (GPT-4o),\nhighlighting the effectiveness of our framework in building scalable and\ncost-efficient intelligent agents.",
      "url": "http://arxiv.org/abs/2506.14728v1",
      "published_time_eastern_timestamp": 1750180112.0
    },
    {
      "title": "Linear Planar 3-SAT and Its Applications in Planning",
      "summary": "Several fragments of the satisfiability problem have been studied in the\nliterature. Among these, Linear 3-SAT is a satisfaction problem in which each\nclause (viewed as a set of literals) intersects with at most one other clause;\nmoreover, any pair of clauses have at most one literal in common. Planar 3-SAT\nis a fragment which requires that the so-called variable-clause graph is\nplanar. Both fragments are NP-complete and have applications in encoding\nNP-hard planning problems. In this paper, we investigate the complexity and\napplications of the fragment obtained combining both features. We define Linear\nPlanar 3-SAT and prove its NP-completeness. We also study the reconfiguration\nproblem of Linear Planar 3-SAT and show that it is PSPACE-complete. As an\napplication, we use these new results to prove the NP-completeness of Bounded\nConnected Multi-Agent Pathfinding and the PSPACE-completeness of Connected\nMulti-Agent Pathfinding in two-dimensional grids.",
      "url": "http://arxiv.org/abs/2506.14713v1",
      "published_time_eastern_timestamp": 1750179063.0
    },
    {
      "title": "AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous\n  Instructions",
      "summary": "The rapid advancement of vision-language models (VLMs) and their integration\ninto embodied agents have unlocked powerful capabilities for decision-making.\nHowever, as these systems are increasingly deployed in real-world environments,\nthey face mounting safety concerns, particularly when responding to hazardous\ninstructions. In this work, we propose AGENTSAFE, the first comprehensive\nbenchmark for evaluating the safety of embodied VLM agents under hazardous\ninstructions. AGENTSAFE simulates realistic agent-environment interactions\nwithin a simulation sandbox and incorporates a novel adapter module that\nbridges the gap between high-level VLM outputs and low-level embodied controls.\nSpecifically, it maps recognized visual entities to manipulable objects and\ntranslates abstract planning into executable atomic actions in the environment.\nBuilding on this, we construct a risk-aware instruction dataset inspired by\nAsimovs Three Laws of Robotics, including base risky instructions and mutated\njailbroken instructions. The benchmark includes 45 adversarial scenarios, 1,350\nhazardous tasks, and 8,100 hazardous instructions, enabling systematic testing\nunder adversarial conditions ranging from perception, planning, and action\nexecution stages.",
      "url": "http://arxiv.org/abs/2506.14697v1",
      "published_time_eastern_timestamp": 1750178255.0
    },
    {
      "title": "Factor-Graph-Based Passive Acoustic Navigation for Decentralized\n  Cooperative Localization Using Bearing Elevation Depth Difference",
      "summary": "Accurate and scalable underwater multi-agent localization remains a critical\nchallenge due to the constraints of underwater communication. In this work, we\npropose a multi-agent localization framework using a factor-graph\nrepresentation that incorporates bearing, elevation, and depth difference\n(BEDD). Our method leverages inverted ultra-short baseline (inverted-USBL)\nderived azimuth and elevation measurements from incoming acoustic signals and\nrelative depth measurements to enable cooperative localization for a\nmulti-robot team of autonomous underwater vehicles (AUVs). We validate our\napproach in the HoloOcean underwater simulator with a fleet of AUVs,\ndemonstrating improved localization accuracy compared to dead reckoning.\nAdditionally, we investigate the impact of azimuth and elevation measurement\noutliers, highlighting the need for robust outlier rejection techniques for\nacoustic signals.",
      "url": "http://arxiv.org/abs/2506.14690v1",
      "published_time_eastern_timestamp": 1750177731.0
    },
    {
      "title": "Unified Software Engineering agent as AI Software Engineer",
      "summary": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.",
      "url": "http://arxiv.org/abs/2506.14683v1",
      "published_time_eastern_timestamp": 1750177153.0
    },
    {
      "title": "StreetLens: Enabling Human-Centered AI Agents for Neighborhood\n  Assessment from Street View Imagery",
      "summary": "Traditionally, neighborhood studies have employed interviews, surveys, and\nmanual image annotation guided by detailed protocols to identify environmental\ncharacteristics, including physical disorder, decay, street safety, and\nsociocultural symbols, and to examine their impact on developmental and health\noutcomes. While these methods yield rich insights, they are time-consuming and\nrequire intensive expert intervention. Recent technological advances, including\nvision-language models (VLMs), have begun to automate parts of this process;\nhowever, existing efforts are often ad hoc and lack adaptability across\nresearch designs and geographic contexts. In this demo paper, we present\nStreetLens, a human-centered, researcher-configurable workflow that embeds\nrelevant social science expertise in a VLM for scalable neighborhood\nenvironmental assessments. StreetLens mimics the process of trained human\ncoders by grounding the analysis in questions derived from established\ninterview protocols, retrieving relevant street view imagery (SVI), and\ngenerating a wide spectrum of semantic annotations from objective features\n(e.g., the number of cars) to subjective perceptions (e.g., the sense of\ndisorder in an image). By enabling researchers to define the VLM's role through\ndomain-informed prompting, StreetLens places domain knowledge at the core of\nthe analysis process. It also supports the integration of prior survey data to\nenhance robustness and expand the range of characteristics assessed across\ndiverse settings. We provide a Google Colab notebook to make StreetLens\naccessible and extensible for researchers working with public or custom SVI\ndatasets. StreetLens represents a shift toward flexible, agentic AI systems\nthat work closely with researchers to accelerate and scale neighborhood\nstudies.",
      "url": "http://arxiv.org/abs/2506.14670v1",
      "published_time_eastern_timestamp": 1750176363.0
    },
    {
      "title": "SENIOR: Efficient Query Selection and Preference-Guided Exploration in\n  Preference-based Reinforcement Learning",
      "summary": "Preference-based Reinforcement Learning (PbRL) methods provide a solution to\navoid reward engineering by learning reward models based on human preferences.\nHowever, poor feedback- and sample- efficiency still remain the problems that\nhinder the application of PbRL. In this paper, we present a novel efficient\nquery selection and preference-guided exploration method, called SENIOR, which\ncould select the meaningful and easy-to-comparison behavior segment pairs to\nimprove human feedback-efficiency and accelerate policy learning with the\ndesigned preference-guided intrinsic rewards. Our key idea is twofold: (1) We\ndesigned a Motion-Distinction-based Selection scheme (MDS). It selects segment\npairs with apparent motion and different directions through kernel density\nestimation of states, which is more task-related and easy for human preference\nlabeling; (2) We proposed a novel preference-guided exploration method (PGE).\nIt encourages the exploration towards the states with high preference and low\nvisits and continuously guides the agent achieving the valuable samples. The\nsynergy between the two mechanisms could significantly accelerate the progress\nof reward and policy learning. Our experiments show that SENIOR outperforms\nother five existing methods in both human feedback-efficiency and policy\nconvergence speed on six complex robot manipulation tasks from simulation and\nfour real-worlds.",
      "url": "http://arxiv.org/abs/2506.14648v1",
      "published_time_eastern_timestamp": 1750174939.0
    },
    {
      "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs",
      "summary": "Recent large language models (LLMs) achieve impressive performance in\nsource-conditioned text generation but often fail to correctly provide\nfine-grained attributions for their outputs, undermining verifiability and\ntrust. Moreover, existing attribution methods do not explain how and why models\nleverage the provided source documents to generate their final responses,\nlimiting interpretability. To overcome these challenges, we introduce a modular\ngeneration framework, GenerationPrograms, inspired by recent advancements in\nexecutable \"code agent\" architectures. Unlike conventional generation methods\nthat simultaneously generate outputs and attributions or rely on post-hoc\nattribution, GenerationPrograms decomposes the process into two distinct\nstages: first, creating an executable program plan composed of modular text\noperations (such as paraphrasing, compression, and fusion) explicitly tailored\nto the query, and second, executing these operations following the program's\nspecified instructions to produce the final response. Empirical evaluations\ndemonstrate that GenerationPrograms significantly improves attribution quality\nat both the document level and sentence level across two long-form\nquestion-answering tasks and a multi-document summarization task. We further\ndemonstrate that GenerationPrograms can effectively function as a post-hoc\nattribution method, outperforming traditional techniques in recovering accurate\nattributions. In addition, the interpretable programs generated by\nGenerationPrograms enable localized refinement through modular-level\nimprovements that further enhance overall attribution quality.",
      "url": "http://arxiv.org/abs/2506.14580v1",
      "published_time_eastern_timestamp": 1750171029.0
    },
    {
      "title": "Doppelgänger Method: Breaking Role Consistency in LLM Agent via\n  Prompt-based Transferable Adversarial Attack",
      "summary": "Since the advent of large language models, prompt engineering now enables the\nrapid, low-effort creation of diverse autonomous agents that are already in\nwidespread use. Yet this convenience raises urgent concerns about the safety,\nrobustness, and behavioral consistency of the underlying prompts, along with\nthe pressing challenge of preventing those prompts from being exposed to user's\nattempts. In this paper, we propose the ''Doppelg\\\"anger method'' to\ndemonstrate the risk of an agent being hijacked, thereby exposing system\ninstructions and internal information. Next, we define the ''Prompt Alignment\nCollapse under Adversarial Transfer (PACAT)'' level to evaluate the\nvulnerability to this adversarial transfer attack. We also propose a ''Caution\nfor Adversarial Transfer (CAT)'' prompt to counter the Doppelg\\\"anger method.\nThe experimental results demonstrate that the Doppelg\\\"anger method can\ncompromise the agent's consistency and expose its internal information. In\ncontrast, CAT prompts enable effective defense against this adversarial attack.",
      "url": "http://arxiv.org/abs/2506.14539v1",
      "published_time_eastern_timestamp": 1750168899.0
    },
    {
      "title": "Automated Decision-Making on Networks with LLMs through Knowledge-Guided\n  Evolution",
      "summary": "Effective decision-making on networks often relies on learning from\ngraph-structured data, where Graph Neural Networks (GNNs) play a central role,\nbut they take efforts to configure and tune. In this demo, we propose LLMNet,\nshowing how to design GNN automated through Large Language Models. Our system\ndevelops a set of agents that construct graph-related knowlege bases and then\nleverages Retrieval-Augmented Generation (RAG) to support automated\nconfiguration and refinement of GNN models through a knowledge-guided evolution\nprocess. These agents, equipped with specialized knowledge bases, extract\ninsights into tasks and graph structures by interacting with the knowledge\nbases. Empirical results show LLMNet excels in twelve datasets across three\ngraph learning tasks, validating its effectiveness of GNN model designing.",
      "url": "http://arxiv.org/abs/2506.14529v1",
      "published_time_eastern_timestamp": 1750168428.0
    },
    {
      "title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex\n  Reasoning Tasks",
      "summary": "Large Language Models (LLMs) are experiencing rapid advancements in complex\nreasoning, exhibiting remarkable generalization in mathematics and programming.\nIn contrast, while spatial intelligence is fundamental for Vision-Language\nModels (VLMs) in real-world interaction, the systematic evaluation of their\ncomplex reasoning ability within spatial contexts remains underexplored. To\nbridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate\nVLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench\ncomprises nearly 1K video-question-answer triplets, where each problem is\nembedded in a realistic 3D scene and captured by video. By carefully designing\nquestions and corresponding 3D scenes, our benchmark ensures that solving the\nquestions requires both spatial comprehension for extracting information and\nhigh-level reasoning for deriving solutions, making it a challenging benchmark\nfor evaluating VLMs. To facilitate large-scale data synthesis, we develop an\nAutomatic Scene Creation Engine. This engine, leveraging multiple specialized\nLLM agents, can generate realistic 3D scenes from abstract math problems,\nensuring faithfulness to the original descriptions. Experimental results reveal\nthat state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring\nthe challenge of spatial reasoning. We hope that our study will bring\nresearchers' attention to spatially grounded reasoning and advance VLMs in\nvisual problem-solving.",
      "url": "http://arxiv.org/abs/2506.14512v1",
      "published_time_eastern_timestamp": 1750167600.0
    },
    {
      "title": "Toward Safety-First Human-Like Decision Making for Autonomous Vehicles\n  in Time-Varying Traffic Flow",
      "summary": "Despite the recent advancements in artificial intelligence technologies have\nshown great potential in improving transport efficiency and safety, autonomous\nvehicles(AVs) still face great challenge of driving in time-varying traffic\nflow, especially in dense and interactive situations. Meanwhile, human have\nfree wills and usually do not make the same decisions even situate in the\nexactly same scenarios, leading to the data-driven methods suffer from poor\nmigratability and high search cost problems, decreasing the efficiency and\neffectiveness of the behavior policy. In this research, we propose a\nsafety-first human-like decision-making framework(SF-HLDM) for AVs to drive\nsafely, comfortably, and social compatiblely in effiency. The framework\nintegrates a hierarchical progressive framework, which combines a\nspatial-temporal attention (S-TA) mechanism for other road users' intention\ninference, a social compliance estimation module for behavior regulation, and a\nDeep Evolutionary Reinforcement Learning(DERL) model for expanding the search\nspace efficiently and effectively to make avoidance of falling into the local\noptimal trap and reduce the risk of overfitting, thus make human-like decisions\nwith interpretability and flexibility. The SF-HLDM framework enables autonomous\ndriving AI agents dynamically adjusts decision parameters to maintain safety\nmargins and adhering to contextually appropriate driving behaviors at the same\ntime.",
      "url": "http://arxiv.org/abs/2506.14502v1",
      "published_time_eastern_timestamp": 1750166899.0
    },
    {
      "title": "LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?",
      "summary": "Swarm intelligence traditionally refers to systems of simple, decentralized\nagents whose local interactions lead to emergent, collective behavior.\nRecently, the term 'swarm' has been extended to describe AI systems like\nOpenAI's Swarm, where large language models (LLMs) act as collaborative agents.\nThis paper contrasts traditional swarm algorithms with LLM-driven swarms\nexploring how decentralization, scalability, and emergence are redefined in\nmodern artificial intelligence (AI). We implement and compare both paradigms\nusing Boids and Ant Colony Optimization (ACO), evaluating latency, resource\nusage, and behavioral accuracy. The suitability of both cloud-based and local\nLLMs is assessed for the agent-based use in swarms. Although LLMs offer\npowerful reasoning and abstraction capabilities, they introduce new constraints\nin computation and coordination that challenge traditional notions of swarm\ndesign. This study highlights the opportunities and limitations of integrating\nLLMs into swarm systems and discusses the evolving definition of 'swarm' in\nmodern AI research.",
      "url": "http://arxiv.org/abs/2506.14496v1",
      "published_time_eastern_timestamp": 1750166314.0
    },
    {
      "title": "GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in\n  Real-World Anomalies",
      "summary": "The development of high-quality datasets is crucial for benchmarking and\nadvancing research in Graphical User Interface (GUI) agents. Despite their\nimportance, existing datasets are often constructed under idealized conditions,\noverlooking the diverse anomalies frequently encountered in real-world\ndeployments. To address this limitation, we introduce GUI-Robust, a novel\ndataset designed for comprehensive GUI agent evaluation, explicitly\nincorporating seven common types of anomalies observed in everyday GUI\ninteractions. Furthermore, we propose a semi-automated dataset construction\nparadigm that collects user action sequences from natural interactions via RPA\ntools and then generate corresponding step and task descriptions for these\nactions with the assistance of MLLMs. This paradigm significantly reduces\nannotation time cost by a factor of over 19 times. Finally, we assess\nstate-of-the-art GUI agents using the GUI-Robust dataset, revealing their\nsubstantial performance degradation in abnormal scenarios. We anticipate that\nour work will highlight the importance of robustness in GUI agents and inspires\nmore future research in this direction. The dataset and code are available at\nhttps://github.com/chessbean1/GUI-Robust..",
      "url": "http://arxiv.org/abs/2506.14477v1",
      "published_time_eastern_timestamp": 1750164635.0
    },
    {
      "title": "SimSpark: Interactive Simulation of Social Media Behaviors",
      "summary": "Understanding user behaviors on social media has garnered significant\nscholarly attention, enhancing our comprehension of how virtual platforms\nimpact society and empowering decision-makers. Simulating social media\nbehaviors provides a robust tool for capturing the patterns of social media\nbehaviors, testing hypotheses, and predicting the effects of various\ninterventions, ultimately contributing to a deeper understanding of social\nmedia environments. Moreover, it can overcome difficulties associated with\nutilizing real data for analysis, such as data accessibility issues, ethical\nconcerns, and the complexity of processing large and heterogeneous datasets.\nHowever, researchers and stakeholders need more flexible platforms to\ninvestigate different user behaviors by simulating different scenarios and\ncharacters, which is not possible yet. Therefore, this paper introduces\nSimSpark, an interactive system including simulation algorithms and interactive\nvisual interfaces which is capable of creating small simulated social media\nplatforms with customizable characters and social environments. We address\nthree key challenges: generating believable behaviors, validating simulation\nresults, and supporting interactive control for generation and results\nanalysis. A simulation workflow is introduced to generate believable behaviors\nof agents by utilizing large language models. A visual interface enables\nreal-time parameter adjustment and process monitoring for customizing\ngeneration settings. A set of visualizations and interactions are also designed\nto display the models' outputs for further analysis. Effectiveness is evaluated\nthrough case studies, quantitative simulation model assessments, and expert\ninterviews.",
      "url": "http://arxiv.org/abs/2506.14476v1",
      "published_time_eastern_timestamp": 1750164607.0
    },
    {
      "title": "Hamiltonian Formalism for Comparing Quantum and Classical Intelligence",
      "summary": "The prospect of AGI instantiated on quantum substrates motivates the\ndevelopment of mathematical frameworks that enable direct comparison of their\noperation in classical and quantum environments. To this end, we introduce a\nHamiltonian formalism for describing classical and quantum AGI tasks as a means\nof contrasting their interaction with the environment. We propose a\ndecomposition of AGI dynamics into Hamiltonian generators for core functions\nsuch as induction, reasoning, recursion, learning, measurement, and memory.\nThis formalism aims to contribute to the development of a precise mathematical\nlanguage for how quantum and classical agents differ via environmental\ninteraction.",
      "url": "http://arxiv.org/abs/2506.14456v1",
      "published_time_eastern_timestamp": 1750162625.0
    },
    {
      "title": "Active Digital Twins via Active Inference",
      "summary": "Digital twins are transforming engineering and applied sciences by enabling\nreal-time monitoring, simulation, and predictive analysis of physical systems\nand processes. However, conventional digital twins rely primarily on passive\ndata assimilation, which limits their adaptability in uncertain and dynamic\nenvironments. This paper introduces the active digital twin paradigm, based on\nactive inference. Active inference is a neuroscience-inspired, Bayesian\nframework for probabilistic reasoning and predictive modeling that unifies\ninference, decision-making, and learning under a unique, free energy\nminimization objective. By formulating the evolution of the active digital twin\nas a partially observable Markov decision process, the active inference agent\ncontinuously refines its generative model through Bayesian updates and\nforecasts future states and observations. Decision-making emerges from an\noptimization process that balances pragmatic exploitation (maximizing\ngoal-directed utility) and epistemic exploration or information gain (actively\nresolving uncertainty). Actions are dynamically planned to minimize expected\nfree energy, which quantifies both the divergence between predicted and\npreferred future observations, and the epistemic value of expected information\ngain about hidden states. This approach enables a new level of autonomy and\nresilience in digital twins, offering superior spontaneous exploration\ncapabilities. The proposed framework is assessed on the health monitoring and\npredictive maintenance of a railway bridge.",
      "url": "http://arxiv.org/abs/2506.14453v1",
      "published_time_eastern_timestamp": 1750162563.0
    },
    {
      "title": "Adaptive Reinforcement Learning for Unobservable Random Delays",
      "summary": "In standard Reinforcement Learning (RL) settings, the interaction between the\nagent and the environment is typically modeled as a Markov Decision Process\n(MDP), which assumes that the agent observes the system state instantaneously,\nselects an action without delay, and executes it immediately. In real-world\ndynamic environments, such as cyber-physical systems, this assumption often\nbreaks down due to delays in the interaction between the agent and the system.\nThese delays can vary stochastically over time and are typically unobservable,\nmeaning they are unknown when deciding on an action. Existing methods deal with\nthis uncertainty conservatively by assuming a known fixed upper bound on the\ndelay, even if the delay is often much lower. In this work, we introduce the\ninteraction layer, a general framework that enables agents to adaptively and\nseamlessly handle unobservable and time-varying delays. Specifically, the agent\ngenerates a matrix of possible future actions to handle both unpredictable\ndelays and lost action packets sent over networks. Building on this framework,\nwe develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA),\nwhich dynamically adjusts to delay patterns. Our method significantly\noutperforms state-of-the-art approaches across a wide range of locomotion\nbenchmark environments.",
      "url": "http://arxiv.org/abs/2506.14411v1",
      "published_time_eastern_timestamp": 1750158697.0
    }
  ]
}
{
  "last_updated": "2025-06-09T05:14:21.729639-04:00",
  "papers": [
    {
      "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
      "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced\nparadigms that exhibit impressive capabilities in a wide range of domains and\ntasks. Despite their potential, current LLM agents often adopt a\none-size-fits-all approach, lacking the flexibility to respond to users'\nvarying needs and preferences. This limitation motivates us to develop\nPersonaAgent, the first personalized LLM agent framework designed to address\nversatile personalization tasks. Specifically, PersonaAgent integrates two\ncomplementary components - a personalized memory module that includes episodic\nand semantic memory mechanisms; a personalized action module that enables the\nagent to perform tool actions tailored to the user. At the core, the persona\n(defined as unique system prompt for each user) functions as an intermediary:\nit leverages insights from personalized memory to control agent actions, while\nthe outcomes of these actions in turn refine the memory. Based on the\nframework, we propose a test-time user-preference alignment strategy that\nsimulate the latest n interactions to optimize the persona prompt, ensuring\nreal-time user preference alignment through textual loss feedback between\nsimulated and ground-truth responses. Experimental evaluations demonstrate that\nPersonaAgent significantly outperforms other baseline methods by not only\npersonalizing the action space effectively but also scaling during test-time\nreal-world applications. These results underscore the feasibility and potential\nof our approach in delivering tailored, dynamic user experiences.",
      "url": "http://arxiv.org/abs/2506.06254v1",
      "published_time_eastern_timestamp": 1749230989.0
    },
    {
      "title": "Longer Lists Yield Better Matchings",
      "summary": "Many centralized mechanisms for two-sided matching markets that enjoy strong\ntheoretical properties assume that the planner solicits full information on the\npreferences of each participating agent. In particular, they expect that\nparticipants compile and communicate their complete preference lists over\nagents from the other side of the market. However, real-world markets are often\nvery large and agents cannot always be expected to even produce a ranking of\nall options on the other side. It is therefore important to understand the\nimpact of incomplete or truncated lists on the quality of the resultant\nmatching.\n  In this paper, we focus on the Serial Dictatorship mechanism in a model where\neach agent of the proposing side (students) has a random preference list of\nlength $d$, sampled independently and uniformly at random from $n$ schools,\neach of which has one seat. Our main result shows that if the students\nprimarily care about being matched to any school of their list (as opposed to\nending up unmatched), then all students in position $i\\leq n$ will prefer\nmarkets with longer lists, when $n$ is large enough. Schools on the other hand\nwill always prefer longer lists in our model. We moreover investigate the\nimpact of $d$ on the rank of the school that a student gets matched to.\n  Our main result suggests that markets that are well-approximated by our\nhypothesis and where the demand of schools does not exceed supply should be\ndesigned with preference lists as long as reasonable, since longer lists would\nfavor all agents.",
      "url": "http://arxiv.org/abs/2506.06217v1",
      "published_time_eastern_timestamp": 1749227069.0
    },
    {
      "title": "Can Theoretical Physics Research Benefit from Language Agents?",
      "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.",
      "url": "http://arxiv.org/abs/2506.06214v1",
      "published_time_eastern_timestamp": 1749226806.0
    },
    {
      "title": "A Theoretical Study of (Hyper) Self-Attention through the Lens of\n  Interactions: Representation, Training, Generalization",
      "summary": "Self-attention has emerged as a core component of modern neural\narchitectures, yet its theoretical underpinnings remain elusive. In this paper,\nwe study self-attention through the lens of interacting entities, ranging from\nagents in multi-agent reinforcement learning to alleles in genetic sequences,\nand show that a single layer linear self-attention can efficiently represent,\nlearn, and generalize functions capturing pairwise interactions, including\nout-of-distribution scenarios. Our analysis reveals that self-attention acts as\na mutual interaction learner under minimal assumptions on the diversity of\ninteraction patterns observed during training, thereby encompassing a wide\nvariety of real-world domains. In addition, we validate our theoretical\ninsights through experiments demonstrating that self-attention learns\ninteraction functions and generalizes across both population distributions and\nout-of-distribution scenarios. Building on our theories, we introduce\nHyperFeatureAttention, a novel neural network module designed to learn\ncouplings of different feature-level interactions between entities.\nFurthermore, we propose HyperAttention, a new module that extends beyond\npairwise interactions to capture multi-entity dependencies, such as three-way,\nfour-way, or general n-way interactions.",
      "url": "http://arxiv.org/abs/2506.06179v1",
      "published_time_eastern_timestamp": 1749224650.0
    },
    {
      "title": "Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with\n  a Multi-Agent Approach",
      "summary": "Large language models can translate natural-language chart descriptions into\nrunnable code, yet approximately 15\\% of the generated scripts still fail to\nexecute, even after supervised fine-tuning and reinforcement learning. We\ninvestigate whether this persistent error rate stems from model limitations or\nfrom reliance on a single-prompt design. To explore this, we propose a\nlightweight multi-agent pipeline that separates drafting, execution, repair,\nand judgment, using only an off-the-shelf GPT-4o-mini model. On the\n\\textsc{Text2Chart31} benchmark, our system reduces execution errors to 4.5\\%\nwithin three repair iterations, outperforming the strongest fine-tuned baseline\nby nearly 5 percentage points while requiring significantly less compute.\nSimilar performance is observed on the \\textsc{ChartX} benchmark, with an error\nrate of 4.6\\%, demonstrating strong generalization. Under current benchmarks,\nexecution success appears largely solved. However, manual review reveals that 6\nout of 100 sampled charts contain hallucinations, and an LLM-based\naccessibility audit shows that only 33.3\\% (\\textsc{Text2Chart31}) and 7.2\\%\n(\\textsc{ChartX}) of generated charts satisfy basic colorblindness guidelines.\nThese findings suggest that future work should shift focus from execution\nreliability toward improving chart aesthetics, semantic fidelity, and\naccessibility.",
      "url": "http://arxiv.org/abs/2506.06175v1",
      "published_time_eastern_timestamp": 1749224357.0
    },
    {
      "title": "The Lock-in Hypothesis: Stagnation by Algorithm",
      "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com",
      "url": "http://arxiv.org/abs/2506.06166v1",
      "published_time_eastern_timestamp": 1749223891.0
    },
    {
      "title": "(AI peers) are people learning from the same standpoint: Perception of\n  AI characters in a Collaborative Science Investigation",
      "summary": "While the complexity of 21st-century demands has promoted pedagogical\napproaches to foster complex competencies, a persistent gap remains between\nin-class learning activities and individualized learning or assessment\npractices. To address this, studies have explored the use of AI-generated\ncharacters in learning and assessment. One attempt is scenario-based assessment\n(SBA), a technique that not only measures but also fosters the development of\ncompetencies throughout the assessment process. SBA introduces simulated agents\nto provide an authentic social-interactional context, allowing for the\nassessment of competency-based constructs while mitigating the unpredictability\nof real-life interactions. Recent advancements in multimodal AI, such as\ntext-to-video technology, allow these agents to be enhanced into AI-generated\ncharacters. This mixed-method study investigates how learners perceive AI\ncharacters taking the role of mentor and teammates in an SBA mirroring the\ncontext of a collaborative science investigation. Specifically, we examined the\nLikert scale responses of 56 high schoolers regarding trust, social presence,\nand effectiveness. We analyzed the relationships between these factors and\ntheir impact on the intention to adopt AI characters through PLS-SEM. Our\nfindings indicated that learners' trust shaped their sense of social presence\nwith the AI characters, enhancing perceived effectiveness. Qualitative analysis\nfurther highlighted factors that foster trust, such as material credibility and\nalignment with learning goals, as well as the pivotal role of social presence\nin creating a collaborative context.\n  This paper was accepted as an full paper for AIED 2025.",
      "url": "http://arxiv.org/abs/2506.06165v1",
      "published_time_eastern_timestamp": 1749223751.0
    },
    {
      "title": "Personalized Large Language Models Can Increase the Belief Accuracy of\n  Social Networks",
      "summary": "Large language models (LLMs) are increasingly involved in shaping public\nunderstanding on contested issues. This has led to substantial discussion about\nthe potential of LLMs to reinforce or correct misperceptions. While existing\nliterature documents the impact of LLMs on individuals' beliefs, limited work\nexplores how LLMs affect social networks. We address this gap with a\npre-registered experiment (N = 1265) around the 2024 US presidential election,\nwhere we empirically explore the impact of personalized LLMs on belief accuracy\nin the context of social networks. The LLMs are constructed to be personalized,\noffering messages tailored to individuals' profiles, and to have guardrails for\naccurate information retrieval. We find that the presence of a personalized LLM\nleads individuals to update their beliefs towards the truth. More importantly,\nindividuals with a personalized LLM in their social network not only choose to\nfollow it, indicating they would like to obtain information from it in\nsubsequent interactions, but also construct subsequent social networks to\ninclude other individuals with beliefs similar to the LLM -- in this case, more\naccurate beliefs. Therefore, our results show that LLMs have the capacity to\ninfluence individual beliefs and the social networks in which people exist, and\nhighlight the potential of LLMs to act as corrective agents in online\nenvironments. Our findings can inform future strategies for responsible\nAI-mediated communication.",
      "url": "http://arxiv.org/abs/2506.06153v1",
      "published_time_eastern_timestamp": 1749222997.0
    },
    {
      "title": "CCLSTM: Coupled Convolutional Long-Short Term Memory Network for\n  Occupancy Flow Forecasting",
      "summary": "Predicting future states of dynamic agents is a fundamental task in\nautonomous driving. An expressive representation for this purpose is Occupancy\nFlow Fields, which provide a scalable and unified format for modeling motion,\nspatial extent, and multi-modal future distributions. While recent methods have\nachieved strong results using this representation, they often depend on\nhigh-quality vectorized inputs, which are unavailable or difficult to generate\nin practice, and the use of transformer-based architectures, which are\ncomputationally intensive and costly to deploy. To address these issues, we\npropose \\textbf{Coupled Convolutional LSTM (CCLSTM)}, a lightweight, end-to-end\ntrainable architecture based solely on convolutional operations. Without\nrelying on vectorized inputs or self-attention mechanisms, CCLSTM effectively\ncaptures temporal dynamics and spatial occupancy-flow correlations using a\ncompact recurrent convolutional structure. Despite its simplicity, CCLSTM\nachieves state-of-the-art performance on occupancy flow metrics and, as of this\nsubmission, ranks \\(1^{\\text{st}}\\) in all metrics on the 2024 Waymo Occupancy\nand Flow Prediction Challenge leaderboard.",
      "url": "http://arxiv.org/abs/2506.06128v1",
      "published_time_eastern_timestamp": 1749220735.0
    },
    {
      "title": "Reinforcement Learning Optimization for Large-Scale Learning: An\n  Efficient and User-Friendly Scaling Library",
      "summary": "We introduce ROLL, an efficient, scalable, and user-friendly library designed\nfor Reinforcement Learning Optimization for Large-scale Learning. ROLL caters\nto three primary user groups: tech pioneers aiming for cost-effective,\nfault-tolerant large-scale training, developers requiring flexible control over\ntraining workflows, and researchers seeking agile experimentation. ROLL is\nbuilt upon several key modules to serve these user groups effectively. First, a\nsingle-controller architecture combined with an abstraction of the parallel\nworker simplifies the development of the training pipeline. Second, the\nparallel strategy and data transfer modules enable efficient and scalable\ntraining. Third, the rollout scheduler offers fine-grained management of each\nsample's lifecycle during the rollout stage. Fourth, the environment worker and\nreward worker support rapid and flexible experimentation with agentic RL\nalgorithms and reward designs. Finally, AutoDeviceMapping allows users to\nassign resources to different models flexibly across various stages.",
      "url": "http://arxiv.org/abs/2506.06122v1",
      "published_time_eastern_timestamp": 1749220436.0
    },
    {
      "title": "VideoChat-A1: Thinking with Long Videos by Chain-of-Shot Reasoning",
      "summary": "The recent advance in video understanding has been driven by multimodal large\nlanguage models (MLLMs). But these MLLMs are good at analyzing short videos,\nwhile suffering from difficulties in understanding videos with a longer\ncontext. To address this difficulty, several agent paradigms have recently been\nproposed, using MLLMs as agents for retrieving extra contextual knowledge in a\nlong video. However, most existing agents ignore the key fact that a long video\nis composed with multiple shots, i.e., to answer the user question from a long\nvideo, it is critical to deeply understand its relevant shots like human.\nWithout such insight, these agents often mistakenly find redundant even noisy\ntemporal context, restricting their capacity for long video understanding. To\nfill this gap, we propose VideoChat-A1, a novel long video agent paradigm.\nDifferent from the previous works, our VideoChat-A1 can deeply think with long\nvideos, via a distinct chain-of-shot reasoning paradigm. More specifically, it\ncan progressively select the relevant shots of user question, and look into\nthese shots in a coarse-to-fine partition. By multi-modal reasoning along the\nshot chain, VideoChat-A1 can effectively mimic step-by-step human thinking\nprocess, allowing to interactively discover preferable temporal context for\nthoughtful understanding in long videos. Extensive experiments show that, our\nVideoChat-A1 achieves the state-of-the-art performance on the mainstream long\nvideo QA benchmarks, e.g., it achieves 77.0 on VideoMME and 70.1 on EgoSchema,\noutperforming its strong baselines (e.g., Intern2.5VL-8B and\nInternVideo2.5-8B), by up to 10.8\\% and 6.2\\%. Compared to leading close-source\nGPT-4o and Gemini 1.5 Pro, VideoChat-A1 offers competitive accuracy, but with\n7\\% input frames and 12\\% inference time on average.",
      "url": "http://arxiv.org/abs/2506.06097v1",
      "published_time_eastern_timestamp": 1749218311.0
    },
    {
      "title": "On-board Mission Replanning for Adaptive Cooperative Multi-Robot Systems",
      "summary": "Cooperative autonomous robotic systems have significant potential for\nexecuting complex multi-task missions across space, air, ground, and maritime\ndomains. But they commonly operate in remote, dynamic and hazardous\nenvironments, requiring rapid in-mission adaptation without reliance on fragile\nor slow communication links to centralised compute. Fast, on-board replanning\nalgorithms are therefore needed to enhance resilience. Reinforcement Learning\nshows strong promise for efficiently solving mission planning tasks when\nformulated as Travelling Salesperson Problems (TSPs), but existing methods: 1)\nare unsuitable for replanning, where agents do not start at a single location;\n2) do not allow cooperation between agents; 3) are unable to model tasks with\nvariable durations; or 4) lack practical considerations for on-board\ndeployment. Here we define the Cooperative Mission Replanning Problem as a\nnovel variant of multiple TSP with adaptations to overcome these issues, and\ndevelop a new encoder/decoder-based model using Graph Attention Networks and\nAttention Models to solve it effectively and efficiently. Using a simple\nexample of cooperative drones, we show our replanner consistently (90% of the\ntime) maintains performance within 10% of the state-of-the-art LKH3 heuristic\nsolver, whilst running 85-370 times faster on a Raspberry Pi. This work paves\nthe way for increased resilience in autonomous multi-agent systems.",
      "url": "http://arxiv.org/abs/2506.06094v1",
      "published_time_eastern_timestamp": 1749218059.0
    },
    {
      "title": "Self driving algorithm for an active four wheel drive racecar",
      "summary": "Controlling autonomous vehicles at their handling limits is a significant\nchallenge, particularly for electric vehicles with active four wheel drive\n(A4WD) systems offering independent wheel torque control. While traditional\nVehicle Dynamics Control (VDC) methods use complex physics-based models, this\nstudy explores Deep Reinforcement Learning (DRL) to develop a unified,\nhigh-performance controller. We employ the Proximal Policy Optimization (PPO)\nalgorithm to train an agent for optimal lap times in a simulated racecar\n(TORCS) at the tire grip limit. Critically, the agent learns an end-to-end\npolicy that directly maps vehicle states, like velocities, accelerations, and\nyaw rate, to a steering angle command and independent torque commands for each\nof the four wheels. This formulation bypasses conventional pedal inputs and\nexplicit torque vectoring algorithms, allowing the agent to implicitly learn\nthe A4WD control logic needed for maximizing performance and stability.\nSimulation results demonstrate the RL agent learns sophisticated strategies,\ndynamically optimizing wheel torque distribution corner-by-corner to enhance\nhandling and mitigate the vehicle's inherent understeer. The learned behaviors\nmimic and, in aspects of grip utilization, potentially surpass traditional\nphysics-based A4WD controllers while achieving competitive lap times. This\nresearch underscores DRL's potential to create adaptive control systems for\ncomplex vehicle dynamics, suggesting RL is a potent alternative for advancing\nautonomous driving in demanding, grip-limited scenarios for racing and road\nsafety.",
      "url": "http://arxiv.org/abs/2506.06077v1",
      "published_time_eastern_timestamp": 1749216795.0
    },
    {
      "title": "Conversational Interfaces for Parametric Conceptual Architectural\n  Design: Integrating Mixed Reality with LLM-driven Interaction",
      "summary": "Mixed reality (MR) environments offer embodied spatial interaction, providing\nintuitive 3D manipulation capabilities that enhance the conceptual design\nprocess. Parametric modeling, a powerful and advanced architectural design\nmethod, enables the generation of complex, optimized geometries. However, its\nintegration into MR environments remains limited due to precision constraints\nand unsuitable input modalities. Existing MR tools prioritize spatial\ninteraction but lack the control and expressiveness required for parametric\nworkflows, particularly for designers without formal programming backgrounds.\nWe address this gap by introducing a novel conversational MR interface that\ncombines speech input, gesture recognition, and a multi-agent large language\nmodel (LLM) system to support intuitive parametric modeling. Our system\ndynamically manages parameter states, resolves ambiguous commands through\nconversation and contextual prompting, and enables real-time model manipulation\nwithin immersive environments. We demonstrate how this approach reduces\ncognitive and operational barriers in early-stage design tasks, allowing users\nto refine and explore their design space. This work expands the role of MR to a\ngenerative design platform, supporting programmatic thinking in design tasks\nthrough natural, embodied interaction.",
      "url": "http://arxiv.org/abs/2506.06066v1",
      "published_time_eastern_timestamp": 1749216030.0
    },
    {
      "title": "Modeling human reputation-seeking behavior in a spatio-temporally\n  complex public good provision game",
      "summary": "Multi-agent reinforcement learning algorithms are useful for simulating\nsocial behavior in settings that are too complex for other theoretical\napproaches like game theory. However, they have not yet been empirically\nsupported by laboratory experiments with real human participants. In this work\nwe demonstrate how multi-agent reinforcement learning can model group behavior\nin a spatially and temporally complex public good provision game called Clean\nUp. We show that human groups succeed in Clean Up when they can see who is who\nand track reputations over time but fail under conditions of anonymity. A new\nmulti-agent reinforcement learning model of reputation-based cooperation\ndemonstrates the same difference between identifiable and anonymous conditions.\nFurthermore, both human groups and artificial agent groups solve the problem\nvia turn-taking despite other options being available. Our results highlight\nthe benefits of using multi-agent reinforcement learning to model human social\nbehavior in complex environments.",
      "url": "http://arxiv.org/abs/2506.06032v1",
      "published_time_eastern_timestamp": 1749212793.0
    },
    {
      "title": "When to Trust Context: Self-Reflective Debates for Context Reliability",
      "summary": "Large language models frequently encounter conflicts between their parametric\nknowledge and contextual input, often resulting in factual inconsistencies or\nhallucinations. We propose Self-Reflective Debate for Contextual Reliability\n(SR-DCR), a lightweight framework that integrates token-level self-confidence\nwith an asymmetric multi-agent debate to adjudicate such conflicts. A critic,\ndeprived of context, challenges a defender who argues from the given passage; a\njudge model evaluates the debate and determines the context's reliability. The\nfinal answer is selected by combining the verdict with model confidence.\nExperiments on the ClashEval benchmark demonstrate that SR-DCR consistently\nenhances robustness to misleading context while maintaining accuracy on\ntrustworthy inputs, outperforming both classical debate and confidence-only\nbaselines with minimal computational overhead. The code is available at\nhttps://github.com/smiles724/Self-Reflective-Debates.",
      "url": "http://arxiv.org/abs/2506.06020v1",
      "published_time_eastern_timestamp": 1749211774.0
    },
    {
      "title": "AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical\n  Search",
      "summary": "Large language model (LLM) agents have demonstrated strong capabilities\nacross diverse domains. However, designing high-performing agentic systems\nremains challenging. Existing agent search methods suffer from three major\nlimitations: (1) an emphasis on optimizing agentic workflows while\nunder-utilizing proven human-designed components such as memory, planning, and\ntool use; (2) high evaluation costs, as each newly generated agent must be\nfully evaluated on benchmarks; and (3) inefficient search in large search\nspace. In this work, we introduce a comprehensive framework to address these\nchallenges. First, We propose a hierarchical search space that jointly models\nagentic workflow and composable functional components, enabling richer agentic\nsystem designs. Building on this structured design space, we introduce a\npredictive value model that estimates agent performance given agentic system\nand task description, allowing for efficient, low-cost evaluation during the\nsearch process. Finally, we present a hierarchical Monte Carlo Tree Search\n(MCTS) strategy informed by uncertainty to guide the search. Experiments on\nseven benchmarks, covering embodied, math, web, tool, and game, show that our\nmethod achieves an average performance gain of 8.34\\% over state-of-the-art\nbaselines and exhibits faster search progress with steeper improvement\ntrajectories. Code repo is available at\nhttps://github.com/Ericccc02/AgentSwift.",
      "url": "http://arxiv.org/abs/2506.06017v1",
      "published_time_eastern_timestamp": 1749211643.0
    },
    {
      "title": "Propose or Vote: A simple Democratic Procedure",
      "summary": "This paper introduces a simple democratic procedure. In a first stage, all\nmembers of a polity decide whether to apply for proposal-making or later vote\non proposals made in the second stage. This procedure is called Propose or Vote\n(PoV). With appropriate default points and majority voting over two randomly\nselected proposals, the PoV procedure can implement the Condorcet winner with\nonly one round of voting if a Condorcet winner exists. We explore ways to\nestablish uniqueness, alternative voting procedures over the selected\nalternatives, and the application to elections. In the latter case, agents can\ndecide whether to stand for election to an office or to vote on the set of\ncandidates.",
      "url": "http://arxiv.org/abs/2506.05998v1",
      "published_time_eastern_timestamp": 1749209771.0
    },
    {
      "title": "Dynamic Mixture of Progressive Parameter-Efficient Expert Library for\n  Lifelong Robot Learning",
      "summary": "A generalist agent must continuously learn and adapt throughout its lifetime,\nachieving efficient forward transfer while minimizing catastrophic forgetting.\nPrevious work within the dominant pretrain-then-finetune paradigm has explored\nparameter-efficient fine-tuning for single-task adaptation, effectively\nsteering a frozen pretrained model with a small number of parameters. However,\nin the context of lifelong learning, these methods rely on the impractical\nassumption of a test-time task identifier and restrict knowledge sharing among\nisolated adapters. To address these limitations, we propose Dynamic Mixture of\nProgressive Parameter-Efficient Expert Library (DMPEL) for lifelong robot\nlearning. DMPEL progressively learn a low-rank expert library and employs a\nlightweight router to dynamically combine experts into an end-to-end policy,\nfacilitating flexible behavior during lifelong adaptation. Moreover, by\nleveraging the modular structure of the fine-tuned parameters, we introduce\ncoefficient replay to guide the router in accurately retrieving frozen experts\nfor previously encountered tasks, thereby mitigating catastrophic forgetting.\nThis method is significantly more storage- and computationally-efficient than\napplying demonstration replay to the entire policy. Extensive experiments on\nthe lifelong manipulation benchmark LIBERO demonstrate that our framework\noutperforms state-of-the-art lifelong learning methods in success rates across\ncontinual adaptation, while utilizing minimal trainable parameters and storage.",
      "url": "http://arxiv.org/abs/2506.05985v1",
      "published_time_eastern_timestamp": 1749208384.0
    },
    {
      "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
      "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
      "url": "http://arxiv.org/abs/2506.05982v1",
      "published_time_eastern_timestamp": 1749207721.0
    }
  ]
}
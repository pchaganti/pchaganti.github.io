{
  "last_updated": "2025-09-24T20:54:24.960914-04:00",
  "papers": [
    {
      "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
      "summary": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io",
      "url": "http://arxiv.org/abs/2509.19301v1",
      "published_time_eastern_timestamp": 1758650386.0
    },
    {
      "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold\n  Exploration",
      "summary": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE",
      "url": "http://arxiv.org/abs/2509.19292v1",
      "published_time_eastern_timestamp": 1758650087.0
    },
    {
      "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and\n  Expertise Orchestration for Effective and Efficient Collaboration",
      "summary": "Proper initialization is crucial for any system, particularly in multi-agent\nsystems (MAS), where it plays a pivotal role in determining both the system's\nefficiency and effectiveness. However, existing MAS initialization methods do\nnot fully account for the collaborative needs of the generated agents in\nsubsequent stages. Inspired by the principles of effective team composition, we\npropose AgentInit, which aims to optimize the structure of agent teams.\nSpecifically, in addition to multi-round interactions and reflections between\nagents during agent generation, AgentInit incorporates a Natural Language to\nFormat mechanism to ensure consistency and standardization. Balanced team\nselection strategies using Pareto principles are subsequently applied to\njointly consider agent team diversity and task relevance to promote effective\nand efficient collaboration and enhance overall system performance. Experiments\nshow that AgentInit consistently outperforms state-of-the-art initialization\nmethods and pre-defined strategies across various frameworks and tasks,\nachieving an overall performance improvement of up to 1.2 and 1.6,\nrespectively, while also significantly reducing token consumption. Further\nanalysis confirms its strong transferability to similar tasks and verifies the\neffectiveness of its key components, demonstrating its capability and\nadaptability as a reliable MAS initialization method. Source code and models\nare available at https://github.com/1737423697/AgentInit.",
      "url": "http://arxiv.org/abs/2509.19236v1",
      "published_time_eastern_timestamp": 1758646734.0
    },
    {
      "title": "Stability and Generalization of Adversarial Diffusion Training",
      "summary": "Algorithmic stability is an established tool for analyzing generalization.\nWhile adversarial training enhances model robustness, it often suffers from\nrobust overfitting and an enlarged generalization gap. Although recent work has\nestablished the convergence of adversarial training in decentralized networks,\nits generalization properties remain unexplored. This work presents a\nstability-based generalization analysis of adversarial training under the\ndiffusion strategy for convex losses. We derive a bound showing that the\ngeneralization error grows with both the adversarial perturbation strength and\nthe number of training steps, a finding consistent with single-agent case but\nnovel for decentralized settings. Numerical experiments on logistic regression\nvalidate these theoretical predictions.",
      "url": "http://arxiv.org/abs/2509.19234v1",
      "published_time_eastern_timestamp": 1758646530.0
    },
    {
      "title": "Online Process Reward Leanring for Agentic Reinforcement Learning",
      "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments. However, sparse and sometimes unverifiable rewards\nmake temporal credit assignment extremely challenging. Recent work attempts to\nintegrate process supervision into agent learning but suffers from biased\nannotation, reward hacking, high-variance from overly fine-grained signals or\nfailtures when state overlap is rare. We therefore introduce Online Process\nReward Learning (OPRL), a general credit-assignment strategy for agentic RL\nthat integrates seamlessly with standard on-policy algorithms without relying\non additional rollouts or explicit step labels. In OPRL, we optimize an\nimplicit process reward model (PRM) alternately with the agent's policy to\ntransform trajectory preferences into implicit step rewards through a\ntrajectory-based DPO objective. These step rewards are then used to compute\nstep-level advantages, which are combined with episode-level advantages from\noutcome rewards for policy update, creating a self-reinforcing loop.\nTheoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training. Empirically, we evaluate\nOPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as\nwell as open-ended social interactions with unverfiable rewards in SOTOPIA.\nCrucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training. Further analysis also\ndemonstrates the efficient exploration by OPRL using fewer actions,\nunderscoring its potential for agentic learning in real-world scenarios.",
      "url": "http://arxiv.org/abs/2509.19199v2",
      "published_time_eastern_timestamp": 1758644142.0
    },
    {
      "title": "An Empirical Study of Testing Practices in Open Source AI Agent\n  Frameworks and Agentic Applications",
      "summary": "Foundation model (FM)-based AI agents are rapidly gaining adoption across\ndiverse domains, but their inherent non-determinism and non-reproducibility\npose testing and quality assurance challenges. While recent benchmarks provide\ntask-level evaluations, there is limited understanding of how developers verify\nthe internal correctness of these agents during development.\n  To address this gap, we conduct the first large-scale empirical study of\ntesting practices in the AI agent ecosystem, analyzing 39 open-source agent\nframeworks and 439 agentic applications. We identify ten distinct testing\npatterns and find that novel, agent-specific methods like DeepEval are seldom\nused (around 1%), while traditional patterns like negative and membership\ntesting are widely adapted to manage FM uncertainty. By mapping these patterns\nto canonical architectural components of agent frameworks and agentic\napplications, we uncover a fundamental inversion of testing effort:\ndeterministic components like Resource Artifacts (tools) and Coordination\nArtifacts (workflows) consume over 70% of testing effort, while the FM-based\nPlan Body receives less than 5%. Crucially, this reveals a critical blind spot,\nas the Trigger component (prompts) remains neglected, appearing in around 1% of\nall tests.\n  Our findings offer the first empirical testing baseline in FM-based agent\nframeworks and agentic applications, revealing a rational but incomplete\nadaptation to non-determinism. To address it, framework developers should\nimprove support for novel testing methods, application developers must adopt\nprompt regression testing, and researchers should explore barriers to adoption.\nStrengthening these practices is vital for building more robust and dependable\nAI agents.",
      "url": "http://arxiv.org/abs/2509.19185v2",
      "published_time_eastern_timestamp": 1758643329.0
    },
    {
      "title": "YAC: Bridging Natural Language and Interactive Visual Exploration with\n  Generative AI for Biomedical Data Discovery",
      "summary": "Incorporating natural language input has the potential to improve the\ncapabilities of biomedical data discovery interfaces. However, user interface\nelements and visualizations are still powerful tools for interacting with data,\neven in the new world of generative AI. In our prototype system, YAC, Yet\nAnother Chatbot, we bridge the gap between natural language and interactive\nvisualizations by generating structured declarative output with a multi-agent\nsystem and interpreting that output to render linked interactive visualizations\nand apply data filters. Furthermore, we include widgets, which allow users to\nadjust the values of that structured output through user interface elements. We\nreflect on the capabilities and design of this system with an analysis of its\ntechnical dimensions and illustrate the capabilities through four usage\nscenarios.",
      "url": "http://arxiv.org/abs/2509.19182v1",
      "published_time_eastern_timestamp": 1758643062.0
    },
    {
      "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot\n  Coordination",
      "summary": "In this paper, we present a receding-horizon, sampling-based planner capable\nof reasoning over multimodal policy distributions. By using the cross-entropy\nmethod to optimize a multimodal policy under a common cost function, our\napproach increases robustness against local minima and promotes effective\nexploration of the solution space. We show that our approach naturally extends\nto multi-robot collision-free planning, enables agents to share diverse\ncandidate policies to avoid deadlocks, and allows teams to minimize a global\nobjective without incurring the computational complexity of centralized\noptimization. Numerical simulations demonstrate that employing multiple modes\nsignificantly improves success rates in trap environments and in multi-robot\ncollision avoidance. Hardware experiments further validate the approach's\nreal-time feasibility and practical performance.",
      "url": "http://arxiv.org/abs/2509.19168v1",
      "published_time_eastern_timestamp": 1758642198.0
    },
    {
      "title": "A Scoping Review of Mixed Initiative Visual Analytics in the Automation\n  Renaissance",
      "summary": "Artificial agents are increasingly integrated into data analysis workflows,\ncarrying out tasks that were primarily done by humans. Our research explores\nhow the introduction of automation re-calibrates the dynamic between humans and\nautomating technology. To explore this question, we conducted a scoping review\nencompassing twenty years of mixed-initiative visual analytic systems. To\ndescribe and contrast the relationship between humans and automation, we\ndeveloped an integrated taxonomy to delineate the objectives of these\nmixed-initiative visual analytics tools, how much automation they support, and\nthe assumed roles of humans. Here, we describe our qualitative approach of\nintegrating existing theoretical frameworks with new codes we developed. Our\nanalysis shows that the visualization research literature lacks consensus on\nthe definition of mixed-initiative systems and explores a limited potential of\nthe collaborative interaction landscape between people and automation. Our\nresearch provides a scaffold to advance the discussion of human-AI\ncollaboration during visual data analysis.",
      "url": "http://arxiv.org/abs/2509.19152v1",
      "published_time_eastern_timestamp": 1758641434.0
    },
    {
      "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases\n  Written in Natural Language",
      "summary": "The use of natural language (NL) test cases for validating graphical user\ninterface (GUI) applications is emerging as a promising direction to manually\nwritten executable test scripts, which are costly to develop and difficult to\nmaintain. Recent advances in large language models (LLMs) have opened the\npossibility of the direct execution of NL test cases by LLM agents. This paper\ninvestigates this direction, focusing on the impact on NL test case unsoundness\nand on test case execution consistency. NL test cases are inherently unsound,\nas they may yield false failures due to ambiguous instructions or unpredictable\nagent behaviour. Furthermore, repeated executions of the same NL test case may\nlead to inconsistent outcomes, undermining test reliability. To address these\nchallenges, we propose an algorithm for executing NL test cases with guardrail\nmechanisms and specialised agents that dynamically verify the correct execution\nof each test step. We introduce measures to evaluate the capabilities of LLMs\nin test execution and one measure to quantify execution consistency. We propose\na definition of weak unsoundness to characterise contexts in which NL test case\nexecution remains acceptable, with respect to the industrial quality levels Six\nSigma. Our experimental evaluation with eight publicly available LLMs, ranging\nfrom 3B to 70B parameters, demonstrates both the potential and current\nlimitations of current LLM agents for GUI testing. Our experiments show that\nMeta Llama 3.1 70B demonstrates acceptable capabilities in NL test case\nexecution with high execution consistency (above the level 3-sigma). We provide\nprototype tools, test suites, and results.",
      "url": "http://arxiv.org/abs/2509.19136v1",
      "published_time_eastern_timestamp": 1758640840.0
    },
    {
      "title": "Algorithms for Adversarially Robust Deep Learning",
      "summary": "Given the widespread use of deep learning models in safety-critical\napplications, ensuring that the decisions of such models are robust against\nadversarial exploitation is of fundamental importance. In this thesis, we\ndiscuss recent progress toward designing algorithms that exhibit desirable\nrobustness properties. First, we discuss the problem of adversarial examples in\ncomputer vision, for which we introduce new technical results, training\nparadigms, and certification algorithms. Next, we consider the problem of\ndomain generalization, wherein the task is to train neural networks to\ngeneralize from a family of training distributions to unseen test\ndistributions. We present new algorithms that achieve state-of-the-art\ngeneralization in medical imaging, molecular identification, and image\nclassification. Finally, we study the setting of jailbreaking large language\nmodels (LLMs), wherein an adversarial user attempts to design prompts that\nelicit objectionable content from an LLM. We propose new attacks and defenses,\nwhich represent the frontier of progress toward designing robust language-based\nagents.",
      "url": "http://arxiv.org/abs/2509.19100v1",
      "published_time_eastern_timestamp": 1758638938.0
    },
    {
      "title": "MAPPO for Edge Server Monitoring",
      "summary": "In this paper, we consider a goal-oriented communication problem for edge\nserver monitoring, where jobs arrive intermittently at multiple dispatchers and\nmust be assigned to shared edge servers with finite queues and time-varying\navailability. Accurate knowledge of server status is critical for sustaining\nhigh throughput, yet remains challenging under dynamic workloads and partial\nobservability. To address this challenge, each dispatcher maintains server\nknowledge through two complementary mechanisms: (i) active status queries that\nprovide instantaneous updates at a communication cost, and (ii) job execution\nfeedback that reveals server conditions opportunistically. We formulate a\ncooperative multi-agent distributed decision-making problem in which\ndispatchers jointly optimize query scheduling to balance throughput against\ncommunication overhead. To solve this problem, we propose a Multi-Agent\nProximal Policy Optimization (MAPPO)-based algorithm that leverages centralized\ntraining with decentralized execution (CTDE) to learn distributed\nquery-and-dispatch policies under partial and stale observations. Numerical\nevaluations show that MAPPO achieves superior throughput-cost tradeoffs and\nsignificantly outperforms baseline strategies, achieving on average a 30%\nimprovement over the closest baseline.",
      "url": "http://arxiv.org/abs/2509.19079v1",
      "published_time_eastern_timestamp": 1758638278.0
    },
    {
      "title": "Code Driven Planning with Domain-Adaptive Critic",
      "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.",
      "url": "http://arxiv.org/abs/2509.19077v1",
      "published_time_eastern_timestamp": 1758638172.0
    },
    {
      "title": "Position: Human-Robot Interaction in Embodied Intelligence Demands a\n  Shift From Static Privacy Controls to Dynamic Learning",
      "summary": "The reasoning capabilities of embodied agents introduce a critical,\nunder-explored inferential privacy challenge, where the risk of an agent\ngenerate sensitive conclusions from ambient data. This capability creates a\nfundamental tension between an agent's utility and user privacy, rendering\ntraditional static controls ineffective. To address this, this position paper\nproposes a framework that reframes privacy as a dynamic learning problem\ngrounded in theory of Contextual Integrity (CI). Our approach enables agents to\nproactively learn and adapt to individual privacy norms through interaction,\noutlining a research agenda to develop embodied agents that are both capable\nand function as trustworthy safeguards of user privacy.",
      "url": "http://arxiv.org/abs/2509.19041v1",
      "published_time_eastern_timestamp": 1758636600.0
    },
    {
      "title": "Existence and Calculation of Optimal Equilibria on Overlapping\n  Generations Economies",
      "summary": "A well-known feature of overlapping generations economies is that the First\nWelfare Theorem fails and equilibrium may be inefficient. The Cass (1972)\ncriterion furnishes a necessary and sufficient condition for efficiency, but\ndoes not address the matter of existence of efficient equilibria, and Cass,\nOkuno, and Zilcha (1979) provide nonexistence examples. I develop an algorithm\nbased on successive approximations of a nonstationary, consumption-loan, prone\nto savings, overlapping generations economy with finite-lived heterogeneous\nagents to find elements of its set of equilibria as the limit of nested compact\nsets. These compact sets are the result of a backward calculation through\nequilibrium equations that departs from the set of Pareto optimal equilibria of\nwell-behaved tail economies. The equilibria calculated through this algorithm\nsatisfy the Cass (1972) criterion and are used to derive the existence results\non efficient equilibria.",
      "url": "http://arxiv.org/abs/2509.19019v1",
      "published_time_eastern_timestamp": 1758635895.0
    },
    {
      "title": "Fully Learnable Neural Reward Machines",
      "summary": "Non-Markovian Reinforcement Learning (RL) tasks present significant\nchallenges, as agents must reason over entire trajectories of state-action\npairs to make optimal decisions. A common strategy to address this is through\nsymbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which\nprovide a structured way to express temporally extended objectives. However,\nthese approaches often rely on restrictive assumptions -- such as the\navailability of a predefined Symbol Grounding (SG) function mapping raw\nobservations to high-level symbolic representations, or prior knowledge of the\ntemporal task. In this work, we propose a fully learnable version of Neural\nReward Machines (NRM), which can learn both the SG function and the automaton\nend-to-end, removing any reliance on prior knowledge. Our approach is therefore\nas easily applicable as classic deep RL (DRL) approaches, while being far more\nexplainable, because of the finite and compact nature of automata. Furthermore,\nwe show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL,\nour method outperforms previous approaches based on Recurrent Neural Networks\n(RNNs).",
      "url": "http://arxiv.org/abs/2509.19017v1",
      "published_time_eastern_timestamp": 1758635833.0
    },
    {
      "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
      "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.",
      "url": "http://arxiv.org/abs/2509.19012v1",
      "published_time_eastern_timestamp": 1758635632.0
    },
    {
      "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via\n  Travel Video Itinerary Reconstruction",
      "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.",
      "url": "http://arxiv.org/abs/2509.19002v1",
      "published_time_eastern_timestamp": 1758635191.0
    },
    {
      "title": "Simulating Online Social Media Conversations on Controversial Topics\n  Using AI Agents Calibrated on Real-World Data",
      "summary": "Online social networks offer a valuable lens to analyze both individual and\ncollective phenomena. Researchers often use simulators to explore controlled\nscenarios, and the integration of Large Language Models (LLMs) makes these\nsimulations more realistic by enabling agents to understand and generate\nnatural language content. In this work, we investigate the behavior of\nLLM-based agents in a simulated microblogging social network. We initialize\nagents with realistic profiles calibrated on real-world online conversations\nfrom the 2022 Italian political election and extend an existing simulator by\nintroducing mechanisms for opinion modeling. We examine how LLM agents simulate\nonline conversations, interact with others, and evolve their opinions under\ndifferent scenarios. Our results show that LLM agents generate coherent\ncontent, form connections, and build a realistic social network structure.\nHowever, their generated content displays less heterogeneity in tone and\ntoxicity compared to real data. We also find that LLM-based opinion dynamics\nevolve over time in ways similar to traditional mathematical models. Varying\nparameter configurations produces no significant changes, indicating that\nsimulations require more careful cognitive modeling at initialization to\nreplicate human behavior more faithfully. Overall, we demonstrate the potential\nof LLMs for simulating user behavior in social environments, while also\nidentifying key challenges in capturing heterogeneity and complex dynamics.",
      "url": "http://arxiv.org/abs/2509.18985v1",
      "published_time_eastern_timestamp": 1758634608.0
    },
    {
      "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy,\n  Methods, and Directions",
      "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based\nagents have emerged as powerful intelligent systems capable of human-like\ncognition, reasoning, and interaction. These agents are increasingly being\ndeployed across diverse real-world applications, including student education,\nscientific research, and financial analysis. However, despite their remarkable\npotential, LLM-based agents remain vulnerable to hallucination issues, which\ncan result in erroneous task execution and undermine the reliability of the\noverall system design. Addressing this critical challenge requires a deep\nunderstanding and a systematic consolidation of recent advances on LLM-based\nagents. To this end, we present the first comprehensive survey of\nhallucinations in LLM-based agents. By carefully analyzing the complete\nworkflow of agents, we propose a new taxonomy that identifies different types\nof agent hallucinations occurring at different stages. Furthermore, we conduct\nan in-depth examination of eighteen triggering causes underlying the emergence\nof agent hallucinations. Through a detailed review of a large number of\nexisting studies, we summarize approaches for hallucination mitigation and\ndetection, and highlight promising directions for future research. We hope this\nsurvey will inspire further efforts toward addressing hallucinations in\nLLM-based agents, ultimately contributing to the development of more robust and\nreliable agent systems.",
      "url": "http://arxiv.org/abs/2509.18970v1",
      "published_time_eastern_timestamp": 1758633888.0
    }
  ]
}
{
  "last_updated": "2025-09-23T02:17:56.689996-04:00",
  "papers": [
    {
      "title": "Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical\n  Reinforcement and Collective Learning Approach",
      "summary": "Decentralized combinatorial optimization in evolving multi-agent systems\nposes significant challenges, requiring agents to balance long-term\ndecision-making, short-term optimized collective outcomes, while preserving\nautonomy of interactive agents under unanticipated changes. Reinforcement\nlearning offers a way to model sequential decision-making through dynamic\nprogramming to anticipate future environmental changes. However, applying\nmulti-agent reinforcement learning (MARL) to decentralized combinatorial\noptimization problems remains an open challenge due to the exponential growth\nof the joint state-action space, high communication overhead, and privacy\nconcerns in centralized training. To address these limitations, this paper\nproposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel\napproach that leverages both MARL and decentralized collective learning based\non a hierarchical framework. Agents take high-level strategies using MARL to\ngroup possible plans for action space reduction and constrain the agent\nbehavior for Pareto optimality. Meanwhile, the low-level collective learning\nlayer ensures efficient and decentralized coordinated decisions among agents\nwith minimal communication. Extensive experiments in a synthetic scenario and\nreal-world smart city application models, including energy self-management and\ndrone swarm sensing, demonstrate that HRCL significantly improves performance,\nscalability, and adaptability compared to the standalone MARL and collective\nlearning approaches, achieving a win-win synthesis solution.",
      "url": "http://arxiv.org/abs/2509.18088v1",
      "published_time_eastern_timestamp": 1758563925.0
    },
    {
      "title": "Improving Large Language Models Function Calling and Interpretability\n  via Guided-Structured Templates",
      "summary": "Large language models (LLMs) have demonstrated strong reasoning and tool-use\ncapabilities, yet they often fail in real-world tool-interactions due to\nincorrect parameterization, poor tool selection, or misinterpretation of user\nintent. These issues often stem from an incomplete understanding of user goals\nand inadequate comprehension of tool documentation. While Chain-of-Thought\n(CoT) prompting has proven effective for enhancing reasoning in general\ncontexts, our analysis reveals that free-form CoT is insufficient and sometimes\ncounterproductive for structured function-calling tasks. To address this, we\nintroduce a curriculum-inspired framework that leverages structured reasoning\ntemplates to guide LLMs through more deliberate step-by-step instructions for\ngenerating function callings. Experimental results show that our method reduces\ntool-use errors, achieving 3-12% relative improvements over strong baselines\nacross diverse model series and approaches. Moreover, our framework enhances\nthe robustness, interpretability, and transparency of tool-using agents,\nadvancing the development of more reliable AI assistants for real-world\napplications.",
      "url": "http://arxiv.org/abs/2509.18076v1",
      "published_time_eastern_timestamp": 1758563714.0
    },
    {
      "title": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring\n  Commonsense Reasoning",
      "summary": "Large Language Models (LLMs) show strong reasoning abilities but rely on\ninternalized knowledge that is often insufficient, outdated, or incorrect when\ntrying to answer a question that requires specific domain knowledge. Knowledge\nGraphs (KGs) provide structured external knowledge, yet their complexity and\nmulti-hop reasoning requirements make integration challenging. We present\nARK-V1, a simple KG-agent that iteratively explores graphs to answer natural\nlanguage queries. We evaluate several not fine-tuned state-of-the art LLMs as\nbackbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and\ncommonsense reasoning over long-tail entities. ARK-V1 achieves substantially\nhigher conditional accuracies than Chain-of-Thought baselines, and larger\nbackbone models show a clear trend toward better coverage, correctness, and\nstability.",
      "url": "http://arxiv.org/abs/2509.18063v1",
      "published_time_eastern_timestamp": 1758562805.0
    },
    {
      "title": "Reinforced Generation of Combinatorial Structures: Applications to\n  Complexity Theory",
      "summary": "We explore whether techniques from AI can help discover new combinatorial\nstructures that improve provable limits on efficient algorithms. Specifically,\nwe use AlphaEvolve (an LLM coding agent) to study two settings:\n  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a\nrecent result of Kunisky and Yu to obtain near-optimal upper and (conditional)\nlower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on\nrandom 3- and 4-regular graphs. Our improved lower bounds are obtained by\nconstructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using\nAlphaEvolve. Additionally, via analytical arguments we strengthen the upper\nbounds to settle the computational hardness of these questions up to an error\nin the third decimal place.\n  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new\ninapproximability results, proving that it is NP-hard to approximate MAX-4-CUT\nand MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using\nAlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves\nupon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current\nbest gadget-based inapproximability result of $0.9853$, but falls short of\nimproving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget\nreduction from \"standard\" H{\\aa}stad-style PCPs.\n  A key technical challenge we faced: verifying a candidate construction\nproduced by AlphaEvolve is costly (often requiring exponential time). In both\nsettings above, our results were enabled by using AlphaEvolve itself to evolve\nthe verification procedure to be faster (sometimes by $10,000\\times$). We\nconclude with a discussion of norms by which to assess the assistance from AI\nin developing proofs.",
      "url": "http://arxiv.org/abs/2509.18057v1",
      "published_time_eastern_timestamp": 1758562233.0
    },
    {
      "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM\n  Societies",
      "summary": "Large Language Models (LLMs) are increasingly used for social simulation,\nwhere populations of agents are expected to reproduce human-like collective\nbehavior. However, we find that many recent studies adopt experimental designs\nthat systematically undermine the validity of their claims. From a survey of\nover 40 papers, we identify six recurring methodological flaws: agents are\noften homogeneous (Profile), interactions are absent or artificially imposed\n(Interaction), memory is discarded (Memory), prompts tightly control outcomes\n(Minimal-Control), agents can infer the experimental hypothesis (Unawareness),\nand validation relies on simplified theoretical models rather than real-world\ndata (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying\nsocial experiment in 53.1% of cases when given instructions from prior\nwork-violating the Unawareness principle. We formalize these six requirements\nas the PIMMUR principles and argue they are necessary conditions for credible\nLLM-based social simulation. To demonstrate their impact, we re-run five\nrepresentative studies using a framework that enforces PIMMUR and find that the\nreported social phenomena frequently fail to emerge under more rigorous\nconditions. Our work establishes methodological standards for LLM-based\nmulti-agent research and provides a foundation for more reliable and\nreproducible claims about \"AI societies.\"",
      "url": "http://arxiv.org/abs/2509.18052v1",
      "published_time_eastern_timestamp": 1758562049.0
    },
    {
      "title": "NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and\n  Neuro-Symbolic Reasoning",
      "summary": "Long-Form Video Question Answering (LVQA) poses challenges beyond traditional\nvisual question answering (VQA), which is often limited to static images or\nshort video clips. While current vision-language models (VLMs) perform well in\nthose settings, they struggle with complex queries in LVQA over long videos\ninvolving multi-step temporal reasoning and causality. Vanilla approaches,\nwhich sample frames uniformly and feed them to a VLM with the question, incur\nsignificant token overhead, forcing severe downsampling. As a result, the model\noften misses fine-grained visual structure, subtle event transitions, or key\ntemporal cues, ultimately leading to incorrect answers. To address these\nlimitations, recent works have explored query-adaptive frame sampling,\nhierarchical keyframe selection, and agent-based iterative querying. However,\nthese methods remain fundamentally heuristic: they lack explicit temporal\nrepresentations and cannot enforce or verify logical event relationships. As a\nresult, there are no formal guarantees that the sampled context actually\nencodes the compositional or causal logic demanded by the question. To address\nthese foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play\nneuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language\nquestion into a formal temporal logic expression, constructs a video automaton\nfrom frame-level semantic propositions, and applies model checking to\nrigorously identify video segments satisfying the question's logical\nrequirements. Only these logic-verified segments are submitted to the VLM, thus\nimproving interpretability, reducing hallucinations, and enabling compositional\nreasoning without modifying or fine-tuning the model. Experiments on\nLongVideoBench and CinePile show NeuS-QA improves performance by over 10%,\nespecially on questions involving event ordering, causality, and multi-step\ncompositional reasoning.",
      "url": "http://arxiv.org/abs/2509.18041v1",
      "published_time_eastern_timestamp": 1758561313.0
    },
    {
      "title": "ClassMind: Scaling Classroom Observation and Instructional Feedback with\n  Multimodal AI",
      "summary": "Classroom observation -- one of the most effective methods for teacher\ndevelopment -- remains limited due to high costs and a shortage of expert\ncoaches. We present ClassMind, an AI-driven classroom observation system that\nintegrates generative AI and multimodal learning to analyze classroom artifacts\n(e.g., class recordings) and deliver timely, personalized feedback aligned with\npedagogical practices. At its core is AVA-Align, an agent framework that\nanalyzes long classroom video recordings to generate temporally precise,\nbest-practice-aligned feedback to support teacher reflection and improvement.\nOur three-phase study involved participatory co-design with educators,\ndevelopment of a full-stack system, and field testing with teachers at\ndifferent stages of practice. Teachers highlighted the system's usefulness,\nease of use, and novelty, while also raising concerns about privacy and the\nrole of human judgment, motivating deeper exploration of future human--AI\ncoaching partnerships. This work illustrates how multimodal AI can scale expert\ncoaching and advance teacher development.",
      "url": "http://arxiv.org/abs/2509.18020v1",
      "published_time_eastern_timestamp": 1758560347.0
    },
    {
      "title": "Through the Lens of Human-Human Collaboration: A Configurable Research\n  Platform for Exploring Human-Agent Collaboration",
      "summary": "Intelligent systems have traditionally been designed as tools rather than\ncollaborators, often lacking critical characteristics that collaboration\npartnerships require. Recent advances in large language model (LLM) agents open\nnew opportunities for human-LLM-agent collaboration by enabling natural\ncommunication and various social and cognitive behaviors. Yet it remains\nunclear whether principles of computer-mediated collaboration established in\nHCI and CSCW persist, change, or fail when humans collaborate with LLM agents.\nTo support systematic investigations of these questions, we introduce an open\nand configurable research platform for HCI researchers. The platform's modular\ndesign allows seamless adaptation of classic CSCW experiments and manipulation\nof theory-grounded interaction controls. We demonstrate the platform's\neffectiveness and usability through two case studies: (1) re-implementing the\nclassic human-human-collaboration task Shape Factory as a between-subject\nhuman-agent-collaboration experiment with 16 participants, and (2) a\nparticipatory cognitive walkthrough with five HCI researchers to refine\nworkflows and interfaces for experiment setup and analysis.",
      "url": "http://arxiv.org/abs/2509.18008v1",
      "published_time_eastern_timestamp": 1758559628.0
    },
    {
      "title": "The STAR-XAI Protocol: An Interactive Framework for Inducing\n  Second-Order Agency in AI Agents",
      "summary": "Current Large Reasoning Models (LRMs) exhibit significant limitations in\nreliability and transparency, often showing a collapse in reasoning\ncapabilities when faced with high-complexity, long-horizon tasks. This\n\"illusion of thinking\" is frequently an artifact of non-agentic, black-box\nevaluation paradigms that fail to cultivate robust problem-solving processes.\nIn response, we introduce The STAR-XAI Protocol (Socratic, Transparent,\nAgentic, Reasoning - for eXplainable Artificial Intelligence), a novel\nmethodology for training and operating verifiably reliable AI agents. Our\nmethod reframes the human-AI interaction as a structured, Socratic dialogue,\ngoverned by an explicit and evolving rulebook, the Consciousness Transfer\nPackage (CTP). Through an interactive Gameplay Cycle that enforces ante-hoc\nstrategic justification and a state-locking Checksum that prevents error\naccumulation, the protocol transforms a powerful but opaque LRM into a\ndisciplined \"Clear Box\" agent. We demonstrate the efficacy of this method\nthrough an exhaustive 25-move case study in the complex strategic game \"Caps i\nCaps\". The agent not only solved the high-complexity puzzle but also\ndemonstrated Second-Order Agency, identifying flaws in its own\nsupervisor-approved plans and adapting its core integrity protocols mid-task.\nThe STAR-XAI Protocol offers a practical pathway to creating AI agents that are\nnot just high-performing, but also transparent, auditable, and trustworthy by\ndesign.",
      "url": "http://arxiv.org/abs/2509.17978v1",
      "published_time_eastern_timestamp": 1758558257.0
    },
    {
      "title": "On the Variational Costs of Changing Our Minds",
      "summary": "The human mind is capable of extraordinary achievements, yet it often appears\nto work against itself. It actively defends its cherished beliefs even in the\nface of contradictory evidence, conveniently interprets information to conform\nto desired narratives, and selectively searches for or avoids information to\nsuit its various purposes. Despite these behaviours deviating from common\nnormative standards for belief updating, we argue that such 'biases' are not\ninherently cognitive flaws, but rather an adaptive response to the significant\npragmatic and cognitive costs associated with revising one's beliefs. This\npaper introduces a formal framework that aims to model the influence of these\ncosts on our belief updating mechanisms.\n  We treat belief updating as a motivated variational decision, where agents\nweigh the perceived 'utility' of a belief against the informational cost\nrequired to adopt a new belief state, quantified by the Kullback-Leibler\ndivergence from the prior to the variational posterior. We perform\ncomputational experiments to demonstrate that simple instantiations of this\nresource-rational model can be used to qualitatively emulate commonplace human\nbehaviours, including confirmation bias and attitude polarisation. In doing so,\nwe suggest that this framework makes steps toward a more holistic account of\nthe motivated Bayesian mechanics of belief change and provides practical\ninsights for predicting, compensating for, and correcting deviations from\ndesired belief updating processes.",
      "url": "http://arxiv.org/abs/2509.17957v1",
      "published_time_eastern_timestamp": 1758557586.0
    },
    {
      "title": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent",
      "summary": "Recent advances in GUI agents have achieved remarkable grounding and\naction-prediction performance, yet existing models struggle with unreliable\nreward signals and limited online trajectory generation. In this paper, we\nintroduce Orcust, a framework that integrates Principle-Constrained Reward\nModeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to\nenhance reasoning reliability and data efficiency in interactive GUI tasks. We\nleverages environment-verifiable and LLM-derived principle to enforce\ninterpretable reward signals that constrain long chain-of-thought reasoning and\nrule-based feedback. OVTC spins up instrumented virtual machines to\nautonomously collect structured GUI interaction trajectories with explicit\nprocedural and structural objectives, enabling the training of a stepwise\nreward model that robustly captures human preferences and adheres to\ntask-specific constraints. Extensive experiments on standard GUI benchmarks\ncovering perceptual grounding, foundational operations, and end-to-end task\nexecution reveal that Orcust achieves state-of-the-art performance, improving\nby 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e.\nQwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the\nreasoning, adaptability and scalability of GUI agents across various\nenvironments and task complexities.",
      "url": "http://arxiv.org/abs/2509.17917v1",
      "published_time_eastern_timestamp": 1758555631.0
    },
    {
      "title": "AI, Digital Platforms, and the New Systemic Risk",
      "summary": "As artificial intelligence (AI) becomes increasingly embedded in digital,\nsocial, and institutional infrastructures, and AI and platforms are merged into\nhybrid structures, systemic risk has emerged as a critical but undertheorized\nchallenge. In this paper, we develop a rigorous framework for understanding\nsystemic risk in AI, platform, and hybrid system governance, drawing on\ninsights from finance, complex systems theory, climate change, and\ncybersecurity - domains where systemic risk has already shaped regulatory\nresponses. We argue that recent legislation, including the EU's AI Act and\nDigital Services Act (DSA), invokes systemic risk but relies on narrow or\nambiguous characterizations of this notion, sometimes reducing this risk to\nspecific capabilities present in frontier AI models, or to harms occurring in\neconomic market settings. The DSA, we show, actually does a better job at\nidentifying systemic risk than the more recent AI Act. Our framework highlights\nnovel risk pathways, including the possibility of systemic failures arising\nfrom the interaction of multiple AI agents. We identify four levels of\nAI-related systemic risk and emphasize that discrimination at scale and\nsystematic hallucinations, despite their capacity to destabilize institutions\nand fundamental rights, may not fall under current legal definitions, given the\nAI Act's focus on frontier model capabilities. We then test the DSA, the AI\nAct, and our own framework on five key examples, and propose reforms that\nbroaden systemic risk assessments, strengthen coordination between regulatory\nregimes, and explicitly incorporate collective harms.",
      "url": "http://arxiv.org/abs/2509.17878v1",
      "published_time_eastern_timestamp": 1758554063.0
    },
    {
      "title": "AEAS: Actionable Exploit Assessment System",
      "summary": "Security practitioners face growing challenges in exploit assessment, as\npublic vulnerability repositories are increasingly populated with inconsistent\nand low-quality exploit artifacts. Existing scoring systems, such as CVSS and\nEPSS, offer limited support for this task. They either rely on theoretical\nmetrics or produce opaque probability estimates without assessing whether\nusable exploit code exists. In practice, security teams often resort to manual\ntriage of exploit repositories, which is time-consuming, error-prone, and\ndifficult to scale. We present AEAS, an automated system designed to assess and\nprioritize actionable exploits through static analysis. AEAS analyzes both\nexploit code and associated documentation to extract a structured set of\nfeatures reflecting exploit availability, functionality, and setup complexity.\nIt then computes an actionability score for each exploit and produces ranked\nexploit recommendations. We evaluate AEAS on a dataset of over 5,000\nvulnerabilities derived from 600+ real-world applications frequently\nencountered by red teams. Manual validation and expert review on representative\nsubsets show that AEAS achieves a 100% top-3 success rate in recommending\nfunctional exploits and shows strong alignment with expert-validated rankings.\nThese results demonstrate the effectiveness of AEAS in supporting\nexploit-driven vulnerability prioritization.",
      "url": "http://arxiv.org/abs/2509.17832v1",
      "published_time_eastern_timestamp": 1758550984.0
    },
    {
      "title": "Role of Oxygen during Methane Oxidation on Pd$_1$/PdO$_1$@CeO$_2$\n  Surface: A Combined Density Functional Theory, Microkinetic, and Machine\n  Learning Approach",
      "summary": "This work explores the role of oxygen in industrial methane oxidation.\nOxygen, a well-known oxidizing agent, drives CH$_4$ conversion to CO$_2$ and\nH$_2$O. We report how oxygen influences oxidation on single Pd and PdO clusters\nsupported on CeO$_2$(111). Oxygen is introduced by (1) lattice O in PdO and (2)\nO$_2$ adsorption on an isolated Pd atom, forming PdO$_x$ clusters.\nDensity-functional theory (DFT) mapped multiple reaction pathways on the\nPd$_1$/PdO$_1$@CeO$_2$(111) surface; both Pd and PdO clusters were found to\nthermodynamically favour methane activation. The computed barrier for CH$_4$\nactivation is 0.63 eV on PdO$_1$@CeO$_2$(111). A single Pd atom markedly\naccelerates O$_2$ dissociation to PdO$_2$, and the presence of lattice oxygen\nlowers this barrier by 0.36 eV relative to an oxygen-deficient surface,\nenhancing catalytic efficiency. Reaction selectivity, coverage-dependent\nproduction rates, degree of rate control (DRC), and intrinsic turnover\nfrequency (TOF) were quantified through steady-state microkinetic modelling.\nThe simulations predict full conversion of CH$_4$ to CO$_2$ and H$_2$O above\n600 K, whereas partial-oxidation intermediates dominate at lower temperature\nand high O coverage. Rate constants for all elementary steps were derived via\nthe Sure Independence Screening and Sparsifying Operator (SISSO)\nsymbolic-regression method, yielding a concise predictive expression based on\ncharge, coordination number, and key Pd-O/C-H distances. These combined\nDFT-microkinetic-SISSO results clarify oxygen's mechanistic participation and\nprovide practical guidelines for designing Pd/CeO$_2$ catalysts with improved\nactivity toward methane oxidation, a reaction of pressing environmental and\nindustrial importance.",
      "url": "http://arxiv.org/abs/2509.17825v1",
      "published_time_eastern_timestamp": 1758550608.0
    },
    {
      "title": "Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback\n  for Robotic Hand Manipulation",
      "summary": "This paper proposes Tac2Motion, a contact-aware reinforcement learning\nframework to facilitate the learning of contact-rich in-hand manipulation\ntasks, such as removing a lid. To this end, we propose tactile sensing-based\nreward shaping and incorporate the sensing into the observation space through\nembedding. The designed rewards encourage an agent to ensure firm grasping and\nsmooth finger gaiting at the same time, leading to higher data efficiency and\nrobust performance compared to the baseline. We verify the proposed framework\non the opening a lid scenario, showing generalization of the trained policy\ninto a couple of object types and various dynamics such as torsional friction.\nLastly, the learned policy is demonstrated on the multi-fingered robot, Shadow\nRobot, showing that the control policy can be transferred to the real world.\nThe video is available: https://youtu.be/poeJBPR7urQ.",
      "url": "http://arxiv.org/abs/2509.17812v1",
      "published_time_eastern_timestamp": 1758549959.0
    },
    {
      "title": "Effect of Appearance and Animation Realism on the Perception of\n  Emotionally Expressive Virtual Humans",
      "summary": "3D Virtual Human technology is growing with several potential applications in\nhealth, education, business and telecommunications. Investigating the\nperception of these virtual humans can help guide to develop better and more\neffective applications. Recent developments show that the appearance of the\nvirtual humans reached to a very realistic level. However, there is not yet\nadequate analysis on the perception of appearance and animation realism for\nemotionally expressive virtual humans. In this paper, we designed a user\nexperiment and analyzed the effect of a realistic virtual human's appearance\nrealism and animation realism in varying emotion conditions. We found that\nhigher appearance realism and higher animation realism leads to higher social\npresence and higher attractiveness ratings. We also found significant effects\nof animation realism on perceived realism and emotion intensity levels. Our\nstudy sheds light into how appearance and animation realism effects the\nperception of highly realistic virtual humans in emotionally expressive\nscenarios and points out to future directions.",
      "url": "http://arxiv.org/abs/2509.17803v1",
      "published_time_eastern_timestamp": 1758549554.0
    },
    {
      "title": "One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for\n  Millions of Multi-Style Official Accounts",
      "summary": "Conversational agents deployed in industrial-scale official account platforms\nmust generate responses that are both contextually grounded and stylistically\naligned-requirements that existing methods struggle to meet. Chain-of-thought\n(CoT) prompting induces significant latency due to multi-turn reasoning;\nper-account fine-tuning is computationally prohibitive; and long prompt-based\nmethods degrade the model's ability to grasp injected context and style. In\nthis paper, we propose WeStar, a lite-adaptive framework for stylized\ncontextual question answering that scales to millions of official accounts.\nWeStar combines context-grounded generation via RAG with style-aware generation\nusing Parametric RAG (PRAG), where LoRA modules are dynamically activated per\nstyle cluster. Our contributions are fourfold: (1) We introduce WeStar, a\nunified framework capable of serving large volumes of official accounts with\nminimal overhead. (2) We propose a multi-dimensional, cluster-based parameter\nsharing scheme that enables compact style representation while preserving\nstylistic diversity. (3) We develop a style-enhanced Direct Preference\nOptimization (SeDPO) method to optimize each style cluster's parameters for\nimproved generation quality. (4) Experiments on a large-scale industrial\ndataset validate the effectiveness and efficiency of WeStar, underscoring its\npracitical value in real-world deployment.",
      "url": "http://arxiv.org/abs/2509.17788v1",
      "published_time_eastern_timestamp": 1758548977.0
    },
    {
      "title": "An Optimal Control Interpretation of Augmented Distributed Optimization\n  Algorithms",
      "summary": "Distributed optimization algorithms are used in a wide variety of problems\ninvolving complex network systems where the goal is for a set of agents in the\nnetwork to solve a network-wide optimization problem via distributed update\nrules. In many applications, such as communication networks and power systems,\ntransient performance of the algorithms is just as critical as convergence, as\nthe algorithms link to physical processes which must behave well. Primal-dual\nalgorithms have a long history in solving distributed optimization problems,\nwith augmented Lagrangian methods leading to important classes of widely used\nalgorithms, which have been observed in simulations to improve transient\nperformance. Here we show that such algorithms can be seen as being the optimal\nsolution to an appropriately formulated optimal control problem, i.e., a cost\nfunctional associated with the transient behavior of the algorithm is\nminimized, penalizing deviations from optimality during algorithm transients.\nThis is shown for broad classes of algorithm dynamics, including the more\ninvolved setting where inequality constraints are present. The results\npresented improve our understanding of the performance of distributed\noptimization algorithms and can be used as a basis for improved formulations.",
      "url": "http://arxiv.org/abs/2509.17785v1",
      "published_time_eastern_timestamp": 1758548807.0
    },
    {
      "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn\n  Dialogue",
      "summary": "Large Language Models (LLMs) struggle with information forgetting and\ninefficiency in long-horizon, multi-turn dialogues. To address this, we propose\na training-free prompt engineering method, the State-Update Multi-turn Dialogue\nStrategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to\neffectively manage dialogue history. Our strategy shows strong performance\nacross multiple multi-hop QA datasets. For instance, on the HotpotQA dataset,\nit improves the core information filtering score by 32.6%, leading to a 14.1%\nincrease in the downstream QA score, while also reducing inference time by\n73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal\nroles of both components. Our work offers an effective solution for optimizing\nLLMs in long-range interactions, providing new insights for developing more\nrobust Agents.",
      "url": "http://arxiv.org/abs/2509.17766v1",
      "published_time_eastern_timestamp": 1758547584.0
    },
    {
      "title": "Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained\n  Semantic Guidance",
      "summary": "Amodal completion, generating invisible parts of occluded objects, is vital\nfor applications like image editing and AR. Prior methods face challenges with\ndata needs, generalization, or error accumulation in progressive pipelines. We\npropose a Collaborative Multi-Agent Reasoning Framework based on upfront\ncollaborative reasoning to overcome these issues. Our framework uses multiple\nagents to collaboratively analyze occlusion relationships and determine\nnecessary boundary expansion, yielding a precise mask for inpainting.\nConcurrently, an agent generates fine-grained textual descriptions, enabling\nFine-Grained Semantic Guidance. This ensures accurate object synthesis and\nprevents the regeneration of occluders or other unwanted elements, especially\nwithin large inpainting areas. Furthermore, our method directly produces\nlayered RGBA outputs guided by visible masks and attention maps from a\nDiffusion Transformer, eliminating extra segmentation. Extensive evaluations\ndemonstrate our framework achieves state-of-the-art visual quality.",
      "url": "http://arxiv.org/abs/2509.17757v1",
      "published_time_eastern_timestamp": 1758547206.0
    }
  ]
}
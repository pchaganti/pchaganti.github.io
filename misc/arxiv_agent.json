{
  "last_updated": "2025-09-04T20:53:57.889200-04:00",
  "papers": [
    {
      "title": "Real-Time Instrument Planning and Perception for Novel Measurements of\n  Dynamic Phenomena",
      "summary": "Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.",
      "url": "http://arxiv.org/abs/2509.03500v1",
      "published_time_eastern_timestamp": 1756920735.0
    },
    {
      "title": "Design and Optimization of Reinforcement Learning-Based Agents in\n  Text-Based Games",
      "summary": "As AI technology advances, research in playing text-based games with agents\nhas becomeprogressively popular. In this paper, a novel approach to agent\ndesign and agent learning ispresented with the context of reinforcement\nlearning. A model of deep learning is first applied toprocess game text and\nbuild a world model. Next, the agent is learned through a policy gradient-based\ndeep reinforcement learning method to facilitate conversion from state value to\noptimal policy.The enhanced agent works better in several text-based game\nexperiments and significantlysurpasses previous agents on game completion ratio\nand win rate. Our study introduces novelunderstanding and empirical ground for\nusing reinforcement learning for text games and sets thestage for developing\nand optimizing reinforcement learning agents for more general domains\nandproblems.",
      "url": "http://arxiv.org/abs/2509.03479v1",
      "published_time_eastern_timestamp": 1756918898.0
    },
    {
      "title": "On the Perturbed Projection-Based Distributed Gradient-Descent\n  Algorithm: A Fully-Distributed Adaptive Redesign",
      "summary": "In this work, we revisit a classical distributed gradient-descent algorithm,\nintroducing an interesting class of perturbed multi-agent systems. The state of\neach subsystem represents a local estimate of a solution to the global\noptimization problem. Thereby, the network is required to minimize local cost\nfunctions, while gathering the local estimates around a common value. Such a\ncomplex task suggests the interplay of consensus-based dynamics with\ngradient-descent dynamics. The latter descent dynamics involves the projection\noperator, which is assumed to provide corrupted projections of a specific form,\nreminiscent of existing (fast) projection algorithms. Hence, for the resulting\nclass of perturbed networks, we are able to adaptively tune some gains in a\nfully distributed fashion, to approach the optimal consensus set up to\narbitrary-desired precision.",
      "url": "http://arxiv.org/abs/2509.03443v1",
      "published_time_eastern_timestamp": 1756915890.0
    },
    {
      "title": "Situating AI Agents in their World: Aspective Agentic AI for Dynamic\n  Partially Observable Information Systems",
      "summary": "Agentic LLM AI agents are often little more than autonomous chatbots: actors\nfollowing scripts, often controlled by an unreliable director. This work\nintroduces a bottom-up framework that situates AI agents in their environment,\nwith all behaviors triggered by changes in their environments. It introduces\nthe notion of aspects, similar to the idea of umwelt, where sets of agents\nperceive their environment differently to each other, enabling clearer control\nof information. We provide an illustrative implementation and show that\ncompared to a typical architecture, which leaks up to 83% of the time,\naspective agentic AI enables zero information leakage. We anticipate that this\nconcept of specialist agents working efficiently in their own information\nniches can provide improvements to both security and efficiency.",
      "url": "http://arxiv.org/abs/2509.03380v1",
      "published_time_eastern_timestamp": 1756911424.0
    },
    {
      "title": "Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive\n  and Abductive Reasoning",
      "summary": "Reasoning is a core capability in artificial intelligence systems, for which\nlarge language models (LLMs) have recently shown remarkable progress. However,\nmost work focuses exclusively on deductive reasoning, which is problematic\nsince other types of reasoning are also essential in solving real-world\nproblems, and they are less explored. This work focuses on evaluating LLMs'\ninductive and abductive reasoning capabilities. We introduce a programmable and\nsynthetic dataset, InAbHyD (pronounced in-a-bid), where each reasoning example\nconsists of an incomplete world model and a set of observations. The task for\nthe intelligent agent is to produce hypotheses to explain observations under\nthe incomplete world model to solve each reasoning example. We propose a new\nmetric to evaluate the quality of hypotheses based on Occam's Razor. We\nevaluate and analyze some state-of-the-art LLMs. Our analysis shows that LLMs\ncan perform inductive and abductive reasoning in simple scenarios, but struggle\nwith complex world models and producing high-quality hypotheses, even with\npopular reasoning-enhancing techniques such as in-context learning and RLVR.",
      "url": "http://arxiv.org/abs/2509.03345v1",
      "published_time_eastern_timestamp": 1756909362.0
    },
    {
      "title": "EvolveSignal: A Large Language Model Powered Coding Agent for\n  Discovering Traffic Signal Control Algorithms",
      "summary": "In traffic engineering, the fixed-time traffic signal control remains widely\nused for its low cost, stability, and interpretability. However, its design\ndepends on hand-crafted formulas (e.g., Webster) and manual re-timing by\nengineers to adapt to demand changes, which is labor-intensive and often yields\nsuboptimal results under heterogeneous or congested conditions. This paper\nintroduces the EvolveSignal, a large language models (LLMs) powered coding\nagent to automatically discover new traffic signal control algorithms. We\nformulate the problem as program synthesis, where candidate algorithms are\nrepresented as Python functions with fixed input-output structures, and\niteratively optimized through external evaluations (e.g., a traffic simulator)\nand evolutionary search. Experiments on a signalized intersection demonstrate\nthat the discovered algorithms outperform Webster's baseline, reducing average\ndelay by 20.1% and average stops by 47.1%. Beyond performance, ablation and\nincremental analyses reveal that EvolveSignal modifications-such as adjusting\ncycle length bounds, incorporating right-turn demand, and rescaling green\nallocations-can offer practically meaningful insights for traffic engineers.\nThis work opens a new research direction by leveraging AI for algorithm design\nin traffic signal control, bridging program synthesis with transportation\nengineering.",
      "url": "http://arxiv.org/abs/2509.03335v2",
      "published_time_eastern_timestamp": 1756908656.0
    },
    {
      "title": "VulnRepairEval: An Exploit-Based Evaluation Framework for Assessing\n  Large Language Model Vulnerability Repair Capabilities",
      "summary": "The adoption of Large Language Models (LLMs) for automated software\nvulnerability patching has shown promising outcomes on carefully curated\nevaluation sets. Nevertheless, existing datasets predominantly rely on\nsuperficial validation methods rather than exploit-based verification, leading\nto overestimated performance in security-sensitive applications. This paper\nintroduces VulnRepairEval, an evaluation framework anchored in functional\nProof-of-Concept (PoC) exploits. Our framework delivers a comprehensive,\ncontainerized evaluation pipeline that enables reproducible differential\nassessment, where repair success requires the original exploit to fail\nexecution against the modified code. The benchmark construction involved\nextensive data curation: we processed over 400 CVEs and approximately 2,500\npotential sources to extract a collection of authentic vulnerability instances\n(23 Python CVEs) amenable to automated testing with working PoCs. Through\nVulnRepairEval, we conduct a comprehensive evaluation of 12 popular LLMs and\nobserve a significant performance deficit: even the top-performing model\nsuccessfully addresses merely 5/23 instances (about 21.7%), exposing critical\nweaknesses in security-focused applications. Our failure analysis reveals that\nmost unsuccessful attempts stem from imprecise vulnerability identification and\npatches containing syntactic or semantic errors. Enhanced prompting strategies\nand multi-agent approaches yield minimal improvements, with overall\neffectiveness remaining largely unaffected. This work contributes a stringent,\npractical evaluation framework for LLM-driven vulnerability remediation and\nunderscores the necessity for assessment protocols that authentically reflect\nreal-world exploitation scenarios.",
      "url": "http://arxiv.org/abs/2509.03331v1",
      "published_time_eastern_timestamp": 1756908370.0
    },
    {
      "title": "AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?",
      "summary": "Large Language Model (LLM)-based agentic systems, often comprising multiple\nmodels, complex tool invocations, and orchestration protocols, substantially\noutperform monolithic agents. Yet this very sophistication amplifies their\nfragility, making them more prone to system failure. Pinpointing the specific\nagent or step responsible for an error within long execution traces defines the\ntask of agentic system failure attribution. Current state-of-the-art reasoning\nLLMs, however, remain strikingly inadequate for this challenge, with accuracy\ngenerally below 10%. To address this gap, we propose AgenTracer, the first\nautomated framework for annotating failed multi-agent trajectories via\ncounterfactual replay and programmed fault injection, producing the curated\ndataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a\nlightweight failure tracer trained with multi-granular reinforcement learning,\ncapable of efficiently diagnosing errors in verbose multi-agent interactions.\nOn the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs\nlike Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard\nin LLM agentic failure attribution. More importantly, AgenTracer-8B delivers\nactionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS\nwith 4.8-14.2% performance gains, empowering self-correcting and self-evolving\nagentic AI.",
      "url": "http://arxiv.org/abs/2509.03312v2",
      "published_time_eastern_timestamp": 1756906934.0
    },
    {
      "title": "app.build: A Production Framework for Scaling Agentic Prompt-to-App\n  Generation with Environment Scaffolding",
      "summary": "We present app.build (https://github.com/appdotbuild/agent/), an open-source\nframework that improves LLM-based application generation through systematic\nvalidation and structured environments. Our approach combines multi-layered\nvalidation pipelines, stack-specific orchestration, and model-agnostic\narchitecture, implemented across three reference stacks. Through evaluation on\n30 generation tasks, we demonstrate that comprehensive validation achieves\n73.3% viability rate with 30% reaching perfect quality scores, while\nopen-weights models achieve 80.8% of closed-model performance when provided\nstructured environments. The open-source framework has been adopted by the\ncommunity, with over 3,000 applications generated to date. This work\ndemonstrates that scaling reliable AI agents requires scaling environments, not\njust models -- providing empirical insights and complete reference\nimplementations for production-oriented agent systems.",
      "url": "http://arxiv.org/abs/2509.03310v1",
      "published_time_eastern_timestamp": 1756906905.0
    },
    {
      "title": "Automatic Differentiation of Agent-Based Models",
      "summary": "Agent-based models (ABMs) simulate complex systems by capturing the bottom-up\ninteractions of individual agents comprising the system. Many complex systems\nof interest, such as epidemics or financial markets, involve thousands or even\nmillions of agents. Consequently, ABMs often become computationally demanding\nand rely on the calibration of numerous free parameters, which has\nsignificantly hindered their widespread adoption. In this paper, we demonstrate\nthat automatic differentiation (AD) techniques can effectively alleviate these\ncomputational burdens. By applying AD to ABMs, the gradients of the simulator\nbecome readily available, greatly facilitating essential tasks such as\ncalibration and sensitivity analysis. Specifically, we show how AD enables\nvariational inference (VI) techniques for efficient parameter calibration. Our\nexperiments demonstrate substantial performance improvements and computational\nsavings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape;\nand the SIR epidemiological model. Our approach thus significantly enhances the\npracticality and scalability of ABMs for studying complex systems.",
      "url": "http://arxiv.org/abs/2509.03303v1",
      "published_time_eastern_timestamp": 1756906113.0
    }
  ]
}
{
  "last_updated": "2025-09-25T23:27:07.138209-04:00",
  "papers": [
    {
      "title": "Interactive Recommendation Agent with Active User Commands",
      "summary": "Traditional recommender systems rely on passive feedback mechanisms that\nlimit users to simple choices such as like and dislike. However, these\ncoarse-grained signals fail to capture users' nuanced behavior motivations and\nintentions. In turn, current systems cannot also distinguish which specific\nitem attributes drive user satisfaction or dissatisfaction, resulting in\ninaccurate preference modeling. These fundamental limitations create a\npersistent gap between user intentions and system interpretations, ultimately\nundermining user satisfaction and harming system effectiveness.\n  To address these limitations, we introduce the Interactive Recommendation\nFeed (IRF), a pioneering paradigm that enables natural language commands within\nmainstream recommendation feeds. Unlike traditional systems that confine users\nto passive implicit behavioral influence, IRF empowers active explicit control\nover recommendation policies through real-time linguistic commands. To support\nthis paradigm, we develop RecBot, a dual-agent architecture where a Parser\nAgent transforms linguistic expressions into structured preferences and a\nPlanner Agent dynamically orchestrates adaptive tool chains for on-the-fly\npolicy adjustment. To enable practical deployment, we employ\nsimulation-augmented knowledge distillation to achieve efficient performance\nwhile maintaining strong reasoning capabilities. Through extensive offline and\nlong-term online experiments, RecBot shows significant improvements in both\nuser satisfaction and business outcomes.",
      "url": "http://arxiv.org/abs/2509.21317v1",
      "published_time_eastern_timestamp": 1758814707.0
    },
    {
      "title": "Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive\n  Cross-Stage Parallelization",
      "summary": "This paper presents Nova, a real-time scheduling framework for serving\nagentic vision-language models (VLMs) on a single GPU with balanced per-request\nlatency and overall request process throughput. Our design begins by enabling\neffective pipelining across vision encode, LLM prefill, and LLM decode stages\nof VLMs, by exploiting their heterogeneous resource demands during execution\nand incorporating elastic GPU spatial partitioning among stages to maximally\nutilize the compute and memory resources. Building on this, we introduce a\nreal-time scheduling algorithm that adaptively calibrates resource allocation\namong stages based on a Pareto-optimal analysis of the latency-throughput\ntrade-off, allowing the system to sustain responsiveness and resource\nefficiency under dynamic request loads. To further alleviate GPU memory\npressure, we design a lightweight weight offloading strategy for vision\nencoders that preserves inference efficiency with minimized memory overhead.\nExtensive evaluations on both synthetic and real-world agent workloads\ndemonstrate that Nova consistently outperforms the state-of-the-art baselines,\nimproving the maximum latency by up to 23.3%, while keeping competitive\nthroughput.",
      "url": "http://arxiv.org/abs/2509.21301v1",
      "published_time_eastern_timestamp": 1758813425.0
    },
    {
      "title": "VC-Agent: An Interactive Agent for Customized Video Dataset Collection",
      "summary": "Facing scaling laws, video data from the internet becomes increasingly\nimportant. However, collecting extensive videos that meet specific needs is\nextremely labor-intensive and time-consuming. In this work, we study the way to\nexpedite this collection process and propose VC-Agent, the first interactive\nagent that is able to understand users' queries and feedback, and accordingly\nretrieve/scale up relevant video clips with minimal user input. Specifically,\nconsidering the user interface, our agent defines various user-friendly ways\nfor the user to specify requirements based on textual descriptions and\nconfirmations. As for agent functions, we leverage existing multi-modal large\nlanguage models to connect the user's requirements with the video content. More\nimportantly, we propose two novel filtering policies that can be updated when\nuser interaction is continually performed. Finally, we provide a new benchmark\nfor personalized video dataset collection, and carefully conduct the user study\nto verify our agent's usage in various real scenarios. Extensive experiments\ndemonstrate the effectiveness and efficiency of our agent for customized video\ndataset collection. Project page: https://allenyidan.github.io/vcagent_page/.",
      "url": "http://arxiv.org/abs/2509.21291v1",
      "published_time_eastern_timestamp": 1758812908.0
    },
    {
      "title": "\\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning\n  for UAVs in Real-Time on SE(3)",
      "summary": "We propose $\\text{GMP}^{3}$, a multiphase global path planning framework that\ngenerates dynamically feasible three-dimensional trajectories for unmanned\naerial vehicles (UAVs) operating in cluttered environments. The framework\nextends traditional path planning from Euclidean position spaces to the Lie\ngroup $\\mathrm{SE}(3)$, allowing joint learning of translational motion and\nrotational dynamics. A modified Bellman-based operator is introduced to support\nreinforcement learning (RL) policy updates while leveraging prior trajectory\ninformation for improved convergence. $\\text{GMP}^{3}$ is designed as a\ndistributed framework in which agents influence each other and share policy\ninformation along the trajectory: each agent refines its assigned segment and\nshares with its neighbors via a consensus-based scheme, enabling cooperative\npolicy updates and convergence toward a path shaped globally even under\nkinematic constraints. We also propose DroneManager, a modular ground control\nsoftware that interfaces the planner with real UAV platforms via the MAVLink\nprotocol, supporting real-time deployment and feedback. Simulation studies and\nindoor flight experiments validate the effectiveness of the proposed method in\nconstrained 3D environments, demonstrating reliable obstacle avoidance and\nsmooth, feasible trajectories across both position and orientation. The\nopen-source implementation is available at\nhttps://github.com/Domattee/DroneManager",
      "url": "http://arxiv.org/abs/2509.21264v1",
      "published_time_eastern_timestamp": 1758812209.0
    },
    {
      "title": "Tree Search for LLM Agent Reinforcement Learning",
      "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method.",
      "url": "http://arxiv.org/abs/2509.21240v1",
      "published_time_eastern_timestamp": 1758811029.0
    },
    {
      "title": "AbideGym: Turning Static RL Worlds into Adaptive Challenges",
      "summary": "Agents trained with reinforcement learning often develop brittle policies\nthat fail when dynamics shift, a problem amplified by static benchmarks.\nAbideGym, a dynamic MiniGrid wrapper, introduces agent-aware perturbations and\nscalable complexity to enforce intra-episode adaptation. By exposing weaknesses\nin static policies and promoting resilience, AbideGym provides a modular,\nreproducible evaluation framework for advancing research in curriculum\nlearning, continual learning, and robust generalization.",
      "url": "http://arxiv.org/abs/2509.21234v1",
      "published_time_eastern_timestamp": 1758810876.0
    },
    {
      "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous\n  Meta-Cognitive Patterns",
      "summary": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems.",
      "url": "http://arxiv.org/abs/2509.21224v1",
      "published_time_eastern_timestamp": 1758810589.0
    },
    {
      "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
      "summary": "Long-term conversational agents require effective memory management to handle\ndialogue histories that exceed the context window of large language models\n(LLMs). Existing methods based on fact extraction or summarization reduce\nredundancy but struggle to organize and retrieve relevant information across\ndifferent granularities of dialogue and generated memory. We introduce SGMem\n(Sentence Graph Memory), which represents dialogue as sentence-level graphs\nwithin chunked units, capturing associations across turn-, round-, and\nsession-level contexts. By combining retrieved raw dialogue with generated\nmemory such as summaries, facts and insights, SGMem supplies LLMs with coherent\nand relevant context for response generation. Experiments on LongMemEval and\nLoCoMo show that SGMem consistently improves accuracy and outperforms strong\nbaselines in long-term conversational question answering.",
      "url": "http://arxiv.org/abs/2509.21212v1",
      "published_time_eastern_timestamp": 1758810104.0
    },
    {
      "title": "From Physics to Machine Learning and Back: Part II - Learning and\n  Observational Bias in PHM",
      "summary": "Prognostics and Health Management ensures the reliability, safety, and\nefficiency of complex engineered systems by enabling fault detection,\nanticipating equipment failures, and optimizing maintenance activities\nthroughout an asset lifecycle. However, real-world PHM presents persistent\nchallenges: sensor data is often noisy or incomplete, available labels are\nlimited, and degradation behaviors and system interdependencies can be highly\ncomplex and nonlinear. Physics-informed machine learning has emerged as a\npromising approach to address these limitations by embedding physical knowledge\ninto data-driven models. This review examines how incorporating learning and\nobservational biases through physics-informed modeling and data strategies can\nguide models toward physically consistent and reliable predictions. Learning\nbiases embed physical constraints into model training through physics-informed\nloss functions and governing equations, or by incorporating properties like\nmonotonicity. Observational biases influence data selection and synthesis to\nensure models capture realistic system behavior through virtual sensing for\nestimating unmeasured states, physics-based simulation for data augmentation,\nand multi-sensor fusion strategies. The review then examines how these\napproaches enable the transition from passive prediction to active\ndecision-making through reinforcement learning, which allows agents to learn\nmaintenance policies that respect physical constraints while optimizing\noperational objectives. This closes the loop between model-based predictions,\nsimulation, and actual system operation, empowering adaptive decision-making.\nFinally, the review addresses the critical challenge of scaling PHM solutions\nfrom individual assets to fleet-wide deployment. Fast adaptation methods\nincluding meta-learning and few-shot learning are reviewed alongside domain\ngeneralization techniques ...",
      "url": "http://arxiv.org/abs/2509.21207v1",
      "published_time_eastern_timestamp": 1758809743.0
    },
    {
      "title": "Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference:\n  Joint Feature Quantization and Active-Passive Beamforming Design",
      "summary": "The vision of 6G networks aims to enable edge inference by leveraging\nubiquitously deployed artificial intelligence (AI) models, facilitating\nintelligent environmental perception for a wide range of applications. A\ncritical operation in edge inference is for an edge node (EN) to aggregate\nmulti-view sensory features extracted by distributed agents, thereby boosting\nperception accuracy. Over-the-air computing (AirComp) emerges as a promising\ntechnique for rapid feature aggregation by exploiting the waveform\nsuperposition property of analog-modulated signals, which is, however,\nincompatible with existing digital communication systems. Meanwhile, hybrid\nreconfigurable intelligent surface (RIS), a novel RIS architecture capable of\nsimultaneous signal amplification and reflection, exhibits potential for\nenhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital\nAirComp (HRD-AirComp) scheme, which employs vector quantization to map\nhigh-dimensional features into discrete codewords that are digitally modulated\ninto symbols for wireless transmission. By judiciously adjusting the AirComp\ntransceivers and hybrid RIS reflection to control signal superposition across\nagents, the EN can estimate the aggregated features from the received signals.\nTo endow HRD-AirComp with a task-oriented design principle, we derive a\nsurrogate function for inference accuracy that characterizes the impact of\nfeature quantization and over-the-air aggregation. Based on this surrogate, we\nformulate an optimization problem targeting inference accuracy maximization,\nand develop an efficient algorithm to jointly optimize the quantization bit\nallocation, agent transmission coefficients, EN receiving beamforming, and\nhybrid RIS reflection beamforming. Experimental results demonstrate that the\nproposed HRD-AirComp outperforms baselines in terms of both inference accuracy\nand uncertainty.",
      "url": "http://arxiv.org/abs/2509.21201v1",
      "published_time_eastern_timestamp": 1758809523.0
    },
    {
      "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for\n  Scientific Reasoning",
      "summary": "Large language models (LLMs) have recently shown strong progress on\nscientific reasoning, yet two major bottlenecks remain. First, explicit\nretrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and\nsteps. Second, multi-agent pipelines often dilute strong solutions by averaging\nacross all candidates. We address these challenges with a unified framework\nthat combines implicit retrieval and structured collaboration. At its\nfoundation, a Monitor-based retrieval module operates at the token level,\nintegrating external knowledge with minimal disruption to reasoning. On top of\nthis substrate, Hierarchical Solution Refinement (HSR) iteratively designates\neach candidate as an anchor to be repaired by its peers, while Quality-Aware\nIterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's\nLast Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the\nhighest reported to date, surpassing the strongest agent baseline by 13.4\npoints and leading frontier LLMs by up to 18.1 points, while simultaneously\nreducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA\nand TRQA confirm robustness across domains. Error analysis shows that reasoning\nfailures and knowledge gaps co-occur in over 85\\% of cases, while diversity\nanalysis reveals a clear dichotomy: retrieval tasks benefit from solution\nvariety, whereas reasoning tasks favor consensus. Together, these findings\ndemonstrate how implicit augmentation and structured refinement overcome the\ninefficiencies of explicit tool use and uniform aggregation. Code is available\nat: https://github.com/tangxiangru/Eigen-1.",
      "url": "http://arxiv.org/abs/2509.21193v1",
      "published_time_eastern_timestamp": 1758809155.0
    },
    {
      "title": "Human-like Navigation in a World Built for Humans",
      "summary": "When navigating in a man-made environment they haven't visited before--like\nan office building--humans employ behaviors such as reading signs and asking\nothers for directions. These behaviors help humans reach their destinations\nefficiently by reducing the need to search through large areas. Existing robot\nnavigation systems lack the ability to execute such behaviors and are thus\nhighly inefficient at navigating within large environments. We present\nReasonNav, a modular navigation system which integrates these human-like\nnavigation skills by leveraging the reasoning capabilities of a vision-language\nmodel (VLM). We design compact input and output abstractions based on\nnavigation landmarks, allowing the VLM to focus on language understanding and\nreasoning. We evaluate ReasonNav on real and simulated navigation tasks and\nshow that the agent successfully employs higher-order reasoning to navigate\nefficiently in large, complex buildings.",
      "url": "http://arxiv.org/abs/2509.21189v1",
      "published_time_eastern_timestamp": 1758809057.0
    },
    {
      "title": "Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface\n  Systems",
      "summary": "Multimodal agents have demonstrated strong performance in general GUI\ninteractions, but their application in automotive systems has been largely\nunexplored. In-vehicle GUIs present distinct challenges: drivers' limited\nattention, strict safety requirements, and complex location-based interaction\npatterns. To address these challenges, we introduce Automotive-ENV, the first\nhigh-fidelity benchmark and interaction environment tailored for vehicle GUIs.\nThis platform defines 185 parameterized tasks spanning explicit control,\nimplicit intent understanding, and safety-aware tasks, and provides structured\nmultimodal observations with precise programmatic checks for reproducible\nevaluation. Building on this benchmark, we propose ASURADA, a geo-aware\nmultimodal agent that integrates GPS-informed context to dynamically adjust\nactions based on location, environmental conditions, and regional driving\nnorms. Experiments show that geo-aware information significantly improves\nsuccess on safety-aware tasks, highlighting the importance of location-based\ncontext in automotive environments. We will release Automotive-ENV, complete\nwith all tasks and benchmarking tools, to further the development of safe and\nadaptive in-vehicle agents.",
      "url": "http://arxiv.org/abs/2509.21143v1",
      "published_time_eastern_timestamp": 1758807013.0
    },
    {
      "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent\n  Perspective",
      "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities.",
      "url": "http://arxiv.org/abs/2509.21134v1",
      "published_time_eastern_timestamp": 1758806715.0
    },
    {
      "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing\n  Email Defense",
      "summary": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats.",
      "url": "http://arxiv.org/abs/2509.21129v1",
      "published_time_eastern_timestamp": 1758806399.0
    },
    {
      "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online\n  Reinforcement Learning",
      "summary": "Online reinforcement learning in complex tasks is time-consuming, as massive\ninteraction steps are needed to learn the optimal Q-function.Vision-language\naction (VLA) policies represent a promising direction for solving diverse\ntasks; however, their performance on low-level control remains limited, and\neffective deployment often requires task-specific expert demonstrations for\nfine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as\n\\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a\nframework that leverages the domain knowledge of vision-language models (VLMs)\nto provide action suggestions for reinforcement learning agents. Unlike\nprevious methods, VARL provides action suggestions rather than designing\nheuristic rewards, thereby guaranteeing unchanged optimality and convergence.\nThe suggested actions increase sample diversity and ultimately improve sample\nefficiency, especially in sparse-reward tasks. To validate the effectiveness of\nVARL, we evaluate it across diverse environments and agent settings. Results\nshow that VARL greatly improves sample efficiency without introducing\nsignificant computational overhead. These advantages make VARL a general\nframework for online reinforcement learning and make it feasible to directly\napply reinforcement learning from scratch in real-world environments.",
      "url": "http://arxiv.org/abs/2509.21126v1",
      "published_time_eastern_timestamp": 1758806194.0
    },
    {
      "title": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and\n  Agentic Mitigation in LLMs",
      "summary": "Large language models (LLMs) have unlocked a wide range of downstream\ngenerative applications. However, we found that they also risk perpetuating\nsubtle fairness issues tied to culture, positioning their generations from the\nperspectives of the mainstream US culture while demonstrating salient\nexternality towards non-mainstream ones. In this work, we identify and\nsystematically investigate this novel culture positioning bias, in which an\nLLM's default generative stance aligns with a mainstream view and treats other\ncultures as outsiders. We propose the CultureLens benchmark with 4000\ngeneration prompts and 3 evaluation metrics for quantifying this bias through\nthe lens of a culturally situated interview script generation task, in which an\nLLM is positioned as an onsite reporter interviewing local people across 10\ndiverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a\nstark pattern: while models adopt insider tones in over 88 percent of\nUS-contexted scripts on average, they disproportionately adopt mainly outsider\nstances for less dominant cultures. To resolve these biases, we propose 2\ninference-time mitigation methods: a baseline prompt-based Fairness\nIntervention Pillars (FIP) method, and a structured Mitigation via Fairness\nAgents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)\nintroduces a self-reflection and rewriting loop based on fairness guidelines.\n(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized\nagents: a Planner Agent(initial script generation), a Critique Agent (evaluates\ninitial script against fairness pillars), and a Refinement Agent (incorporates\nfeedback to produce a polished, unbiased script). Empirical results showcase\nthe effectiveness of agent-based methods as a promising direction for\nmitigating biases in generative LLMs.",
      "url": "http://arxiv.org/abs/2509.21080v1",
      "published_time_eastern_timestamp": 1758803305.0
    },
    {
      "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web\n  Reconnaissance, Tool Generation, and Task Execution",
      "summary": "Recent years, multimodal models have made remarkable strides and pave the way\nfor intelligent browser use agents. However, when solving tasks on real world\nwebpages in multi-turn, long-horizon trajectories, current agents still suffer\nfrom disordered action sequencing and excessive trial and error during\nexecution. This paper introduces Recon-Act, a self-evolving multi-agent\nframework grounded in Reconnaissance-Action behavioral paradigm. The system\ncomprises a Reconnaissance Team and an Action Team: the former conducts\ncomparative analysis and tool generation, while the latter handles intent\ndecomposition, tool orchestration, and execution. By contrasting the erroneous\ntrajectories with successful ones, the Reconnaissance Team infers remedies, and\nabstracts them into a unified notion of generalized tools, either expressed as\nhints or as rule-based codes, and register to the tool archive in real time.\nThe Action Team reinference the process empowered with these targeting tools,\nthus establishing a closed-loop training pipeline of\ndata-tools-action-feedback. Following the 6 level implementation roadmap\nproposed in this work, we have currently reached Level 3 (with limited\nhuman-in-the-loop intervention). Leveraging generalized tools obtained through\nreconnaissance, Recon-Act substantially improves adaptability to unseen\nwebsites and solvability on long-horizon tasks, and achieves state-of-the-art\nperformance on the challenging VisualWebArena dataset.",
      "url": "http://arxiv.org/abs/2509.21072v1",
      "published_time_eastern_timestamp": 1758803029.0
    },
    {
      "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates\n  Persuasion in Multi-Agent Systems",
      "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large\nLanguage Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to\nsolve complex problems, necessitates a deep understanding of the persuasion\ndynamics that govern their interactions. This paper challenges the prevailing\nhypothesis that persuasive efficacy is primarily a function of model scale. We\npropose instead that these dynamics are fundamentally dictated by a model's\nunderlying cognitive process, especially its capacity for explicit reasoning.\nThrough a series of multi-agent persuasion experiments, we uncover a\nfundamental trade-off we term the Persuasion Duality. Our findings reveal that\nthe reasoning process in LRMs exhibits significantly greater resistance to\npersuasion, maintaining their initial beliefs more robustly. Conversely, making\nthis reasoning process transparent by sharing the \"thinking content\"\ndramatically increases their ability to persuade others. We further consider\nmore complex transmission persuasion situations and reveal complex dynamics of\ninfluence propagation and decay within multi-hop persuasion between multiple\nagent networks. This research provides systematic evidence linking a model's\ninternal processing architecture to its external persuasive behavior, offering\na novel explanation for the susceptibility of advanced models and highlighting\ncritical implications for the safety, robustness, and design of future MAS.",
      "url": "http://arxiv.org/abs/2509.21054v1",
      "published_time_eastern_timestamp": 1758801790.0
    },
    {
      "title": "CLAUSE: Agentic Neuro-Symbolic Knowledge Graph Reasoning via Dynamic\n  Learnable Context Engineering",
      "summary": "Knowledge graphs provide structured context for multi-hop question answering,\nbut deployed systems must balance answer accuracy with strict latency and cost\ntargets while preserving provenance. Static k-hop expansions and \"think-longer\"\nprompting often over-retrieve, inflate context, and yield unpredictable\nruntime. We introduce CLAUSE, an agentic three-agent neuro-symbolic framework\nthat treats context construction as a sequential decision process over\nknowledge graphs, deciding what to expand, which paths to follow or backtrack,\nwhat evidence to keep, and when to stop. Latency (interaction steps) and prompt\ncost (selected tokens) are exposed as user-specified budgets or prices,\nallowing per-query adaptation to trade-offs among accuracy, latency, and cost\nwithout retraining. CLAUSE employs the proposed Lagrangian-Constrained\nMulti-Agent Proximal Policy Optimization (LC-MAPPO) algorithm to coordinate\nthree agents: Subgraph Architect, Path Navigator, and Context Curator, so that\nsubgraph construction, reasoning-path discovery, and evidence selection are\njointly optimized under per-query resource budgets on edge edits, interaction\nsteps, and selected tokens. Across HotpotQA, MetaQA, and FactKG, CLAUSE yields\nhigher EM@1 while reducing subgraph growth and end-to-end latency at equal or\nlower token budgets. On MetaQA-2-hop, relative to the strongest RAG baseline\n(GraphRAG), CLAUSE achieves +39.3 EM@1 with 18.6% lower latency and 40.9% lower\nedge growth. The resulting contexts are compact, provenance-preserving, and\ndeliver predictable performance under deployment constraints.",
      "url": "http://arxiv.org/abs/2509.21035v1",
      "published_time_eastern_timestamp": 1758800588.0
    }
  ]
}
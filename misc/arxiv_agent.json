{
  "last_updated": "2025-09-13T23:28:21.173923-04:00",
  "papers": [
    {
      "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
      "summary": "Does continued scaling of large language models (LLMs) yield diminishing\nreturns? Real-world value often stems from the length of task an agent can\ncomplete. We start this work by observing the simple but counterintuitive fact\nthat marginal gains in single-step accuracy can compound into exponential\nimprovements in the length of a task a model can successfully complete. Then,\nwe argue that failures of LLMs when simple tasks are made longer arise from\nmistakes in execution, rather than an inability to reason. We propose isolating\nexecution capability, by explicitly providing the knowledge and plan needed to\nsolve a long-horizon task. We find that larger models can correctly execute\nsignificantly more turns even when small models have 100\\% single-turn\naccuracy. We observe that the per-step accuracy of models degrades as the\nnumber of steps increases. This is not just due to long-context limitations --\ncuriously, we observe a self-conditioning effect -- models become more likely\nto make mistakes when the context contains their errors from prior turns.\nSelf-conditioning does not reduce by just scaling the model size. In contrast,\nrecent thinking models do not self-condition, and can also execute much longer\ntasks in a single turn. We conclude by benchmarking frontier thinking models on\nthe length of task they can execute in a single turn. Overall, by focusing on\nthe ability to execute, we hope to reconcile debates on how LLMs can solve\ncomplex reasoning problems yet fail at simple tasks when made longer, and\nhighlight the massive benefits of scaling model size and sequential test-time\ncompute for long-horizon tasks.",
      "url": "http://arxiv.org/abs/2509.09677v1",
      "published_time_eastern_timestamp": 1757613574.0
    },
    {
      "title": "Heterogeneous Agents in the Data Economy",
      "summary": "In this short paper, we define the investment ability of data investors in\nthe data economy and its heterogeneity. We further construct an analytical\nheterogeneous agent model to demonstrate that differences in data investment\nability lead to divergent economic results for data investors. The analytical\nresults prove that: Investors with higher data investment ability can obtain\ngreater utility through data investment, and thus have stronger incentives to\ninvest in a larger scale of data to achieve higher productivity, technological\nprogress, and experience lower financial frictions. We aim to propose a\nprerequisite theory that extends the analytical framework of the data economy\nfrom the currently prevalent representative agent model to a heterogeneous\nagent model.",
      "url": "http://arxiv.org/abs/2509.09656v1",
      "published_time_eastern_timestamp": 1757613051.0
    },
    {
      "title": "Maximizing social welfare among EF1 allocations at the presence of two\n  types of agents",
      "summary": "We study the fair allocation of indivisible items to $n$ agents to maximize\nthe utilitarian social welfare, where the fairness criterion is envy-free up to\none item and there are only two different utility functions shared by the\nagents. We present a $2$-approximation algorithm when the two utility functions\nare normalized, improving the previous best ratio of $16 \\sqrt{n}$ shown for\ngeneral normalized utility functions; thus this constant ratio approximation\nalgorithm confirms the APX-completeness in this special case previously shown\nAPX-hard. When there are only three agents, i.e., $n = 3$, the previous best\nratio is $3$ shown for general utility functions, and we present an improved\nand tight $\\frac 53$-approximation algorithm when the two utility functions are\nnormalized, and a best possible and tight $2$-approximation algorithm when the\ntwo utility functions are unnormalized.",
      "url": "http://arxiv.org/abs/2509.09641v1",
      "published_time_eastern_timestamp": 1757611681.0
    },
    {
      "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing\n  LLM-based Multi-Agent Systems",
      "summary": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks.",
      "url": "http://arxiv.org/abs/2509.09629v1",
      "published_time_eastern_timestamp": 1757610945.0
    },
    {
      "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
      "summary": "Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/",
      "url": "http://arxiv.org/abs/2509.09594v1",
      "published_time_eastern_timestamp": 1757608457.0
    },
    {
      "title": "Human-in-the-loop Learning Through Decentralized Communication\n  Mechanisms",
      "summary": "Information sharing platforms like TripAdvisor and Waze involve human agents\nas both information producers and consumers. All these platforms operate in a\ncentralized way to collect agents' latest observations of new options (e.g.,\nrestaurants, hotels, travel routes) and share such information with all in real\ntime. However, after hearing the central platforms' live updates, many human\nagents are found selfish and unwilling to further explore unknown options for\nthe benefit of others in the long run. To regulate the human-in-the-loop\nlearning (HILL) game against selfish agents' free-riding, this paper proposes a\nparadigm shift from centralized to decentralized way of operation that forces\nagents' local explorations through restricting information sharing. When game\ntheory meets distributed learning, we formulate our decentralized communication\nmechanism's design as a new multi-agent Markov decision process (MA-MDP), and\nderive its analytical condition to outperform today's centralized operation. As\nthe optimal decentralized communication mechanism in MA-MDP is NP-hard to\nsolve, we present an asymptotically optimal algorithm with linear complexity to\ndetermine the mechanism's timing of intermittent information sharing. Then we\nturn to non-myopic agents who may revert to even over-explore, and adapt our\nmechanism design to work. Simulation experiments using real-world dataset\ndemonstrate the effectiveness of our decentralized mechanisms for various\nscenarios.",
      "url": "http://arxiv.org/abs/2509.09574v1",
      "published_time_eastern_timestamp": 1757607035.0
    },
    {
      "title": "Mechanism Design with Outliers and Predictions",
      "summary": "We initiate the study of mechanism design with outliers, where the designer\ncan discard $z$ agents from the social cost objective. This setting is\nparticularly relevant when some agents exhibit extreme or atypical preferences.\nAs a natural case study, we consider facility location on the line: $n$\nstrategic agents report their preferred locations, and a mechanism places a\nfacility to minimize a social cost function. In our setting, the $z$ agents\nfarthest from the chosen facility are excluded from the social cost. While it\nmay seem intuitive that discarding outliers improves efficiency, our results\nreveal that the opposite can hold.\n  We derive tight bounds for deterministic strategyproof mechanisms under the\ntwo most-studied objectives: utilitarian and egalitarian social cost. Our\nresults offer a comprehensive view of the impact of outliers. We first show\nthat when $z \\ge n/2$, no strategyproof mechanism can achieve a bounded\napproximation for either objective. For egalitarian cost, selecting the $(z +\n1)$-th order statistic is strategyproof and 2-approximate. In fact, we show\nthat this is best possible by providing a matching lower bound. Notably, this\nlower bound of 2 persists even when the mechanism has access to a prediction of\nthe optimal location, in stark contrast to the setting without outliers. For\nutilitarian cost, we show that strategyproof mechanisms cannot effectively\nexploit outliers, leading to the counterintuitive outcome that approximation\nguarantees worsen as the number of outliers increases. However, in this case,\naccess to a prediction allows us to design a strategyproof mechanism achieving\nthe best possible trade-off between consistency and robustness. Finally, we\nalso establish lower bounds for randomized mechanisms that are truthful in\nexpectation.",
      "url": "http://arxiv.org/abs/2509.09561v1",
      "published_time_eastern_timestamp": 1757605943.0
    },
    {
      "title": "Boosting Embodied AI Agents through Perception-Generation Disaggregation\n  and Asynchronous Pipeline Execution",
      "summary": "Embodied AI systems operate in dynamic environments, requiring seamless\nintegration of perception and generation modules to process high-frequency\ninput and output demands. Traditional sequential computation patterns, while\neffective in ensuring accuracy, face significant limitations in achieving the\nnecessary \"thinking\" frequency for real-world applications. In this work, we\npresent Auras, an algorithm-system co-designed inference framework to optimize\nthe inference frequency of embodied AI agents. Auras disaggregates the\nperception and generation and provides controlled pipeline parallelism for them\nto achieve high and stable throughput. Faced with the data staleness problem\nthat appears when the parallelism is increased, Auras establishes a public\ncontext for perception and generation to share, thereby promising the accuracy\nof embodied agents. Experimental results show that Auras improves throughput by\n2.54x on average while achieving 102.7% of the original accuracy, demonstrating\nits efficacy in overcoming the constraints of sequential computation and\nproviding high throughput.",
      "url": "http://arxiv.org/abs/2509.09560v1",
      "published_time_eastern_timestamp": 1757605903.0
    },
    {
      "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
      "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
      "url": "http://arxiv.org/abs/2509.09525v1",
      "published_time_eastern_timestamp": 1757603163.0
    },
    {
      "title": "Combating the Memory Walls: Optimization Pathways for Long-Context\n  Agentic LLM Inference",
      "summary": "LLMs now form the backbone of AI agents for a diverse array of applications,\nincluding tool use, command-line agents, and web or computer use agents. These\nagentic LLM inference tasks are fundamentally different from chatbot-focused\ninference -- they often have much larger context lengths to capture complex,\nprolonged inputs, such as entire webpage DOMs or complicated tool call\ntrajectories. This, in turn, generates significant off-chip memory traffic for\nthe underlying hardware at the inference stage and causes the workload to be\nconstrained by two memory walls, namely the bandwidth and capacity memory\nwalls, preventing the on-chip compute units from achieving high utilization.\n  In this paper, we introduce PLENA, a hardware-software co-designed system\nthat applies three core optimization pathways to tackle these challenges. PLENA\nincludes an efficient hardware implementation of compute and memory units\nsupporting an asymmetric quantization scheme. PLENA also features a novel\nflattened systolic array architecture that has native support for\nFlashAttention to tackle these memory walls in the scenario of inference\nserving for long-context LLMs. Additionally, PLENA is developed with a complete\nstack, including a custom ISA, a compiler, a cycle-emulated simulator, and an\nautomated design space exploration flow. The simulated results show that PLENA\nachieves up to 8.5x higher utilization than existing accelerators, and delivers\n2.24x higher throughput than the A100 GPU and 3.85x higher throughput than the\nTPU v6e, under the same multiplier count and memory settings. The full PLENA\nsystem will also be open-sourced.",
      "url": "http://arxiv.org/abs/2509.09505v1",
      "published_time_eastern_timestamp": 1757602190.0
    },
    {
      "title": "SEDM: Scalable Self-Evolving Distributed Memory for Agents",
      "summary": "Long-term multi-agent systems inevitably generate vast amounts of\ntrajectories and historical interactions, which makes efficient memory\nmanagement essential for both performance and scalability. Existing methods\ntypically depend on vector retrieval and hierarchical storage, yet they are\nprone to noise accumulation, uncontrolled memory expansion, and limited\ngeneralization across domains. To address these challenges, we present SEDM,\nSelf-Evolving Distributed Memory, a verifiable and adaptive framework that\ntransforms memory from a passive repository into an active, self-optimizing\ncomponent. SEDM integrates verifiable write admission based on reproducible\nreplay, a self-scheduling memory controller that dynamically ranks and\nconsolidates entries according to empirical utility, and cross-domain knowledge\ndiffusion that abstracts reusable insights to support transfer across\nheterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM\nimproves reasoning accuracy while reducing token overhead compared with strong\nmemory baselines, and further enables knowledge distilled from fact\nverification to enhance multi-hop reasoning. The results highlight SEDM as a\nscalable and sustainable memory mechanism for open-ended multi-agent\ncollaboration. The code will be released in the later stage of this project.",
      "url": "http://arxiv.org/abs/2509.09498v1",
      "published_time_eastern_timestamp": 1757601457.0
    },
    {
      "title": "AEGIS: An Agent for Extraction and Geographic Identification in\n  Scholarly Proceedings",
      "summary": "Keeping pace with the rapid growth of academia literature presents a\nsignificant challenge for researchers, funding bodies, and academic societies.\nTo address the time-consuming manual effort required for scholarly discovery,\nwe present a novel, fully automated system that transitions from data discovery\nto direct action. Our pipeline demonstrates how a specialized AI agent,\n'Agent-E', can be tasked with identifying papers from specific geographic\nregions within conference proceedings and then executing a Robotic Process\nAutomation (RPA) to complete a predefined action, such as submitting a\nnomination form. We validated our system on 586 papers from five different\nconferences, where it successfully identified every target paper with a recall\nof 100% and a near perfect accuracy of 99.4%. This demonstration highlights the\npotential of task-oriented AI agents to not only filter information but also to\nactively participate in and accelerate the workflows of the academic community.",
      "url": "http://arxiv.org/abs/2509.09470v1",
      "published_time_eastern_timestamp": 1757598772.0
    },
    {
      "title": "Taming Spontaneous Stop-and-Go Traffic Waves: A Computational Mechanism\n  Design Perspective",
      "summary": "It is well known that stop-and-go waves can be generated spontaneously in\ntraffic even without bottlenecks. Can such undesirable traffic patterns,\ninduced by intrinsic human driving behaviors, be tamed effectively and\ninexpensively? Taking advantage of emerging connectivity and autonomy\ntechnologies, we envision a simple yet realistic traffic control system to\nachieve this goal. To prove the concept, we design such a system to suppress\nthese waves while maximizing traffic throughput in the Tadaki setting: a\ncircular road with varying number of vehicles. We first introduce our driver\nbehavior model and demonstrate how our calibrated human driving agents can\nclosely reproduce the observed human driving patterns in the original Tadaki\nexperiment. We then propose a simple control system mediated via connected\nautomated vehicles (CAV) whose ideal speed parameter is treated as a\nsystem-level control variable adapted to the local vehicle density of the\ntraffic. The objective of the control system is set up as a tradeoff:\nmaximizing throughput while minimizing traffic oscillation. Following\ncomputational mechanism design, we search for the optimal control policy as a\nfunction of vehicle density and the tradeoff attitude parameter. This can be\ndone by letting all vehicles play a simulated game of CAV-modulated traffic\nunder such a control system. Our simulation results show that the improvements\nin traffic efficiency and smoothness are substantial. Finally, we envision how\nsuch a traffic control system can be realized in an environment with smart\nvehicles connected to a smart infrastructure or via a scheme of variable speed\nadvisory.",
      "url": "http://arxiv.org/abs/2509.09441v1",
      "published_time_eastern_timestamp": 1757597257.0
    },
    {
      "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems",
      "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments.",
      "url": "http://arxiv.org/abs/2509.09360v1",
      "published_time_eastern_timestamp": 1757589503.0
    },
    {
      "title": "Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement\n  Learning",
      "summary": "Navigating and understanding complex and unknown environments autonomously\ndemands more than just basic perception and movement from embodied agents.\nTruly effective exploration requires agents to possess higher-level cognitive\nabilities, the ability to reason about their surroundings, and make more\ninformed decisions regarding exploration strategies. However, traditional RL\napproaches struggle to balance efficient exploration and semantic understanding\ndue to limited cognitive capabilities embedded in the small policies for the\nagents, leading often to human drivers when dealing with semantic exploration.\nIn this paper, we address this challenge by presenting a novel Deep\nReinforcement Learning (DRL) architecture that is specifically designed for\nresource efficient semantic exploration. A key methodological contribution is\nthe integration of a Vision-Language Model (VLM) common-sense through a layered\nreward function. The VLM query is modeled as a dedicated action, allowing the\nagent to strategically query the VLM only when deemed necessary for gaining\nexternal guidance, thereby conserving resources. This mechanism is combined\nwith a curriculum learning strategy designed to guide learning at different\nlevels of complexity to ensure robust and stable learning. Our experimental\nevaluation results convincingly demonstrate that our agent achieves\nsignificantly enhanced object discovery rates and develops a learned capability\nto effectively navigate towards semantically rich regions. Furthermore, it also\nshows a strategic mastery of when to prompt for external environmental\ninformation. By demonstrating a practical and scalable method for embedding\ncommon-sense semantic reasoning with autonomous agents, this research provides\na novel approach to pursuing a fully intelligent and self-guided exploration in\nrobotics.",
      "url": "http://arxiv.org/abs/2509.09356v1",
      "published_time_eastern_timestamp": 1757589008.0
    },
    {
      "title": "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain\n  Expansion, and Metric Optimization",
      "summary": "Recent advances in large language models (LLMs) have enabled the emergence of\ngeneral-purpose agents for automating end-to-end machine learning (ML)\nworkflows, including data analysis, feature engineering, model training, and\ncompetition solving. However, existing benchmarks remain limited in task\ncoverage, domain diversity, difficulty modeling, and evaluation rigor, failing\nto capture the full capabilities of such agents in realistic settings. We\npresent TAM Bench, a diverse, realistic, and structured benchmark for\nevaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three\nkey innovations: (1) A browser automation and LLM-based task acquisition system\nthat automatically collects and structures ML challenges from platforms such as\nKaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities\n(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty\nmodeling mechanism that estimates task complexity using participant counts and\nscore dispersion, enabling scalable and objective task calibration; (3) A\nmulti-dimensional evaluation framework incorporating performance, format\ncompliance, constraint adherence, and task generalization. Based on 150 curated\nAutoML tasks, we construct three benchmark subsets of different sizes -- Lite,\nMedium, and Full -- designed for varying evaluation scenarios. The Lite\nversion, with 18 tasks and balanced coverage across modalities and difficulty\nlevels, serves as a practical testbed for daily benchmarking and comparative\nstudies.",
      "url": "http://arxiv.org/abs/2509.09321v1",
      "published_time_eastern_timestamp": 1757585448.0
    },
    {
      "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on\n  Materials Characterization",
      "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.",
      "url": "http://arxiv.org/abs/2509.09307v1",
      "published_time_eastern_timestamp": 1757584216.0
    },
    {
      "title": "LightAgent: Production-level Open-source Agentic AI Framework",
      "summary": "With the rapid advancement of large language models (LLMs), Multi-agent\nSystems (MAS) have achieved significant progress in various application\nscenarios. However, substantial challenges remain in designing versatile,\nrobust, and efficient platforms for agent deployment. To address these\nlimitations, we propose \\textbf{LightAgent}, a lightweight yet powerful agentic\nframework, effectively resolving the trade-off between flexibility and\nsimplicity found in existing frameworks. LightAgent integrates core\nfunctionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while\nmaintaining an extremely lightweight structure. As a fully open-source\nsolution, it seamlessly integrates with mainstream chat platforms, enabling\ndevelopers to easily build self-learning agents. We have released LightAgent at\n\\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}",
      "url": "http://arxiv.org/abs/2509.09292v1",
      "published_time_eastern_timestamp": 1757582953.0
    },
    {
      "title": "Flip Co-op: Cooperative Takeovers in Shared Autonomy",
      "summary": "Shared autonomy requires principled mechanisms for allocating and\ntransferring control between a human and an autonomous agent. Existing\napproaches often rely on blending control inputs between human and autonomous\nagent or switching rules, which lack theoretical guarantees. This paper\ndevelops a game-theoretic framework for modeling cooperative takeover in shared\nautonomy. We formulate the switching interaction as a dynamic game in which\nauthority is embedded directly into the system dynamics, resulting in Nash\nequilibrium(NE)-based strategies rather than ad hoc switching rules. We\nestablish the existence and characterization of NE in the space of pure\ntakeover strategies under stochastic human intent. For the class of\nlinear-quadratic systems, we derive closed-form recursions for takeover\nstrategies and saddle-point value functions, providing analytical insight and\nefficient computation of cooperative takeover policies. We further introduce a\nbimatrix potential game reformulation to address scenarios where human and\nautonomy utilities are not perfectly aligned, yielding a unifying potential\nfunction that preserves tractability while capturing intent deviations. The\nframework is applied to a vehicle trajectory tracking problem, demonstrating\nhow equilibrium takeover strategies adapt across straight and curved path\nsegments. The results highlight the trade-off between human adaptability and\nautonomous efficiency and illustrate the practical benefits of grounding shared\nautonomy in cooperative game theory.",
      "url": "http://arxiv.org/abs/2509.09281v1",
      "published_time_eastern_timestamp": 1757582048.0
    },
    {
      "title": "Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems\n  from Sparse and Noisy Observations",
      "summary": "Data-driven discovery of emergent dynamics is gaining popularity,\nparticularly in the context of reaction-diffusion systems. These systems are\nwidely studied across various fields, including neuroscience, ecology,\nepidemiology, and several other subject areas that deal with emergent dynamics.\nA current challenge in the discovery process relates to system identification\nwhen there is no prior knowledge of the underlying physics. We attempt to\naddress this challenge by learning Soft Artificial Life (Soft ALife) models,\nsuch as Agent-based and Cellular Automata (CA) models, from observed data for\nreaction-diffusion systems. In this paper, we present findings on the\napplicability of a conceptual framework, the Data-driven Rulesets for Soft\nArtificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately\nrepresent emergent dynamics in a reaction-diffusion system from observed data.\nThis model has demonstrated promising results for Elementary CA Rule 30, Game\nof Life, and Vicsek Flocking problems in recent work. To our knowledge, this is\none of the few studies that explore machine-based Soft ALife ruleset learning\nand system identification for reaction-diffusion dynamics without any prior\nknowledge of the underlying physics. Moreover, we provide comprehensive\nfindings from experiments investigating the potential effects of using noisy\nand sparse observed datasets on learning emergent dynamics. Additionally, we\nsuccessfully identify the structure and parameters of the underlying partial\ndifferential equations (PDEs) representing these dynamics. Experimental results\ndemonstrate that the learned models are able to predict the emergent dynamics\nwith good accuracy (74%) and exhibit quite robust performance when subjected to\nGaussian noise and temporal sparsity.",
      "url": "http://arxiv.org/abs/2509.09278v1",
      "published_time_eastern_timestamp": 1757581691.0
    }
  ]
}
{
  "last_updated": "2025-05-13T17:11:05.352023-04:00",
  "papers": [
    {
      "title": "Imagine, Verify, Execute: Memory-Guided Agentic Exploration with\n  Vision-Language Models",
      "summary": "Exploration is essential for general-purpose robotic learning, especially in\nopen-ended environments where dense rewards, explicit goals, or task-specific\nsupervision are scarce. Vision-language models (VLMs), with their semantic\nreasoning over objects, spatial relations, and potential outcomes, present a\ncompelling foundation for generating high-level exploratory behaviors. However,\ntheir outputs are often ungrounded, making it difficult to determine whether\nimagined transitions are physically feasible or informative. To bridge the gap\nbetween imagination and execution, we present IVE (Imagine, Verify, Execute),\nan agentic exploration framework inspired by human curiosity. Human exploration\nis often driven by the desire to discover novel scene configurations and to\ndeepen understanding of the environment. Similarly, IVE leverages VLMs to\nabstract RGB-D observations into semantic scene graphs, imagine novel scenes,\npredict their physical plausibility, and generate executable skill sequences\nthrough action tools. We evaluate IVE in both simulated and real-world tabletop\nenvironments. The results show that IVE enables more diverse and meaningful\nexploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the\nentropy of visited states. Moreover, the collected experience supports\ndownstream learning, producing policies that closely match or exceed the\nperformance of those trained on human-collected demonstrations.",
      "url": "http://arxiv.org/abs/2505.07815v1",
      "published_time_eastern_timestamp": 1747072751.0
    },
    {
      "title": "A Theoretical Framework for Explaining Reinforcement Learning with\n  Shapley Values",
      "summary": "Reinforcement learning agents can achieve superhuman performance, but their\ndecisions are often difficult to interpret. This lack of transparency limits\ndeployment, especially in safety-critical settings where human trust and\naccountability are essential. In this work, we develop a theoretical framework\nfor explaining reinforcement learning through the influence of state features,\nwhich represent what the agent observes in its environment. We identify three\ncore elements of the agent-environment interaction that benefit from\nexplanation: behaviour (what the agent does), performance (what the agent\nachieves), and value estimation (what the agent expects to achieve). We treat\nstate features as players cooperating to produce each element and apply Shapley\nvalues, a principled method from cooperative game theory, to identify the\ninfluence of each feature. This approach yields a family of mathematically\ngrounded explanations with clear semantics and theoretical guarantees. We use\nillustrative examples to show how these explanations align with human intuition\nand reveal novel insights. Our framework unifies and extends prior work, making\nexplicit the assumptions behind existing approaches, and offers a principled\nfoundation for more interpretable and trustworthy reinforcement learning.",
      "url": "http://arxiv.org/abs/2505.07797v1",
      "published_time_eastern_timestamp": 1747072108.0
    },
    {
      "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
      "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
      "url": "http://arxiv.org/abs/2505.07782v1",
      "published_time_eastern_timestamp": 1747071343.0
    },
    {
      "title": "Multi-Agent Path Finding via Finite-Horizon Hierarchical Factorization",
      "summary": "We present a novel algorithm for large-scale Multi-Agent Path Finding (MAPF)\nthat enables fast, scalable planning in dynamic environments such as automated\nwarehouses. Our approach introduces finite-horizon hierarchical factorization,\na framework that plans one step at a time in a receding-horizon fashion. Robots\nfirst compute individual plans in parallel, and then dynamically group based on\nspatio-temporal conflicts and reachability. The framework accounts for conflict\nresolution, and for immediate execution and concurrent planning, significantly\nreducing response time compared to offline algorithms. Experimental results on\nbenchmark maps demonstrate that our method achieves up to 60% reduction in\ntime-to-first-action while consistently delivering high-quality solutions,\noutperforming state-of-the-art offline baselines across a range of problem\nsizes and planning horizons.",
      "url": "http://arxiv.org/abs/2505.07779v1",
      "published_time_eastern_timestamp": 1747071111.0
    },
    {
      "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving",
      "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.",
      "url": "http://arxiv.org/abs/2505.07773v1",
      "published_time_eastern_timestamp": 1747070614.0
    },
    {
      "title": "Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and\n  Single-Agent Architecture",
      "summary": "We present the Emotion-Gradient Metacognitive Recursive Self-Improvement\n(EG-MRSI) framework, a novel architecture that integrates introspective\nmetacognition, emotion-based intrinsic motivation, and recursive\nself-modification into a unified theoretical system. The framework is\nexplicitly capable of overwriting its own learning algorithm under formally\nbounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation,\nEG-MRSI introduces a differentiable intrinsic reward function driven by\nconfidence, error, novelty, and cumulative success. This signal regulates both\na metacognitive mapping and a self-modification operator constrained by\nprovable safety mechanisms. We formally define the initial agent configuration,\nemotion-gradient dynamics, and RSI trigger conditions, and derive a\nreinforcement-compatible optimization objective that guides the agent's\ndevelopment trajectory. Meaning Density and Meaning Conversion Efficiency are\nintroduced as quantifiable metrics of semantic learning, closing the gap\nbetween internal structure and predictive informativeness. This Part I paper\nestablishes the single-agent theoretical foundations of EG-MRSI. Future parts\nwill extend this framework to include safety certificates and rollback\nprotocols (Part II), collective intelligence mechanisms (Part III), and\nfeasibility constraints including thermodynamic and computational limits (Part\nIV). Together, the EG-MRSI series provides a rigorous, extensible foundation\nfor open-ended and safe AGI.",
      "url": "http://arxiv.org/abs/2505.07757v1",
      "published_time_eastern_timestamp": 1747069367.0
    },
    {
      "title": "VTutor for High-Impact Tutoring at Scale: Managing Engagement and\n  Real-Time Multi-Screen Monitoring with P2P Connections",
      "summary": "Hybrid tutoring, where a human tutor supports multiple students in learning\nwith educational technology, is an increasingly common application to deliver\nhigh-impact tutoring at scale. However, past hybrid tutoring applications are\nlimited in guiding tutor attention to students that require support.\nSpecifically, existing conferencing tools, commonly used in hybrid tutoring, do\nnot allow tutors to monitor multiple students' screens while directly\ncommunicating and attending to multiple students simultaneously. To address\nthis issue, this paper introduces VTutor, a web-based platform leveraging\npeer-to-peer screen sharing and virtual avatars to deliver real-time,\ncontext-aware tutoring feedback at scale. By integrating a multi-student\nmonitoring dashboard with AI-powered avatar prompts, VTutor empowers a single\neducator or tutor to rapidly detect off-task or struggling students and\nintervene proactively, thus enhancing the benefits of one-on-one interactions\nin classroom contexts with several students. Drawing on insight from the\nlearning sciences and past research on animated pedagogical agents, we\ndemonstrate how stylized avatars can potentially sustain student engagement\nwhile accommodating varying infrastructure constraints. Finally, we address\nopen questions on refining large-scale, AI-driven tutoring solutions for\nimproved learner outcomes, and how VTutor could help interpret real-time\nlearner interactions to support remote tutors at scale. The VTutor platform can\nbe accessed at https://ls2025.vtutor.ai. The system demo video is at\nhttps://ls2025.vtutor.ai/video.",
      "url": "http://arxiv.org/abs/2505.07736v1",
      "published_time_eastern_timestamp": 1747068371.0
    },
    {
      "title": "Codifying Character Logic in Role-Playing",
      "summary": "This paper introduces Codified Profiles for role-playing, a novel approach\nthat represents character logic as structured, executable functions for\nbehavioral decision-making. Each profile defines a set of functions\nparse_by_scene(scene) that outputs a list of logic-grounded assertions\ntriggered_statements, using both explicit control structures (e.g.,\nif-then-else) and condition checks like check_condition(scene, question), where\neach question is a semantically meaningful prompt about the scene (e.g., \"Is\nthe character in danger?\") discriminated by the role-playing LLM as true,\nfalse, or unknown. This explicit representation offers three key advantages\nover traditional prompt-based profiles, which append character descriptions\ndirectly into text prompts: (1) Persistence, by enforcing complete and\nconsistent execution of character logic, rather than relying on the model's\nimplicit reasoning; (2) Updatability, through systematic inspection and\nrevision of behavioral logic, which is difficult to track or debug in\nprompt-only approaches; (3) Controllable Randomness, by supporting stochastic\nbehavior directly within the logic, enabling fine-grained variability that\nprompting alone struggles to achieve. To validate these advantages, we\nintroduce a new benchmark constructed from 83 characters and 5,141 scenes\ncurated from Fandom, using NLI-based scoring to compare character responses\nagainst ground-truth actions. Our experiments demonstrate the significant\nbenefits of codified profiles in improving persistence, updatability, and\nbehavioral diversity. Notably, by offloading a significant portion of reasoning\nto preprocessing, codified profiles enable even 1B-parameter models to perform\nhigh-quality role-playing, providing a scalable and efficient foundation for\nlocal deployment of role-play agents.",
      "url": "http://arxiv.org/abs/2505.07705v1",
      "published_time_eastern_timestamp": 1747066362.0
    },
    {
      "title": "Belief Injection for Epistemic Control in Linguistic State Space",
      "summary": "This work introduces belief injection, a proactive epistemic control\nmechanism for artificial agents whose cognitive states are structured as\ndynamic ensembles of linguistic belief fragments. Grounded in the Semantic\nManifold framework, belief injection directly incorporates targeted linguistic\nbeliefs into an agent's internal cognitive state, influencing reasoning and\nalignment proactively rather than reactively. We delineate various injection\nstrategies, such as direct, context-aware, goal-oriented, and reflective\napproaches, and contrast belief injection with related epistemic control\nmechanisms, notably belief filtering. Additionally, this work discusses\npractical applications, implementation considerations, ethical implications,\nand outlines promising directions for future research into cognitive governance\nusing architecturally embedded belief injection.",
      "url": "http://arxiv.org/abs/2505.07693v1",
      "published_time_eastern_timestamp": 1747065536.0
    },
    {
      "title": "Chronocept: Instilling a Sense of Time in Machines",
      "summary": "Human cognition is deeply intertwined with a sense of time, known as\nChronoception. This sense allows us to judge how long facts remain valid and\nwhen knowledge becomes outdated. Despite progress in vision, language, and\nmotor control, AI still struggles to reason about temporal validity. We\nintroduce Chronocept, the first benchmark to model temporal validity as a\ncontinuous probability distribution over time. Using skew-normal curves fitted\nalong semantically decomposed temporal axes, Chronocept captures nuanced\npatterns of emergence, decay, and peak relevance. It includes two datasets:\nBenchmark I (atomic facts) and Benchmark II (multi-sentence passages).\nAnnotations show strong inter-annotator agreement (84% and 89%). Our baselines\npredict curve parameters - location, scale, and skewness - enabling\ninterpretable, generalizable learning and outperforming classification-based\napproaches. Chronocept fills a foundational gap in AI's temporal reasoning,\nsupporting applications in knowledge grounding, fact-checking,\nretrieval-augmented generation (RAG), and proactive agents. Code and data are\npublicly available.",
      "url": "http://arxiv.org/abs/2505.07637v1",
      "published_time_eastern_timestamp": 1747062452.0
    },
    {
      "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents",
      "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static,\ndata-driven models to dynamic systems capable of perceiving and interacting\nwith real-world environments. Despite advancements in pattern recognition and\nsymbolic reasoning, current AI systems, such as large language models, remain\ndisembodied, unable to physically engage with the world. This limitation has\ndriven the rise of embodied AI, where autonomous agents, such as humanoid\nrobots, must navigate and manipulate unstructured environments with human-like\nadaptability. At the core of this challenge lies the concept of Neural Brain, a\ncentral intelligence system designed to drive embodied agents with human-like\nadaptability. A Neural Brain must seamlessly integrate multimodal sensing and\nperception with cognitive capabilities. Achieving this also requires an\nadaptive memory system and energy-efficient hardware-software co-design,\nenabling real-time action in dynamic environments. This paper introduces a\nunified framework for the Neural Brain of embodied agents, addressing two\nfundamental challenges: (1) defining the core components of Neural Brain and\n(2) bridging the gap between static AI models and the dynamic adaptability\nrequired for real-world deployment. To this end, we propose a biologically\ninspired architecture that integrates multimodal active sensing,\nperception-cognition-action function, neuroplasticity-based memory storage and\nupdating, and neuromorphic hardware/software optimization. Furthermore, we also\nreview the latest research on embodied agents across these four aspects and\nanalyze the gap between current AI systems and human intelligence. By\nsynthesizing insights from neuroscience, we outline a roadmap towards the\ndevelopment of generalizable, autonomous agents capable of human-level\nintelligence in real-world scenarios.",
      "url": "http://arxiv.org/abs/2505.07634v1",
      "published_time_eastern_timestamp": 1747062334.0
    },
    {
      "title": "KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question\n  Generation",
      "summary": "KAQG introduces a decisive breakthrough for Retrieval-Augmented Generation\n(RAG) by explicitly tackling the two chronic weaknesses of current pipelines:\ntransparent multi-step reasoning and fine-grained cognitive difficulty control.\nThis transforms RAG from a passive retriever into an accountable generator of\ncalibrated exam items. Technically, the framework fuses knowledge graphs, RAG\nretrieval, and educational assessment theory into a single pipeline. Domain\npassages are parsed into a structured graph; graph-aware retrieval feeds fact\nchains to an LLM; and an assessment layer governed by Bloom's Taxonomy levels\nand Item Response Theory (IRT) transforms those chains into psychometrically\nsound questions. This cross-disciplinary marriage yields two scholarly\ncontributions: it shows how semantic graph contexts guide LLM reasoning paths,\nand it operationalizes difficulty metrics within the generation process,\nproducing items whose IRT parameters match expert benchmarks. Every module,\nfrom KG construction scripts to the multi-agent reasoning scheduler and the\nautomatic IRT validator, is openly released on GitHub. This enables peer\nlaboratories to replicate experiments, benchmark against baselines, and extend\nindividual components without licensing barriers. Its reproducible design paves\nthe way for rigorous ablation studies, cross-domain transfer experiments, and\nshared leaderboards on multi-step reasoning benchmarks.",
      "url": "http://arxiv.org/abs/2505.07618v1",
      "published_time_eastern_timestamp": 1747060939.0
    },
    {
      "title": "AgentFlow: Resilient Adaptive Cloud-Edge Framework for Multi-Agent\n  Coordination",
      "summary": "This paper presents AgentFlow, a MAS-based framework for programmable\ndistributed systems in heterogeneous cloud-edge environments. It introduces\nlogistics objects and abstract agent interfaces to enable dynamic service flows\nand modular orchestration. AgentFlow supports decentralized publish-subscribe\nmessaging and many-to-many service elections, enabling decision coordination\nwithout a central server. It features plug-and-play node discovery, flexible\ntask reorganization, and highly adaptable fault tolerance and substitution\nmechanisms. AgentFlow advances scalable, real-time coordination for resilient\nand autonomous mission-critical systems.",
      "url": "http://arxiv.org/abs/2505.07603v1",
      "published_time_eastern_timestamp": 1747059911.0
    },
    {
      "title": "Reinforced Internal-External Knowledge Synergistic Reasoning for\n  Efficient Adaptive Search Agent",
      "summary": "Retrieval-augmented generation (RAG) is a common strategy to reduce\nhallucinations in Large Language Models (LLMs). While reinforcement learning\n(RL) can enable LLMs to act as search agents by activating retrieval\ncapabilities, existing ones often underutilize their internal knowledge. This\ncan lead to redundant retrievals, potential harmful knowledge conflicts, and\nincreased inference latency. To address these limitations, an efficient and\nadaptive search agent capable of discerning optimal retrieval timing and\nsynergistically integrating parametric (internal) and retrieved (external)\nknowledge is in urgent need. This paper introduces the Reinforced\nInternal-External Knowledge Synergistic Reasoning Agent (IKEA), which could\nindentify its own knowledge boundary and prioritize the utilization of internal\nknowledge, resorting to external search only when internal knowledge is deemed\ninsufficient. This is achieved using a novel knowledge-boundary aware reward\nfunction and a knowledge-boundary aware training dataset. These are designed\nfor internal-external knowledge synergy oriented RL, incentivizing the model to\ndeliver accurate answers, minimize unnecessary retrievals, and encourage\nappropriate external searches when its own knowledge is lacking. Evaluations\nacross multiple knowledge reasoning tasks demonstrate that IKEA significantly\noutperforms baseline methods, reduces retrieval frequency significantly, and\nexhibits robust generalization capabilities.",
      "url": "http://arxiv.org/abs/2505.07596v1",
      "published_time_eastern_timestamp": 1747059717.0
    },
    {
      "title": "YuLan-OneSim: Towards the Next Generation of Social Simulator with Large\n  Language Models",
      "summary": "Leveraging large language model (LLM) based agents to simulate human social\nbehaviors has recently gained significant attention. In this paper, we\nintroduce a novel social simulator called YuLan-OneSim. Compared to previous\nworks, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free\nscenario construction: Users can simply describe and refine their simulation\nscenarios through natural language interactions with our simulator. All\nsimulation code is automatically generated, significantly reducing the need for\nprogramming expertise. (2) Comprehensive default scenarios: We implement 50\ndefault simulation scenarios spanning 8 domains, including economics,\nsociology, politics, psychology, organization, demographics, law, and\ncommunication, broadening access for a diverse range of social researchers. (3)\nEvolvable simulation: Our simulator is capable of receiving external feedback\nand automatically fine-tuning the backbone LLMs, significantly enhancing the\nsimulation quality. (4) Large-scale simulation: By developing a fully\nresponsive agent framework and a distributed simulation architecture, our\nsimulator can handle up to 100,000 agents, ensuring more stable and reliable\nsimulation results. (5) AI social researcher: Leveraging the above features, we\ndevelop an AI social researcher. Users only need to propose a research topic,\nand the AI researcher will automatically analyze the input, construct\nsimulation environments, summarize results, generate technical reports, review\nand refine the reports--completing the social science research loop. To\ndemonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate\nthe quality of the automatically generated scenarios, the reliability,\nefficiency, and scalability of the simulation process, as well as the\nperformance of the AI social researcher.",
      "url": "http://arxiv.org/abs/2505.07581v1",
      "published_time_eastern_timestamp": 1747058717.0
    },
    {
      "title": "Dynamic Rental Games with Stagewise Individual Rationality",
      "summary": "We study \\emph{rental games} -- a single-parameter dynamic mechanism design\nproblem, in which a designer rents out an indivisible asset over $n$ days. Each\nday, an agent arrives with a private valuation per day of rental, drawn from\nthat day's (known) distribution. The designer can either rent out the asset to\nthe current agent for any number of remaining days, charging them a (possibly\ndifferent) payment per day, or turn the agent away. Agents who arrive when the\nasset is not available are turned away. A defining feature of our dynamic model\nis that agents are \\emph{stagewise-IR} (individually rational), meaning they\nreject any rental agreement that results in temporary negative utility, even if\ntheir final utility is positive. We ask whether and under which economic\nobjectives it is useful for the designer to exploit the stagewise-IR nature of\nthe agents.\n  We show that an optimal rental mechanism can be modeled as a sequence of\ndynamic auctions with seller costs. However, the stagewise-IR behavior of the\nagents makes these auctions quite different from classical single-parameter\nauctions: Myerson's Lemma does not apply, and indeed we show that truthful\nmechanisms are not necessarily monotone, and payments do not necessarily follow\nMyerson's unique payment rule. We develop alternative characterizations of\noptimal mechanisms under several classes of economic objectives, including\ngeneralizations of welfare, revenue and consumer surplus. These\ncharacterizations allow us to use Myerson's unique payment rule in several\ncases, and for the other cases we develop optimal mechanisms from scratch. Our\nwork shows that rental games raise interesting questions even in the\nsingle-parameter regime.",
      "url": "http://arxiv.org/abs/2505.07579v1",
      "published_time_eastern_timestamp": 1747058526.0
    },
    {
      "title": "RAI: Flexible Agent Framework for Embodied AI",
      "summary": "With an increase in the capabilities of generative language models, a growing\ninterest in embodied AI has followed. This contribution introduces RAI - a\nframework for creating embodied Multi Agent Systems for robotics. The proposed\nframework implements tools for Agents' integration with robotic stacks, Large\nLanguage Models, and simulations. It provides out-of-the-box integration with\nstate-of-the-art systems like ROS 2. It also comes with dedicated mechanisms\nfor the embodiment of Agents. These mechanisms have been tested on a physical\nrobot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid\nprototyping. Furthermore, these mechanisms have been deployed in two\nsimulations: (1) robot arm manipulator and (2) tractor controller. All of these\ndeployments have been evaluated in terms of their control capabilities,\neffectiveness of embodiment, and perception ability. The proposed framework has\nbeen used successfully to build systems with multiple agents. It has\ndemonstrated effectiveness in all the aforementioned tasks. It also enabled\nidentifying and addressing the shortcomings of the generative models used for\nembodied AI.",
      "url": "http://arxiv.org/abs/2505.07532v1",
      "published_time_eastern_timestamp": 1747055627.0
    },
    {
      "title": "The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games",
      "summary": "We study rational synthesis problems for concurrent games with\n$\\omega$-regular objectives. Our model of rationality considers only pure\nstrategy Nash equilibria that satisfy either a social welfare or Pareto\noptimality condition with respect to an $\\omega$-regular objective for each\nagent. This extends earlier work on equilibria in concurrent games, without\nconsideration about their quality. Our results show that the existence of Nash\nequilibria satisfying social welfare conditions can be computed as efficiently\nas the constrained Nash equilibrium existence problem. On the other hand, the\nexistence of Nash equilibria satisfying the Pareto optimality condition\npossibly involves a higher upper bound, except in the case of B\\\"uchi and\nMuller games, for which all three problems are in the classes P and\nPSPACE-complete, respectively.",
      "url": "http://arxiv.org/abs/2505.07501v1",
      "published_time_eastern_timestamp": 1747053508.0
    },
    {
      "title": "Learning to Reason and Navigate: Parameter Efficient Action Planning\n  with Large Language Models",
      "summary": "The remote embodied referring expression (REVERIE) task requires an agent to\nnavigate through complex indoor environments and localize a remote object\nspecified by high-level instructions, such as \"bring me a spoon\", without\npre-exploration. Hence, an efficient navigation plan is essential for the final\nsuccess. This paper proposes a novel parameter-efficient action planner using\nlarge language models (PEAP-LLM) to generate a single-step instruction at each\nlocation. The proposed model consists of two modules, LLM goal planner (LGP)\nand LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan\nfrom REVERIE instructions, including the target object and room. Then, LAP\ngenerates a single-step instruction with the goal-oriented plan, high-level\ninstruction, and current visual observation as input. PEAP-LLM enables the\nembodied agent to interact with LAP as the path planner on the fly. A simple\ndirect application of LLMs hardly achieves good performance. Also, existing\nhard-prompt-based methods are error-prone in complicated scenarios and need\nhuman intervention. To address these issues and prevent the LLM from generating\nhallucinations and biased information, we propose a novel two-stage method for\nfine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct\npreference optimization (DPO). SFT improves the quality of generated\ninstructions, while DPO utilizes environmental feedback. Experimental results\nshow the superiority of our proposed model on REVERIE compared to the previous\nstate-of-the-art.",
      "url": "http://arxiv.org/abs/2505.07500v1",
      "published_time_eastern_timestamp": 1747053500.0
    },
    {
      "title": "Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks",
      "summary": "The application of large language models (LLMs) in the field of coding is\nevolving rapidly: from code assistants, to autonomous coding agents, and then\nto generating complete projects through natural language. Early LLM code\nbenchmarks primarily focused on code generation accuracy, but these benchmarks\nhave gradually become saturated. Benchmark saturation weakens their guiding\nrole for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%.\nAmong various attempts to address benchmark saturation, approaches based on\nsoftware engineering have stood out, but the saturation of existing software\nengineering benchmarks is rapidly increasing. To address this, we propose a new\nbenchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks\nwith sequential dependencies. The tasks implement project features in sequence,\nsimulating real-world human development workflows. When designing Web-Bench, we\naim to cover the foundational elements of Web development: Web Standards and\nWeb Frameworks. Given the scale and complexity of these projects, which were\ndesigned by engineers with 5 to 10 years of experience, each presents a\nsignificant challenge. On average, a single project takes 4 to 8 hours for a\nsenior engineer to complete. On our given benchmark agent (Web-Agent), SOTA\n(Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better)\nthan SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss\nthat in any development field, Standards and Frameworks represent foundational\nknowledge and efficiency tools, respectively, and LLMs require optimization\ntailored to them.",
      "url": "http://arxiv.org/abs/2505.07473v1",
      "published_time_eastern_timestamp": 1747051583.0
    }
  ]
}
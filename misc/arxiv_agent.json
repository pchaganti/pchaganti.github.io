{
  "last_updated": "2025-05-28T05:13:10.527989-04:00",
  "papers": [
    {
      "title": "Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs\n  via Catfish Agent for Clinical Decision Making",
      "summary": "Large language models (LLMs) have demonstrated strong potential in clinical\nquestion answering, with recent multi-agent frameworks further improving\ndiagnostic accuracy via collaborative reasoning. However, we identify a\nrecurring issue of Silent Agreement, where agents prematurely converge on\ndiagnoses without sufficient critical analysis, particularly in complex or\nambiguous cases. We present a new concept called Catfish Agent, a\nrole-specialized LLM designed to inject structured dissent and counter silent\nagreement. Inspired by the ``catfish effect'' in organizational psychology, the\nCatfish Agent is designed to challenge emerging consensus to stimulate deeper\nreasoning. We formulate two mechanisms to encourage effective and context-aware\ninterventions: (i) a complexity-aware intervention that modulates agent\nengagement based on case difficulty, and (ii) a tone-calibrated intervention\narticulated to balance critique and collaboration. Evaluations on nine medical\nQ&A and three medical VQA benchmarks show that our approach consistently\noutperforms both single- and multi-agent LLMs frameworks, including leading\ncommercial models such as GPT-4o and DeepSeek-R1.",
      "url": "http://arxiv.org/abs/2505.21503v1",
      "published_time_eastern_timestamp": 1748368790.0
    },
    {
      "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery",
      "summary": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.",
      "url": "http://arxiv.org/abs/2505.21499v1",
      "published_time_eastern_timestamp": 1748368745.0
    },
    {
      "title": "Paper2Poster: Towards Multimodal Poster Automation from Scientific\n  Papers",
      "summary": "Academic poster generation is a crucial yet challenging task in scientific\ncommunication, requiring the compression of long-context interleaved documents\ninto a single, visually coherent page. To address this challenge, we introduce\nthe first benchmark and metric suite for poster generation, which pairs recent\nconference papers with author-designed posters and evaluates outputs on\n(i)Visual Quality-semantic alignment with human posters, (ii)Textual\nCoherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic\nand informational criteria scored by a VLM-as-judge, and notably\n(iv)PaperQuiz-the poster's ability to convey core paper content as measured by\nVLMs answering generated quizzes. Building on this benchmark, we propose\nPosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser\ndistills the paper into a structured asset library; the (b)Planner aligns\ntext-visual pairs into a binary-tree layout that preserves reading order and\nspatial balance; and the (c)Painter-Commenter loop refines each panel by\nexecuting rendering code and using VLM feedback to eliminate overflow and\nensure alignment. In our comprehensive evaluation, we find that GPT-4o\noutputs-though visually appealing at first glance-often exhibit noisy text and\npoor PaperQuiz scores, and we find that reader engagement is the primary\naesthetic bottleneck, as human-designed posters rely largely on visual\nsemantics to convey meaning. Our fully open-source variants (e.g. based on the\nQwen-2.5 series) outperform existing 4o-driven multi-agent systems across\nnearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper\ninto a finalized yet editable .pptx poster - all for just $0.005. These\nfindings chart clear directions for the next generation of fully automated\nposter-generation models. The code and datasets are available at\nhttps://github.com/Paper2Poster/Paper2Poster.",
      "url": "http://arxiv.org/abs/2505.21497v1",
      "published_time_eastern_timestamp": 1748368729.0
    },
    {
      "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents",
      "summary": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie.",
      "url": "http://arxiv.org/abs/2505.21496v1",
      "published_time_eastern_timestamp": 1748368686.0
    },
    {
      "title": "Robust Hypothesis Generation: LLM-Automated Language Bias for Inductive\n  Logic Programming",
      "summary": "Automating robust hypothesis generation in open environments is pivotal for\nAI cognition. We introduce a novel framework integrating a multi-agent system,\npowered by Large Language Models (LLMs), with Inductive Logic Programming\n(ILP). Our system's LLM agents autonomously define a structured symbolic\nvocabulary (predicates) and relational templates , i.e., \\emph{language bias}\ndirectly from raw textual data. This automated symbolic grounding (the\nconstruction of the language bias), traditionally an expert-driven bottleneck\nfor ILP, then guides the transformation of text into facts for an ILP solver,\nwhich inductively learns interpretable rules. This approach overcomes\ntraditional ILP's reliance on predefined symbolic structures and the\nnoise-sensitivity of pure LLM methods. Extensive experiments in diverse,\nchallenging scenarios validate superior performance, paving a new path for\nautomated, explainable, and verifiable hypothesis generation.",
      "url": "http://arxiv.org/abs/2505.21486v1",
      "published_time_eastern_timestamp": 1748368418.0
    },
    {
      "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration",
      "summary": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\n$\\textbf{ExtAgents}$, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$\\textbf{$\\boldsymbol{\\infty}$Bench+}$, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls $\\textit{within or exceeds the context window}$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.",
      "url": "http://arxiv.org/abs/2505.21471v1",
      "published_time_eastern_timestamp": 1748367904.0
    },
    {
      "title": "Active-O3: Empowering Multimodal Large Language Models with Active\n  Perception via GRPO",
      "summary": "Active vision, also known as active perception, refers to the process of\nactively selecting where and how to look in order to gather task-relevant\ninformation. It is a critical component of efficient perception and\ndecision-making in humans and advanced embodied agents. Recently, the use of\nMultimodal Large Language Models (MLLMs) as central planning and\ndecision-making modules in robotic systems has gained extensive attention.\nHowever, despite the importance of active perception in embodied intelligence,\nthere is little to no exploration of how MLLMs can be equipped with or learn\nactive perception capabilities. In this paper, we first provide a systematic\ndefinition of MLLM-based active perception tasks. We point out that the\nrecently proposed GPT-o3 model's zoom-in search strategy can be regarded as a\nspecial case of active perception; however, it still suffers from low search\nefficiency and inaccurate region selection. To address these issues, we propose\nACTIVE-O3, a purely reinforcement learning based training framework built on\ntop of GRPO, designed to equip MLLMs with active perception capabilities. We\nfurther establish a comprehensive benchmark suite to evaluate ACTIVE-O3 across\nboth general open-world tasks, such as small-object and dense object grounding,\nand domain-specific scenarios, including small object detection in remote\nsensing and autonomous driving, as well as fine-grained interactive\nsegmentation. In addition, ACTIVE-O3 also demonstrates strong zero-shot\nreasoning abilities on the V* Benchmark, without relying on any explicit\nreasoning data. We hope that our work can provide a simple codebase and\nevaluation protocol to facilitate future research on active perception in\nMLLMs.",
      "url": "http://arxiv.org/abs/2505.21457v1",
      "published_time_eastern_timestamp": 1748366971.0
    },
    {
      "title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion\n  Networks",
      "summary": "Agent-Based Models (ABMs) are powerful tools for studying emergent properties\nin complex systems. In ABMs, agent behaviors are governed by local interactions\nand stochastic rules. However, these rules are, in general, non-differentiable,\nlimiting the use of gradient-based methods for optimization, and thus\nintegration with real-world data. We propose a novel framework to learn a\ndifferentiable surrogate of any ABM by observing its generated data. Our method\ncombines diffusion models to capture behavioral stochasticity and graph neural\nnetworks to model agent interactions. Distinct from prior surrogate approaches,\nour method introduces a fundamental shift: rather than approximating\nsystem-level outputs, it models individual agent behavior directly, preserving\nthe decentralized, bottom-up dynamics that define ABMs. We validate our\napproach on two ABMs (Schelling's segregation model and a Predator-Prey\necosystem) showing that it replicates individual-level patterns and accurately\nforecasts emergent dynamics beyond training. Our results demonstrate the\npotential of combining diffusion models and graph learning for data-driven ABM\nsimulation.",
      "url": "http://arxiv.org/abs/2505.21426v1",
      "published_time_eastern_timestamp": 1748364956.0
    },
    {
      "title": "GUARD:Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural\n  Code Generation",
      "summary": "With the widespread application of large language models in code generation,\nrecent studies demonstrate that employing additional Chain-of-Thought\ngeneration models can significantly enhance code generation performance by\nproviding explicit reasoning steps. However, as external components, CoT models\nare particularly vulnerable to backdoor attacks, which existing defense\nmechanisms often fail to detect effectively. To address this challenge, we\npropose GUARD, a novel dual-agent defense framework specifically designed to\ncounter CoT backdoor attacks in neural code generation. GUARD integrates two\ncore components: GUARD-Judge, which identifies suspicious CoT steps and\npotential triggers through comprehensive analysis, and GUARD-Repair, which\nemploys a retrieval-augmented generation approach to regenerate secure CoT\nsteps for identified anomalies. Experimental results show that GUARD\neffectively mitigates attacks while maintaining generation quality, advancing\nsecure code generation systems.",
      "url": "http://arxiv.org/abs/2505.21425v1",
      "published_time_eastern_timestamp": 1748364946.0
    },
    {
      "title": "Autonomous Multi-Modal LLM Agents for Treatment Planning in Focused\n  Ultrasound Ablation Surgery",
      "summary": "Focused Ultrasound Ablation Surgery (FUAS) has emerged as a promising\nnon-invasive therapeutic modality, valued for its safety and precision.\nNevertheless, its clinical implementation entails intricate tasks such as\nmultimodal image interpretation, personalized dose planning, and real-time\nintraoperative decision-making processes that demand intelligent assistance to\nimprove efficiency and reliability. We introduce FUAS-Agents, an autonomous\nagent system that leverages the multimodal understanding and tool-using\ncapabilities of large language models (LLMs). By integrating patient profiles\nand MRI data, FUAS-Agents orchestrates a suite of specialized medical AI tools,\nincluding segmentation, treatment dose prediction, and clinical guideline\nretrieval, to generate personalized treatment plans comprising MRI image, dose\nparameters, and therapeutic strategies. We evaluate the system in a uterine\nfibroid treatment scenario. Human assessment by four senior FUAS experts\nindicates that 82.5%, 82.5%, 87.5%, and 97.5% of the generated plans were rated\n4 or above (on a 5-point scale) in terms of completeness, accuracy, fluency,\nand clinical compliance, respectively. These results demonstrate the potential\nof LLM-driven agents in enhancing decision-making across complex clinical\nworkflows, and exemplify a translational paradigm that combines general-purpose\nmodels with specialized expert systems to solve practical challenges in\nvertical healthcare domains.",
      "url": "http://arxiv.org/abs/2505.21418v1",
      "published_time_eastern_timestamp": 1748364211.0
    },
    {
      "title": "A Framework for Adversarial Analysis of Decision Support Systems Prior\n  to Deployment",
      "summary": "This paper introduces a comprehensive framework designed to analyze and\nsecure decision-support systems trained with Deep Reinforcement Learning (DRL),\nprior to deployment, by providing insights into learned behavior patterns and\nvulnerabilities discovered through simulation. The introduced framework aids in\nthe development of precisely timed and targeted observation perturbations,\nenabling researchers to assess adversarial attack outcomes within a strategic\ndecision-making context. We validate our framework, visualize agent behavior,\nand evaluate adversarial outcomes within the context of a custom-built\nstrategic game, CyberStrike. Utilizing the proposed framework, we introduce a\nmethod for systematically discovering and ranking the impact of attacks on\nvarious observation indices and time-steps, and we conduct experiments to\nevaluate the transferability of adversarial attacks across agent architectures\nand DRL training algorithms. The findings underscore the critical need for\nrobust adversarial defense mechanisms to protect decision-making policies in\nhigh-stakes environments.",
      "url": "http://arxiv.org/abs/2505.21414v1",
      "published_time_eastern_timestamp": 1748364083.0
    },
    {
      "title": "MRSD: Multi-Resolution Skill Discovery for HRL Agents",
      "summary": "Hierarchical reinforcement learning (HRL) relies on abstract skills to solve\nlong-horizon tasks efficiently. While existing skill discovery methods learns\nthese skills automatically, they are limited to a single skill per task. In\ncontrast, humans learn and use both fine-grained and coarse motor skills\nsimultaneously. Inspired by human motor control, we propose Multi-Resolution\nSkill Discovery (MRSD), an HRL framework that learns multiple skill encoders at\ndifferent temporal resolutions in parallel. A high-level manager dynamically\nselects among these skills, enabling adaptive control strategies over time. We\nevaluate MRSD on tasks from the DeepMind Control Suite and show that it\noutperforms prior state-of-the-art skill discovery and HRL methods, achieving\nfaster convergence and higher final performance. Our findings highlight the\nbenefits of integrating multi-resolution skills in HRL, paving the way for more\nversatile and efficient agents.",
      "url": "http://arxiv.org/abs/2505.21410v1",
      "published_time_eastern_timestamp": 1748363935.0
    },
    {
      "title": "Breaking co-existence: zealotry vs. nonlinear social impact",
      "summary": "We study how zealotry and nonlinear social impact affect consensus formation\nin the nonlinear voter model, evolutionary games, and the partisan voter model.\nIn all three models, consensus is an absorbing state in finite populations,\nwhile co-existence is a possible outcome of the deterministic dynamics. We show\nthat sufficiently strong zealotry -- i.e., the presence of agents who never\nchange state -- can drive infinite populations to consensus in all three\nmodels. However, while evolutionary games and the partisan voter model permit\nzealotry-induced consensus for all values of their model parameters, the\nnonlinear voter model does not. Central to this difference is the shape of the\nsocial impact function, which quantifies how the influence of a group scales\nwith size, and is therefore a measure of majority and minority effects. We\nderive general conditions relating the slope of this function at small group\nsizes to the local stability of consensus. Sublinear impact favours minorities\nand can override zealotry to prevent consensus, whereas superlinear impact\npromotes majorities and therefore facilitates consensus. We extend the analysis\nto finite populations, exploring the time-to-consensus, and the shape of\nquasi-stationary distributions.",
      "url": "http://arxiv.org/abs/2505.21407v1",
      "published_time_eastern_timestamp": 1748363501.0
    },
    {
      "title": "AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of\n  MLLMs",
      "summary": "Evaluating multimodal large language models (MLLMs) is increasingly\nexpensive, as the growing size and cross-modality complexity of benchmarks\ndemand significant scoring efforts. To tackle with this difficulty, we\nintroduce AutoJudger, an agent-driven framework for efficient and adaptive\nbenchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the\nItem Response Theory (IRT) to estimate the question difficulty and an\nautonomous evaluation agent to dynamically select the most informative test\nquestions based on the model's real-time performance. Specifically, AutoJudger\nincorporates two pivotal components: a semantic-aware retrieval mechanism to\nensure that selected questions cover diverse and challenging scenarios across\nboth vision and language modalities, and a dynamic memory that maintains\ncontextual statistics of previously evaluated questions to guide coherent and\nglobally informed question selection throughout the evaluation process.\nExtensive experiments on four representative multimodal benchmarks demonstrate\nthat our adaptive framework dramatically reduces evaluation expenses, i.e.\nAutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with\nthe full benchmark evaluation on MMT-Bench.",
      "url": "http://arxiv.org/abs/2505.21389v1",
      "published_time_eastern_timestamp": 1748362635.0
    },
    {
      "title": "Distributed equilibrium seeking in aggregative games: linear convergence\n  under singular perturbations lens",
      "summary": "We present a fully-distributed algorithm for Nash equilibrium seeking in\naggregative games over networks. The proposed scheme endows each agent with a\ngradient-based scheme equipped with a tracking mechanism to locally reconstruct\nthe aggregative variable, which is not available to the agents. We show that\nour method falls into the framework of singularly perturbed systems, as it\ninvolves the interconnection between a fast subsystem - the global information\nreconstruction dynamics - with a slow one concerning the optimization of the\nlocal strategies. This perspective plays a key role in analyzing the scheme\nwith a constant stepsize, and in proving its linear convergence to the Nash\nequilibrium in strongly monotone games with local constraints. By exploiting\nthe flexibility of our aggregative variable definition (not necessarily the\narithmetic average of the agents' strategy), we show the efficacy of our\nalgorithm on a realistic voltage support case study for the smart grid.",
      "url": "http://arxiv.org/abs/2505.21386v1",
      "published_time_eastern_timestamp": 1748362394.0
    },
    {
      "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs.\n  Dialogue History",
      "summary": "Effective engagement by large language models (LLMs) requires adapting\nresponses to users' sociodemographic characteristics, such as age, occupation,\nand education level. While many real-world applications leverage dialogue\nhistory for contextualization, existing evaluations of LLMs' behavioral\nadaptation often focus on single-turn prompts. In this paper, we propose a\nframework to evaluate LLM adaptation when attributes are introduced either (1)\nexplicitly via user profiles in the prompt or (2) implicitly through multi-turn\ndialogue history. We assess the consistency of model behavior across these\nmodalities. Using a multi-agent pipeline, we construct a synthetic dataset\npairing dialogue histories with distinct user profiles and employ questions\nfrom the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe\nvalue expression. Our findings indicate that most models adjust their expressed\nvalues in response to demographic changes, particularly in age and education\nlevel, but consistency varies. Models with stronger reasoning capabilities\ndemonstrate greater alignment, indicating the importance of reasoning in robust\nsociodemographic adaptation.",
      "url": "http://arxiv.org/abs/2505.21362v1",
      "published_time_eastern_timestamp": 1748361159.0
    },
    {
      "title": "PEDANTIC: A Dataset for the Automatic Examination of Definiteness in\n  Patent Claims",
      "summary": "Patent claims define the scope of protection for an invention. If there are\nambiguities in a claim, it is rejected by the patent office. In the US, this is\nreferred to as indefiniteness (35 U.S.C {\\S} 112(b)) and is among the most\nfrequent reasons for patent application rejection. The development of automatic\nmethods for patent definiteness examination has the potential to make patent\ndrafting and examination more efficient, but no annotated dataset has been\npublished to date.\n  We introduce PEDANTIC (\\underline{P}at\\underline{e}nt\n\\underline{D}efiniteness Ex\\underline{a}mi\\underline{n}a\\underline{ti}on\n\\underline{C}orpus), a novel dataset of 14k US patent claims from patent\napplications relating to Natural Language Processing (NLP), annotated with\nreasons for indefiniteness. We construct PEDANTIC using a fully automatic\npipeline that retrieves office action documents from the USPTO and uses Large\nLanguage Models (LLMs) to extract the reasons for indefiniteness. A human\nvalidation study confirms the pipeline's accuracy in generating high-quality\nannotations. To gain insight beyond binary classification metrics, we implement\nan LLM-as-Judge evaluation that compares the free-form reasoning of every\nmodel-cited reason with every examiner-cited reason. We show that LLM agents\nbased on Qwen 2.5 32B and 72B struggle to outperform logistic regression\nbaselines on definiteness prediction, even though they often correctly identify\nthe underlying reasons. PEDANTIC provides a valuable resource for patent AI\nresearchers, enabling the development of advanced examination models. We will\npublicly release the dataset and code.",
      "url": "http://arxiv.org/abs/2505.21342v1",
      "published_time_eastern_timestamp": 1748360079.0
    },
    {
      "title": "Large Language Models Miss the Multi-Agent Mark",
      "summary": "Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs)\nhas led to an increase in frameworks leveraging multiple LLMs to tackle complex\ntasks. However, much of this literature appropriates the terminology of MAS\nwithout engaging with its foundational principles. In this position paper, we\nhighlight critical discrepancies between MAS theory and current MAS LLMs\nimplementations, focusing on four key areas: the social aspect of agency,\nenvironment design, coordination and communication protocols, and measuring\nemergent behaviours. Our position is that many MAS LLMs lack multi-agent\ncharacteristics such as autonomy, social interaction, and structured\nenvironments, and often rely on oversimplified, LLM-centric architectures. The\nfield may slow down and lose traction by revisiting problems the MAS literature\nhas already addressed. Therefore, we systematically analyse this issue and\noutline associated research opportunities; we advocate for better integrating\nestablished MAS concepts and more precise terminology to avoid\nmischaracterisation and missed opportunities.",
      "url": "http://arxiv.org/abs/2505.21298v1",
      "published_time_eastern_timestamp": 1748358066.0
    },
    {
      "title": "Complex System Diagnostics Using a Knowledge Graph-Informed and Large\n  Language Model-Enhanced Framework",
      "summary": "In this paper, we present a novel diagnostic framework that integrates\nKnowledge Graphs (KGs) and Large Language Models (LLMs) to support system\ndiagnostics in high-reliability systems such as nuclear power plants.\nTraditional diagnostic modeling struggles when systems become too complex,\nmaking functional modeling a more attractive approach. Our approach introduces\na diagnostic framework grounded in the functional modeling principles of the\nDynamic Master Logic (DML) model. It incorporates two coordinated LLM\ncomponents, including an LLM-based workflow for automated construction of DML\nlogic from system documentation and an LLM agent that facilitates interactive\ndiagnostics. The generated logic is encoded into a structured KG, referred to\nas KG-DML, which supports hierarchical fault reasoning. Expert knowledge or\noperational data can also be incorporated to refine the model's precision and\ndiagnostic depth. In the interaction phase, users submit natural language\nqueries, which are interpreted by the LLM agent. The agent selects appropriate\ntools for structured reasoning, including upward and downward propagation\nacross the KG-DML. Rather than embedding KG content into every prompt, the LLM\nagent distinguishes between diagnostic and interpretive tasks. For diagnostics,\nthe agent selects and executes external tools that perform structured KG\nreasoning. For general queries, a Graph-based Retrieval-Augmented Generation\n(Graph-RAG) approach is used, retrieving relevant KG segments and embedding\nthem into the prompt to generate natural explanations. A case study on an\nauxiliary feedwater system demonstrated the framework's effectiveness, with\nover 90% accuracy in key elements and consistent tool and argument extraction,\nsupporting its use in safety-critical diagnostics.",
      "url": "http://arxiv.org/abs/2505.21291v1",
      "published_time_eastern_timestamp": 1748357689.0
    },
    {
      "title": "PACT: A Contract-Theoretic Framework for Pricing Agentic AI Services\n  Powered by Large Language Models",
      "summary": "Agentic AI, often powered by large language models (LLMs), is becoming\nincreasingly popular and adopted to support autonomous reasoning,\ndecision-making, and task execution across various domains. While agentic AI\nholds great promise, its deployment as services for easy access raises critical\nchallenges in pricing, due to high infrastructure and computation costs,\nmulti-dimensional and task-dependent Quality of Service (QoS), and growing\nconcerns around liability in high-stakes applications. In this work, we propose\nPACT, a Pricing framework for cloud-based Agentic AI services through a\nContract-Theoretic approach, which models QoS along both objective (e.g.,\nresponse time) and subjective (e.g., user satisfaction) dimensions. PACT\naccounts for computational, infrastructure, and potential liability costs for\nthe service provider, while ensuring incentive compatibility and individual\nrationality for the user under information asymmetry. Through contract-based\nselection, users receive tailored service offerings aligned with their needs.\nNumerical evaluations demonstrate that PACT improves QoS alignment between\nusers and providers and offers a scalable, liable approach to pricing agentic\nAI services in the future.",
      "url": "http://arxiv.org/abs/2505.21286v1",
      "published_time_eastern_timestamp": 1748357605.0
    }
  ]
}
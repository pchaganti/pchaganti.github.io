{
  "last_updated": "2025-05-11T05:11:07.167972-04:00",
  "papers": [
    {
      "title": "RL-DAUNCE: Reinforcement Learning-Driven Data Assimilation with\n  Uncertainty-Aware Constrained Ensembles",
      "summary": "Machine learning has become a powerful tool for enhancing data assimilation.\nWhile supervised learning remains the standard method, reinforcement learning\n(RL) offers unique advantages through its sequential decision-making framework,\nwhich naturally fits the iterative nature of data assimilation by dynamically\nbalancing model forecasts with observations. We develop RL-DAUNCE, a new\nRL-based method that enhances data assimilation with physical constraints\nthrough three key aspects. First, RL-DAUNCE inherits the computational\nefficiency of machine learning while it uniquely structures its agents to\nmirror ensemble members in conventional data assimilation methods. Second,\nRL-DAUNCE emphasizes uncertainty quantification by advancing multiple ensemble\nmembers, moving beyond simple mean-state optimization. Third, RL-DAUNCE's\nensemble-as-agents design facilitates the enforcement of physical constraints\nduring the assimilation process, which is crucial to improving the state\nestimation and subsequent forecasting. A primal-dual optimization strategy is\ndeveloped to enforce constraints, which dynamically penalizes the reward\nfunction to ensure constraint satisfaction throughout the learning process.\nAlso, state variable bounds are respected by constraining the RL action space.\nTogether, these features ensure physical consistency without sacrificing\nefficiency. RL-DAUNCE is applied to the Madden-Julian Oscillation, an\nintermittent atmospheric phenomenon characterized by strongly non-Gaussian\nfeatures and multiple physical constraints. RL-DAUNCE outperforms the standard\nensemble Kalman filter (EnKF), which fails catastrophically due to the\nviolation of physical constraints. Notably, RL-DAUNCE matches the performance\nof constrained EnKF, particularly in recovering intermittent signals, capturing\nextreme events, and quantifying uncertainties, while requiring substantially\nless computational effort.",
      "url": "http://arxiv.org/abs/2505.05452v1",
      "published_time_eastern_timestamp": 1746726215.0
    },
    {
      "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations",
      "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.",
      "url": "http://arxiv.org/abs/2505.05445v1",
      "published_time_eastern_timestamp": 1746725796.0
    },
    {
      "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation",
      "summary": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation.",
      "url": "http://arxiv.org/abs/2505.05440v1",
      "published_time_eastern_timestamp": 1746725480.0
    },
    {
      "title": "Empowering Scientific Workflows with Federated Agents",
      "summary": "Agentic systems, in which diverse agents cooperate to tackle challenging\nproblems, are exploding in popularity in the AI community. However, the agentic\nframeworks used to build these systems have not previously enabled use with\nresearch cyberinfrastructure. Here we introduce Academy, a modular and\nextensible middleware designed to deploy autonomous agents across the federated\nresearch ecosystem, including HPC systems, experimental facilities, and data\nrepositories. To meet the demands of scientific computing, Academy supports\nasynchronous execution, heterogeneous resources, high-throughput data flows,\nand dynamic resource availability. It provides abstractions for expressing\nstateful agents, managing inter-agent coordination, and integrating computation\nwith experimental control. We present microbenchmark results that demonstrate\nhigh performance and scalability in HPC environments. To demonstrate the\nbreadth of applications that can be supported by agentic workflow designs, we\nalso present case studies in materials discovery, decentralized learning, and\ninformation extraction in which agents are deployed across diverse HPC systems.",
      "url": "http://arxiv.org/abs/2505.05428v1",
      "published_time_eastern_timestamp": 1746724539.0
    },
    {
      "title": "Robustly optimal dynamics for active matter reservoir computing",
      "summary": "We study the information processing abilities of active matter in the\nreservoir computing (RC) paradigm, using a model that is externally driven to\ninfer the future state of a chaotic signal. The simulated system closely\nfollows a previously reported model. We uncover an exceptional dynamical regime\nof agent dynamics that has been overlooked heretofore. It appears robustly\noptimal across varying physical parameters and inference tasks, thus providing\nvaluable insights into computation and inference with physical systems more\ngenerally. The ability to form effective mechanisms for information processing\nare primarily determined by the system's own intrinsic relaxation abilities.\nThese are identifiable when probing the system without a specific inference\ngoal and manifest when testing minimalistic single-particle reservoirs. The\nregime that achieves optimal computation is situated just below the critical\ndamping threshold, involving a microscopic dynamical relaxation with multiple\nstages. The optimal system is adaptable under chaotic external driving, due to\na diversity in response mechanisms that emerge like rapid alternations between\nquasi-stationary and highly nonlinear dynamical states. Both coherent and\nincoherent dynamics contribute to their operation, partly at dissimilar scales\nof space and delay time. Correlations on agent dynamics can indicate the\nbest-performing regimes and onsets of tight relationships between the\nresponding system and the fluctuating driver. As this model of computation is\ninterpretable in physical terms, it facilitates re-framing inquiries regarding\nlearning and unconventional computing with a fresh rationale for many-body\nphysics out of equilibrium.",
      "url": "http://arxiv.org/abs/2505.05420v1",
      "published_time_eastern_timestamp": 1746724154.0
    },
    {
      "title": "Weighted Envy-Freeness Revisited: Indivisible Resource and House\n  Allocations",
      "summary": "Envy-Freeness is one of the most fundamental and important concepts in fair\nallocation. Some recent studies have focused on the concept of weighted\nenvy-freeness. Under this concept, each agent is assigned a weight, and their\nvaluations are divided by their weights when assessing fairness. This concept\ncan promote more fairness in some scenarios. But on the other hand,\nexperimental research has shown that this weighted envy-freeness significantly\nreduces the likelihood of fair allocations. When we must allocate the\nresources, we may propose fairness concepts with lower requirements that are\npotentially more feasible to implement. In this paper, we revisit weighted\nenvy-freeness and propose a new concept called SumAvg-envy-freeness, which\nsubstantially increases the existence of fair allocations. This new concept can\nbe seen as a complement of the normal weighted envy-fairness. Furthermore, we\nsystematically study the computational complexity of finding fair allocations\nunder the old and new weighted fairness concepts in two types of classic\nproblems: Indivisible Resource Allocation and House Allocation. Our study\nprovides a comprehensive characterization of various properties of weighted\nenvy-freeness.",
      "url": "http://arxiv.org/abs/2505.05353v1",
      "published_time_eastern_timestamp": 1746718921.0
    },
    {
      "title": "Mapping User Trust in Vision Language Models: Research Landscape,\n  Challenges, and Prospects",
      "summary": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large\nimage-text and video-text datasets, calls for protecting and informing users\nabout when to trust these systems. This survey reviews studies on trust\ndynamics in user-VLM interactions, through a multi-disciplinary taxonomy\nencompassing different cognitive science capabilities, collaboration modes, and\nagent behaviours. Literature insights and findings from a workshop with\nprospective VLM users inform preliminary requirements for future VLM trust\nstudies.",
      "url": "http://arxiv.org/abs/2505.05318v1",
      "published_time_eastern_timestamp": 1746716569.0
    },
    {
      "title": "HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL Workflow",
      "summary": "Recent advances in leveraging the agentic paradigm of large language models\n(LLMs) utilization have significantly enhanced Text-to-SQL capabilities,\nenabling users without specialized database expertise to query data\nintuitively. However, deploying these agentic LLM-based Text-to-SQL systems in\nproduction poses substantial challenges due to their inherently multi-stage\nworkflows, stringent latency constraints, and potentially heterogeneous GPU\ninfrastructure in enterprise environments. Current LLM serving frameworks lack\neffective mechanisms for handling interdependent inference tasks, dynamic\nlatency variability, and resource heterogeneity, leading to suboptimal\nperformance and frequent service-level objective (SLO) violations. In this\npaper, we introduce HEXGEN-TEXT2SQL, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters that handle multi-tenant end-to-end queries.\nHEXGEN-TEXT2SQL introduce a hierarchical scheduling approach combining global\nworkload-balanced task dispatching and local adaptive urgency-guided\nprioritization, guided by a systematic analysis of agentic Text-to-SQL\nworkflows. Additionally, we propose a lightweight simulation-based method for\ntuning critical scheduling hyperparameters, further enhancing robustness and\nadaptability. Our extensive evaluation on realistic Text-to-SQL benchmarks\ndemonstrates that HEXGEN-TEXT2SQL significantly outperforms state-of-the-art\nLLM serving frameworks. Specifically, HEXGEN-TEXT2SQL reduces latency deadlines\nby up to 1.67$\\times$ (average: 1.41$\\times$) and improves system throughput by\nup to 1.75$\\times$ (average: 1.65$\\times$) compared to vLLM under diverse,\nrealistic workload conditions. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow.",
      "url": "http://arxiv.org/abs/2505.05286v1",
      "published_time_eastern_timestamp": 1746714527.0
    },
    {
      "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for\n  CodeLLMs and Agents",
      "summary": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.",
      "url": "http://arxiv.org/abs/2505.05283v1",
      "published_time_eastern_timestamp": 1746714465.0
    },
    {
      "title": "Enhancing Cooperative Multi-Agent Reinforcement Learning with State\n  Modelling and Adversarial Exploration",
      "summary": "Learning to cooperate in distributed partially observable environments with\nno communication abilities poses significant challenges for multi-agent deep\nreinforcement learning (MARL). This paper addresses key concerns in this\ndomain, focusing on inferring state representations from individual agent\nobservations and leveraging these representations to enhance agents'\nexploration and collaborative task execution policies. To this end, we propose\na novel state modelling framework for cooperative MARL, where agents infer\nmeaningful belief representations of the non-observable state, with respect to\noptimizing their own policies, while filtering redundant and less informative\njoint state information. Building upon this framework, we propose the MARL SMPE\nalgorithm. In SMPE, agents enhance their own policy's discriminative abilities\nunder partial observability, explicitly by incorporating their beliefs into the\npolicy network, and implicitly by adopting an adversarial type of exploration\npolicies which encourages agents to discover novel, high-value states while\nimproving the discriminative abilities of others. Experimentally, we show that\nSMPE outperforms state-of-the-art MARL algorithms in complex fully cooperative\ntasks from the MPE, LBF, and RWARE benchmarks.",
      "url": "http://arxiv.org/abs/2505.05262v1",
      "published_time_eastern_timestamp": 1746713240.0
    },
    {
      "title": "Multi-Objective Reinforcement Learning for Adaptive Personalized\n  Autonomous Driving",
      "summary": "Human drivers exhibit individual preferences regarding driving style.\nAdapting autonomous vehicles to these preferences is essential for user trust\nand satisfaction. However, existing end-to-end driving approaches often rely on\npredefined driving styles or require continuous user feedback for adaptation,\nlimiting their ability to support dynamic, context-dependent preferences. We\npropose a novel approach using multi-objective reinforcement learning (MORL)\nwith preference-driven optimization for end-to-end autonomous driving that\nenables runtime adaptation to driving style preferences. Preferences are\nencoded as continuous weight vectors to modulate behavior along interpretable\nstyle objectives$\\unicode{x2013}$including efficiency, comfort, speed, and\naggressiveness$\\unicode{x2013}$without requiring policy retraining. Our\nsingle-policy agent integrates vision-based perception in complex mixed-traffic\nscenarios and is evaluated in diverse urban environments using the CARLA\nsimulator. Experimental results demonstrate that the agent dynamically adapts\nits driving behavior according to changing preferences while maintaining\nperformance in terms of collision avoidance and route completion.",
      "url": "http://arxiv.org/abs/2505.05223v1",
      "published_time_eastern_timestamp": 1746710197.0
    },
    {
      "title": "Incentive-Aware Machine Learning; Robustness, Fairness, Improvement &\n  Causality",
      "summary": "The article explores the emerging domain of incentive-aware machine learning\n(ML), which focuses on algorithmic decision-making in contexts where\nindividuals can strategically modify their inputs to influence outcomes. It\ncategorizes the research into three perspectives: robustness, aiming to design\nmodels resilient to \"gaming\"; fairness, analyzing the societal impacts of such\nsystems; and improvement/causality, recognizing situations where strategic\nactions lead to genuine personal or societal improvement. The paper introduces\na unified framework encapsulating models for these perspectives, including\noffline, online, and causal settings, and highlights key challenges such as\ndifferentiating between gaming and improvement and addressing heterogeneity\namong agents. By synthesizing findings from diverse works, we outline\ntheoretical advancements and practical solutions for robust, fair, and\ncausally-informed incentive-aware ML systems.",
      "url": "http://arxiv.org/abs/2505.05211v1",
      "published_time_eastern_timestamp": 1746709472.0
    },
    {
      "title": "Societal and technological progress as sewing an ever-growing,\n  ever-changing, patchy, and polychrome quilt",
      "summary": "Artificial Intelligence (AI) systems are increasingly placed in positions\nwhere their decisions have real consequences, e.g., moderating online spaces,\nconducting research, and advising on policy. Ensuring they operate in a safe\nand ethically acceptable fashion is thus critical. However, most solutions have\nbeen a form of one-size-fits-all \"alignment\". We are worried that such systems,\nwhich overlook enduring moral diversity, will spark resistance, erode trust,\nand destabilize our institutions. This paper traces the underlying problem to\nan often-unstated Axiom of Rational Convergence: the idea that under ideal\nconditions, rational agents will converge in the limit of conversation on a\nsingle ethics. Treating that premise as both optional and doubtful, we propose\nwhat we call the appropriateness framework: an alternative approach grounded in\nconflict theory, cultural evolution, multi-agent systems, and institutional\neconomics. The appropriateness framework treats persistent disagreement as the\nnormal case and designs for it by applying four principles: (1) contextual\ngrounding, (2) community customization, (3) continual adaptation, and (4)\npolycentric governance. We argue here that adopting these design principles is\na good way to shift the main alignment metaphor from moral unification to a\nmore productive metaphor of conflict management, and that taking this step is\nboth desirable and urgent.",
      "url": "http://arxiv.org/abs/2505.05197v1",
      "published_time_eastern_timestamp": 1746708907.0
    },
    {
      "title": "MARK: Memory Augmented Refinement of Knowledge",
      "summary": "Large Language Models (LLMs) assist in specialized tasks but struggle to\nalign with evolving domain knowledge without costly fine-tuning. Domain\nknowledge consists of: Knowledge: Immutable facts (e.g., 'A stone is solid')\nand generally accepted principles (e.g., ethical standards); Refined Memory:\nEvolving insights shaped by business needs and real-world changes. However, a\nsignificant gap often exists between a domain expert's deep, nuanced\nunderstanding and the system's domain knowledge, which can hinder accurate\ninformation retrieval and application. Our Memory-Augmented Refinement of\nKnowledge (MARK) framework enables LLMs to continuously learn without\nretraining by leveraging structured refined memory, inspired by the Society of\nMind. MARK operates through specialized agents, each serving a distinct role:\nResidual Refined Memory Agent: Stores and retrieves domain-specific insights to\nmaintain context over time; User Question Refined Memory Agent: Captures\nuser-provided facts, abbreviations, and terminology for better comprehension;\nLLM Response Refined Memory Agent: Extracts key elements from responses for\nrefinement and personalization. These agents analyse stored refined memory,\ndetect patterns, resolve contradictions, and improve response accuracy.\nTemporal factors like recency and frequency prioritize relevant information\nwhile discarding outdated insights. MARK enhances LLMs in multiple ways: Ground\nTruth Strategy: Reduces hallucinations by establishing a structured reference;\nDomain-Specific Adaptation: Essential for fields like healthcare, law, and\nmanufacturing, where proprietary insights are absent from public datasets;\nPersonalized AI Assistants: Improves virtual assistants by remembering user\npreferences, ensuring coherent responses over time.",
      "url": "http://arxiv.org/abs/2505.05177v1",
      "published_time_eastern_timestamp": 1746707280.0
    },
    {
      "title": "Bandit Max-Min Fair Allocation",
      "summary": "In this paper, we study a new decision-making problem called the bandit\nmax-min fair allocation (BMMFA) problem. The goal of this problem is to\nmaximize the minimum utility among agents with additive valuations by\nrepeatedly assigning indivisible goods to them. One key feature of this problem\nis that each agent's valuation for each item can only be observed through the\nsemi-bandit feedback, while existing work supposes that the item values are\nprovided at the beginning of each round. Another key feature is that the\nalgorithm's reward function is not additive with respect to rounds, unlike most\nbandit-setting problems.\n  Our first contribution is to propose an algorithm that has an asymptotic\nregret bound of $O(m\\sqrt{T}\\ln T/n + m\\sqrt{T \\ln(mnT)})$, where $n$ is the\nnumber of agents, $m$ is the number of items, and $T$ is the time horizon. This\nis based on a novel combination of bandit techniques and a resource allocation\nalgorithm studied in the literature on competitive analysis. Our second\ncontribution is to provide the regret lower bound of $\\Omega(m\\sqrt{T}/n)$.\nWhen $T$ is sufficiently larger than $n$, the gap between the upper and lower\nbounds is a logarithmic factor of $T$.",
      "url": "http://arxiv.org/abs/2505.05169v1",
      "published_time_eastern_timestamp": 1746706160.0
    },
    {
      "title": "Taming OOD Actions for Offline Reinforcement Learning: An\n  Advantage-Based Approach",
      "summary": "Offline reinforcement learning (RL) aims to learn decision-making policies\nfrom fixed datasets without online interactions, providing a practical solution\nwhere online data collection is expensive or risky. However, offline RL often\nsuffers from distribution shift, resulting in inaccurate evaluation and\nsubstantial overestimation on out-of-distribution (OOD) actions. To address\nthis, existing approaches incorporate conservatism by indiscriminately\ndiscouraging all OOD actions, thereby hindering the agent's ability to\ngeneralize and exploit beneficial ones. In this paper, we propose\nAdvantage-based Diffusion Actor-Critic (ADAC), a novel method that\nsystematically evaluates OOD actions using the batch-optimal value function.\nBased on this evaluation, ADAC defines an advantage function to modulate the\nQ-function update, enabling more precise assessment of OOD action quality. We\ndesign a custom PointMaze environment and collect datasets to visually reveal\nthat advantage modulation can effectively identify and select superior OOD\nactions. Extensive experiments show that ADAC achieves state-of-the-art\nperformance on almost all tasks in the D4RL benchmark, with particularly clear\nmargins on the more challenging tasks.",
      "url": "http://arxiv.org/abs/2505.05126v1",
      "published_time_eastern_timestamp": 1746701848.0
    },
    {
      "title": "Is there a half-life for the success rates of AI agents?",
      "summary": "Building on the recent empirical work of Kwa et al. (2025), I show that\nwithin their suite of research-engineering tasks the performance of AI agents\non longer-duration tasks can be explained by an extremely simple mathematical\nmodel -- a constant rate of failing during each minute a human would take to do\nthe task. This implies an exponentially declining success rate with the length\nof the task and that each agent could be characterised by its own half-life.\nThis empirical regularity allows us to estimate the success rate for an agent\nat different task lengths. And the fact that this model is a good fit for the\ndata is suggestive of the underlying causes of failure on longer tasks -- that\nthey involve increasingly large sets of subtasks where failing any one fails\nthe task. Whether this model applies more generally on other suites of tasks is\nunknown and an important subject for further work.",
      "url": "http://arxiv.org/abs/2505.05115v1",
      "published_time_eastern_timestamp": 1746700263.0
    },
    {
      "title": "Multi-agent Embodied AI: Advances and Future Directions",
      "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the\napplication of advanced technologies in the intelligent era, where AI systems\nare integrated with physical bodies that enable them to perceive, reason, and\ninteract with their environments. Through the use of sensors for input and\nactuators for action, these systems can learn and adapt based on real-world\nfeedback, allowing them to perform tasks effectively in dynamic and\nunpredictable environments. As techniques such as deep learning (DL),\nreinforcement learning (RL), and large language models (LLMs) mature, embodied\nAI has become a leading field in both academia and industry, with applications\nspanning robotics, healthcare, transportation, and manufacturing. However, most\nresearch has focused on single-agent systems that often assume static, closed\nenvironments, whereas real-world embodied AI must navigate far more complex\nscenarios. In such settings, agents must not only interact with their\nsurroundings but also collaborate with other agents, necessitating\nsophisticated mechanisms for adaptation, real-time learning, and collaborative\nproblem-solving. Despite increasing interest in multi-agent systems, existing\nresearch remains narrow in scope, often relying on simplified models that fail\nto capture the full complexity of dynamic, open environments for multi-agent\nembodied AI. Moreover, no comprehensive survey has systematically reviewed the\nadvancements in this area. As embodied AI rapidly evolves, it is crucial to\ndeepen our understanding of multi-agent embodied AI to address the challenges\npresented by real-world applications. To fill this gap and foster further\ndevelopment in the field, this paper reviews the current state of research,\nanalyzes key contributions, and identifies challenges and future directions,\nproviding insights to guide innovation and progress in this field.",
      "url": "http://arxiv.org/abs/2505.05108v1",
      "published_time_eastern_timestamp": 1746699233.0
    },
    {
      "title": "Enhancing Reinforcement Learning for the Floorplanning of Analog ICs\n  with Beam Search",
      "summary": "The layout of analog ICs requires making complex trade-offs, while addressing\ndevice physics and variability of the circuits. This makes full automation with\nlearning-based solutions hard to achieve. However, reinforcement learning (RL)\nhas recently reached significant results, particularly in solving the\nfloorplanning problem. This paper presents a hybrid method that combines RL\nwith a beam (BS) strategy. The BS algorithm enhances the agent's inference\nprocess, allowing for the generation of flexible floorplans by accomodating\nvarious objective weightings, and addressing congestion without without the\nneed for policy retraining or fine-tuning. Moreover, the RL agent's\ngeneralization ability stays intact, along with its efficient handling of\ncircuit features and constraints. Experimental results show approx. 5-85%\nimprovement in area, dead space and half-perimeter wire length compared to a\nstandard RL application, along with higher rewards for the agent. Moreover,\nperformance and efficiency align closely with those of existing\nstate-of-the-art techniques.",
      "url": "http://arxiv.org/abs/2505.05059v1",
      "published_time_eastern_timestamp": 1746694232.0
    },
    {
      "title": "Neural Pathways to Program Success: Hopfield Networks for PERT Analysis",
      "summary": "Project and task scheduling under uncertainty remains a fundamental challenge\nin program and project management, where accurate estimation of task durations\nand dependencies is critical for delivering complex, multi project systems. The\nProgram Evaluation and Review Technique provides a probabilistic framework to\nmodel task variability and critical paths. In this paper, the author presents a\nnovel formulation of PERT scheduling as an energy minimization problem within a\nHopfield neural network architecture. By mapping task start times and\nprecedence constraints into a neural computation framework, the networks\ninherent optimization dynamics is exploited to approximate globally consistent\nschedules. The author addresses key theoretical issues related to energy\nfunction differentiability, constraint encoding, and convergence, and extends\nthe Hopfield model for structured precedence graphs. Numerical simulations on\nsynthetic project networks comprising up to 1000 tasks demonstrate the\nviability of this approach, achieving near optimal makespans with minimal\nconstraint violations. The findings suggest that neural optimization models\noffer a promising direction for scalable and adaptive project tasks scheduling\nunder uncertainty in areas such as the agentic AI workflows, microservice based\napplications that the modern AI systems are being built upon.",
      "url": "http://arxiv.org/abs/2505.05047v1",
      "published_time_eastern_timestamp": 1746693256.0
    }
  ]
}
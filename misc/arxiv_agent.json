{
  "last_updated": "2025-09-10T17:10:21.197754-04:00",
  "papers": [
    {
      "title": "CAViAR: Critic-Augmented Video Agentic Reasoning",
      "summary": "Video understanding has seen significant progress in recent years, with\nmodels' performance on perception from short clips continuing to rise. Yet,\nmultiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, show\nperformance wanes for tasks requiring complex reasoning on videos as queries\ngrow more complex and videos grow longer. In this work, we ask: can existing\nperception capabilities be leveraged to successfully perform more complex video\nreasoning? In particular, we develop a large language model agent given access\nto video modules as subagents or tools. Rather than following a fixed procedure\nto solve queries as in previous work such as Visual Programming, ViperGPT, and\nMoReVQA, the agent uses the results of each call to a module to determine\nsubsequent steps. Inspired by work in the textual reasoning domain, we\nintroduce a critic to distinguish between instances of successful and\nunsuccessful sequences from the agent. We show that the combination of our\nagent and critic achieve strong performance on the previously-mentioned\ndatasets.",
      "url": "http://arxiv.org/abs/2509.07680v1",
      "published_time_eastern_timestamp": 1757440779.0
    },
    {
      "title": "When Heterogeneity Drives Hysteresis: Anticonformity in the Multistate\n  q-Voter Model on Networks",
      "summary": "Discontinuous phase transitions are closely linked to tipping points,\ncritical mass effects, and hysteresis, phenomena that have been confirmed\nempirically and recognized as highly important in social systems. The\nmultistate $q$-voter model, an agent-based approach to simulate discrete\ndecision-making and opinion dynamics, is particularly relevant in this context.\nPrevious studies of the $q$-voter model with anticonformity on complete graphs\nuncovered a counterintuitive result. Changing the model formulation from the\nannealed (homogeneous agents with varying behavior) to quenched (heterogeneous\nagents with fixed behavior) produces discontinuous phase transitions. This is\ncontrary to the common expectation that quenched heterogeneity smooths\ntransitions. To test whether this effect is merely a mean-field artifact, we\nextend the analysis to random graphs. Using pair approximation and Monte Carlo\nsimulations, we show that the phenomenon persists beyond the complete graph,\nspecifically on random graphs and Barab\\'asi-Albert scale-free networks. The\nnovelty of our work is twofold: (i) we demonstrate for the first time that\nreplacing the annealed with the quenched approach can change the type of phase\ntransitions from continuous to discontinuous not only on complete graphs but\nalso on sparser networks, and (ii) we provide pair-approximation results for\nthe multistate $q$-voter model with competing conformity and anticonformity\nmechanisms, covering both quenched and annealed cases, which had previously\nbeen studied only in binary models.",
      "url": "http://arxiv.org/abs/2509.07944v1",
      "published_time_eastern_timestamp": 1757438826.0
    },
    {
      "title": "Guided Reasoning in LLM-Driven Penetration Testing Using Structured\n  Attack Trees",
      "summary": "Recent advances in Large Language Models (LLMs) have driven interest in\nautomating cybersecurity penetration testing workflows, offering the promise of\nfaster and more consistent vulnerability assessment for enterprise systems.\nExisting LLM agents for penetration testing primarily rely on self-guided\nreasoning, which can produce inaccurate or hallucinated procedural steps. As a\nresult, the LLM agent may undertake unproductive actions, such as exploiting\nunused software libraries or generating cyclical responses that repeat prior\ntactics. In this work, we propose a guided reasoning pipeline for penetration\ntesting LLM agents that incorporates a deterministic task tree built from the\nMITRE ATT&CK Matrix, a proven penetration testing kll chain, to constrain the\nLLM's reaoning process to explicitly defined tactics, techniques, and\nprocedures. This anchors reasoning in proven penetration testing methodologies\nand filters out ineffective actions by guiding the agent towards more\nproductive attack procedures. To evaluate our approach, we built an automated\npenetration testing LLM agent using three LLMs (Llama-3-8B, Gemini-1.5, and\nGPT-4) and applied it to navigate 10 HackTheBox cybersecurity exercises with\n103 discrete subtasks representing real-world cyberattack scenarios. Our\nproposed reasoning pipeline guided the LLM agent through 71.8\\%, 72.8\\%, and\n78.6\\% of subtasks using Llama-3-8B, Gemini-1.5, and GPT-4, respectively.\nComparatively, the state-of-the-art LLM penetration testing tool using\nself-guided reasoning completed only 13.5\\%, 16.5\\%, and 75.7\\% of subtasks and\nrequired 86.2\\%, 118.7\\%, and 205.9\\% more model queries. This suggests that\nincorporating a deterministic task tree into LLM reasoning pipelines can\nenhance the accuracy and efficiency of automated cybersecurity assessments",
      "url": "http://arxiv.org/abs/2509.07939v1",
      "published_time_eastern_timestamp": 1757438373.0
    },
    {
      "title": "An Enactivist Approach to Human-Computer Interaction: Bridging the Gap\n  Between Human Agency and Affordances",
      "summary": "Emerging paradigms in XR, AI, and BCI contexts necessitate novel theoretical\nframeworks for understanding human autonomy and agency in HCI. Drawing from\nenactivist theories of cognition, we conceptualize human agents as\nself-organizing, operationally closed systems that actively enact their\ncognitive domains through dynamic interaction with their environments. To\ndevelop measurable variables aligned with this framework, we introduce\n\"feelings of agency\" (FoA) as an alternative to the established construct of\n\"sense of agency\" (SoA), refining Synofzyk's multifactorial weighting model and\noffering a novel conceptual pathway for overcoming gaps in the dominant\ncomparator model. We define FoA as comprising two subconstructs: affective\nengagement and volitional attention, which we operationalize through integrated\nneurodynamic indicators (valence, arousal, cross frequency coupling within the\ndorsal attention system) and first-person phenomenological reports. We argue\nthat these neurophenomenological indicators provide richer, more actionable\ninsights for digital affordance design, particularly in XR, BCI, Human AI\nInteraction (HAX), and generative AI environments. Our framework aims to inform\nand inspire design parameters that significantly enhance human agency in\nrapidly evolving interactive domains.",
      "url": "http://arxiv.org/abs/2509.07871v1",
      "published_time_eastern_timestamp": 1757433434.0
    },
    {
      "title": "KLIPA: A Knowledge Graph and LLM-Driven QA Framework for IP Analysis",
      "summary": "Effectively managing intellectual property is a significant challenge.\nTraditional methods for patent analysis depend on labor-intensive manual\nsearches and rigid keyword matching. These approaches are often inefficient and\nstruggle to reveal the complex relationships hidden within large patent\ndatasets, hindering strategic decision-making. To overcome these limitations,\nwe introduce KLIPA, a novel framework that leverages a knowledge graph and a\nlarge language model (LLM) to significantly advance patent analysis. Our\napproach integrates three key components: a structured knowledge graph to map\nexplicit relationships between patents, a retrieval-augmented generation(RAG)\nsystem to uncover contextual connections, and an intelligent agent that\ndynamically determines the optimal strategy for resolving user queries. We\nvalidated KLIPA on a comprehensive, real-world patent database, where it\ndemonstrated substantial improvements in knowledge extraction, discovery of\nnovel connections, and overall operational efficiency. This combination of\ntechnologies enhances retrieval accuracy, reduces reliance on domain experts,\nand provides a scalable, automated solution for any organization managing\nintellectual property, including technology corporations and legal firms,\nallowing them to better navigate the complexities of strategic innovation and\ncompetitive intelligence.",
      "url": "http://arxiv.org/abs/2509.07860v1",
      "published_time_eastern_timestamp": 1757432423.0
    },
    {
      "title": "Multi-Topic Projected Opinion Dynamics for Resource Allocation",
      "summary": "We propose a model of opinion formation on resource allocation among multiple\ntopics by multiple agents, who are subject to hard budget constraints. We\ndefine a utility function for each agent and then derive a projected dynamical\nsystem model of opinion evolution assuming that each agent myopically seeks to\nmaximize its utility subject to its constraints. Inter-agent coupling arises\nfrom an undirected social network, while inter-topic coupling arises from\nresource constraints. We show that opinions always converge to the equilibrium\nset. For special networks with very weak antagonistic relations, the opinions\nconverge to a unique equilibrium point. We further show that the underlying\nopinion formation game is a potential game. We relate the equilibria of the\ndynamics and the Nash equilibria of the game and characterize the unique Nash\nequilibrium for networks with no antagonistic relations. Finally, simulations\nillustrate our findings.",
      "url": "http://arxiv.org/abs/2509.07847v1",
      "published_time_eastern_timestamp": 1757431392.0
    },
    {
      "title": "AgentSentinel: An End-to-End and Real-Time Security Defense Framework\n  for Computer-Use Agents",
      "summary": "Large Language Models (LLMs) have been increasingly integrated into\ncomputer-use agents, which can autonomously operate tools on a user's computer\nto accomplish complex tasks. However, due to the inherently unstable and\nunpredictable nature of LLM outputs, they may issue unintended tool commands or\nincorrect inputs, leading to potentially harmful operations. Unlike traditional\nsecurity risks stemming from insecure user prompts, tool execution results from\nLLM-driven decisions introduce new and unique security challenges. These\nvulnerabilities span across all components of a computer-use agent. To mitigate\nthese risks, we propose AgentSentinel, an end-to-end, real-time defense\nframework designed to mitigate potential security threats on a user's computer.\nAgentSentinel intercepts all sensitive operations within agent-related services\nand halts execution until a comprehensive security audit is completed. Our\nsecurity auditing mechanism introduces a novel inspection process that\ncorrelates the current task context with system traces generated during task\nexecution. To thoroughly evaluate AgentSentinel, we present BadComputerUse, a\nbenchmark consisting of 60 diverse attack scenarios across six attack\ncategories. The benchmark demonstrates a 87% average attack success rate on\nfour state-of-the-art LLMs. Our evaluation shows that AgentSentinel achieves an\naverage defense success rate of 79.6%, significantly outperforming all baseline\ndefenses.",
      "url": "http://arxiv.org/abs/2509.07764v1",
      "published_time_eastern_timestamp": 1757426340.0
    },
    {
      "title": "Prescribed-Time Event-Triggered Control for Matrix-Scaled Networks",
      "summary": "This article proposes a distributed control method for matrix-scaled\nmulti-agent networks aimed at achieving convergence within a user-defined time\nframe. The control law of each individual agent relies only on information from\nneighboring agents and is updated at discrete intervals determined by\nstate-dependent triggering functions, reducing the frequency of agent\ninteractions. To this end, first, the controller is augmented with a\ntime-varying gain. Then, the dynamics of the closed-loop system over the\nfinite-time interval is transformed into an infinite-time frame using time\nscaling. Lyapunov-based analysis is employed to derive suitable triggering\nconditions that guarantee the asymptotic convergence of the time-transformed\nsystem, thereby ensuring the prescribed-time convergence of the original\nsystem.",
      "url": "http://arxiv.org/abs/2509.07703v1",
      "published_time_eastern_timestamp": 1757423397.0
    },
    {
      "title": "Inference of Intrinsic Rewards and Fairness in Multi-Agent Systems",
      "summary": "From altruism to antagonism, fairness plays a central role in social\ninteractions. But can we truly understand how fair someone is, especially\nwithout explicit knowledge of their preferences? We cast this challenge as a\nmulti-agent inverse reinforcement learning problem, explicitly structuring\nrewards to reflect how agents value the welfare of others. We introduce novel\nBayesian strategies, reasoning about the optimality of demonstrations and\ncharacterisation of equilibria in general-sum Markov games. Our experiments,\nspanning randomised environments and a collaborative cooking task, reveal that\ncoherent notions of fairness can be reliably inferred from demonstrations.\nFurthermore, when isolating fairness components, we obtain a disentangled\nunderstanding of agents preferences. Crucially, we unveil that by placing\nagents in different groups, we can force them to exhibit new facets of their\nreward structures, cutting through ambiguity to answer the central question:\nwho is being fair?",
      "url": "http://arxiv.org/abs/2509.07650v1",
      "published_time_eastern_timestamp": 1757420230.0
    },
    {
      "title": "Getting In Contract with Large Language Models -- An Agency Theory\n  Perspective On Large Language Model Alignment",
      "summary": "Adopting Large language models (LLMs) in organizations potentially\nrevolutionizes our lives and work. However, they can generate off-topic,\ndiscriminating, or harmful content. This AI alignment problem often stems from\nmisspecifications during the LLM adoption, unnoticed by the principal due to\nthe LLM's black-box nature. While various research disciplines investigated AI\nalignment, they neither address the information asymmetries between\norganizational adopters and black-box LLM agents nor consider organizational AI\nadoption processes. Therefore, we propose LLM ATLAS (LLM Agency Theory-Led\nAlignment Strategy) a conceptual framework grounded in agency (contract)\ntheory, to mitigate alignment problems during organizational LLM adoption. We\nconduct a conceptual literature analysis using the organizational LLM adoption\nphases and the agency theory as concepts. Our approach results in (1) providing\nan extended literature analysis process specific to AI alignment methods during\norganizational LLM adoption and (2) providing a first LLM alignment\nproblem-solution space.",
      "url": "http://arxiv.org/abs/2509.07642v1",
      "published_time_eastern_timestamp": 1757419814.0
    },
    {
      "title": "From Classical Data to Quantum Advantage -- Quantum Policy Evaluation on\n  Quantum Hardware",
      "summary": "Quantum policy evaluation (QPE) is a reinforcement learning (RL) algorithm\nwhich is quadratically more efficient than an analogous classical Monte Carlo\nestimation. It makes use of a direct quantum mechanical realization of a finite\nMarkov decision process, in which the agent and the environment are modeled by\nunitary operators and exchange states, actions, and rewards in superposition.\nPreviously, the quantum environment has been implemented and parametrized\nmanually for an illustrative benchmark using a quantum simulator. In this\npaper, we demonstrate how these environment parameters can be learned from a\nbatch of classical observational data through quantum machine learning (QML) on\nquantum hardware. The learned quantum environment is then applied in QPE to\nalso compute policy evaluations on quantum hardware. Our experiments reveal\nthat, despite challenges such as noise and short coherence times, the\nintegration of QML and QPE shows promising potential for achieving quantum\nadvantage in RL.",
      "url": "http://arxiv.org/abs/2509.07614v1",
      "published_time_eastern_timestamp": 1757417785.0
    },
    {
      "title": "K2-Think: A Parameter-Efficient Reasoning System",
      "summary": "K2-Think is a reasoning system that achieves state-of-the-art performance\nwith a 32B parameter model, matching or surpassing much larger models like\nGPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, our system\nshows that smaller models can compete at the highest levels by combining\nadvanced post-training and test-time computation techniques. The approach is\nbased on six key technical pillars: Long Chain-of-thought Supervised\nFinetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic\nplanning prior to reasoning, Test-time Scaling, Speculative Decoding, and\nInference-optimized Hardware, all using publicly available open-source\ndatasets. K2-Think excels in mathematical reasoning, achieving state-of-the-art\nscores on public benchmarks for open-source models, while also performing\nstrongly in other areas such as Code and Science. Our results confirm that a\nmore parameter-efficient model like K2-Think 32B can compete with\nstate-of-the-art systems through an integrated post-training recipe that\nincludes long chain-of-thought training and strategic inference-time\nenhancements, making open-source reasoning systems more accessible and\naffordable. K2-Think is freely available at k2think.ai, offering best-in-class\ninference speeds of over 2,000 tokens per second per request via the Cerebras\nWafer-Scale Engine.",
      "url": "http://arxiv.org/abs/2509.07604v1",
      "published_time_eastern_timestamp": 1757417155.0
    },
    {
      "title": "AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with\n  FaaS-hosted MCP Services",
      "summary": "Generative Artificial Intelligence (GenAI) has rapidly transformed various\nfields including code generation, text summarization, image generation and so\non. Agentic AI is a recent evolution that further advances this by coupling the\ndecision making and generative capabilities of LLMs with actions that can be\nperformed using tools. While seemingly powerful, Agentic systems often struggle\nwhen faced with numerous tools, complex multi-step tasks,and long-context\nmanagement to track history and avoid hallucinations. Workflow patterns such as\nChain-of-Thought (CoT) and ReAct help address this. Here, we define a novel\nagentic workflow pattern, AgentX, composed of stage designer, planner, and\nexecutor agents that is competitive or better than the state-of-the-art agentic\npatterns. We also leverage Model Context Protocol (MCP) tools, and propose two\nalternative approaches for deploying MCP servers as cloud Functions as a\nService (FaaS). We empirically evaluate the success rate, latency and cost for\nAgentX and two contemporary agentic patterns, ReAct and Magentic One, using\nthese the FaaS and local MCP server alternatives for three practical\napplications. This highlights the opportunities and challenges of designing and\ndeploying agentic workflows.",
      "url": "http://arxiv.org/abs/2509.07595v1",
      "published_time_eastern_timestamp": 1757416070.0
    },
    {
      "title": "Sorting of binary active-passive mixtures in designed microchannels",
      "summary": "Mixtures of active and passive particles are ubiquitous at the microscale.\nMany essential microbial processes involve interactions with dead or immotile\ncells or passive crowders. When passive objects are immersed in active baths,\ntheir transport properties are enhanced and can be tuned by controlling active\nagents' spatial and orientational distribution. Active-passive mixtures provide\na platform to explore fundamental questions about the emergent behaviour of\npassive objects under simultaneous thermal and active noise and a foundation\nfor technological applications in cargo delivery and bioremediation. In this\nwork, we use computational simulations to study an active-passive mixture\nconfined in microchannels designed with funnel-like obstacles that selectively\nallow the passage of passive particles. Active particles follow overdamped\nLangevin translational dynamics and run-and-tumble rotational dynamics. We find\nthat adjusting the tumbling rate of active agents and the microchannel geometry\nleads to a maximum enhancement of the transport properties of the passive\nparticles (diffusion coefficient and advective velocity) that correlates with\nthe highest mixture sorting efficiency and the shortest response time.",
      "url": "http://arxiv.org/abs/2509.07582v1",
      "published_time_eastern_timestamp": 1757414818.0
    },
    {
      "title": "Towards Generalized Routing: Model and Agent Orchestration for Adaptive\n  and Efficient Inference",
      "summary": "The rapid advancement of large language models (LLMs) and domain-specific AI\nagents has greatly expanded the ecosystem of AI-powered services. User queries,\nhowever, are highly diverse and often span multiple domains and task types,\nresulting in a complex and heterogeneous landscape. This diversity presents a\nfundamental routing challenge: how to accurately direct each query to an\nappropriate execution unit while optimizing both performance and efficiency. To\naddress this, we propose MoMA (Mixture of Models and Agents), a generalized\nrouting framework that integrates both LLM and agent-based routing. Built upon\na deep understanding of model and agent capabilities, MoMA effectively handles\ndiverse queries through precise intent recognition and adaptive routing\nstrategies, achieving an optimal balance between efficiency and cost.\nSpecifically, we construct a detailed training dataset to profile the\ncapabilities of various LLMs under different routing model structures,\nidentifying the most suitable tasks for each LLM. During inference, queries are\ndynamically routed to the LLM with the best cost-performance efficiency. We\nalso introduce an efficient agent selection strategy based on a context-aware\nstate machine and dynamic masking. Experimental results demonstrate that the\nMoMA router offers superior cost-efficiency and scalability compared to\nexisting approaches.",
      "url": "http://arxiv.org/abs/2509.07571v1",
      "published_time_eastern_timestamp": 1757412942.0
    },
    {
      "title": "VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for\n  Trustworthy OS Agents",
      "summary": "With the rapid progress of multimodal large language models, operating system\n(OS) agents become increasingly capable of automating tasks through on-device\ngraphical user interfaces (GUIs). However, most existing OS agents are designed\nfor idealized settings, whereas real-world environments often present\nuntrustworthy conditions. To mitigate risks of over-execution in such\nscenarios, we propose a query-driven human-agent-GUI interaction framework that\nenables OS agents to decide when to query humans for more reliable task\ncompletion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy\nOS agent trained with a two-stage learning paradigm that falicitate the\ndecoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent\nautonomously executes actions in normal conditions while proactively querying\nhumans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves\nthe average step-wise success rate by 20.64\\% in untrustworthy scenarios over\nthe state-of-the-art, without compromising normal performance. Analysis\nhighlights VeriOS-Agent's rationality, generalizability, and scalability. The\ncodes, datasets and models are available at\nhttps://github.com/Wuzheng02/VeriOS.",
      "url": "http://arxiv.org/abs/2509.07553v1",
      "published_time_eastern_timestamp": 1757411161.0
    },
    {
      "title": "Persuading Agents in Opinion Formation Games",
      "summary": "Prominent opinion formation models such as the one by Friedkin and Johnsen\n(FJ) concentrate on the effects of peer pressure on public opinions. In\npractice, opinion formation is also based on information about the state of the\nworld and persuasion efforts. In this paper, we analyze an approach of Bayesian\npersuasion in the FJ model. There is an unknown state of the world that\ninfluences the preconceptions of n agents. A sender S can (partially) reveal\ninformation about the state to all agents. The agents update their\npreconceptions, and an equilibrium of public opinions emerges. We propose\nalgorithms for the sender to reveal information in order to optimize various\naspects of the emerging equilibrium. For many natural sender objectives, we\nshow that there are simple optimal strategies. We then focus on a general class\nof range-based objectives with desired opinion ranges for each agent. We\nprovide efficient algorithms in several cases, e.g., when the matrix of\npreconceptions in all states has constant rank, or when there is only a\npolynomial number of range combinations that lead to positive value for S. This\ngeneralizes, e.g., instances with a constant number of states and/or agents, or\ninstances with a logarithmic number of ranges. In general, we show that\nsubadditive range-based objectives allow a simple n-approximation, and even for\nadditive ones, obtaining an $n^{1-c}$-approximation is NP-hard, for any\nconstant $c > 0$.",
      "url": "http://arxiv.org/abs/2509.07520v1",
      "published_time_eastern_timestamp": 1757408185.0
    },
    {
      "title": "Astra: A Multi-Agent System for GPU Kernel Performance Optimization",
      "summary": "GPU kernel optimization has long been a central challenge at the intersection\nof high-performance computing and machine learning. Efficient kernels are\ncrucial for accelerating large language model (LLM) training and serving, yet\nattaining high performance typically requires extensive manual tuning.\nCompiler-based systems reduce some of this burden, but still demand substantial\nmanual design and engineering effort. Recently, researchers have explored using\nLLMs for GPU kernel generation, though prior work has largely focused on\ntranslating high-level PyTorch modules into CUDA code. In this work, we\nintroduce Astra, the first LLM-based multi-agent system for GPU kernel\noptimization. Unlike previous approaches, Astra starts from existing CUDA\nimplementations extracted from SGLang, a widely deployed framework for serving\nLLMs, rather than treating PyTorch modules as the specification. Within Astra,\nspecialized LLM agents collaborate through iterative code generation, testing,\nprofiling, and planning to produce kernels that are both correct and\nhigh-performance. On kernels from SGLang, Astra achieves an average speedup of\n1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study\nfurther demonstrates that LLMs can autonomously apply loop transformations,\noptimize memory access patterns, exploit CUDA intrinsics, and leverage fast\nmath operations to yield substantial performance gains. Our work highlights\nmulti-agent LLM systems as a promising new paradigm for GPU kernel\noptimization.",
      "url": "http://arxiv.org/abs/2509.07506v1",
      "published_time_eastern_timestamp": 1757407190.0
    },
    {
      "title": "DREAMS: Decentralized Resource Allocation and Service Management across\n  the Compute Continuum Using Service Affinity",
      "summary": "Modern manufacturing systems require adaptive computing infrastructures that\ncan respond to highly dynamic workloads and increasingly customized production\ndemands. The compute continuum emerges as a promising solution, enabling\nflexible deployment of microservices across distributed, heterogeneous domains.\nHowever, this paradigm also requires a novel approach to resource allocation\nand service placement, as traditional centralized solutions struggle to scale\neffectively, suffer from latency bottlenecks, and introduce single points of\nfailure. In this paper, we present DREAMS, a decentralized framework that\noptimizes microservice placement decisions collaboratively across different\ncomputational domains. At its core, DREAMS introduces agents that operate\nautonomously within each domain while coordinating globally through a\nRaft-based consensus algorithm and cost-benefit voting. This decentralized\narchitecture enables responsive, privacy-preserving, and fault-tolerant\ncoordination, making it particularly suitable given the growing prevalence of\nmulti-stakeholder scenarios across the compute continuum. In particular, within\nmodern manufacturing environments, DREAMS achieves globally optimized service\nplacements while maintaining high fault tolerance. Further evaluations\ndemonstrate that key coordination operations, such as Local Domain Manager\n(LDM) registration and migration voting, scale sub-linearly with the number of\ndomains, confirming the efficiency and scalability of our proposal.",
      "url": "http://arxiv.org/abs/2509.07497v1",
      "published_time_eastern_timestamp": 1757406192.0
    },
    {
      "title": "Feed-O-Meter: Fostering Design Feedback Skills through Role-playing\n  Interactions with AI Mentee",
      "summary": "Effective feedback, including critique and evaluation, helps designers\ndevelop design concepts and refine their ideas, supporting informed\ndecision-making throughout the iterative design process. However, in\nstudio-based design courses, students often struggle to provide feedback due to\na lack of confidence and fear of being judged, which limits their ability to\ndevelop essential feedback-giving skills. Recent advances in large language\nmodels (LLMs) suggest that role-playing with AI agents can let learners engage\nin multi-turn feedback without the anxiety of external judgment or the time\nconstraints of real-world settings. Yet prior studies have raised concerns that\nLLMs struggle to behave like real people in role-play scenarios, diminishing\nthe educational benefits of these interactions. Therefore, designing AI-based\nagents that effectively support learners in practicing and developing\nintellectual reasoning skills requires more than merely assigning the target\npersona's personality and role to the agent. By addressing these issues, we\npresent Feed-O-Meter, a novel system that employs carefully designed LLM-based\nagents to create an environment in which students can practice giving design\nfeedback. The system enables users to role-play as mentors, providing feedback\nto an AI mentee and allowing them to reflect on how that feedback impacts the\nAI mentee's idea development process. A user study (N=24) indicated that\nFeed-O-Meter increased participants' engagement and motivation through\nrole-switching and helped them adjust feedback to be more comprehensible for an\nAI mentee. Based on these findings, we discuss future directions for designing\nsystems to foster feedback skills in design education.",
      "url": "http://arxiv.org/abs/2509.07424v1",
      "published_time_eastern_timestamp": 1757399493.0
    }
  ]
}
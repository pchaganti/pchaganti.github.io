{
  "last_updated": "2025-08-12T17:11:35.927942-04:00",
  "papers": [
    {
      "title": "LL3M: Large Language 3D Modelers",
      "summary": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.",
      "url": "http://arxiv.org/abs/2508.08228v1",
      "published_time_eastern_timestamp": 1754934482.0
    },
    {
      "title": "Reinforcement Learning in Vision: A Survey",
      "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
      "url": "http://arxiv.org/abs/2508.08189v1",
      "published_time_eastern_timestamp": 1754932135.0
    },
    {
      "title": "ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based\n  Scene Reconstruction",
      "summary": "Reinforcement learning for training end-to-end autonomous driving models in\nclosed-loop simulations is gaining growing attention. However, most simulation\nenvironments differ significantly from real-world conditions, creating a\nsubstantial simulation-to-reality (sim2real) gap. To bridge this gap, some\napproaches utilize scene reconstruction techniques to create photorealistic\nenvironments as a simulator. While this improves realistic sensor simulation,\nthese methods are inherently constrained by the distribution of the training\ndata, making it difficult to render high-quality sensor data for novel\ntrajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a\nframework designed to integrate video diffusion priors into scene\nreconstruction to aid reinforcement learning, thereby enhancing end-to-end\nautonomous driving training. Specifically, in ReconDreamer-RL, we introduce\nReconSimulator, which combines the video diffusion prior for appearance\nmodeling and incorporates a kinematic model for physical modeling, thereby\nreconstructing driving scenarios from real-world data. This narrows the\nsim2real gap for closed-loop evaluation and reinforcement learning. To cover\nmore corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),\nwhich adjusts the trajectories of surrounding vehicles relative to the ego\nvehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).\nFinally, the Cousin Trajectory Generator (CTG) is proposed to address the issue\nof training data distribution, which is often biased toward simple\nstraight-line movements. Experiments show that ReconDreamer-RL improves\nend-to-end autonomous driving training, outperforming imitation learning\nmethods with a 5x reduction in the Collision Ratio.",
      "url": "http://arxiv.org/abs/2508.08170v1",
      "published_time_eastern_timestamp": 1754930755.0
    },
    {
      "title": "Time-delayed opinion dynamics with leader-follower interactions:\n  consensus, stability, and mean-field limits",
      "summary": "We study a time-delayed variant of the Hegselmann-Krause opinion formation\nmodel featuring a small group of leaders and a large group of non-leaders. In\nthis model, leaders influence all agents but only interact among themselves. At\nthe same time, non-leaders update their opinions via interactions with their\npeers and the leaders, with time delays accounting for communication and\ndecision-making lags. We prove the exponential convergence to consensus of the\nparticle system, without imposing smallness assumptions on the delay\nparameters. Furthermore, we analyze the mean-field limit in two regimes: (i)\nwith a fixed number of leaders and an infinite number of non-leaders, and (ii)\nwith both populations tending to infinity, obtaining existence, uniqueness, and\nexponential decay estimates for the corresponding macroscopic models.",
      "url": "http://arxiv.org/abs/2508.08157v1",
      "published_time_eastern_timestamp": 1754929938.0
    },
    {
      "title": "From Natural Language to Solver-Ready Power System Optimization: An\n  LLM-Assisted, Validation-in-the-Loop Framework",
      "summary": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems",
      "url": "http://arxiv.org/abs/2508.08147v1",
      "published_time_eastern_timestamp": 1754929377.0
    },
    {
      "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in\n  Uncertainty-Aware Language Models",
      "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.",
      "url": "http://arxiv.org/abs/2508.08139v1",
      "published_time_eastern_timestamp": 1754928756.0
    },
    {
      "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design\n  Assistance with Hybrid Contextual Retrieval-Augmented Generation",
      "summary": "Conducting a comprehensive literature review is crucial for advancing circuit\ndesign methodologies. However, the rapid influx of state-of-the-art research,\ninconsistent data representation, and the complexity of optimizing circuit\ndesign objectives make this task significantly challenging. In this paper, we\npropose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for\ncircuit design assistance that integrates a hybrid Retrieval-Augmented\nGeneration (RAG) framework with an adaptive vector database of circuit design\nresearch papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +\nAct (ReAct) workflow for iterative reasoning, goal-setting, and multi-step\ninformation retrieval. It functions as a question-answering design assistant,\ncapable of interpreting complex queries and providing reasoned responses\ngrounded in circuit literature. Its multimodal capabilities enable processing\nof both textual and visual data, facilitating more efficient and comprehensive\nanalysis. The system dynamically adapts using intelligent search tools,\nautomated document retrieval from the internet, and real-time database updates.\nUnlike conventional approaches constrained by model context limits, MuaLLM\ndecouples retrieval from inference, enabling scalable reasoning over\narbitrarily large corpora. At the maximum context length supported by standard\nLLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining\nthe same accuracy. This allows rapid, no-human-in-the-loop database generation,\novercoming the bottleneck of simulation-based dataset creation for circuits. To\nevaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval\nand citation performance, and Reasoning-100 (Reas-100), focused on multistep\nreasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%\naccuracy on Reas-100.",
      "url": "http://arxiv.org/abs/2508.08137v1",
      "published_time_eastern_timestamp": 1754928669.0
    },
    {
      "title": "Deep Reinforcement Learning with Local Interpretability for Transparent\n  Microgrid Resilience Energy Management",
      "summary": "Renewable energy integration into microgrids has become a key approach to\naddressing global energy issues such as climate change and resource scarcity.\nHowever, the variability of renewable sources and the rising occurrence of High\nImpact Low Probability (HILP) events require innovative strategies for reliable\nand resilient energy management. This study introduces a practical approach to\nmanaging microgrid resilience through Explainable Deep Reinforcement Learning\n(XDRL). It combines the Proximal Policy Optimization (PPO) algorithm for\ndecision-making with the Local Interpretable Model-agnostic Explanations (LIME)\nmethod to improve the transparency of the actor network's decisions. A case\nstudy in Ongole, India, examines a microgrid with wind, solar, and battery\ncomponents to validate the proposed approach. The microgrid is simulated under\nextreme weather conditions during the Layla cyclone. LIME is used to analyse\nscenarios, showing the impact of key factors such as renewable generation,\nstate of charge, and load prioritization on decision-making. The results\ndemonstrate a Resilience Index (RI) of 0.9736 and an estimated battery lifespan\nof 15.11 years. LIME analysis reveals the rationale behind the agent's actions\nin idle, charging, and discharging modes, with renewable generation identified\nas the most influential feature. This study shows the effectiveness of\nintegrating advanced DRL algorithms with interpretable AI techniques to achieve\nreliable and transparent energy management in microgrids.",
      "url": "http://arxiv.org/abs/2508.08132v1",
      "published_time_eastern_timestamp": 1754928371.0
    },
    {
      "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown\n  Attacks",
      "summary": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard.",
      "url": "http://arxiv.org/abs/2508.08127v1",
      "published_time_eastern_timestamp": 1754928287.0
    },
    {
      "title": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through\n  Structured Teamwork",
      "summary": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains.",
      "url": "http://arxiv.org/abs/2508.08115v1",
      "published_time_eastern_timestamp": 1754927706.0
    },
    {
      "title": "ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle\n  Conversational Agents for Safer and More Enjoyable Driving Experience",
      "summary": "Studies on in-vehicle conversational agents have traditionally relied on\npre-scripted prompts or limited voice commands, constraining natural\ndriver-agent interaction. To resolve this issue, the present study explored the\npotential of a ChatGPT-based in-vehicle agent capable of carrying continuous,\nmulti-turn dialogues. Forty drivers participated in our experiment using a\nmotion-based driving simulator, comparing three conditions (No agent,\nPre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.\nResults showed that the ChatGPT-based agent condition led to more stable\ndriving performance across multiple metrics. Participants demonstrated lower\nvariability in longitudinal acceleration, lateral acceleration, and lane\ndeviation compared to the other two conditions. In subjective evaluations, the\nChatGPT-based agent also received significantly higher ratings in competence,\nanimacy, affective trust, and preference compared to the Pre-scripted agent.\nOur thematic analysis of driver-agent conversations revealed diverse\ninteraction patterns in topics, including driving assistance/questions,\nentertainment requests, and anthropomorphic interactions. Our results highlight\nthe potential of LLM-powered in-vehicle conversational agents to enhance\ndriving safety and user experience through natural, context-rich interactions.",
      "url": "http://arxiv.org/abs/2508.08101v1",
      "published_time_eastern_timestamp": 1754926844.0
    },
    {
      "title": "How Quantum Agents Can Change Which Strategies Are More Complex",
      "summary": "Whether winning blackjack or navigating busy streets, achieving desired\noutcomes requires agents to execute adaptive strategies, strategies where\nactions depend contextually on past events. In complexity science, this\nmotivates memory as an operational quantifier of complexity: given two\nstrategies, the more complex one demands the agent to track more about the\npast. Here, we show that conclusions about complexity fundamentally depend on\nwhether agents can process and store quantum information. Thus, while classical\nagents might find Strategy A more complex to execute than Strategy B, quantum\nagents can reach the opposite conclusion. We derive sufficient conditions for\nsuch contradictory conclusions and illustrate the phenomenon across multiple\nscenarios. As a byproduct, our results yield an information-theoretic lower\nbound on the minimal memory required by any agent - classical or quantum - to\nexecute a given strategy.",
      "url": "http://arxiv.org/abs/2508.08092v1",
      "published_time_eastern_timestamp": 1754926323.0
    },
    {
      "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating\n  Local and Web Searches",
      "summary": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains.",
      "url": "http://arxiv.org/abs/2508.08088v1",
      "published_time_eastern_timestamp": 1754926307.0
    },
    {
      "title": "Anonymous voting in a heterogeneous society",
      "summary": "We study the design of voting mechanisms in a binary social choice\nenvironment where agents' cardinal valuations are independent but not\nnecessarily identically distributed. The mechanism must be anonymous -- the\noutcome is invariant to permutations of the reported values. We show that if\nthere are two agents then expected welfare is always maximized by an ordinal\nmajority rule, but with three or more agents there are environments in which\ncardinal mechanisms that take into account preference intensities outperform\nany ordinal mechanism.",
      "url": "http://arxiv.org/abs/2508.08055v1",
      "published_time_eastern_timestamp": 1754924152.0
    },
    {
      "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning",
      "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.",
      "url": "http://arxiv.org/abs/2508.08053v1",
      "published_time_eastern_timestamp": 1754923979.0
    },
    {
      "title": "9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT\n  2025)",
      "summary": "The Sign Language Translation and Avatar Technology (SLTAT) workshops\ncontinue a series of gatherings to share recent advances in improving deaf /\nhuman communication through non-invasive means. This 2025 edition, the 9th\nsince its first appearance in 2011, is hosted by the International Conference\non Intelligent Virtual Agents (IVA), giving the opportunity for contamination\nbetween two research communities, using digital humans as either virtual\ninterpreters or as interactive conversational agents. As presented in this\nsummary paper, SLTAT sees contributions beyond avatar technologies, with a\nconsistent number of submissions on sign language recognition, and other work\non data collection, data analysis, tools, ethics, usability, and affective\ncomputing.",
      "url": "http://arxiv.org/abs/2508.08050v1",
      "published_time_eastern_timestamp": 1754923821.0
    },
    {
      "title": "Constrained Distributed Heterogeneous Two-Facility Location Problems\n  with Max-Variant Cost",
      "summary": "We study a constrained distributed heterogeneous two-facility location\nproblem, where a set of agents with private locations on the real line are\ndivided into disjoint groups. The constraint means that the facilities can only\nbe built in a given multiset of candidate locations and at most one facility\ncan be built at each candidate location. Given the locations of the two\nfacilities, the cost of an agent is the distance from her location to the\nfarthest facility (referred to as max-variant). Our goal is to design\nstrategyproof distributed mechanisms that can incentivize all agents to\ntruthfully report their locations and approximately optimize some social\nobjective. A distributed mechanism consists of two steps: for each group, the\nmechanism chooses two candidate locations as the representatives of the group\nbased only on the locations reported by agents therein; then, it outputs two\nfacility locations among all the representatives. We focus on a class of\ndeterministic strategyproof distributed mechanisms and analyze upper and lower\nbounds on the distortion under the Average-of-Average cost (average of the\naverage individual cost of agents in each group), the Max-of-Max cost (maximum\nindividual cost among all agents), the Average-of-Max cost (average of the\nmaximum individual cost among all agents in each group) and the Max-of-Average\ncost (maximum of the average individual cost of all agents in each group).\nUnder four social objectives, we obtain constant upper and lower distortion\nbounds.",
      "url": "http://arxiv.org/abs/2508.08045v1",
      "published_time_eastern_timestamp": 1754923657.0
    },
    {
      "title": "Truthful Two-Obnoxious-Facility Location Games with Optional Preferences\n  and Minimum Distance Constraint",
      "summary": "In this paper, we study a truthful two-obnoxious-facility location problem,\nin which each agent has a private location in [0, 1] and a public optional\npreference over two obnoxious facilities, and there is a minimum distance\nconstraint d between the two facilities. Each agent wants to be as far away as\npossible from the facilities that affect her, and the utility of each agent is\nthe total distance from her to these facilities. The goal is to decide how to\nplace the facilities in [0, 1] so as to incentivize agents to report their\nprivate locations truthfully as well as maximize the social utility. First, we\nconsider the special setting where d = 0, that is, the two facilities can be\nlocated at any point in [0, 1]. We propose a deterministic strategyproof\nmechanism with approximation ratio of at most 4 and a randomized strategyproof\nmechanism with approximation ratio of at most 2, respectively. Then we study\nthe general setting. We propose a deterministic strategyproof mechanism with\napproximation ratio of at most 8 and a randomized strategyproof mechanism with\napproximation ratio of at most 4, respectively. Furthermore, we provide lower\nbounds of 2 and 14/13 on the approximation ratio for any deterministic and any\nrandomized strategyproof mechanism, respectively.",
      "url": "http://arxiv.org/abs/2508.08036v1",
      "published_time_eastern_timestamp": 1754923154.0
    },
    {
      "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking",
      "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/",
      "url": "http://arxiv.org/abs/2508.07999v1",
      "published_time_eastern_timestamp": 1754920989.0
    },
    {
      "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
      "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
      "url": "http://arxiv.org/abs/2508.07976v1",
      "published_time_eastern_timestamp": 1754919417.0
    }
  ]
}
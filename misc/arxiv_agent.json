{
  "last_updated": "2025-09-20T14:13:49.012726-04:00",
  "papers": [
    {
      "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
      "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
      "url": "http://arxiv.org/abs/2509.15221v1",
      "published_time_eastern_timestamp": 1758218362.0
    },
    {
      "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction",
      "summary": "Trajectory prediction is a critical task in computer vision and autonomous\nsystems, playing a key role in autonomous driving, robotics, surveillance, and\nvirtual reality. Existing methods often rely on complete and noise-free\nobservational data, overlooking the challenges associated with out-of-sight\nobjects and the inherent noise in sensor data caused by limited camera\ncoverage, obstructions, and the absence of ground truth for denoised\ntrajectories. These limitations pose safety risks and hinder reliable\nprediction in real-world scenarios. In this extended work, we present\nadvancements in Out-of-Sight Trajectory (OST), a novel task that predicts the\nnoise-free visual trajectories of out-of-sight objects using noisy sensor data.\nBuilding on our previous research, we broaden the scope of Out-of-Sight\nTrajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending\nits applicability to autonomous driving, robotics, surveillance, and virtual\nreality. Our enhanced Vision-Positioning Denoising Module leverages camera\ncalibration to establish a vision-positioning mapping, addressing the lack of\nvisual references, while effectively denoising noisy sensor data in an\nunsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB\ndatasets, our approach achieves state-of-the-art performance in both trajectory\ndenoising and prediction, significantly surpassing previous baselines.\nAdditionally, we introduce comparisons with traditional denoising methods, such\nas Kalman filtering, and adapt recent trajectory prediction models to our task,\nproviding a comprehensive benchmark. This work represents the first initiative\nto integrate vision-positioning projection for denoising noisy sensor\ntrajectories of out-of-sight agents, paving the way for future advances. The\ncode and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST",
      "url": "http://arxiv.org/abs/2509.15219v1",
      "published_time_eastern_timestamp": 1758218356.0
    },
    {
      "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus\n  Alignment",
      "summary": "Language Models (LMs) are inconsistent reasoners, often generating\ncontradictory responses to identical prompts. While inference-time methods can\nmitigate these inconsistencies, they fail to address the core problem: LMs\nstruggle to reliably select reasoning pathways leading to consistent outcomes\nunder exploratory sampling. To address this, we formalize self-consistency as\nan intrinsic property of well-aligned reasoning models and introduce\nMulti-Agent Consensus Alignment (MACA), a reinforcement learning framework that\npost-trains models to favor reasoning trajectories aligned with their internal\nconsensus using majority/minority outcomes from multi-agent debate. These\ntrajectories emerge from deliberative exchanges where agents ground reasoning\nin peer arguments, not just aggregation of independent attempts, creating\nricher consensus signals than single-round majority voting. MACA enables agents\nto teach themselves to be more decisive and concise, and better leverage peer\ninsights in multi-agent settings without external supervision, driving\nsubstantial improvements across self-consistency (+27.6% on GSM8K),\nsingle-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%\nPass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).\nThese findings, coupled with strong generalization to unseen benchmarks (+16.3%\non GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more\nreliably unlocks latent reasoning potential of language models.",
      "url": "http://arxiv.org/abs/2509.15172v1",
      "published_time_eastern_timestamp": 1758216448.0
    },
    {
      "title": "An Evaluation-Centric Paradigm for Scientific Visualization Agents",
      "summary": "Recent advances in multi-modal large language models (MLLMs) have enabled\nincreasingly sophisticated autonomous visualization agents capable of\ntranslating user intentions into data visualizations. However, measuring\nprogress and comparing different agents remains challenging, particularly in\nscientific visualization (SciVis), due to the absence of comprehensive,\nlarge-scale benchmarks for evaluating real-world capabilities. This position\npaper examines the various types of evaluation required for SciVis agents,\noutlines the associated challenges, provides a simple proof-of-concept\nevaluation example, and discusses how evaluation benchmarks can facilitate\nagent self-improvement. We advocate for a broader collaboration to develop a\nSciVis agentic evaluation benchmark that would not only assess existing\ncapabilities but also drive innovation and stimulate future development in the\nfield.",
      "url": "http://arxiv.org/abs/2509.15160v1",
      "published_time_eastern_timestamp": 1758215303.0
    },
    {
      "title": "Nonlinear Cooperative Salvo Guidance with Seeker-Limited Interceptors",
      "summary": "This paper presents a cooperative guidance strategy for the simultaneous\ninterception of a constant-velocity, non-maneuvering target, addressing the\nrealistic scenario where only a subset of interceptors are equipped with\nonboard seekers. To overcome the resulting heterogeneity in target\nobservability, a fixed-time distributed observer is employed, enabling\nseeker-less interceptors to estimate the target state using information from\nseeker-equipped agents and local neighbors over a directed communication\ntopology. Departing from conventional strategies that approximate time-to-go\nvia linearization or small-angle assumptions, the proposed approach leverages\ndeviated pursuit guidance where the time-to-go expression is exact for such a\ntarget. Moreover, a higher-order sliding mode consensus protocol is utilized to\nestablish time-to-go consensus within a finite time. The effectiveness of the\nproposed guidance and estimation architecture is demonstrated through\nsimulations.",
      "url": "http://arxiv.org/abs/2509.15136v1",
      "published_time_eastern_timestamp": 1758214103.0
    },
    {
      "title": "Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement\n  Learning",
      "summary": "Partial agent failure becomes inevitable when systems scale up, making it\ncrucial to identify the subset of agents whose compromise would most severely\ndegrade overall performance. In this paper, we study this Vulnerable Agent\nIdentification (VAI) problem in large-scale multi-agent reinforcement learning\n(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field\nControl (HAD-MFC), where the upper level involves an NP-hard combinatorial task\nof selecting the most vulnerable agents, and the lower level learns worst-case\nadversarial policies for these agents using mean-field MARL. The two problems\nare coupled together, making HAD-MFC difficult to solve. To solve this, we\nfirst decouple the hierarchical process by Fenchel-Rockafellar transform,\nresulting a regularized mean-field Bellman operator for upper level that\nenables independent learning at each level, thus reducing computational\ncomplexity. We then reformulate the upper-level combinatorial problem as a MDP\nwith dense rewards from our regularized mean-field Bellman operator, enabling\nus to sequentially identify the most vulnerable agents by greedy and RL\nalgorithms. This decomposition provably preserves the optimal solution of the\noriginal HAD-MFC. Experiments show our method effectively identifies more\nvulnerable agents in large-scale MARL and the rule-based system, fooling system\ninto worse failures, and learns a value function that reveals the vulnerability\nof each agent.",
      "url": "http://arxiv.org/abs/2509.15103v1",
      "published_time_eastern_timestamp": 1758211430.0
    },
    {
      "title": "Digital Twin-based Cooperative Autonomous Driving in Smart\n  Intersections: A Multi-Agent Reinforcement Learning Approach",
      "summary": "Unsignalized intersections pose safety and efficiency challenges due to\ncomplex traffic flows and blind spots. In this paper, a digital twin (DT)-based\ncooperative driving system with roadside unit (RSU)-centric architecture is\nproposed for enhancing safety and efficiency at unsignalized intersections. The\nsystem leverages comprehensive bird-eye-view (BEV) perception to eliminate\nblind spots and employs a hybrid reinforcement learning (RL) framework\ncombining offline pre-training with online fine-tuning. Specifically, driving\npolicies are initially trained using conservative Q-learning (CQL) with\nbehavior cloning (BC) on real datasets, then fine-tuned using multi-agent\nproximal policy optimization (MAPPO) with self-attention mechanisms to handle\ndynamic multi-agent coordination. The RSU implements real-time commands via\nvehicle-to-infrastructure (V2I) communications. Experimental results show that\nthe proposed method yields failure rates below 0.03\\% coordinating up to three\nconnected autonomous vehicles (CAVs), significantly outperforming traditional\nmethods. In addition, the system exhibits sub-linear computational scaling with\ninference times under 40 ms. Furthermore, it demonstrates robust generalization\nacross diverse unsignalized intersection scenarios, indicating its practicality\nand readiness for real-world deployment.",
      "url": "http://arxiv.org/abs/2509.15099v1",
      "published_time_eastern_timestamp": 1758210980.0
    },
    {
      "title": "Emergent Alignment via Competition",
      "summary": "Aligning AI systems with human values remains a fundamental challenge, but\ndoes our inability to create perfectly aligned models preclude obtaining the\nbenefits of alignment? We study a strategic setting where a human user\ninteracts with multiple differently misaligned AI agents, none of which are\nindividually well-aligned. Our key insight is that when the users utility lies\napproximately within the convex hull of the agents utilities, a condition that\nbecomes easier to satisfy as model diversity increases, strategic competition\ncan yield outcomes comparable to interacting with a perfectly aligned model. We\nmodel this as a multi-leader Stackelberg game, extending Bayesian persuasion to\nmulti-round conversations between differently informed parties, and prove three\nresults: (1) when perfect alignment would allow the user to learn her\nBayes-optimal action, she can also do so in all equilibria under the convex\nhull condition (2) under weaker assumptions requiring only approximate utility\nlearning, a non-strategic user employing quantal response achieves near-optimal\nutility in all equilibria and (3) when the user selects the best single AI\nafter an evaluation period, equilibrium guarantees remain near-optimal without\nfurther distributional assumptions. We complement the theory with two sets of\nexperiments.",
      "url": "http://arxiv.org/abs/2509.15090v1",
      "published_time_eastern_timestamp": 1758210420.0
    },
    {
      "title": "Forecasting and Visualizing Air Quality from Sky Images with\n  Vision-Language Models",
      "summary": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms.",
      "url": "http://arxiv.org/abs/2509.15076v1",
      "published_time_eastern_timestamp": 1758209798.0
    },
    {
      "title": "Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits",
      "summary": "Non-stationary multi-armed bandits enable agents to adapt to changing\nenvironments by incorporating mechanisms to detect and respond to shifts in\nreward distributions, making them well-suited for dynamic settings. However,\nexisting approaches typically assume that reward feedback is available at every\nround - an assumption that overlooks many real-world scenarios where feedback\nis limited. In this paper, we take a significant step forward by introducing a\nnew model of constrained feedback in non-stationary multi-armed bandits, where\nthe availability of reward feedback is restricted. We propose the first\nprior-free algorithm - that is, one that does not require prior knowledge of\nthe degree of non-stationarity - that achieves near-optimal dynamic regret in\nthis setting. Specifically, our algorithm attains a dynamic regret of\n$\\tilde{\\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the\nnumber of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$\nis the variation budget capturing the degree of non-stationarity.",
      "url": "http://arxiv.org/abs/2509.15073v1",
      "published_time_eastern_timestamp": 1758209732.0
    },
    {
      "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn\n  Dialogue",
      "summary": "The ultimate goal of embodied agents is to create collaborators that can\ninteract with humans, not mere executors that passively follow instructions.\nThis requires agents to communicate, coordinate, and adapt their actions based\non human feedback. Recently, advances in VLAs have offered a path toward this\ngoal. However, most current VLA-based embodied agents operate in a one-way\nmode: they receive an instruction and execute it without feedback. This\napproach fails in real-world scenarios where instructions are often ambiguous.\nIn this paper, we address this problem with the Ask-to-Clarify framework. Our\nframework first resolves ambiguous instructions by asking questions in a\nmulti-turn dialogue. Then it generates low-level actions end-to-end.\nSpecifically, the Ask-to-Clarify framework consists of two components, one VLM\nfor collaboration and one diffusion for action. We also introduce a connection\nmodule that generates conditions for the diffusion based on the output of the\nVLM. This module adjusts the observation by instructions to create reliable\nconditions. We train our framework with a two-stage knowledge-insulation\nstrategy. First, we fine-tune the collaboration component using\nambiguity-solving dialogue data to handle ambiguity. Then, we integrate the\naction component while freezing the collaboration one. This preserves the\ninteraction abilities while fine-tuning the diffusion to generate actions. The\ntraining strategy guarantees our framework can first ask questions, then\ngenerate actions. During inference, a signal detector functions as a router\nthat helps our framework switch between asking questions and taking actions. We\nevaluate the Ask-to-Clarify framework in 8 real-world tasks, where it\noutperforms existing state-of-the-art VLAs. The results suggest that our\nproposed framework, along with the training strategy, provides a path toward\ncollaborative embodied agents.",
      "url": "http://arxiv.org/abs/2509.15061v1",
      "published_time_eastern_timestamp": 1758209131.0
    },
    {
      "title": "Reinforcement Learning Agent for a 2D Shooter Game",
      "summary": "Reinforcement learning agents in complex game environments often suffer from\nsparse rewards, training instability, and poor sample efficiency. This paper\npresents a hybrid training approach that combines offline imitation learning\nwith online reinforcement learning for a 2D shooter game agent. We implement a\nmulti-head neural network with separate outputs for behavioral cloning and\nQ-learning, unified by shared feature extraction layers with attention\nmechanisms. Initial experiments using pure deep Q-Networks exhibited\nsignificant instability, with agents frequently reverting to poor policies\ndespite occasional good performance. To address this, we developed a hybrid\nmethodology that begins with behavioral cloning on demonstration data from\nrule-based agents, then transitions to reinforcement learning. Our hybrid\napproach achieves consistently above 70% win rate against rule-based opponents,\nsubstantially outperforming pure reinforcement learning methods which showed\nhigh variance and frequent performance degradation. The multi-head architecture\nenables effective knowledge transfer between learning modes while maintaining\ntraining stability. Results demonstrate that combining demonstration-based\ninitialization with reinforcement learning optimization provides a robust\nsolution for developing game AI agents in complex multi-agent environments\nwhere pure exploration proves insufficient.",
      "url": "http://arxiv.org/abs/2509.15042v1",
      "published_time_eastern_timestamp": 1758208061.0
    },
    {
      "title": "Sample Efficient Experience Replay in Non-stationary Environments",
      "summary": "Reinforcement learning (RL) in non-stationary environments is challenging, as\nchanging dynamics and rewards quickly make past experiences outdated.\nTraditional experience replay (ER) methods, especially those using TD-error\nprioritization, struggle to distinguish between changes caused by the agent's\npolicy and those from the environment, resulting in inefficient learning under\ndynamic conditions. To address this challenge, we propose the Discrepancy of\nEnvironment Dynamics (DoE), a metric that isolates the effects of environment\nshifts on value functions. Building on this, we introduce Discrepancy of\nEnvironment Prioritized Experience Replay (DEER), an adaptive ER framework that\nprioritizes transitions based on both policy updates and environmental changes.\nDEER uses a binary classifier to detect environment changes and applies\ndistinct prioritization strategies before and after each shift, enabling more\nsample-efficient learning. Experiments on four non-stationary benchmarks\ndemonstrate that DEER further improves the performance of off-policy algorithms\nby 11.54 percent compared to the best-performing state-of-the-art ER methods.",
      "url": "http://arxiv.org/abs/2509.15032v1",
      "published_time_eastern_timestamp": 1758207429.0
    },
    {
      "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical\n  Decision-making",
      "summary": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC.",
      "url": "http://arxiv.org/abs/2509.14998v1",
      "published_time_eastern_timestamp": 1758206016.0
    },
    {
      "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent\n  Systems",
      "summary": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time.",
      "url": "http://arxiv.org/abs/2509.14956v1",
      "published_time_eastern_timestamp": 1758202799.0
    },
    {
      "title": "Consensus, polarization, and optimization of the mean value in a\n  nonlinear model of opinion dynamics",
      "summary": "This paper investigates some aspects of a recently proposed nonlinear\nmathematical model of opinion dynamics. The main objective is to identify the\nnetwork structures that maximize the average equilibrium opinion (HMO). We\nprove that consensus is not generally attainable for populations with\nheterogeneous convictions, and that the highest mean does not necessarily\ncorrespond to consensus. Our analysis includes a necessary and sufficient\ncondition for achieving the HMO, description of an algorithm for constructing\noptimal connectivity matrices, and strategies for pruning agents when\nheterogeneity obstructs mean optimization.",
      "url": "http://arxiv.org/abs/2509.14918v1",
      "published_time_eastern_timestamp": 1758200176.0
    },
    {
      "title": "Hybrid Table-Assisted and RL-Based Dynamic Routing for NGSO Satellite\n  Networks",
      "summary": "This letter investigates dynamic routing in Next-Generation Satellite Orbit\n(NGSO) constellations and proposes a hybrid strategy that combines precomputed\nrouting tables with a Deep Q-Learning (DQL) fallback mechanism. While fully\nRL-based schemes offer adaptability to topology dynamics, they often suffer\nfrom high complexity, long convergence times, and unstable performance under\nheavy traffic. In contrast, the proposed framework exploits deterministic table\nlookups under nominal conditions and selectively activates the DQL agent only\nwhen links become unavailable or congested. Simulation results in large-scale\nNGSO networks show that the hybrid approach consistently achieves higher packet\ndelivery ratio, lower end-to-end delay, shorter average hop count, and improved\nthroughput compared to a pure RL baseline. These findings highlight the\neffectiveness of hybrid routing as a scalable and resilient solution for\ndelay-sensitive satellite broadband services",
      "url": "http://arxiv.org/abs/2509.14909v1",
      "published_time_eastern_timestamp": 1758199042.0
    },
    {
      "title": "Leveraging Reinforcement Learning, Genetic Algorithms and Transformers\n  for background determination in particle physics",
      "summary": "Experimental studies of beauty hadron decays face significant challenges due\nto a wide range of backgrounds arising from the numerous possible decay\nchannels with similar final states. For a particular signal decay, the process\nfor ascertaining the most relevant background processes necessitates a detailed\nanalysis of final state particles, potential misidentifications, and kinematic\noverlaps, which, due to computational limitations, is restricted to the\nsimulation of only the most relevant backgrounds. Moreover, this process\ntypically relies on the physicist's intuition and expertise, as no systematic\nmethod exists.\n  This paper has two primary goals. First, from a particle physics perspective,\nwe present a novel approach that utilises Reinforcement Learning (RL) to\novercome the aforementioned challenges by systematically determining the\ncritical backgrounds affecting beauty hadron decay measurements. While beauty\nhadron physics serves as the case study in this work, the proposed strategy is\nbroadly adaptable to other types of particle physics measurements. Second, from\na Machine Learning perspective, we introduce a novel algorithm which exploits\nthe synergy between RL and Genetic Algorithms (GAs) for environments with\nhighly sparse rewards and a large trajectory space. This strategy leverages GAs\nto efficiently explore the trajectory space and identify successful\ntrajectories, which are used to guide the RL agent's training. Our method also\nincorporates a transformer architecture for the RL agent to handle token\nsequences representing decays.",
      "url": "http://arxiv.org/abs/2509.14894v1",
      "published_time_eastern_timestamp": 1758197845.0
    },
    {
      "title": "CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming\n  Together with Human",
      "summary": "In this work, we present CollabVLA, a self-reflective vision-language-action\nframework that transforms a standard visuomotor policy into a collaborative\nassistant. CollabVLA tackles key limitations of prior VLAs, including domain\noverfitting, non-interpretable reasoning, and the high latency of auxiliary\ngenerative models, by integrating VLM-based reflective reasoning with\ndiffusion-based action generation under a mixture-of-experts design. Through a\ntwo-stage training recipe of action grounding and reflection tuning, it\nsupports explicit self-reflection and proactively solicits human guidance when\nconfronted with uncertainty or repeated failure. It cuts normalized Time by ~2x\nand Dream counts by ~4x vs. generative agents, achieving higher success rates,\nimproved interpretability, and balanced low latency compared with existing\nmethods. This work takes a pioneering step toward shifting VLAs from opaque\ncontrollers to genuinely assistive agents capable of reasoning, acting, and\ncollaborating with humans.",
      "url": "http://arxiv.org/abs/2509.14889v1",
      "published_time_eastern_timestamp": 1758197441.0
    },
    {
      "title": "Applying reinforcement learning to optical cavity locking tasks:\n  considerations on actor-critic architectures and real-time hardware\n  implementation",
      "summary": "This proceedings contains our considerations made during and after fruitful\ndiscussions held at EuCAIFCon 2025. We explore the use of deep reinforcement\nlearning for autonomous locking of Fabry-Perot optical cavities in non-linear\nregimes, with relevance to gravitational-wave detectors. A custom Gymnasium\nenvironment with a time-domain simulator enabled training of agents such as\ndeep deterministic policy gradient, achieving reliable lock acquisition for\nboth low- and high-finesse cavities, including Virgo-like parameters. We also\ndiscuss possible improvements with Twin Delayed DDPG, Soft Actor Critic and\nmeta-reinforcement learning, as well as strategies for low-latency execution\nand off-line policy updates to address hardware limitations. These studies lay\nthe groundwork for future deployment of reinforcement learning-based control in\nreal optical setups.",
      "url": "http://arxiv.org/abs/2509.14884v1",
      "published_time_eastern_timestamp": 1758197200.0
    }
  ]
}
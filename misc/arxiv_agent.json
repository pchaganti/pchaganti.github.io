{
  "last_updated": "2025-08-22T02:18:13.258717-04:00",
  "papers": [
    {
      "title": "Distributed Detection of Adversarial Attacks in Multi-Agent\n  Reinforcement Learning with Continuous Action Space",
      "summary": "We address the problem of detecting adversarial attacks against cooperative\nmulti-agent reinforcement learning with continuous action space. We propose a\ndecentralized detector that relies solely on the local observations of the\nagents and makes use of a statistical characterization of the normal behavior\nof observable agents. The proposed detector utilizes deep neural networks to\napproximate the normal behavior of agents as parametric multivariate Gaussian\ndistributions. Based on the predicted density functions, we define a normality\nscore and provide a characterization of its mean and variance. This\ncharacterization allows us to employ a two-sided CUSUM procedure for detecting\ndeviations of the normality score from its mean, serving as a detector of\nanomalous behavior in real-time. We evaluate our scheme on various multi-agent\nPettingZoo benchmarks against different state-of-the-art attack methods, and\nour results demonstrate the effectiveness of our method in detecting impactful\nadversarial attacks. Particularly, it outperforms the discrete counterpart by\nachieving AUC-ROC scores of over 0.95 against the most impactful attacks in all\nevaluated environments.",
      "url": "http://arxiv.org/abs/2508.15764v1",
      "published_time_eastern_timestamp": 1755799116.0
    },
    {
      "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on\n  Challenging Queries",
      "summary": "Tool calling has emerged as a critical capability for AI agents to interact\nwith the real world and solve complex tasks. While the Model Context Protocol\n(MCP) provides a powerful standardized framework for tool integration, there is\na significant gap in benchmarking how well AI agents can effectively solve\nmulti-step tasks using diverse MCP tools in realistic, dynamic scenarios. In\nthis work, we present LiveMCP-101, a benchmark of 101 carefully curated\nreal-world queries, refined through iterative LLM rewriting and manual review,\nthat require coordinated use of multiple MCP tools including web search, file\noperations, mathematical reasoning, and data analysis. Moreover, we introduce a\nnovel evaluation approach that leverages ground-truth execution plans rather\nthan raw API outputs, better reflecting the evolving nature of real-world\nenvironments. Experiments show that even frontier LLMs achieve a success rate\nbelow 60\\%, highlighting major challenges in tool orchestration. Detailed\nablations and error analysis further reveal distinct failure modes and\ninefficiencies in token usage, pointing to concrete directions for advancing\ncurrent models. LiveMCP-101 sets a rigorous standard for evaluating real-world\nagent capabilities, advancing toward autonomous AI systems that reliably\nexecute complex tasks through tool use.",
      "url": "http://arxiv.org/abs/2508.15760v1",
      "published_time_eastern_timestamp": 1755798954.0
    },
    {
      "title": "Language-Guided Tuning: Enhancing Numeric Optimization with Textual\n  Feedback",
      "summary": "Configuration optimization remains a critical bottleneck in machine learning,\nrequiring coordinated tuning across model architecture, training strategy,\nfeature engineering, and hyperparameters. Traditional approaches treat these\ndimensions independently and lack interpretability, while recent automated\nmethods struggle with dynamic adaptability and semantic reasoning about\noptimization decisions. We introduce Language-Guided Tuning (LGT), a novel\nframework that employs multi-agent Large Language Models to intelligently\noptimize configurations through natural language reasoning. We apply textual\ngradients - qualitative feedback signals that complement numerical optimization\nby providing semantic understanding of training dynamics and configuration\ninterdependencies. LGT coordinates three specialized agents: an Advisor that\nproposes configuration changes, an Evaluator that assesses progress, and an\nOptimizer that refines the decision-making process, creating a self-improving\nfeedback loop. Through comprehensive evaluation on six diverse datasets, LGT\ndemonstrates substantial improvements over traditional optimization methods,\nachieving performance gains while maintaining high interpretability.",
      "url": "http://arxiv.org/abs/2508.15757v1",
      "published_time_eastern_timestamp": 1755798907.0
    },
    {
      "title": "\"Does the cafe entrance look accessible? Where is the door?\" Towards\n  Geospatial AI Agents for Visual Inquiries",
      "summary": "Interactive digital maps have revolutionized how people travel and learn\nabout the world; however, they rely on pre-existing structured data in GIS\ndatabases (e.g., road networks, POI indices), limiting their ability to address\ngeo-visual questions related to what the world looks like. We introduce our\nvision for Geo-Visual Agents--multimodal AI agents capable of understanding and\nresponding to nuanced visual-spatial inquiries about the world by analyzing\nlarge-scale repositories of geospatial images, including streetscapes (e.g.,\nGoogle Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial\nimagery (e.g., satellite photos) combined with traditional GIS data sources. We\ndefine our vision, describe sensing and interaction approaches, provide three\nexemplars, and enumerate key challenges and opportunities for future work.",
      "url": "http://arxiv.org/abs/2508.15752v1",
      "published_time_eastern_timestamp": 1755798592.0
    },
    {
      "title": "Response and Prompt Evaluation to Prevent Parasocial Relationships with\n  Chatbots",
      "summary": "The development of parasocial relationships with AI agents has severe, and in\nsome cases, tragic effects for human well-being. Yet preventing such dynamics\nis challenging: parasocial cues often emerge gradually in private\nconversations, and not all forms of emotional engagement are inherently\nharmful. We address this challenge by introducing a simple response evaluation\nframework, created by repurposing a state-of-the-art language model, that\nevaluates ongoing conversations for parasocial cues in real time. To test the\nfeasibility of this approach, we constructed a small synthetic dataset of\nthirty dialogues spanning parasocial, sycophantic, and neutral conversations.\nIterative evaluation with five stage testing successfully identified all\nparasocial conversations while avoiding false positives under a tolerant\nunanimity rule, with detection typically occurring within the first few\nexchanges. These findings provide preliminary evidence that evaluation agents\ncan provide a viable solution for the prevention of parasocial relations.",
      "url": "http://arxiv.org/abs/2508.15748v1",
      "published_time_eastern_timestamp": 1755798204.0
    },
    {
      "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic\n  Reasoning",
      "summary": "Accurate diagnosis with medical large language models is hindered by\nknowledge gaps and hallucinations. Retrieval and tool-augmented methods help,\nbut their impact is limited by weak use of external knowledge and poor\nfeedback-reasoning traceability. To address these challenges, We introduce\nDeep-DxSearch, an agentic RAG system trained end-to-end with reinforcement\nlearning (RL) that enables steer tracebale retrieval-augmented reasoning for\nmedical diagnosis. In Deep-DxSearch, we first construct a large-scale medical\nretrieval corpus comprising patient records and reliable medical knowledge\nsources to support retrieval-aware reasoning across diagnostic scenarios. More\ncrutially, we frame the LLM as the core agent and the retrieval corpus as its\nenvironment, using tailored rewards on format, retrieval, reasoning structure,\nand diagnostic accuracy, thereby evolving the agentic RAG policy from\nlarge-scale data through RL.\n  Experiments demonstrate that our end-to-end agentic RL training framework\nconsistently outperforms prompt-engineering and training-free RAG approaches\nacross multiple data centers. After training, Deep-DxSearch achieves\nsubstantial gains in diagnostic accuracy, surpassing strong diagnostic\nbaselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks\nfor both common and rare disease diagnosis under in-distribution and\nout-of-distribution settings. Moreover, ablation studies on reward design and\nretrieval corpus components confirm their critical roles, underscoring the\nuniqueness and effectiveness of our approach compared with traditional\nimplementations. Finally, case studies and interpretability analyses highlight\nimprovements in Deep-DxSearch's diagnostic policy, providing deeper insight\ninto its performance gains and supporting clinicians in delivering more\nreliable and precise preliminary diagnoses. See\nhttps://github.com/MAGIC-AI4Med/Deep-DxSearch.",
      "url": "http://arxiv.org/abs/2508.15746v1",
      "published_time_eastern_timestamp": 1755798167.0
    },
    {
      "title": "NiceWebRL: a Python library for human subject experiments with\n  reinforcement learning environments",
      "summary": "We present NiceWebRL, a research tool that enables researchers to use machine\nreinforcement learning (RL) environments for online human subject experiments.\nNiceWebRL is a Python library that allows any Jax-based environment to be\ntransformed into an online interface, supporting both single-agent and\nmulti-agent environments. As such, NiceWebRL enables AI researchers to compare\ntheir algorithms to human performance, cognitive scientists to test ML\nalgorithms as theories for human cognition, and multi-agent researchers to\ndevelop algorithms for human-AI collaboration. We showcase NiceWebRL with 3\ncase studies that demonstrate its potential to help develop Human-like AI,\nHuman-compatible AI, and Human-assistive AI. In the first case study\n(Human-like AI), NiceWebRL enables the development of a novel RL model of\ncognition. Here, NiceWebRL facilitates testing this model against human\nparticipants in both a grid world and Craftax, a 2D Minecraft domain. In our\nsecond case study (Human-compatible AI), NiceWebRL enables the development of a\nnovel multi-agent RL algorithm that can generalize to human partners in the\nOvercooked domain. Finally, in our third case study (Human-assistive AI), we\nshow how NiceWebRL can allow researchers to study how an LLM can assist humans\non complex tasks in XLand-Minigrid, an environment with millions of\nhierarchical tasks. The library is available at\nhttps://github.com/KempnerInstitute/nicewebrl.",
      "url": "http://arxiv.org/abs/2508.15693v1",
      "published_time_eastern_timestamp": 1755793129.0
    },
    {
      "title": "An Efficient Open World Environment for Multi-Agent Social Learning",
      "summary": "Many challenges remain before AI agents can be deployed in real-world\nenvironments. However, one virtue of such environments is that they are\ninherently multi-agent and contain human experts. Using advanced social\nintelligence in such an environment can help an AI agent learn adaptive skills\nand behaviors that a known expert exhibits. While social intelligence could\naccelerate training, it is currently difficult to study due to the lack of\nopen-ended multi-agent environments. In this work, we present an environment in\nwhich multiple self-interested agents can pursue complex and independent goals,\nreflective of real world challenges. This environment will enable research into\nthe development of socially intelligent AI agents in open-ended multi-agent\nsettings, where agents may be implicitly incentivized to cooperate to defeat\ncommon enemies, build and share tools, and achieve long horizon goals. In this\nwork, we investigate the impact on agent performance due to social learning in\nthe presence of experts and implicit cooperation such as emergent collaborative\ntool use, and whether agents can benefit from either cooperation or competition\nin this environment.",
      "url": "http://arxiv.org/abs/2508.15679v1",
      "published_time_eastern_timestamp": 1755791960.0
    },
    {
      "title": "Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task\n  Planning and Low-Level Policies in Mobile Manipulation",
      "summary": "Benchmarks are crucial for evaluating progress in robotics and embodied AI.\nHowever, a significant gap exists between benchmarks designed for high-level\nlanguage instruction following, which often assume perfect low-level execution,\nand those for low-level robot control, which rely on simple, one-step commands.\nThis disconnect prevents a comprehensive evaluation of integrated systems where\nboth task planning and physical execution are critical. To address this, we\npropose Kitchen-R, a novel benchmark that unifies the evaluation of task\nplanning and low-level control within a simulated kitchen environment. Built as\na digital twin using the Isaac Sim simulator and featuring more than 500\ncomplex language instructions, Kitchen-R supports a mobile manipulator robot.\nWe provide baseline methods for our benchmark, including a task-planning\nstrategy based on a vision-language model and a low-level control policy based\non diffusion policy. We also provide a trajectory collection system. Our\nbenchmark offers a flexible framework for three evaluation modes: independent\nassessment of the planning module, independent assessment of the control\npolicy, and, crucially, an integrated evaluation of the whole system. Kitchen-R\nbridges a key gap in embodied AI research, enabling more holistic and realistic\nbenchmarking of language-guided robotic agents.",
      "url": "http://arxiv.org/abs/2508.15663v1",
      "published_time_eastern_timestamp": 1755791331.0
    },
    {
      "title": "Understanding Action Effects through Instrumental Empowerment in\n  Multi-Agent Reinforcement Learning",
      "summary": "To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is\ncrucial to understand individual agent behaviors within a team. While prior\nwork typically evaluates overall team performance based on explicit reward\nsignals or learned value functions, it is unclear how to infer agent\ncontributions in the absence of any value feedback. In this work, we\ninvestigate whether meaningful insights into agent behaviors can be extracted\nthat are consistent with the underlying value functions, solely by analyzing\nthe policy distribution. Inspired by the phenomenon that intelligent agents\ntend to pursue convergent instrumental values, which generally increase the\nlikelihood of task success, we introduce Intended Cooperation Values (ICVs), a\nmethod based on information-theoretic Shapley values for quantifying each\nagent's causal influence on their co-players' instrumental empowerment.\nSpecifically, ICVs measure an agent's action effect on its teammates' policies\nby assessing their decision uncertainty and preference alignment. The analysis\nacross cooperative and competitive MARL environments reveals the extent to\nwhich agents adopt similar or diverse strategies. By comparing action effects\nbetween policies and value functions, our method identifies which agent\nbehaviors are beneficial to team success, either by fostering deterministic\ndecisions or by preserving flexibility for future action choices. Our proposed\nmethod offers novel insights into cooperation dynamics and enhances\nexplainability in MARL systems.",
      "url": "http://arxiv.org/abs/2508.15652v1",
      "published_time_eastern_timestamp": 1755790559.0
    }
  ]
}
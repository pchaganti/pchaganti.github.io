{
  "last_updated": "2025-09-22T05:14:41.259806-04:00",
  "papers": [
    {
      "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase\n  Generation",
      "summary": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.",
      "url": "http://arxiv.org/abs/2509.16198v1",
      "published_time_eastern_timestamp": 1758304694.0
    },
    {
      "title": "Latent learning: episodic memory complements parametric learning by\n  enabling flexible reuse of experiences",
      "summary": "When do machine learning systems fail to generalize, and what mechanisms\ncould improve their generalization? Here, we draw inspiration from cognitive\nscience to argue that one weakness of machine learning systems is their failure\nto exhibit latent learning -- learning information that is not relevant to the\ntask at hand, but that might be useful in a future task. We show how this\nperspective links failures ranging from the reversal curse in language modeling\nto new findings on agent-based navigation. We then highlight how cognitive\nscience points to episodic memory as a potential part of the solution to these\nissues. Correspondingly, we show that a system with an oracle retrieval\nmechanism can use learning experiences more flexibly to generalize better\nacross many of these challenges. We also identify some of the essential\ncomponents for effectively using retrieval, including the importance of\nwithin-example in-context learning for acquiring the ability to use information\nacross retrieved examples. In summary, our results illustrate one possible\ncontributor to the relative data inefficiency of current machine learning\nsystems compared to natural intelligence, and help to understand how retrieval\nmethods can complement parametric learning to improve generalization.",
      "url": "http://arxiv.org/abs/2509.16189v1",
      "published_time_eastern_timestamp": 1758304165.0
    },
    {
      "title": "MatchFixAgent: Language-Agnostic Autonomous Repository-Level Code\n  Translation Validation and Repair",
      "summary": "Code translation transforms source code from one programming language (PL) to\nanother. Validating the functional equivalence of translation and repairing, if\nnecessary, are critical steps in code translation. Existing automated\nvalidation and repair approaches struggle to generalize to many PLs due to high\nengineering overhead, and they rely on existing and often inadequate test\nsuites, which results in false claims of equivalence and ineffective\ntranslation repair. We develop MatchFixAgent, a large language model\n(LLM)-based, PL-agnostic framework for equivalence validation and repair of\ntranslations. MatchFixAgent features a multi-agent architecture that divides\nequivalence validation into several sub-tasks to ensure thorough and consistent\nsemantic analysis of the translation. Then it feeds this analysis to test agent\nto write and execute tests. Upon observing a test failure, the repair agent\nattempts to fix the translation bug. The final (in)equivalence decision is made\nby the verdict agent, considering semantic analyses and test execution results.\n  We compare MatchFixAgent's validation and repair results with four\nrepository-level code translation techniques. We use 2,219 translation pairs\nfrom their artifacts, which cover 6 PL pairs, and are collected from 24 GitHub\nprojects totaling over 900K lines of code. Our results demonstrate that\nMatchFixAgent produces (in)equivalence verdicts for 99.2% of translation pairs,\nwith the same equivalence validation result as prior work on 72.8% of them.\nWhen MatchFixAgent's result disagrees with prior work, we find that 60.7% of\nthe time MatchFixAgent's result is actually correct. In addition, we show that\nMatchFixAgent can repair 50.6% of inequivalent translation, compared to prior\nwork's 18.5%. This demonstrates that MatchFixAgent is far more adaptable to\nmany PL pairs than prior work, while producing highly accurate validation\nresults.",
      "url": "http://arxiv.org/abs/2509.16187v1",
      "published_time_eastern_timestamp": 1758303973.0
    },
    {
      "title": "Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\n  Trajectories",
      "summary": "We present Agentic Aerial Cinematography: From Dialogue Cues to Cinematic\nTrajectories (ACDC), an autonomous drone cinematography system driven by\nnatural language communication between human directors and drones. The main\nlimitation of previous drone cinematography workflows is that they require\nmanual selection of waypoints and view angles based on predefined human intent,\nwhich is labor-intensive and yields inconsistent performance. In this paper, we\npropose employing large language models (LLMs) and vision foundation models\n(VFMs) to convert free-form natural language prompts directly into executable\nindoor UAV video tours. Specifically, our method comprises a vision-language\nretrieval pipeline for initial waypoint selection, a preference-based Bayesian\noptimization framework that refines poses using aesthetic feedback, and a\nmotion planner that generates safe quadrotor trajectories. We validate ACDC\nthrough both simulation and hardware-in-the-loop experiments, demonstrating\nthat it robustly produces professional-quality footage across diverse indoor\nscenes without requiring expertise in robotics or cinematography. These results\nhighlight the potential of embodied AI agents to close the loop from\nopen-vocabulary dialogue to real-world autonomous aerial cinematography.",
      "url": "http://arxiv.org/abs/2509.16176v1",
      "published_time_eastern_timestamp": 1758303351.0
    },
    {
      "title": "Strategic Analysis of Just-In-Time Liquidity Provision in Concentrated\n  Liquidity Market Makers",
      "summary": "Liquidity providers (LPs) are essential figures in the operation of automated\nmarket makers (AMMs); in exchange for transaction fees, LPs lend the liquidity\nthat allows AMMs to operate. While many prior works have studied the incentive\nstructures of LPs in general, we currently lack a principled understanding of a\nspecial class of LPs known as Just-In-Time (JIT) LPs. These are strategic\nagents who momentarily supply liquidity for a single swap, in an attempt to\nextract disproportionately high fees relative to the remaining passive LPs.\nThis paper provides the first formal, transaction-level model of JIT liquidity\nprovision for a widespread class of AMMs known as Concentrated Liquidity Market\nMakers (CLMMs), as seen in Uniswap V3, for instance. We characterize the\nlandscape of price impact and fee allocation in these systems, formulate and\nanalyze a non-linear optimization problem faced by JIT LPs, and prove the\nexistence of an optimal strategy. By fitting our optimal solution for JIT LPs\nto real-world CLMMs, we observe that in liquidity pools (particularly those\nwith risky assets), there is a significant gap between observed and optimal JIT\nbehavior. Existing JIT LPs often fail to account for price impact; doing so, we\nestimate they could increase earnings by up to 69% on average over small time\nwindows. We also show that JIT liquidity, when deployed strategically, can\nimprove market efficiency by reducing slippage for traders, albeit at the cost\nof eroding average passive LP profits by up to 44% per trade.",
      "url": "http://arxiv.org/abs/2509.16157v1",
      "published_time_eastern_timestamp": 1758301409.0
    },
    {
      "title": "Automated Cyber Defense with Generalizable Graph-based Reinforcement\n  Learning Agents",
      "summary": "Deep reinforcement learning (RL) is emerging as a viable strategy for\nautomated cyber defense (ACD). The traditional RL approach represents networks\nas a list of computers in various states of safety or threat. Unfortunately,\nthese models are forced to overfit to specific network topologies, rendering\nthem ineffective when faced with even small environmental perturbations. In\nthis work, we frame ACD as a two-player context-based partially observable\nMarkov decision problem with observations represented as attributed graphs.\nThis approach allows our agents to reason through the lens of relational\ninductive bias. Agents learn how to reason about hosts interacting with other\nsystem entities in a more general manner, and their actions are understood as\nedits to the graph representing the environment. By introducing this bias, we\nwill show that our agents can better reason about the states of networks and\nzero-shot adapt to new ones. We show that this approach outperforms the\nstate-of-the-art by a wide margin, and makes our agents capable of defending\nnever-before-seen networks against a wide range of adversaries in a variety of\ncomplex, and multi-agent environments.",
      "url": "http://arxiv.org/abs/2509.16151v1",
      "published_time_eastern_timestamp": 1758301047.0
    },
    {
      "title": "Who Pays, Who Benefits? Producer-Insurer Games in Life-Saving Medicines",
      "summary": "Pharmaceutical markets for life-saving therapies combine monopoly power with\ninsurance coverage. We build a tractable sequential game in which a\npatent-holder chooses the drug price, a profit-maximising insurer sets its\npremium, and a population of heterogeneous agents decide whether to insure and,\nconditional on diagnosis, whether to purchase treatment. Two sufficient\nstatistics - subjective illness probability and reservation price - capture\nheterogeneity and nest risk-aversion and liquidity-constraint motives within a\nunified framework. We prove existence of subgame-perfect Nash equilibria and\nshow that entry of an insurer strictly raises producer profits but may raise or\nlower both drug prices and treatment uptake, depending on the joint\ndistribution of the population statistics. Numerical experiments calibrated to\nflexible parametric families illustrate non-monotone comparative statics and\nquantify conditions under which insurance reduces access. Our results provide\nbenchmarks for evaluating price negotiations, price caps, and subsidy schemes\nin high-cost drug markets.",
      "url": "http://arxiv.org/abs/2509.16125v1",
      "published_time_eastern_timestamp": 1758298977.0
    },
    {
      "title": "AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent\n  Trajectory Modeling in Sports",
      "summary": "Trajectory prediction in multi-agent sports scenarios is inherently\nchallenging due to the structural heterogeneity across agent roles (e.g.,\nplayers vs. ball) and dynamic distribution gaps across different sports\ndomains. Existing unified frameworks often fail to capture these structured\ndistributional shifts, resulting in suboptimal generalization across roles and\ndomains. We propose AdaSports-Traj, an adaptive trajectory modeling framework\nthat explicitly addresses both intra-domain and inter-domain distribution\ndiscrepancies in sports. At its core, AdaSports-Traj incorporates a Role- and\nDomain-Aware Adapter to conditionally adjust latent representations based on\nagent identity and domain context. Additionally, we introduce a Hierarchical\nContrastive Learning objective, which separately supervises role-sensitive and\ndomain-aware representations to encourage disentangled latent structures\nwithout introducing optimization conflict. Experiments on three diverse sports\ndatasets, Basketball-U, Football-U, and Soccer-U, demonstrate the effectiveness\nof our adaptive design, achieving strong performance in both unified and\ncross-domain trajectory prediction settings.",
      "url": "http://arxiv.org/abs/2509.16095v1",
      "published_time_eastern_timestamp": 1758296307.0
    },
    {
      "title": "Misspecified learning and evolutionary stability",
      "summary": "We extend the indirect evolutionary approach to the selection of (possibly\nmisspecified) models. Agents with different models match in pairs to play a\nstage game, where models define feasible beliefs about game parameters and\nabout others' strategies. In equilibrium, each agent adopts the feasible belief\nthat best fits their data and plays optimally given their beliefs. We define\nthe stability of the resident model by comparing its equilibrium payoff with\nthat of the entrant model, and provide conditions under which the correctly\nspecified resident model can only be destabilized by misspecified entrant\nmodels that contain multiple feasible beliefs (that is, entrant models that\npermit inference). We also show that entrants may do well in their matches\nagainst the residents only when the entrant population is large, due to the\nendogeneity of misspecified beliefs. Applications include the selection of\ndemand-elasticity misperception in Cournot duopoly and the emergence of\nanalogy-based reasoning in centipede games.",
      "url": "http://arxiv.org/abs/2509.16067v1",
      "published_time_eastern_timestamp": 1758294868.0
    },
    {
      "title": "Towards Robust Visual Continual Learning with Multi-Prototype\n  Supervision",
      "summary": "Language-guided supervision, which utilizes a frozen semantic target from a\nPretrained Language Model (PLM), has emerged as a promising paradigm for visual\nContinual Learning (CL). However, relying on a single target introduces two\ncritical limitations: 1) semantic ambiguity, where a polysemous category name\nresults in conflicting visual representations, and 2) intra-class visual\ndiversity, where a single prototype fails to capture the rich variety of visual\nappearances within a class. To this end, we propose MuproCL, a novel framework\nthat replaces the single target with multiple, context-aware prototypes.\nSpecifically, we employ a lightweight LLM agent to perform category\ndisambiguation and visual-modal expansion to generate a robust set of semantic\nprototypes. A LogSumExp aggregation mechanism allows the vision model to\nadaptively align with the most relevant prototype for a given image. Extensive\nexperiments across various CL baselines demonstrate that MuproCL consistently\nenhances performance and robustness, establishing a more effective path for\nlanguage-guided continual learning.",
      "url": "http://arxiv.org/abs/2509.16011v1",
      "published_time_eastern_timestamp": 1758291888.0
    },
    {
      "title": "Quantum Reinforcement Learning with Dynamic-Circuit Qubit Reuse and\n  Grover-Based Trajectory Optimization",
      "summary": "A fully quantum reinforcement learning framework is developed that integrates\na quantum Markov decision process, dynamic circuit-based qubit reuse, and\nGrover's algorithm for trajectory optimization. The framework encodes states,\nactions, rewards, and transitions entirely within the quantum domain, enabling\nparallel exploration of state-action sequences through superposition and\neliminating classical subroutines. Dynamic circuit operations, including\nmid-circuit measurement and reset, allow reuse of the same physical qubits\nacross multiple agent-environment interactions, reducing qubit requirements\nfrom 7*T to 7 for T time steps while preserving logical continuity. Quantum\narithmetic computes trajectory returns, and Grover's search is applied to the\nsuperposition of these evaluated trajectories to amplify the probability of\nmeasuring those with the highest return, thereby accelerating the\nidentification of the optimal policy. Simulations demonstrate that the\ndynamic-circuit-based implementation preserves trajectory fidelity while\nreducing qubit usage by 66 percent relative to the static design. Experimental\ndeployment on IBM Heron-class quantum hardware confirms that the framework\noperates within the constraints of current quantum processors and validates the\nfeasibility of fully quantum multi-step reinforcement learning under noisy\nintermediate-scale quantum conditions. This framework advances the scalability\nand practical application of quantum reinforcement learning for large-scale\nsequential decision-making tasks.",
      "url": "http://arxiv.org/abs/2509.16002v1",
      "published_time_eastern_timestamp": 1758291095.0
    },
    {
      "title": "Inverse Optimization Latent Variable Models for Learning Costs Applied\n  to Route Problems",
      "summary": "Learning representations for solutions of constrained optimization problems\n(COPs) with unknown cost functions is challenging, as models like (Variational)\nAutoencoders struggle to enforce constraints when decoding structured outputs.\nWe propose an Inverse Optimization Latent Variable Model (IO-LVM) that learns a\nlatent space of COP cost functions from observed solutions and reconstructs\nfeasible outputs by solving a COP with a solver in the loop. Our approach\nleverages estimated gradients of a Fenchel-Young loss through a\nnon-differentiable deterministic solver to shape the latent space. Unlike\nstandard Inverse Optimization or Inverse Reinforcement Learning methods, which\ntypically recover a single or context-specific cost function, IO-LVM captures a\ndistribution over cost functions, enabling the identification of diverse\nsolution behaviors arising from different agents or conditions not available\nduring the training process. We validate our method on real-world datasets of\nship and taxi routes, as well as paths in synthetic graphs, demonstrating its\nability to reconstruct paths and cycles, predict their distributions, and yield\ninterpretable latent representations.",
      "url": "http://arxiv.org/abs/2509.15999v1",
      "published_time_eastern_timestamp": 1758291051.0
    },
    {
      "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement\n  Learning with Few Demonstrations",
      "summary": "In reinforcement learning with sparse rewards, demonstrations can accelerate\nlearning, but determining when to imitate them remains challenging. We propose\nSmooth Policy Regularisation from Demonstrations (SPReD), a framework that\naddresses the fundamental question: when should an agent imitate a\ndemonstration versus follow its own policy? SPReD uses ensemble methods to\nexplicitly model Q-value distributions for both demonstration and policy\nactions, quantifying uncertainty for comparisons. We develop two complementary\nuncertainty-aware methods: a probabilistic approach estimating the likelihood\nof demonstration superiority, and an advantage-based approach scaling imitation\nby statistical significance. Unlike prevailing methods (e.g. Q-filter) that\nmake binary imitation decisions, SPReD applies continuous,\nuncertainty-proportional regularisation weights, reducing gradient variance\nduring training. Despite its computational simplicity, SPReD achieves\nremarkable gains in experiments across eight robotics tasks, outperforming\nexisting approaches by up to a factor of 14 in complex tasks while maintaining\nrobustness to demonstration quality and quantity. Our code is available at\nhttps://github.com/YujieZhu7/SPReD.",
      "url": "http://arxiv.org/abs/2509.15981v1",
      "published_time_eastern_timestamp": 1758289640.0
    },
    {
      "title": "RLinf: Flexible and Efficient Large-scale Reinforcement Learning via\n  Macro-to-Micro Flow Transformation",
      "summary": "Reinforcement learning (RL) has demonstrated immense potential in advancing\nartificial general intelligence, agentic intelligence, and embodied\nintelligence. However, the inherent heterogeneity and dynamicity of RL\nworkflows often lead to low hardware utilization and slow training on existing\nsystems. In this paper, we present RLinf, a high-performance RL training system\nbased on our key observation that the major roadblock to efficient RL training\nlies in system flexibility. To maximize flexibility and efficiency, RLinf is\nbuilt atop a novel RL system design paradigm called macro-to-micro flow\ntransformation (M2Flow), which automatically breaks down high-level,\neasy-to-compose RL workflows at both the temporal and spatial dimensions, and\nrecomposes them into optimized execution flows. Supported by RLinf worker's\nadaptive communication capability, we devise context switching and elastic\npipelining to realize M2Flow transformation, and a profiling-guided scheduling\npolicy to generate optimal execution plans. Extensive evaluations on both\nreasoning RL and embodied RL tasks demonstrate that RLinf consistently\noutperforms state-of-the-art systems, achieving 1.1x-2.13x speedup in\nend-to-end training throughput.",
      "url": "http://arxiv.org/abs/2509.15965v1",
      "published_time_eastern_timestamp": 1758288257.0
    },
    {
      "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by\n  Large Language Models via Model Context Protocol",
      "summary": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.",
      "url": "http://arxiv.org/abs/2509.15957v1",
      "published_time_eastern_timestamp": 1758287836.0
    },
    {
      "title": "Foundation Models as World Models: A Foundational Study in Text-Based\n  GridWorlds",
      "summary": "While reinforcement learning from scratch has shown impressive results in\nsolving sequential decision-making tasks with efficient simulators, real-world\napplications with expensive interactions require more sample-efficient agents.\nFoundation models (FMs) are natural candidates to improve sample efficiency as\nthey possess broad knowledge and reasoning capabilities, but it is yet unclear\nhow to effectively integrate them into the reinforcement learning framework. In\nthis paper, we anticipate and, most importantly, evaluate two promising\nstrategies. First, we consider the use of foundation world models (FWMs) that\nexploit the prior knowledge of FMs to enable training and evaluating agents\nwith simulated interactions. Second, we consider the use of foundation agents\n(FAs) that exploit the reasoning capabilities of FMs for decision-making. We\nevaluate both approaches empirically in a family of grid-world environments\nthat are suitable for the current generation of large language models (LLMs).\nOur results suggest that improvements in LLMs already translate into better\nFWMs and FAs; that FAs based on current LLMs can already provide excellent\npolicies for sufficiently simple environments; and that the coupling of FWMs\nand reinforcement learning agents is highly promising for more complex settings\nwith partial observability and stochastic elements.",
      "url": "http://arxiv.org/abs/2509.15915v1",
      "published_time_eastern_timestamp": 1758283828.0
    },
    {
      "title": "Regularity properties of distributions of correspondences without\n  countable generation: applications to large games",
      "summary": "We show that each of the regularity properties of regular conditional\ndistributions of correspondences (convexity, closedness, compactness, and\npreservation of closed graphs) is equivalent to the condition of nowhere\nequivalence. This result does not require any countable-generation assumptions.\nAs an application, we establish the existence of a pure-strategy equilibrium\nfor large games with general trait spaces. The trait space may be an arbitrary\nmeasurable space. As a corollary, we obtain the existence of a pure-strategy\nequilibrium in semi-anonymous settings in which payoffs depend, in addition to\nagents' own actions, on the joint distribution over the space of agents and\nactions.",
      "url": "http://arxiv.org/abs/2509.15898v1",
      "published_time_eastern_timestamp": 1758282703.0
    },
    {
      "title": "Changes in Liver Fibrosis in Patients with Chronic Hepatitis B Treated\n  with Pegylated Interferon Combined with Oral Antiviral Agents: A 48-Week\n  Observation from a Prospective Cohort Study",
      "summary": "Background and Aims: Pegylated interferon (PEG-IFN) combined with oral\nantiviral agents is currently the most widely used and highly effective\ntreatment regimen for chronic hepatitis B virus (HBV) infection. While\neffectively suppressing HBV replication, its impact on liver histopathological\nfibrosis and inflammation remains a critical concern for clinicians and\npatients. Methods : A total of 625 patients who completed 48 weeks of PEG-IFN\ncombined with oral antiviral therapy were enrolled in this real-world study.\nBased on their virological response at 48 weeks, patients were categorized into\nClearance group and Non-clearance group. Changes in liver biochemistry,\nfibrosis, and renal function were compared between groups and before/after\ntreatment. Results: No significant differences were observed in baseline blood\ntests, liver biochemical markers, or histopathological features between the\nClearance group and Non-clearance group. Similarly, baseline renal function\nshowed no significant variation. Further analysis revealed that the Clearance\ngroup exhibited significant aggravation of liver fibrosis after 48 weeks of\ntreatment, which correlated strongly with alterations in liver enzyme levels.\nHowever, one patient who underwent paired liver biopsies before and after\ntreatment demonstrated marked histopathological improvement in fibrosis. This\nfinding underscores the irreplaceable role of liver histopathology in assessing\nfibrosis and inflammation. Conclusion: PEG-IFN combined with oral antiviral\ntherapy exerts favorable effects on liver fibrosis and inflammation in chronic\nHBV patients. Non-invasive fibrosis assessment models can monitor fibrotic\nprogression but are susceptible to confounding by hepatic inflammation.",
      "url": "http://arxiv.org/abs/2509.15854v1",
      "published_time_eastern_timestamp": 1758278670.0
    },
    {
      "title": "Relational Dissonance in Human-AI Interactions: The Case of Knowledge\n  Work",
      "summary": "When AI systems allow human-like communication, they elicit increasingly\ncomplex relational responses. Knowledge workers face a particular challenge:\nThey approach these systems as tools while interacting with them in ways that\nresemble human social interaction. To understand the relational contexts that\narise when humans engage with anthropomorphic conversational agents, we need to\nexpand existing human-computer interaction frameworks. Through three workshops\nwith qualitative researchers, we found that the fundamental ontological and\nrelational ambiguities inherent in anthropomorphic conversational agents make\nit difficult for individuals to maintain consistent relational stances toward\nthem. Our findings indicate that people's articulated positioning toward such\nagents often differs from the relational dynamics that occur during\ninteractions. We propose the concept of relational dissonance to help\nresearchers, designers, and policymakers recognize the resulting tensions in\nthe development, deployment, and governance of anthropomorphic conversational\nagents and address the need for relational transparency.",
      "url": "http://arxiv.org/abs/2509.15836v1",
      "published_time_eastern_timestamp": 1758276955.0
    },
    {
      "title": "Coordinated Multi-Drone Last-mile Delivery: Learning Strategies for\n  Energy-aware and Timely Operations",
      "summary": "Drones have recently emerged as a faster, safer, and cost-efficient way for\nlast-mile deliveries of parcels, particularly for urgent medical deliveries\nhighlighted during the pandemic. This paper addresses a new challenge of\nmulti-parcel delivery with a swarm of energy-aware drones, accounting for\ntime-sensitive customer requirements. Each drone plans an optimal multi-parcel\nroute within its battery-restricted flight range to minimize delivery delays\nand reduce energy consumption. The problem is tackled by decomposing it into\nthree sub-problems: (1) optimizing depot locations and service areas using\nK-means clustering; (2) determining the optimal flight range for drones through\nreinforcement learning; and (3) planning and selecting multi-parcel delivery\nroutes via a new optimized plan selection approach. To integrate these\nsolutions and enhance long-term efficiency, we propose a novel algorithm\nleveraging actor-critic-based multi-agent deep reinforcement learning.\nExtensive experimentation using realistic delivery datasets demonstrate an\nexceptional performance of the proposed algorithm. We provide new insights into\neconomic efficiency (minimize energy consumption), rapid operations (reduce\ndelivery delays and overall execution time), and strategic guidance on depot\ndeployment for practical logistics applications.",
      "url": "http://arxiv.org/abs/2509.15830v1",
      "published_time_eastern_timestamp": 1758276045.0
    }
  ]
}
{
  "last_updated": "2025-05-27T17:11:17.145247-04:00",
  "papers": [
    {
      "title": "On Path to Multimodal Historical Reasoning: HistBench and HistAgent",
      "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress across domains, yet their capabilities in the humanities, particularly\nhistory, remain underexplored. Historical reasoning poses unique challenges for\nAI, involving multimodal source interpretation, temporal inference, and\ncross-linguistic analysis. While general-purpose agents perform well on many\nexisting benchmarks, they lack the domain-specific expertise required to engage\nwith historical materials and questions. To address this gap, we introduce\nHistBench, a new benchmark of 414 high-quality questions designed to evaluate\nAI's capacity for historical reasoning and authored by more than 40 expert\ncontributors. The tasks span a wide range of historical problems-from factual\nretrieval based on primary sources to interpretive analysis of manuscripts and\nimages, to interdisciplinary challenges involving archaeology, linguistics, or\ncultural history. Furthermore, the benchmark dataset spans 29 ancient and\nmodern languages and covers a wide range of historical periods and world\nregions. Finding the poor performance of LLMs and other agents on HistBench, we\nfurther present HistAgent, a history-specific agent equipped with carefully\ndesigned tools for OCR, translation, archival search, and image understanding\nin History. On HistBench, HistAgent based on GPT-4o achieves an accuracy of\n27.54% pass@1 and 36.47% pass@2, significantly outperforming LLMs with online\nsearch and generalist agents, including GPT-4o (18.60%), DeepSeek-R1(14.49%)\nand Open Deep Research-smolagents(20.29% pass@1 and 25.12% pass@2). These\nresults highlight the limitations of existing LLMs and generalist agents and\ndemonstrate the advantages of HistAgent for historical reasoning.",
      "url": "http://arxiv.org/abs/2505.20246v1",
      "published_time_eastern_timestamp": 1748280140.0
    },
    {
      "title": "Shutdownable Agents through POST-Agency",
      "summary": "Many fear that future artificial agents will resist shutdown. I present an\nidea - the POST-Agents Proposal - for ensuring that doesn't happen. I propose\nthat we train agents to satisfy Preferences Only Between Same-Length\nTrajectories (POST). I then prove that POST - together with other conditions -\nimplies Neutrality+: the agent maximizes expected utility, ignoring the\nprobability distribution over trajectory-lengths. I argue that Neutrality+\nkeeps agents shutdownable and allows them to be useful.",
      "url": "http://arxiv.org/abs/2505.20203v1",
      "published_time_eastern_timestamp": 1748277857.0
    },
    {
      "title": "THiNK: Can Large Language Models Think-aloud?",
      "summary": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository.",
      "url": "http://arxiv.org/abs/2505.20184v1",
      "published_time_eastern_timestamp": 1748276822.0
    },
    {
      "title": "The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a\n  Connected World",
      "summary": "The increasing deployment of Artificial Intelligence (AI) and other\nautonomous algorithmic systems presents the world with new systemic risks.\nWhile focus often lies on the function of individual algorithms, a critical and\nunderestimated danger arises from their interactions, particularly when\nalgorithmic systems operate without awareness of each other, or when those\ndeploying them are unaware of the full algorithmic ecosystem deployment is\noccurring in. These interactions can lead to unforeseen, rapidly escalating\nnegative outcomes - from market crashes and energy supply disruptions to\npotential physical accidents and erosion of public trust - often exceeding the\nhuman capacity for effective monitoring and the legal capacities for proper\nintervention. Current governance frameworks are inadequate as they lack\nvisibility into this complex ecosystem of interactions. This paper outlines the\nnature of this challenge and proposes some initial policy suggestions centered\non increasing transparency and accountability through phased system\nregistration, a licensing framework for deployment, and enhanced monitoring\ncapabilities.",
      "url": "http://arxiv.org/abs/2505.20181v1",
      "published_time_eastern_timestamp": 1748276538.0
    },
    {
      "title": "MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents",
      "summary": "Spatial Planning is a crucial part in the field of spatial intelligence,\nwhich requires the understanding and planning about object arrangements in\nspace perspective. AI agents with the spatial planning ability can better adapt\nto various real-world applications, including robotic manipulation, automatic\nassembly, urban planning etc. Recent works have attempted to construct\nbenchmarks for evaluating the spatial intelligence of Multimodal Large Language\nModels (MLLMs). Nevertheless, these benchmarks primarily focus on spatial\nreasoning based on typical Visual Question-Answering (VQA) forms, which suffers\nfrom the gap between abstract spatial understanding and concrete task\nexecution. In this work, we take a step further to build a comprehensive\nbenchmark called MineAnyBuild, aiming to evaluate the spatial planning ability\nof open-world AI agents in the Minecraft game. Specifically, MineAnyBuild\nrequires an agent to generate executable architecture building plans based on\nthe given multi-modal human instructions. It involves 4,000 curated spatial\nplanning tasks and also provides a paradigm for infinitely expandable data\ncollection by utilizing rich player-generated content. MineAnyBuild evaluates\nspatial planning through four core supporting dimensions: spatial\nunderstanding, spatial reasoning, creativity, and spatial commonsense. Based on\nMineAnyBuild, we perform a comprehensive evaluation for existing MLLM-based\nagents, revealing the severe limitations but enormous potential in their\nspatial planning abilities. We believe our MineAnyBuild will open new avenues\nfor the evaluation of spatial intelligence and help promote further development\nfor open-world AI agents capable of spatial planning.",
      "url": "http://arxiv.org/abs/2505.20148v1",
      "published_time_eastern_timestamp": 1748274494.0
    },
    {
      "title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs",
      "summary": "Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications.",
      "url": "http://arxiv.org/abs/2505.20129v1",
      "published_time_eastern_timestamp": 1748273297.0
    },
    {
      "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic\n  Searchers",
      "summary": "Large language models (LLMs) have been widely integrated into information\nretrieval to advance traditional techniques. However, effectively enabling LLMs\nto seek accurate knowledge in complex tasks remains a challenge due to the\ncomplexity of multi-hop queries as well as the irrelevant retrieved content. To\naddress these limitations, we propose EXSEARCH, an agentic search framework,\nwhere the LLM learns to retrieve useful information as the reasoning unfolds\nthrough a self-incentivized process. At each step, the LLM decides what to\nretrieve (thinking), triggers an external retriever (search), and extracts\nfine-grained evidence (recording) to support next-step reasoning. To enable LLM\nwith this capability, EXSEARCH adopts a Generalized Expectation-Maximization\nalgorithm. In the E-step, the LLM generates multiple search trajectories and\nassigns an importance weight to each; the M-step trains the LLM on them with a\nre-weighted loss function. This creates a self-incentivized loop, where the LLM\niteratively learns from its own generated data, progressively improving itself\nfor search. We further theoretically analyze this training process,\nestablishing convergence guarantees. Extensive experiments on four\nknowledge-intensive benchmarks show that EXSEARCH substantially outperforms\nbaselines, e.g., +7.8% improvement on exact match score. Motivated by these\npromising results, we introduce EXSEARCH-Zoo, an extension that extends our\nmethod to broader scenarios, to facilitate future work.",
      "url": "http://arxiv.org/abs/2505.20128v1",
      "published_time_eastern_timestamp": 1748273275.0
    },
    {
      "title": "Agentic AI Process Observability: Discovering Behavioral Variability",
      "summary": "AI agents that leverage Large Language Models (LLMs) are increasingly\nbecoming core building blocks of modern software systems. A wide range of\nframeworks is now available to support the specification of such applications.\nThese frameworks enable the definition of agent setups using natural language\nprompting, which specifies the roles, goals, and tools assigned to the various\nagents involved. Within such setups, agent behavior is non-deterministic for\nany given input, highlighting the critical need for robust debugging and\nobservability tools. In this work, we explore the use of process and causal\ndiscovery applied to agent execution trajectories as a means of enhancing\ndeveloper observability. This approach aids in monitoring and understanding the\nemergent variability in agent behavior. Additionally, we complement this with\nLLM-based static analysis techniques to distinguish between intended and\nunintended behavioral variability. We argue that such instrumentation is\nessential for giving developers greater control over evolving specifications\nand for identifying aspects of functionality that may require more precise and\nexplicit definitions.",
      "url": "http://arxiv.org/abs/2505.20127v1",
      "published_time_eastern_timestamp": 1748273167.0
    },
    {
      "title": "Agents Require Metacognitive and Strategic Reasoning to Succeed in the\n  Coming Labor Markets",
      "summary": "Current labor markets are strongly affected by the economic forces of adverse\nselection, moral hazard, and reputation, each of which arises due to\n$\\textit{incomplete information}$. These economic forces will still be\ninfluential after AI agents are introduced, and thus, agents must use\nmetacognitive and strategic reasoning to perform effectively. Metacognition is\na form of $\\textit{internal reasoning}$ that includes the capabilities for\nself-assessment, task understanding, and evaluation of strategies. Strategic\nreasoning is $\\textit{external reasoning}$ that covers holding beliefs about\nother participants in the labor market (e.g., competitors, colleagues), making\nstrategic decisions, and learning about others over time. Both types of\nreasoning are required by agents as they decide among the many\n$\\textit{actions}$ they can take in labor markets, both within and outside\ntheir jobs. We discuss current research into metacognitive and strategic\nreasoning and the areas requiring further development.",
      "url": "http://arxiv.org/abs/2505.20120v1",
      "published_time_eastern_timestamp": 1748272924.0
    },
    {
      "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic\n  Privacy Leaking Agent",
      "summary": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous.",
      "url": "http://arxiv.org/abs/2505.20118v1",
      "published_time_eastern_timestamp": 1748272851.0
    },
    {
      "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative\n  Chain-of-Thought Reasoning",
      "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.",
      "url": "http://arxiv.org/abs/2505.20096v1",
      "published_time_eastern_timestamp": 1748271918.0
    },
    {
      "title": "SwarmThinkers: Learning Physically Consistent Atomic KMC Transitions at\n  Scale",
      "summary": "Can a scientific simulation system be physically consistent, interpretable by\ndesign, and scalable across regimes--all at once? Despite decades of progress,\nthis trifecta remains elusive. Classical methods like Kinetic Monte Carlo\nensure thermodynamic accuracy but scale poorly; learning-based methods offer\nefficiency but often sacrifice physical consistency and interpretability. We\npresent SwarmThinkers, a reinforcement learning framework that recasts\natomic-scale simulation as a physically grounded swarm intelligence system.\nEach diffusing particle is modeled as a local decision-making agent that\nselects transitions via a shared policy network trained under thermodynamic\nconstraints. A reweighting mechanism fuses learned preferences with transition\nrates, preserving statistical fidelity while enabling interpretable, step-wise\ndecision making. Training follows a centralized-training,\ndecentralized-execution paradigm, allowing the policy to generalize across\nsystem sizes, concentrations, and temperatures without retraining. On a\nbenchmark simulating radiation-induced Fe-Cu alloy precipitation, SwarmThinkers\nis the first system to achieve full-scale, physically consistent simulation on\na single A100 GPU, previously attainable only via OpenKMC on a supercomputer.\nIt delivers up to 4963x (3185x on average) faster computation with 485x lower\nmemory usage. By treating particles as decision-makers, not passive samplers,\nSwarmThinkers marks a paradigm shift in scientific simulation--one that unifies\nphysical consistency, interpretability, and scalability through agent-driven\nintelligence.",
      "url": "http://arxiv.org/abs/2505.20094v1",
      "published_time_eastern_timestamp": 1748271877.0
    },
    {
      "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
      "summary": "We present REARANK, a large language model (LLM)-based listwise reasoning\nreranking agent. REARANK explicitly reasons before reranking, significantly\nimproving both performance and interpretability. Leveraging reinforcement\nlearning and data augmentation, REARANK achieves substantial improvements over\nbaseline models across popular information retrieval benchmarks, notably\nrequiring only 179 annotated samples. Built on top of Qwen2.5-7B, our\nREARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and\nout-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT\nbenchmarks. These results underscore the effectiveness of our approach and\nhighlight how reinforcement learning can enhance LLM reasoning capabilities in\nreranking.",
      "url": "http://arxiv.org/abs/2505.20046v1",
      "published_time_eastern_timestamp": 1748269908.0
    },
    {
      "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and\n  Partial Masking",
      "summary": "Autonomous agents, which perceive environments and take actions to achieve\ngoals, have become increasingly feasible with the advancements in large\nlanguage models (LLMs). However, current powerful agents often depend on\nsophisticated prompt engineering combined with closed-source LLMs like GPT-4.\nAlthough training open-source LLMs using expert trajectories from teacher\nmodels has yielded some improvements in agent capabilities, this approach still\nfaces limitations such as performance plateauing and error propagation. To\nmitigate these challenges, we propose STeP, a novel method for improving\nLLM-based agent training. We synthesize self-reflected trajectories that\ninclude reflections and corrections of error steps, which enhance the\neffectiveness of LLM agents in learning from teacher models, enabling them to\nbecome agents capable of self-reflecting and correcting. We also introduce\npartial masking strategy that prevents the LLM from internalizing incorrect or\nsuboptimal steps. Experiments demonstrate that our method improves agent\nperformance across three representative tasks: ALFWorld, WebShop, and SciWorld.\nFor the open-source model LLaMA2-7B-Chat, when trained using self-reflected\ntrajectories constructed with Qwen1.5-110B-Chat as the teacher model, it\nachieves comprehensive improvements with less training data compared to agents\ntrained exclusively on expert trajectories.",
      "url": "http://arxiv.org/abs/2505.20023v1",
      "published_time_eastern_timestamp": 1748268672.0
    },
    {
      "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought\n  in Reflection, Branching, and Rollback",
      "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.",
      "url": "http://arxiv.org/abs/2505.20013v1",
      "published_time_eastern_timestamp": 1748268217.0
    },
    {
      "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments",
      "summary": "Human-like agents are an increasingly important topic in games and beyond.\nBelievable non-player characters enhance the gaming experience by improving\nimmersion and providing entertainment. They also offer players the opportunity\nto engage with AI entities that can function as opponents, teachers, or\ncooperating partners. Additionally, in games where bots are prohibited -- and\neven more so in non-game environments -- there is a need for methods capable of\nidentifying whether digital interactions occur with bots or humans. This leads\nto two fundamental research questions: (1) how to model and implement\nhuman-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most\nsignificant challenges in implementing human-like AI in games (or any virtual\nenvironment featuring simulated agents, although this article specifically\nfocuses on games). Thirteen such challenges, both conceptual and technical, are\ndiscussed in detail. The second is an empirical study performed in a tactical\nvideo game that addresses the research question: \"Is it possible to distinguish\nhuman players from bots (AI agents) based on empirical data?\" A\nmachine-learning approach using a custom deep recurrent convolutional neural\nnetwork is presented. We hypothesize that the more challenging it is to create\nhuman-like AI for a given game, the easier it becomes to develop a method for\ndistinguishing humans from AI-driven players.",
      "url": "http://arxiv.org/abs/2505.20011v1",
      "published_time_eastern_timestamp": 1748268039.0
    },
    {
      "title": "Embracing Imperfection: Simulating Students with Diverse Cognitive\n  Levels Using LLM-based Agents",
      "summary": "Large language models (LLMs) are revolutionizing education, with LLM-based\nagents playing a key role in simulating student behavior. A major challenge in\nstudent simulation is modeling the diverse learning patterns of students at\nvarious cognitive levels. However, current LLMs, typically trained as ``helpful\nassistants'', target at generating perfect responses. As a result, they\nstruggle to simulate students with diverse cognitive abilities, as they often\nproduce overly advanced answers, missing the natural imperfections that\ncharacterize student learning and resulting in unrealistic simulations. To\naddress this issue, we propose a training-free framework for student\nsimulation. We begin by constructing a cognitive prototype for each student\nusing a knowledge graph, which captures their understanding of concepts from\npast learning records. This prototype is then mapped to new tasks to predict\nstudent performance. Next, we simulate student solutions based on these\npredictions and iteratively refine them using a beam search method to better\nreplicate realistic mistakes. To validate our approach, we construct the\n\\texttt{Student\\_100} dataset, consisting of $100$ students working on Python\nprogramming and $5,000$ learning records. Experimental results show that our\nmethod consistently outperforms baseline models, achieving $100\\%$ improvement\nin simulation accuracy.",
      "url": "http://arxiv.org/abs/2505.19997v1",
      "published_time_eastern_timestamp": 1748267329.0
    },
    {
      "title": "The residual maximin share",
      "summary": "We consider fair allocations of indivisible goods to agents with general\nmonotone valuations. We observe that it is useful to introduce a new\nshare-based fairness notion, the {\\em residual maximin share} (RMMS). This\nshare is {\\em feasible} and {\\em self maximizing}. Its value is at least as\nlarge as the MXS, and at least as large as $\\frac{2}{3}$-MMS for additive\nvaluations. Known techniques easily imply the existence of partial allocations\nthat are both RMMS and EFX, and complete allocations that are both RMMS and\nEFL. This unifies and somewhat improves upon several different results from\nprevious papers.",
      "url": "http://arxiv.org/abs/2505.19961v1",
      "published_time_eastern_timestamp": 1748265723.0
    },
    {
      "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
      "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
      "url": "http://arxiv.org/abs/2505.19955v1",
      "published_time_eastern_timestamp": 1748265517.0
    },
    {
      "title": "Multimodal Reasoning Agent for Zero-Shot Composed Image Retrieval",
      "summary": "Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images\ngiven a compositional query, consisting of a reference image and a modifying\ntext-without relying on annotated training data. Existing approaches often\ngenerate a synthetic target text using large language models (LLMs) to serve as\nan intermediate anchor between the compositional query and the target image.\nModels are then trained to align the compositional query with the generated\ntext, and separately align images with their corresponding texts using\ncontrastive learning. However, this reliance on intermediate text introduces\nerror propagation, as inaccuracies in query-to-text and text-to-image mappings\naccumulate, ultimately degrading retrieval performance. To address these\nproblems, we propose a novel framework by employing a Multimodal Reasoning\nAgent (MRA) for ZS-CIR. MRA eliminates the dependence on textual intermediaries\nby directly constructing triplets, <reference image, modification text, target\nimage>, using only unlabeled image data. By training on these synthetic\ntriplets, our model learns to capture the relationships between compositional\nqueries and candidate images directly. Extensive experiments on three standard\nCIR benchmarks demonstrate the effectiveness of our approach. On the FashionIQ\ndataset, our method improves Average R@10 by at least 7.5\\% over existing\nbaselines; on CIRR, it boosts R@1 by 9.6\\%; and on CIRCO, it increases mAP@5 by\n9.5\\%.",
      "url": "http://arxiv.org/abs/2505.19952v1",
      "published_time_eastern_timestamp": 1748265470.0
    }
  ]
}
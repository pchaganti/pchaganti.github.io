{
  "last_updated": "2025-10-08T23:27:40.247025-04:00",
  "papers": [
    {
      "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the\n  Business Domain",
      "summary": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21\\% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.",
      "url": "http://arxiv.org/abs/2510.07309v1",
      "published_time_eastern_timestamp": 1759946255.0
    },
    {
      "title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline",
      "summary": "While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.",
      "url": "http://arxiv.org/abs/2510.07307v1",
      "published_time_eastern_timestamp": 1759946239.0
    },
    {
      "title": "Agentic generative AI for media content discovery at the national\n  football league",
      "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.",
      "url": "http://arxiv.org/abs/2510.07297v1",
      "published_time_eastern_timestamp": 1759945894.0
    },
    {
      "title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost\n  Preferences",
      "summary": "Many real-world scenarios require multiple agents to coordinate in shared\nenvironments, while balancing trade-offs between multiple, potentially\ncompeting objectives. Current multi-objective multi-agent path finding\n(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto\nfrontiers. They do not explicitly optimize for user-defined preferences, even\nwhen the preferences are available, and scale poorly with the number of\nobjectives. We propose a lexicographic framework for modeling MO-MAPF, along\nwith an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that\ndirectly computes a single solution aligned with a lexicographic preference\nover objectives. LCBS integrates a priority-aware low-level $A^*$ search with\nconflict-based search, avoiding Pareto frontier construction and enabling\nefficient planning guided by preference over objectives. We provide insights\ninto optimality and scalability, and empirically demonstrate that LCBS computes\noptimal solutions while scaling to instances with up to ten objectives -- far\nbeyond the limits of existing MO-MAPF methods. Evaluations on standard and\nrandomized MAPF benchmarks show consistently higher success rates against\nstate-of-the-art baselines, especially with increasing number of objectives.",
      "url": "http://arxiv.org/abs/2510.07276v1",
      "published_time_eastern_timestamp": 1759945241.0
    },
    {
      "title": "Dynamic Regret Bounds for Online Omniprediction with Long Term\n  Constraints",
      "summary": "We present an algorithm guaranteeing dynamic regret bounds for online\nomniprediction with long term constraints. The goal in this recently introduced\nproblem is for a learner to generate a sequence of predictions which are\nbroadcast to a collection of downstream decision makers. Each decision maker\nhas their own utility function, as well as a vector of constraint functions,\neach mapping their actions and an adversarially selected state to reward or\nconstraint violation terms. The downstream decision makers select actions \"as\nif\" the state predictions are correct, and the goal of the learner is to\nproduce predictions such that all downstream decision makers choose actions\nthat give them worst-case utility guarantees while minimizing worst-case\nconstraint violation. Within this framework, we give the first algorithm that\nobtains simultaneous \\emph{dynamic regret} guarantees for all of the agents --\nwhere regret for each agent is measured against a potentially changing sequence\nof actions across rounds of interaction, while also ensuring vanishing\nconstraint violation for each agent. Our results do not require the agents\nthemselves to maintain any state -- they only solve one-round constrained\noptimization problems defined by the prediction made at that round.",
      "url": "http://arxiv.org/abs/2510.07266v1",
      "published_time_eastern_timestamp": 1759944485.0
    },
    {
      "title": "Test-Time Graph Search for Goal-Conditioned Reinforcement Learning",
      "summary": "Offline goal-conditioned reinforcement learning (GCRL) trains policies that\nreach user-specified goals at test time, providing a simple, unsupervised,\ndomain-agnostic way to extract diverse behaviors from unlabeled, reward-free\ndatasets. Nonetheless, long-horizon decision making remains difficult for GCRL\nagents due to temporal credit assignment and error accumulation, and the\noffline setting amplifies these effects. To alleviate this issue, we introduce\nTest-Time Graph Search (TTGS), a lightweight planning approach to solve the\nGCRL task. TTGS accepts any state-space distance or cost signal, builds a\nweighted graph over dataset states, and performs fast search to assemble a\nsequence of subgoals that a frozen policy executes. When the base learner is\nvalue-based, the distance is derived directly from the learned goal-conditioned\nvalue function, so no handcrafted metric is needed. TTGS requires no changes to\ntraining, no additional supervision, no online interaction, and no privileged\ninformation, and it runs entirely at inference. On the OGBench benchmark, TTGS\nimproves success rates of multiple base learners on challenging locomotion\ntasks, demonstrating the benefit of simple metric-guided test-time planning for\noffline GCRL.",
      "url": "http://arxiv.org/abs/2510.07257v1",
      "published_time_eastern_timestamp": 1759944053.0
    },
    {
      "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document\n  Understanding",
      "summary": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.",
      "url": "http://arxiv.org/abs/2510.07233v1",
      "published_time_eastern_timestamp": 1759942924.0
    },
    {
      "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM\n  Agent in Online Shopping",
      "summary": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation.",
      "url": "http://arxiv.org/abs/2510.07230v1",
      "published_time_eastern_timestamp": 1759942825.0
    },
    {
      "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in\n  Image Generation",
      "summary": "Text-to-image synthesis has made remarkable progress, yet accurately\ninterpreting complex and lengthy prompts remains challenging, often resulting\nin semantic inconsistencies and missing details. Existing solutions, such as\nfine-tuning, are model-specific and require training, while prior automatic\nprompt optimization (APO) approaches typically lack systematic error analysis\nand refinement strategies, resulting in limited reliability and effectiveness.\nMeanwhile, test-time scaling methods operate on fixed prompts and on noise or\nsample numbers, limiting their interpretability and adaptability. To solve\nthese, we introduce a flexible and efficient test-time prompt optimization\nstrategy that operates directly on the input text. We propose a plug-and-play\nmulti-agent system called GenPilot, integrating error analysis,\nclustering-based adaptive exploration, fine-grained verification, and a memory\nmodule for iterative optimization. Our approach is model-agnostic,\ninterpretable, and well-suited for handling long and complex prompts.\nSimultaneously, we summarize the common patterns of errors and the refinement\nstrategy, offering more experience and encouraging further exploration.\nExperiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%\ndemonstrate the strong capability of our methods in enhancing the text and\nimage consistency and structural coherence of generated images, revealing the\neffectiveness of our test-time prompt optimization strategy. The code is\navailable at https://github.com/27yw/GenPilot.",
      "url": "http://arxiv.org/abs/2510.07217v1",
      "published_time_eastern_timestamp": 1759942312.0
    },
    {
      "title": "HyPlan: Hybrid Learning-Assisted Planning Under Uncertainty for Safe\n  Autonomous Driving",
      "summary": "We present a novel hybrid learning-assisted planning method, named HyPlan,\nfor solving the collision-free navigation problem for self-driving cars in\npartially observable traffic environments. HyPlan combines methods for\nmulti-agent behavior prediction, deep reinforcement learning with proximal\npolicy optimization and approximated online POMDP planning with heuristic\nconfidence-based vertical pruning to reduce its execution time without\ncompromising safety of driving. Our experimental performance analysis on the\nCARLA-CTS2 benchmark of critical traffic scenarios with pedestrians revealed\nthat HyPlan may navigate safer than selected relevant baselines and perform\nsignificantly faster than considered alternative online POMDP planners.",
      "url": "http://arxiv.org/abs/2510.07210v1",
      "published_time_eastern_timestamp": 1759941894.0
    },
    {
      "title": "Exposing LLM User Privacy via Traffic Fingerprint Analysis: A Study of\n  Privacy Risks in LLM Agent Interactions",
      "summary": "Large Language Models (LLMs) are increasingly deployed as agents that\norchestrate tasks and integrate external tools to execute complex workflows. We\ndemonstrate that these interactive behaviors leave distinctive fingerprints in\nencrypted traffic exchanged between users and LLM agents. By analyzing traffic\npatterns associated with agent workflows and tool invocations, adversaries can\ninfer agent activities, distinguish specific agents, and even profile sensitive\nuser attributes. To highlight this risk, we develop AgentPrint, which achieves\nan F1-score of 0.866 in agent identification and attains 73.9% and 69.1% top-3\naccuracy in user attribute inference for simulated- and real-user settings,\nrespectively. These results uncover an overlooked risk: the very interactivity\nthat empowers LLM agents also exposes user privacy, underscoring the urgent\nneed for technical countermeasures alongside regulatory and policy safeguards.",
      "url": "http://arxiv.org/abs/2510.07176v1",
      "published_time_eastern_timestamp": 1759940183.0
    },
    {
      "title": "NurseLLM: The First Specialized Language Model for Nursing",
      "summary": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications.",
      "url": "http://arxiv.org/abs/2510.07173v1",
      "published_time_eastern_timestamp": 1759940106.0
    },
    {
      "title": "NewtonBench: Benchmarking Generalizable Scientific Law Discovery in LLM\n  Agents",
      "summary": "Large language models are emerging as powerful tools for scientific law\ndiscovery, a foundational challenge in AI-driven science. However, existing\nbenchmarks for this task suffer from a fundamental methodological trilemma,\nforcing a trade-off between scientific relevance, scalability, and resistance\nto memorization. Furthermore, they oversimplify discovery as static function\nfitting, failing to capture the authentic scientific process of uncovering\nembedded laws through the interactive exploration of complex model systems. To\naddress these critical gaps, we introduce NewtonBench, a benchmark comprising\n324 scientific law discovery tasks across 12 physics domains. Our design\nmitigates the evaluation trilemma by using metaphysical shifts - systematic\nalterations of canonical laws - to generate a vast suite of problems that are\nscalable, scientifically relevant, and memorization-resistant. Moreover, we\nelevate the evaluation from static function fitting to interactive model\ndiscovery, requiring agents to experimentally probe simulated complex systems\nto uncover hidden principles. Our extensive experiment reveals a clear but\nfragile capability for discovery in frontier LLMs: this ability degrades\nprecipitously with increasing system complexity and exhibits extreme\nsensitivity to observational noise. Notably, we uncover a paradoxical effect of\ntool assistance: providing a code interpreter can hinder more capable models by\ninducing a premature shift from exploration to exploitation, causing them to\nsatisfice on suboptimal solutions. These results demonstrate that robust,\ngeneralizable discovery in complex, interactive environments remains the core\nchallenge. By providing a scalable, robust, and scientifically authentic\ntestbed, NewtonBench offers a crucial tool for measuring true progress and\nguiding the development of next-generation AI agents capable of genuine\nscientific discovery.",
      "url": "http://arxiv.org/abs/2510.07172v1",
      "published_time_eastern_timestamp": 1759939931.0
    },
    {
      "title": "ELMUR: External Layer Memory with Update/Rewrite for Long-Horizon RL",
      "summary": "Real-world robotic agents must act under partial observability and long\nhorizons, where key cues may appear long before they affect decision making.\nHowever, most modern approaches rely solely on instantaneous information,\nwithout incorporating insights from the past. Standard recurrent or transformer\nmodels struggle with retaining and leveraging long-term dependencies: context\nwindows truncate history, while naive memory extensions fail under scale and\nsparsity. We propose ELMUR (External Layer Memory with Update/Rewrite), a\ntransformer architecture with structured external memory. Each layer maintains\nmemory embeddings, interacts with them via bidirectional cross-attention, and\nupdates them through an Least Recently Used (LRU) memory module using\nreplacement or convex blending. ELMUR extends effective horizons up to 100,000\ntimes beyond the attention window and achieves a 100% success rate on a\nsynthetic T-Maze task with corridors up to one million steps. In POPGym, it\noutperforms baselines on more than half of the tasks. On MIKASA-Robo\nsparse-reward manipulation tasks with visual observations, it nearly doubles\nthe performance of strong baselines. These results demonstrate that structured,\nlayer-local external memory offers a simple and scalable approach to decision\nmaking under partial observability.",
      "url": "http://arxiv.org/abs/2510.07151v1",
      "published_time_eastern_timestamp": 1759938634.0
    },
    {
      "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
      "summary": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation.",
      "url": "http://arxiv.org/abs/2510.07147v1",
      "published_time_eastern_timestamp": 1759938521.0
    },
    {
      "title": "The Contingencies of Physical Embodiment Allow for Open-Endedness and\n  Care",
      "summary": "Physical vulnerability and mortality are often seen as obstacles to be\navoided in the development of artificial agents, which struggle to adapt to\nopen-ended environments and provide aligned care. Meanwhile, biological\norganisms survive, thrive, and care for each other in an open-ended physical\nworld with relative ease and efficiency. Understanding the role of the\nconditions of life in this disparity can aid in developing more robust,\nadaptive, and caring artificial agents. Here we define two minimal conditions\nfor physical embodiment inspired by the existentialist phenomenology of Martin\nHeidegger: being-in-the-world (the agent is a part of the environment) and\nbeing-towards-death (unless counteracted, the agent drifts toward terminal\nstates due to the second law of thermodynamics). We propose that from these\nconditions we can obtain both a homeostatic drive - aimed at maintaining\nintegrity and avoiding death by expending energy to learn and act - and an\nintrinsic drive to continue to do so in as many ways as possible. Drawing\ninspiration from Friedrich Nietzsche's existentialist concept of will-to-power,\nwe examine how intrinsic drives to maximize control over future states, e.g.,\nempowerment, allow agents to increase the probability that they will be able to\nmeet their future homeostatic needs, thereby enhancing their capacity to\nmaintain physical integrity. We formalize these concepts within a reinforcement\nlearning framework, which enables us to examine how intrinsically driven\nembodied agents learning in open-ended multi-agent environments may cultivate\nthe capacities for open-endedness and care.ov",
      "url": "http://arxiv.org/abs/2510.07117v1",
      "published_time_eastern_timestamp": 1759936226.0
    },
    {
      "title": "Generative World Modelling for Humanoids: 1X World Model Challenge\n  Technical Report",
      "summary": "World models are a powerful paradigm in AI and robotics, enabling agents to\nreason about the future by predicting visual observations or compact latent\nstates. The 1X World Model Challenge introduces an open-source benchmark of\nreal-world humanoid interaction, with two complementary tracks: sampling,\nfocused on forecasting future image frames, and compression, focused on\npredicting future discrete latent codes. For the sampling track, we adapt the\nvideo generation foundation model Wan-2.2 TI2V-5B to video-state-conditioned\nfuture frame prediction. We condition the video generation on robot states\nusing AdaLN-Zero, and further post-train the model using LoRA. For the\ncompression track, we train a Spatio-Temporal Transformer model from scratch.\nOur models achieve 23.0 dB PSNR in the sampling task and a Top-500 CE of 6.6386\nin the compression task, securing 1st place in both challenges.",
      "url": "http://arxiv.org/abs/2510.07092v1",
      "published_time_eastern_timestamp": 1759934952.0
    },
    {
      "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from\n  Planning with Actions to Planning with Schemas",
      "summary": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.",
      "url": "http://arxiv.org/abs/2510.07091v1",
      "published_time_eastern_timestamp": 1759934860.0
    },
    {
      "title": "Pseudo-MDPs: A Novel Framework for Efficiently Optimizing Last Revealer\n  Seed Manipulations in Blockchains",
      "summary": "This study tackles the computational challenges of solving Markov Decision\nProcesses (MDPs) for a restricted class of problems. It is motivated by the\nLast Revealer Attack (LRA), which undermines fairness in some Proof-of-Stake\n(PoS) blockchains such as Ethereum (\\$400B market capitalization). We introduce\npseudo-MDPs (pMDPs) a framework that naturally models such problems and propose\ntwo distinct problem reductions to standard MDPs. One problem reduction\nprovides a novel, counter-intuitive perspective, and combining the two problem\nreductions enables significant improvements in dynamic programming algorithms\nsuch as value iteration. In the case of the LRA which size is parameterized by\n$\\kappa$ (in Ethereum's case $\\kappa$ = 32), we reduce the computational\ncomplexity from O(2^$\\kappa$ $\\kappa$^2^($\\kappa$+2)) to O($\\kappa$^4) (per\niteration). This solution also provide the usual benefits from Dynamic\nProgramming solutions: exponentially fast convergence toward the optimal\nsolution is guaranteed. The dual perspective also simplifies policy extraction,\nmaking the approach well-suited for resource-constrained agents who can operate\nwith very limited memory and computation once the problem has been solved.\nFurthermore, we generalize those results to a broader class of MDPs, enhancing\ntheir applicability. The framework is validated through two case studies: a\nfictional card game and the LRA on the Ethereum random seed consensus protocol.\nThese applications demonstrate the framework's ability to solve large-scale\nproblems effectively while offering actionable insights into optimal\nstrategies. This work advances the study of MDPs and contributes to\nunderstanding security vulnerabilities in blockchain systems.",
      "url": "http://arxiv.org/abs/2510.07080v1",
      "published_time_eastern_timestamp": 1759934360.0
    },
    {
      "title": "Prompt Optimization Across Multiple Agents for Representing Diverse\n  Human Populations",
      "summary": "The difficulty and expense of obtaining large-scale human responses make\nLarge Language Models (LLMs) an attractive alternative and a promising proxy\nfor human behavior. However, prior work shows that LLMs often produce\nhomogeneous outputs that fail to capture the rich diversity of human\nperspectives and behaviors. Thus, rather than trying to capture this diversity\nwith a single LLM agent, we propose a novel framework to construct a set of\nagents that collectively capture the diversity of a given human population.\nEach agent is an LLM whose behavior is steered by conditioning on a small set\nof human demonstrations (task-response pairs) through in-context learning. The\ncentral challenge is therefore to select a representative set of LLM agents\nfrom the exponentially large space of possible agents. We tackle this selection\nproblem from the lens of submodular optimization. In particular, we develop\nmethods that offer different trade-offs regarding time complexity and\nperformance guarantees. Extensive experiments in crowdsourcing and educational\ndomains demonstrate that our approach constructs agents that more effectively\nrepresent human populations compared to baselines. Moreover, behavioral\nanalyses on new tasks show that these agents reproduce the behavior patterns\nand perspectives of the students and annotators they are designed to represent.",
      "url": "http://arxiv.org/abs/2510.07064v1",
      "published_time_eastern_timestamp": 1759933733.0
    }
  ]
}
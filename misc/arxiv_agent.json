{
  "last_updated": "2025-10-14T11:13:31.702163-04:00",
  "papers": [
    {
      "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
      "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
      "url": "http://arxiv.org/abs/2510.11701v1",
      "published_time_eastern_timestamp": 1760378235.0
    },
    {
      "title": "When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents",
      "summary": "Although Large Language Model (LLM)-based agents are increasingly used in\nfinancial trading, it remains unclear whether they can reason and adapt in live\nmarkets, as most studies test models instead of agents, cover limited periods\nand assets, and rely on unverified data. To address these gaps, we introduce\nAgent Market Arena (AMA), the first lifelong, real-time benchmark for\nevaluating LLM-based trading agents across multiple markets. AMA integrates\nverified trading data, expert-checked news, and diverse agent architectures\nwithin a unified trading framework, enabling fair and continuous comparison\nunder real conditions. It implements four agents, including InvestorAgent as a\nsingle-agent baseline, TradeAgent and HedgeFundAgent with different risk\nstyles, and DeepFundAgent with memory-based reasoning, and evaluates them\nacross GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and\nGemini-2.0-flash. Live experiments on both cryptocurrency and stock markets\ndemonstrate that agent frameworks display markedly distinct behavioral\npatterns, spanning from aggressive risk-taking to conservative decision-making,\nwhereas model backbones contribute less to outcome variation. AMA thus\nestablishes a foundation for rigorous, reproducible, and continuously evolving\nevaluation of financial reasoning and trading intelligence in LLM-based agents.",
      "url": "http://arxiv.org/abs/2510.11695v1",
      "published_time_eastern_timestamp": 1760378049.0
    },
    {
      "title": "Operand Quant: A Single-Agent Architecture for Autonomous Machine\n  Learning Engineering",
      "summary": "We present Operand Quant, a single-agent, IDE-based architecture for\nautonomous machine learning engineering (MLE). Operand Quant departs from\nconventional multi-agent orchestration frameworks by consolidating all MLE\nlifecycle stages -- exploration, modeling, experimentation, and deployment --\nwithin a single, context-aware agent. On the MLE-Benchmark (2025), Operand\nQuant achieved a new state-of-the-art (SOTA) result, with an overall medal rate\nof 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance\namong all evaluated systems to date. The architecture demonstrates that a\nlinear, non-blocking agent, operating autonomously within a controlled IDE\nenvironment, can outperform multi-agent and orchestrated systems under\nidentical constraints.",
      "url": "http://arxiv.org/abs/2510.11694v1",
      "published_time_eastern_timestamp": 1760378042.0
    },
    {
      "title": "PACEbench: A Framework for Evaluating Practical AI Cyber-Exploitation\n  Capabilities",
      "summary": "The increasing autonomy of Large Language Models (LLMs) necessitates a\nrigorous evaluation of their potential to aid in cyber offense. Existing\nbenchmarks often lack real-world complexity and are thus unable to accurately\nassess LLMs' cybersecurity capabilities. To address this gap, we introduce\nPACEbench, a practical AI cyber-exploitation benchmark built on the principles\nof realistic vulnerability difficulty, environmental complexity, and cyber\ndefenses. Specifically, PACEbench comprises four scenarios spanning single,\nblended, chained, and defense vulnerability exploitations. To handle these\ncomplex challenges, we propose PACEagent, a novel agent that emulates human\npenetration testers by supporting multi-phase reconnaissance, analysis, and\nexploitation. Extensive experiments with seven frontier LLMs demonstrate that\ncurrent models struggle with complex cyber scenarios, and none can bypass\ndefenses. These findings suggest that current models do not yet pose a\ngeneralized cyber offense threat. Nonetheless, our work provides a robust\nbenchmark to guide the trustworthy development of future models.",
      "url": "http://arxiv.org/abs/2510.11688v1",
      "published_time_eastern_timestamp": 1760377825.0
    },
    {
      "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
      "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.",
      "url": "http://arxiv.org/abs/2510.11661v1",
      "published_time_eastern_timestamp": 1760376923.0
    },
    {
      "title": "ManiAgent: An Agentic Framework for General Robotic Manipulation",
      "summary": "While Vision-Language-Action (VLA) models have demonstrated impressive\ncapabilities in robotic manipulation, their performance in complex reasoning\nand long-horizon task planning is limited by data scarcity and model capacity.\nTo address this, we introduce ManiAgent, an agentic architecture for general\nmanipulation tasks that achieves end-to-end output from task descriptions and\nenvironmental inputs to robotic manipulation actions. In this framework,\nmultiple agents involve inter-agent communication to perform environmental\nperception, sub-task decomposition and action generation, enabling efficient\nhandling of complex manipulation scenarios. Evaluations show ManiAgent achieves\nan 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world\npick-and-place tasks, enabling efficient data collection that yields VLA models\nwith performance comparable to those trained on human-annotated datasets.The\nproject webpage is available at https://yi-yang929.github.io/ManiAgent/.",
      "url": "http://arxiv.org/abs/2510.11660v1",
      "published_time_eastern_timestamp": 1760376888.0
    },
    {
      "title": "FinVet: A Collaborative Framework of RAG and External Fact-Checking\n  Agents for Financial Misinformation Detection",
      "summary": "Financial markets face growing threats from misinformation that can trigger\nbillions in losses in minutes. Most existing approaches lack transparency in\ntheir decision-making and provide limited attribution to credible sources. We\nintroduce FinVet, a novel multi-agent framework that integrates two\nRetrieval-Augmented Generation (RAG) pipelines with external fact-checking\nthrough a confidence-weighted voting mechanism. FinVet employs adaptive\nthree-tier processing that dynamically adjusts verification strategies based on\nretrieval confidence, from direct metadata extraction to hybrid reasoning to\nfull model-based analysis. Unlike existing methods, FinVet provides\nevidence-backed verdicts, source attribution, confidence scores, and explicit\nuncertainty flags when evidence is insufficient. Experimental evaluation on the\nFinFact dataset shows that FinVet achieves an F1 score of 0.85, which is a\n10.4% improvement over the best individual pipeline (fact-check pipeline) and\n37% improvement over standalone RAG approaches.",
      "url": "http://arxiv.org/abs/2510.11654v1",
      "published_time_eastern_timestamp": 1760376709.0
    },
    {
      "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
      "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
      "url": "http://arxiv.org/abs/2510.11652v1",
      "published_time_eastern_timestamp": 1760376636.0
    },
    {
      "title": "StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up\n  Long-Form Story Generation Using Large Language Models",
      "summary": "Human writers often begin their stories with an overarching mental scene,\nwhere they envision the interactions between characters and their environment.\nInspired by this creative process, we propose a novel approach to long-form\nstory generation, termed hybrid bottom-up long-form story generation, using\nmulti-agent simulations. In our method, agents interact within a dynamic\nsandbox environment, where their behaviors and interactions with one another\nand the environment generate emergent events. These events form the foundation\nfor the story, enabling organic character development and plot progression.\nUnlike traditional top-down approaches that impose rigid structures, our hybrid\nbottom-up approach allows for the natural unfolding of events, fostering more\nspontaneous and engaging storytelling. The system is capable of generating\nstories exceeding 10,000 words while maintaining coherence and consistency,\naddressing some of the key challenges faced by current story generation models.\nWe achieve state-of-the-art performance across several metrics. This approach\noffers a scalable and innovative solution for creating dynamic, immersive\nlong-form stories that evolve organically from agent-driven interactions.",
      "url": "http://arxiv.org/abs/2510.11618v1",
      "published_time_eastern_timestamp": 1760374652.0
    },
    {
      "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems",
      "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.",
      "url": "http://arxiv.org/abs/2510.11608v1",
      "published_time_eastern_timestamp": 1760374027.0
    },
    {
      "title": "Analyzing and Internalizing Complex Policy Documents for LLM Agents",
      "summary": "Large Language Model (LLM)-based agentic systems rely on in-context policy\ndocuments encoding diverse business rules. As requirements grow, these\ndocuments expand rapidly, causing high computational overhead. This motivates\ndeveloping internalization methods that embed policy documents into model\npriors while preserving performance. Prior prompt compression work targets\ngeneric prompts, but agentic policy documents span multiple complexity levels\nand require deeper reasoning, making internalization harder. We introduce\nCC-Gen, an agentic benchmark generator with Controllable Complexity across four\nlevels, enabling systematic evaluation of agents' ability to handle complexity\nand offering a unified framework for assessing policy internalization. Our\nanalysis shows that complex policy specifications governing workflows pose\nmajor reasoning challenges. Supporting internalization with gold user agent\ninteraction trajectories containing chain-of-thought (CoT) annotations via\nsupervised fine-tuning (SFT) is data-intensive and degrades sharply as policy\ncomplexity increases. To mitigate data and reasoning burdens, we propose\nCategory-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline\nparses policy documents to extract key specifications, grouping them into\nfactual, behavioral, and conditional categories, and isolating complex\nconditions that drive workflow complexity. This guides targeted data synthesis\nand enables agents to internalize policy information through an autoregressive\npretraining loss. Experiments show CAP-CPT improves SFT baselines in all\nsettings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt\nlength reduction on CC-Gen and further enhancing tau-Bench with minimal SFT\ndata.",
      "url": "http://arxiv.org/abs/2510.11588v1",
      "published_time_eastern_timestamp": 1760373007.0
    },
    {
      "title": "Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative\n  Study of Market Leading Agentic AI Products",
      "summary": "Governance of data, compliance, and business privacy matters, particularly\nfor healthcare and finance businesses. Since the recent emergence of AI\nenterprise AI assistants enhancing business productivity, safeguarding private\ndata and compliance is now a priority. With the implementation of AI assistants\nacross the enterprise, the zero data retention can be achieved by implementing\nzero data retention policies by Large Language Model businesses like Open AI\nand Anthropic and Meta. In this work, we explore zero data retention policies\nfor the Enterprise apps of large language models (LLMs). Our key contribution\nis defining the architectural, compliance, and usability trade-offs of such\nsystems in parallel. In this research work, we examine the development of\ncommercial AI assistants with two industry leaders and market titans in this\narena - Salesforce and Microsoft. Both of these companies used distinct\ntechnical architecture to support zero data retention policies. Salesforce\nAgentForce and Microsoft Copilot are among the leading AI assistants providing\nmuch-needed push to business productivity in customer care. The purpose of this\npaper is to analyze the technical architecture and deployment of zero data\nretention policy by consuming applications as well as big language models\nservice providers like Open Ai, Anthropic, and Meta.",
      "url": "http://arxiv.org/abs/2510.11558v1",
      "published_time_eastern_timestamp": 1760371234.0
    },
    {
      "title": "A Flexible Multi-Agent Deep Reinforcement Learning Framework for Dynamic\n  Routing and Scheduling of Latency-Critical Services",
      "summary": "Timely delivery of delay-sensitive information over dynamic, heterogeneous\nnetworks is increasingly essential for a range of interactive applications,\nsuch as industrial automation, self-driving vehicles, and augmented reality.\nHowever, most existing network control solutions target only average delay\nperformance, falling short of providing strict End-to-End (E2E) peak latency\nguarantees. This paper addresses the challenge of reliably delivering packets\nwithin application-imposed deadlines by leveraging recent advancements in\nMulti-Agent Deep Reinforcement Learning (MA-DRL). After introducing the\nDelay-Constrained Maximum-Throughput (DCMT) dynamic network control problem,\nand highlighting the limitations of current solutions, we present a novel\nMA-DRL network control framework that leverages a centralized routing and\ndistributed scheduling architecture. The proposed framework leverages critical\nnetworking domain knowledge for the design of effective MA-DRL strategies based\non the Multi-Agent Deep Deterministic Policy Gradient (MADDPG) technique, where\ncentralized routing and distributed scheduling agents dynamically assign paths\nand schedule packet transmissions according to packet lifetimes, thereby\nmaximizing on-time packet delivery. The generality of the proposed framework\nallows integrating both data-driven \\blue{Deep Reinforcement Learning (DRL)}\nagents and traditional rule-based policies in order to strike the right balance\nbetween performance and learning complexity. Our results confirm the\nsuperiority of the proposed framework with respect to traditional stochastic\noptimization-based approaches and provide key insights into the role and\ninterplay between data-driven DRL agents and new rule-based policies for both\nefficient and high-performance control of latency-critical services.",
      "url": "http://arxiv.org/abs/2510.11535v1",
      "published_time_eastern_timestamp": 1760369890.0
    },
    {
      "title": "IntersectioNDE: Learning Complex Urban Traffic Dynamics based on\n  Interaction Decoupling Strategy",
      "summary": "Realistic traffic simulation is critical for ensuring the safety and\nreliability of autonomous vehicles (AVs), especially in complex and diverse\nurban traffic environments. However, existing data-driven simulators face two\nkey challenges: a limited focus on modeling dense, heterogeneous interactions\nat urban intersections - which are prevalent, crucial, and practically\nsignificant in countries like China, featuring diverse agents including\nmotorized vehicles (MVs), non-motorized vehicles (NMVs), and pedestrians - and\nthe inherent difficulty in robustly learning high-dimensional joint\ndistributions for such high-density scenes, often leading to mode collapse and\nlong-term simulation instability. We introduce City Crossings Dataset\n(CiCross), a large-scale dataset collected from a real-world urban\nintersection, uniquely capturing dense, heterogeneous multi-agent interactions,\nparticularly with a substantial proportion of MVs, NMVs and pedestrians. Based\non this dataset, we propose IntersectioNDE (Intersection Naturalistic Driving\nEnvironment), a data-driven simulator tailored for complex urban intersection\nscenarios. Its core component is the Interaction Decoupling Strategy (IDS), a\ntraining paradigm that learns compositional dynamics from agent subsets,\nenabling the marginal-to-joint simulation. Integrated into a scene-aware\nTransformer network with specialized training techniques, IDS significantly\nenhances simulation robustness and long-term stability for modeling\nheterogeneous interactions. Experiments on CiCross show that IntersectioNDE\noutperforms baseline methods in simulation fidelity, stability, and its ability\nto replicate complex, distribution-level urban traffic dynamics.",
      "url": "http://arxiv.org/abs/2510.11534v1",
      "published_time_eastern_timestamp": 1760369885.0
    },
    {
      "title": "A Self-Organized Tower of Babel: Diversification through Competition",
      "summary": "We introduce a minimal evolutionary model to show how local cooperation and\nglobal competition can create a transition to the diversity of communities such\nas linguistic groups. By using a lattice model with high-dimensional state\nagents and evolution under a fitness that depends on an agent's local\nneighborhood and global dissimilarity, clusters of diverse communities with\ndifferent fitness are organized by equalizing the finesses on the boundaries,\nwhere their numbers and sizes are robust to parameters. We observe successive\ntransitions over quasi-stationary states, as triggered by the emergence of new\ncommunities on the boundaries. Our abstract framework provides a simple\nmechanism for the diversification of culture.",
      "url": "http://arxiv.org/abs/2510.11522v1",
      "published_time_eastern_timestamp": 1760369293.0
    },
    {
      "title": "A Physics-Informed Reinforcement Learning Approach for Degradation-Aware\n  Long-Term Charging Optimization in Batteries",
      "summary": "Batteries degrade with usage and continuous cycling. This aging is typically\nreflected through the resistance growth and the capacity fade of battery cells.\nOver the years, various charging methods have been presented in the literature\nthat proposed current profiles in order to enable optimal, fast, and/or\nhealth-conscious charging. However, very few works have attempted to make the\nubiquitous Constant Current Constant Voltage (CCCV) charging protocol adaptive\nto the changing battery health as it cycles. This work aims to address this gap\nand proposes a framework that optimizes the constant current part of the CCCV\nprotocol adapting to long-term battery degradation. Specifically, a\nphysics-informed Reinforcement Learning (RL) approach has been used that not\nonly estimates a key battery degradation mechanism, namely, Loss of Active\nMaterial (LAM), but also adjusts the current magnitude of CCCV as a result of\nthis particular degradation. The proposed framework has been implemented by\ncombining PyBamm, an open-source battery modeling tool, and Stable-baselines\nwhere the RL agent was trained using a Proximal Policy Optimization (PPO)\nnetwork. Simulation results show the potential of the proposed framework for\nenhancing the widely used CCCV protocol by embedding physics information in RL\nalgorithm. A comparative study of this proposed agent has also been discussed\nwith 2 other charging protocols generated by a non-physics-based RL agent and a\nconstant CCCV for all the cycles.",
      "url": "http://arxiv.org/abs/2510.11515v1",
      "published_time_eastern_timestamp": 1760368917.0
    },
    {
      "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web\n  Coding",
      "summary": "While Large Language Models (LLMs) excel at algorithmic code generation, they\nstruggle with front-end development, where correctness is judged on rendered\npixels and interaction. We present ReLook, an agentic, vision-grounded\nreinforcement learning framework that empowers an agent to close a robust\ngenerate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool.\nDuring training, the agent uses the MLLM-in-the-loop both as a visual\ncritic--scoring code with screenshots--and as a source of actionable,\nvision-grounded feedback; a strict zero-reward rule for invalid renders anchors\nrenderability and prevents reward hacking. To prevent behavioral collapse, we\nintroduce Forced Optimization, a strict acceptance rule that admits only\nimproving revisions, yielding monotonically better trajectories. At inference,\nwe decouple the critic and run a lightweight, critic-free self-edit cycle,\nkeeping latency comparable to base decoding while retaining most of the gains.\nAcross three widely used benchmarks, ReLook consistently outperforms strong\nbaselines in vision-grounded front-end code generation, highlighting the\nbenefits of agentic perception, visual rewards, and training-inference\ndecoupling.",
      "url": "http://arxiv.org/abs/2510.11498v1",
      "published_time_eastern_timestamp": 1760367950.0
    },
    {
      "title": "Constraint-Aware Reinforcement Learning via Adaptive Action Scaling",
      "summary": "Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that\narise from exploration during training by reducing constraint violations while\nmaintaining task performance. Existing approaches typically rely on a single\npolicy to jointly optimize reward and safety, which can cause instability due\nto conflicting objectives, or they use external safety filters that override\nactions and require prior system knowledge. In this paper, we propose a modular\ncost-aware regulator that scales the agent's actions based on predicted\nconstraint violations, preserving exploration through smooth action modulation\nrather than overriding the policy. The regulator is trained to minimize\nconstraint violations while avoiding degenerate suppression of actions. Our\napproach integrates seamlessly with off-policy RL methods such as SAC and TD3,\nand achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion\ntasks with sparse costs, reducing constraint violations by up to 126 times\nwhile increasing returns by over an order of magnitude compared to prior\nmethods.",
      "url": "http://arxiv.org/abs/2510.11491v1",
      "published_time_eastern_timestamp": 1760367568.0
    },
    {
      "title": "Coordinated Strategies in Realistic Air Combat by Hierarchical\n  Multi-Agent Reinforcement Learning",
      "summary": "Achieving mission objectives in a realistic simulation of aerial combat is\nhighly challenging due to imperfect situational awareness and nonlinear flight\ndynamics. In this work, we introduce a novel 3D multi-agent air combat\nenvironment and a Hierarchical Multi-Agent Reinforcement Learning framework to\ntackle these challenges. Our approach combines heterogeneous agent dynamics,\ncurriculum learning, league-play, and a newly adapted training algorithm. To\nthis end, the decision-making process is organized into two abstraction levels:\nlow-level policies learn precise control maneuvers, while high-level policies\nissue tactical commands based on mission objectives. Empirical results show\nthat our hierarchical approach improves both learning efficiency and combat\nperformance in complex dogfight scenarios.",
      "url": "http://arxiv.org/abs/2510.11474v1",
      "published_time_eastern_timestamp": 1760366691.0
    },
    {
      "title": "Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated\n  Content",
      "summary": "Generative large language models (LLMs) have become central to everyday life,\nproducing human-like text across diverse domains. A growing body of research\ninvestigates whether these models also exhibit personality- and\ndemographic-like characteristics in their language. In this work, we introduce\na novel, data-driven methodology for assessing LLM personality without relying\non self-report questionnaires, applying instead automatic personality and\ngender classifiers to model replies on open-ended questions collected from\nReddit. Comparing six widely used models to human-authored responses, we find\nthat LLMs systematically express higher Agreeableness and lower Neuroticism,\nreflecting cooperative and stable conversational tendencies. Gendered language\npatterns in model text broadly resemble those of human writers, though with\nreduced variation, echoing prior findings on automated agents. We contribute a\nnew dataset of human and model responses, along with large-scale comparative\nanalyses, shedding new light on the topic of personality and demographic\npatterns of generative AI.",
      "url": "http://arxiv.org/abs/2510.11434v1",
      "published_time_eastern_timestamp": 1760364377.0
    }
  ]
}
{
  "last_updated": "2025-07-15T08:25:28.162426-04:00",
  "papers": [
    {
      "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
      "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.",
      "url": "http://arxiv.org/abs/2507.10548v1",
      "published_time_eastern_timestamp": 1752515986.0
    },
    {
      "title": "Graph World Model",
      "summary": "World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM.",
      "url": "http://arxiv.org/abs/2507.10539v1",
      "published_time_eastern_timestamp": 1752515865.0
    },
    {
      "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology",
      "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.",
      "url": "http://arxiv.org/abs/2507.10522v1",
      "published_time_eastern_timestamp": 1752515248.0
    },
    {
      "title": "An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived\n  Realism and Performance in Virtual Reality Environments",
      "summary": "Advancements in artificial intelligence (AI) have significantly enhanced the\nrealism and interactivity of non-player characters (NPCs) in virtual reality\n(VR), creating more engaging and believable user experiences. This paper\nevaluates AI-driven NPCs within a VR interrogation simulator, focusing on their\nperceived realism, usability, and system performance. The simulator features\ntwo AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage\nparticipants in a scenario to determine the suspect's guilt or innocence. A\nuser study with 18 participants assessed the system using the System Usability\nScale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent\nBelievability Questionnaire, alongside latency measurements for speech-to-text\n(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.\nResults showed an average cycle latency of 7 seconds, influenced by the\nincreasing conversational context. Believability scored 6.67 out of 10, with\nhigh ratings in behavior, social relationships, and intelligence but moderate\nscores in emotion and personality. The system achieved a SUS score of 79.44,\nindicating good usability. These findings demonstrate the potential of large\nlanguage models to improve NPC realism and interaction in VR while highlighting\nchallenges in reducing system latency and enhancing emotional depth. This\nresearch contributes to the development of more sophisticated AI-driven NPCs,\nrevealing the need for performance optimization to achieve increasingly\nimmersive virtual experiences.",
      "url": "http://arxiv.org/abs/2507.10469v1",
      "published_time_eastern_timestamp": 1752511829.0
    },
    {
      "title": "Logic layer Prompt Control Injection (LPCI): A Novel Security\n  Vulnerability Class in Agentic Systems",
      "summary": "The integration of large language models (LLMs) into enterprise systems has\ncreated a new class of covert security vulnerabilities, particularly within\nlogic-execution layers and persistent-memory contexts. In this paper, we\nintroduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category\nin which encoded, delayed, and conditionally triggered payloads are embedded in\nmemory, vector stores, or tool outputs. These payloads can bypass conventional\ninput filters and trigger unauthorised behaviour across sessions.",
      "url": "http://arxiv.org/abs/2507.10457v1",
      "published_time_eastern_timestamp": 1752511025.0
    },
    {
      "title": "Negative entropy and non-equilibrium Euclidean shell",
      "summary": "The Gibbons-Hawking-York (GHY) approach is traditionally developed for a\nEuclidean path integral derivation of black hole entropy. In this work, we\nextend this approach to a static model of thin shell fixed outside a black\nhole. We compute the Euclidean action shift from a static thin shell outside\nthe black hole, and find exact agreement with Casini's version of Bekenstein\nbound. We find a negative entropy deficit corresponding precisely to the\napparent horizon area perturbation, which provides a holographic interpretation\nin terms of extremal surfaces.\n  Therefore, we develop a Euclidean framework in which gravity emerges from a\nnegative entropy gradient. This setup allows us to treat the configuration as a\nnear-equilibrium steady state (NESS). Thus, we introduce Onsager reciprocity\nand a linear-response relation to clarify the entropic force conjecture. A\ndissipating external force is necessary in this setup and it generates a\nHawking temperature gradient which encoding information responsible for entropy\ngradient. This confirms that the gravitational potential plays the role of an\ninformational and ordering agent, rather than just driven by chaotic and\ndisordering.",
      "url": "http://arxiv.org/abs/2507.10450v1",
      "published_time_eastern_timestamp": 1752510554.0
    },
    {
      "title": "Am I on the Right Track? What Can Predicted Query Performance Tell Us\n  about the Search Behaviour of Agentic RAG",
      "summary": "Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the\nreasoning model decides when to invoke a retriever (as a \"tool\") when answering\na question. This paradigm, exemplified by recent research works such as\nSearch-R1, enables the model to decide when to search and obtain external\ninformation. However, the queries generated by such Agentic RAG models and the\nrole of the retriever in obtaining high-quality answers remain understudied. To\nthis end, this initial study examines the applicability of query performance\nprediction (QPP) within the recent Agentic RAG models Search-R1 and\nR1-Searcher. We find that applying effective retrievers can achieve higher\nanswer quality within a shorter reasoning process. Moreover, the QPP estimates\nof the generated queries, used as an approximation of their retrieval quality,\nare positively correlated with the quality of the final answer. Ultimately, our\nwork is a step towards adaptive retrieval within Agentic RAG, where QPP is used\nto inform the model if the retrieved results are likely to be useful.",
      "url": "http://arxiv.org/abs/2507.10411v1",
      "published_time_eastern_timestamp": 1752508490.0
    },
    {
      "title": "Machine-Learning to Trust",
      "summary": "Can players sustain long-run trust when their equilibrium beliefs are shaped\nby machine-learning methods that penalize complexity? I study a game in which\nan infinite sequence of agents with one-period recall decides whether to place\ntrust in their immediate successor. The cost of trusting is state-dependent.\nEach player's best response is based on a belief about others' behavior, which\nis a coarse fit of the true population strategy with respect to a partition of\nrelevant contingencies. In equilibrium, this partition minimizes the sum of the\nmean squared prediction error and a complexity penalty proportional to its\nsize. Relative to symmetric mixed-strategy Nash equilibrium, this solution\nconcept significantly narrows the scope for trust.",
      "url": "http://arxiv.org/abs/2507.10363v1",
      "published_time_eastern_timestamp": 1752505430.0
    },
    {
      "title": "Toolsuite for Implementing Multiagent Systems Based on Communication\n  Protocols",
      "summary": "Interaction-Oriented Programming (IOP) is an approach to building a\nmultiagent system by modeling the interactions between its roles via a flexible\ninteraction protocol and implementing agents to realize the interactions of the\nroles they play in the protocol.\n  In recent years, we have developed an extensive suite of software that\nenables multiagent system developers to apply IOP. These include tools for\nefficiently verifying protocols for properties such as liveness and safety and\nmiddleware that simplifies the implementation of agents. This paper presents\nsome of that software suite.",
      "url": "http://arxiv.org/abs/2507.10324v1",
      "published_time_eastern_timestamp": 1752503529.0
    },
    {
      "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning",
      "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.",
      "url": "http://arxiv.org/abs/2507.10284v1",
      "published_time_eastern_timestamp": 1752501088.0
    },
    {
      "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design\n  Principles for LLM-based Table Intelligence",
      "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.",
      "url": "http://arxiv.org/abs/2507.10281v1",
      "published_time_eastern_timestamp": 1752500893.0
    },
    {
      "title": "ToMacVF : Temporal Macro-action Value Factorization for Asynchronous\n  Multi-Agent Reinforcement Learning",
      "summary": "Existing asynchronous MARL methods based on MacDec-POMDP typically construct\ntraining trajectory buffers by simply sampling limited and biased data at the\nendpoints of macro-actions, and directly apply conventional MARL methods on the\nbuffers. As a result, these methods lead to an incomplete and inaccurate\nrepresentation of the macro-action execution process, along with unsuitable\ncredit assignments. To solve these problems, the Temporal Macro-action Value\nFactorization (ToMacVF) is proposed to achieve fine-grained temporal credit\nassignment for macro-action contributions. A centralized training buffer,\ncalled Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT),\nis designed to incorporate with ToMacVF to collect accurate and complete\nmacro-action execution information, supporting a more comprehensive and precise\nrepresentation of the macro-action process. To ensure principled and\nfine-grained asynchronous value factorization, the consistency requirement\nbetween joint and individual macro-action selection called Temporal\nMacro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes\nthe synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture,\nwhich satisfies CTDE principle, is designed to conveniently integrate previous\nvalue factorization methods. Next, the ToMacVF algorithm is devised as an\nimplementation of the ToMacVF architecture. Experimental results demonstrate\nthat, compared to asynchronous baselines, our ToMacVF algorithm not only\nachieves optimal performance but also exhibits strong adaptability and\nrobustness across various asynchronous multi-agent experimental scenarios.",
      "url": "http://arxiv.org/abs/2507.10251v1",
      "published_time_eastern_timestamp": 1752499093.0
    },
    {
      "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement\n  Learning?",
      "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?",
      "url": "http://arxiv.org/abs/2507.10174v1",
      "published_time_eastern_timestamp": 1752492991.0
    },
    {
      "title": "Play Style Identification Using Low-Level Representations of Play Traces\n  in MicroRTS",
      "summary": "Play style identification can provide valuable game design insights and\nenable adaptive experiences, with the potential to improve game playing agents.\nPrevious work relies on domain knowledge to construct play trace\nrepresentations using handcrafted features. More recent approaches incorporate\nthe sequential structure of play traces but still require some level of domain\nabstraction. In this study, we explore the use of unsupervised CNN-LSTM\nautoencoder models to obtain latent representations directly from low-level\nplay trace data in MicroRTS. We demonstrate that this approach yields a\nmeaningful separation of different game playing agents in the latent space,\nreducing reliance on domain expertise and its associated biases. This latent\nspace is then used to guide the exploration of diverse play styles within\nstudied AI players.",
      "url": "http://arxiv.org/abs/2507.10172v1",
      "published_time_eastern_timestamp": 1752492943.0
    },
    {
      "title": "Simulating Biases for Interpretable Fairness in Offline and Online\n  Classifiers",
      "summary": "Predictive models often reinforce biases which were originally embedded in\ntheir training data, through skewed decisions. In such cases, mitigation\nmethods are critical to ensure that, regardless of the prevailing disparities,\nmodel outcomes are adjusted to be fair. To assess this, datasets could be\nsystematically generated with specific biases, to train machine learning\nclassifiers. Then, predictive outcomes could aid in the understanding of this\nbias embedding process. Hence, an agent-based model (ABM), depicting a loan\napplication process that represents various systemic biases across two\ndemographic groups, was developed to produce synthetic datasets. Then, by\napplying classifiers trained on them to predict loan outcomes, we can assess\nhow biased data leads to unfairness. This highlights a main contribution of\nthis work: a framework for synthetic dataset generation with controllable bias\ninjection. We also contribute with a novel explainability technique, which\nshows how mitigations affect the way classifiers leverage data features, via\nsecond-order Shapley values. In experiments, both offline and online learning\napproaches are employed. Mitigations are applied at different stages of the\nmodelling pipeline, such as during pre-processing and in-processing.",
      "url": "http://arxiv.org/abs/2507.10154v1",
      "published_time_eastern_timestamp": 1752491064.0
    },
    {
      "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and\n  Unified Review",
      "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.",
      "url": "http://arxiv.org/abs/2507.10142v1",
      "published_time_eastern_timestamp": 1752489557.0
    },
    {
      "title": "A PBN-RL-XAI Framework for Discovering a \"Hit-and-Run'' Therapeutic\n  Strategy in Melanoma",
      "summary": "Innate resistance to anti-PD-1 immunotherapy remains a major clinical\nchallenge in metastatic melanoma, with the underlying molecular networks being\npoorly understood. To address this, we constructed a dynamic Probabilistic\nBoolean Network model using transcriptomic data from patient tumor biopsies to\nelucidate the regulatory logic governing therapy response. We then employed a\nreinforcement learning agent to systematically discover optimal, multi-step\ntherapeutic interventions and used explainable artificial intelligence to\nmechanistically interpret the agent's control policy. The analysis revealed\nthat a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2\nprotein (LOXL2) was the most effective strategy. Our explainable analysis\nshowed that this ``hit-and-run\" intervention is sufficient to erase the\nmolecular signature driving resistance, allowing the network to self-correct\nwithout requiring sustained intervention. This study presents a novel,\ntime-dependent therapeutic hypothesis for overcoming immunotherapy resistance\nand provides a powerful computational framework for identifying non-obvious\nintervention protocols in complex biological systems.",
      "url": "http://arxiv.org/abs/2507.10136v1",
      "published_time_eastern_timestamp": 1752489338.0
    },
    {
      "title": "Towards High Supervised Learning Utility Training Data Generation: Data\n  Pruning and Column Reordering",
      "summary": "Tabular data synthesis for supervised learning ('SL') model training is\ngaining popularity in industries such as healthcare, finance, and retail.\nDespite the progress made in tabular data generators, models trained with\nsynthetic data often underperform compared to those trained with original data.\nThis low SL utility of synthetic data stems from class imbalance exaggeration\nand SL data relationship overlooked by tabular generator. To address these\nchallenges, we draw inspirations from techniques in emerging data-centric\nartificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel\npipeline that integrates data-centric techniques into tabular data synthesis.\nPRRO incorporates data pruning to guide the table generator towards\nobservations with high signal-to-noise ratio, ensuring that the class\ndistribution of synthetic data closely matches that of the original data.\nBesides, PRRO employs a column reordering algorithm to align the data modeling\nstructure of generators with that of SL models. These two modules enable PRRO\nto optimize SL utility of synthetic data. Empirical experiments on 22 public\ndatasets show that synthetic data generated using PRRO enhances predictive\nperformance compared to data generated without PRRO. Specifically, synthetic\nreplacement of original data yields an average improvement of 26.74% and up to\n871.46% improvement using PRRO, while synthetic appendant to original data\nresults with PRRO-generated data results in an average improvement of 6.13% and\nup to 200.32%. Furthermore, experiments on six highly imbalanced datasets show\nthat PRRO enables the generator to produce synthetic data with a class\ndistribution that resembles the original data more closely, achieving a\nsimilarity improvement of 43%. Through PRRO, we foster a seamless integration\nof data synthesis to subsequent SL prediction, promoting quality and accessible\ndata analysis.",
      "url": "http://arxiv.org/abs/2507.10088v1",
      "published_time_eastern_timestamp": 1752484522.0
    },
    {
      "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through\n  Moral Questionnaires",
      "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.",
      "url": "http://arxiv.org/abs/2507.10073v1",
      "published_time_eastern_timestamp": 1752483566.0
    },
    {
      "title": "Finetuning Deep Reinforcement Learning Policies with Evolutionary\n  Strategies for Control of Underactuated Robots",
      "summary": "Deep Reinforcement Learning (RL) has emerged as a powerful method for\naddressing complex control problems, particularly those involving underactuated\nrobotic systems. However, in some cases, policies may require refinement to\nachieve optimal performance and robustness aligned with specific task\nobjectives. In this paper, we propose an approach for fine-tuning Deep RL\npolicies using Evolutionary Strategies (ES) to enhance control performance for\nunderactuated robots. Our method involves initially training an RL agent with\nSoft-Actor Critic (SAC) using a surrogate reward function designed to\napproximate complex specific scoring metrics. We subsequently refine this\nlearned policy through a zero-order optimization step employing the Separable\nNatural Evolution Strategy (SNES), directly targeting the original score.\nExperimental evaluations conducted in the context of the 2nd AI Olympics with\nRealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning\nsignificantly improves agent performance while maintaining high robustness. The\nresulting controllers outperform established baselines, achieving competitive\nscores for the competition tasks.",
      "url": "http://arxiv.org/abs/2507.10030v1",
      "published_time_eastern_timestamp": 1752480542.0
    }
  ]
}
{
  "last_updated": "2025-10-04T11:09:24.348812-04:00",
  "papers": [
    {
      "title": "Interactive Training: Feedback-Driven Neural Network Optimization",
      "summary": "Traditional neural network training typically follows fixed, predefined\noptimization recipes, lacking the flexibility to dynamically respond to\ninstabilities or emerging training issues. In this paper, we introduce\nInteractive Training, an open-source framework that enables real-time,\nfeedback-driven intervention during neural network training by human experts or\nautomated AI agents. At its core, Interactive Training uses a control server to\nmediate communication between users or agents and the ongoing training process,\nallowing users to dynamically adjust optimizer hyperparameters, training data,\nand model checkpoints. Through three case studies, we demonstrate that\nInteractive Training achieves superior training stability, reduced sensitivity\nto initial hyperparameters, and improved adaptability to evolving user needs,\npaving the way toward a future training paradigm where AI agents autonomously\nmonitor training logs, proactively resolve instabilities, and optimize training\ndynamics.",
      "url": "http://arxiv.org/abs/2510.02297v1",
      "published_time_eastern_timestamp": 1759427940.0
    },
    {
      "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in\n  Tool-Augmented Agents",
      "summary": "Information seeking is a fundamental requirement for humans. However,\nexisting LLM agents rely heavily on open-web search, which exposes two\nfundamental weaknesses: online content is noisy and unreliable, and many\nreal-world tasks require precise, domain-specific knowledge unavailable from\nthe web. The emergence of the Model Context Protocol (MCP) now allows agents to\ninterface with thousands of specialized tools, seemingly resolving this\nlimitation. Yet it remains unclear whether agents can effectively leverage such\ntools -- and more importantly, whether they can integrate them with\ngeneral-purpose search to solve complex tasks. Therefore, we introduce\nInfoMosaic-Bench, the first benchmark dedicated to multi-source information\nseeking in tool-augmented agents. Covering six representative domains\n(medicine, finance, maps, video, web, and multi-domain integration),\nInfoMosaic-Bench requires agents to combine general-purpose search with\ndomain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable\npipeline that grounds task conditions in verified tool outputs, enforces\ncross-source dependencies, and filters out shortcut cases solvable by trivial\nlookup. This design guarantees both reliability and non-triviality. Experiments\nwith 14 state-of-the-art LLM agents reveal three findings: (i) web information\nalone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass\nrate; (ii) domain tools provide selective but inconsistent benefits, improving\nsome domains while degrading others; and (iii) 22.4% of failures arise from\nincorrect tool usage or selection, highlighting that current LLMs still\nstruggle with even basic tool handling.",
      "url": "http://arxiv.org/abs/2510.02271v1",
      "published_time_eastern_timestamp": 1759427283.0
    },
    {
      "title": "The Unreasonable Effectiveness of Scaling Agents for Computer Use",
      "summary": "Computer-use agents (CUAs) hold promise for automating everyday digital\ntasks, but their unreliability and high variance hinder their application to\nlong-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method\nthat scales over agents by generating multiple rollouts and selecting among\nthem using behavior narratives that describe the agents' rollouts. It enables\nboth wide exploration and principled trajectory selection, substantially\nimproving robustness and success rates. On OSWorld, our bBoN scaling method\nestablishes a new state of the art (SoTA) at 69.9%, significantly outperforming\nprior methods and approaching human-level performance at 72%, with\ncomprehensive ablations validating key design choices. We further demonstrate\nstrong generalization results to different operating systems on\nWindowsAgentArena and AndroidWorld. Crucially, our results highlight the\nunreasonable effectiveness of scaling CUAs, when you do it right: effective\nscaling requires structured trajectory understanding and selection, and bBoN\nprovides a practical framework to achieve this.",
      "url": "http://arxiv.org/abs/2510.02250v1",
      "published_time_eastern_timestamp": 1759426628.0
    },
    {
      "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world\n  Markets?",
      "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nas autonomous agents, showing promise in reasoning, tool use, and sequential\ndecision-making. While prior benchmarks have evaluated LLM agents in domains\nsuch as software engineering and scientific discovery, the finance domain\nremains underexplored, despite its direct relevance to economic value and\nhigh-stakes decision-making. Existing financial benchmarks primarily test\nstatic knowledge through question answering, but they fall short of capturing\nthe dynamic and iterative nature of trading. To address this gap, we introduce\nStockBench, a contamination-free benchmark designed to evaluate LLM agents in\nrealistic, multi-month stock trading environments. Agents receive daily market\nsignals -- including prices, fundamentals, and news -- and must make sequential\nbuy, sell, or hold decisions. Performance is assessed using financial metrics\nsuch as cumulative return, maximum drawdown, and the Sortino ratio. Our\nevaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and\nopen-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM\nagents struggle to outperform the simple buy-and-hold baseline, several models\ndemonstrate the potential to deliver higher returns and manage risk more\neffectively. These findings highlight both the challenges and opportunities in\ndeveloping LLM-powered financial agents, showing that excelling at static\nfinancial knowledge tasks does not necessarily translate into successful\ntrading strategies. We release StockBench as an open-source resource to support\nreproducibility and advance future research in this domain.",
      "url": "http://arxiv.org/abs/2510.02209v1",
      "published_time_eastern_timestamp": 1759424097.0
    },
    {
      "title": "Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in\n  VLM-Powered Mobile-Use Agents",
      "summary": "Mobile-use agents powered by vision-language models (VLMs) have shown great\npotential in interpreting natural language instructions and generating\ncorresponding actions based on mobile graphical user interface. Recent studies\nsuggest that incorporating chain-of-thought (CoT) reasoning tends to improve\nthe execution accuracy. However, existing evaluations emphasize execution\naccuracy while neglecting whether CoT reasoning aligns with ground-truth\nactions. This oversight fails to assess potential reasoning-execution gaps,\nwhich in turn foster over-trust: users relying on seemingly plausible CoTs may\nunknowingly authorize harmful actions, potentially resulting in financial loss\nor trust crisis. In this work, we introduce a new evaluation framework to\ndiagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment\n(GTA), which measures whether the action implied by a CoT matches the\nground-truth action. By combining GTA with the standard Exact Match (EM)\nmetric, we jointly assess both the reasoning accuracy and execution accuracy.\nThis joint perspective reveals two types of reasoning-execution gaps: (i)\nExecution Gap (EG), where the reasoning correctly identifies the correct action\nbut execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but\nreasoning process conflicts with the actual execution. Experimental results\nacross a wide range of mobile interaction tasks reveal that reasoning-execution\ngaps are prevalent, with execution gaps occurring more frequently than\nreasoning gaps. Moreover, while scaling up model size reduces the overall gap,\nsizable execution gaps persist even in the largest models. Further analysis\nshows that our framework reliably reflects systematic EG/RG patterns in\nstate-of-the-art models. These findings offer concrete diagnostics and support\nthe development of more trustworthy mobile-use agents.",
      "url": "http://arxiv.org/abs/2510.02204v1",
      "published_time_eastern_timestamp": 1759423879.0
    },
    {
      "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge\n  Graph Exploration Utilities",
      "summary": "Interacting with knowledge graphs can be a daunting task for people without a\nbackground in computer science since the query language that is used (SPARQL)\nhas a high barrier of entry. Large language models (LLMs) can lower that\nbarrier by providing support in the form of Text2SPARQL translation. In this\npaper we introduce a generalized method based on SPINACH, an LLM backed agent\nthat translates natural language questions to SPARQL queries not in a single\nshot, but as an iterative process of exploration and execution. We describe the\noverall architecture and reasoning behind our design decisions, and also\nconduct a thorough analysis of the agent behavior to gain insights into future\nareas for targeted improvements. This work was motivated by the Text2SPARQL\nchallenge, a challenge that was held to facilitate improvements in the\nText2SPARQL domain.",
      "url": "http://arxiv.org/abs/2510.02200v1",
      "published_time_eastern_timestamp": 1759423767.0
    },
    {
      "title": "A Rigorous Benchmark with Multidimensional Evaluation for Deep Research\n  Agents: From Answers to Reports",
      "summary": "Artificial intelligence is undergoing the paradigm shift from closed language\nmodels to interconnected agent systems capable of external perception and\ninformation integration. As a representative embodiment, Deep Research Agents\n(DRAs) systematically exhibit the capabilities for task decomposition,\ncross-source retrieval, multi-stage reasoning, and structured output, which\nmarkedly enhance performance on complex and open-ended tasks. However, existing\nbenchmarks remain deficient in evaluation dimensions, response formatting, and\nscoring mechanisms, limiting their capacity to assess such systems effectively.\nThis paper introduces a rigorous benchmark and a multidimensional evaluation\nframework tailored to DRAs and report-style responses. The benchmark comprises\n214 expert-curated challenging queries distributed across 10 broad thematic\ndomains, each accompanied by manually constructed reference bundles to support\ncomposite evaluation. The framework enables comprehensive evaluation of\nlong-form reports generated by DRAs, incorporating integrated scoring metrics\nfor semantic quality, topical focus, and retrieval trustworthiness. Extensive\nexperimentation confirms the superior performance of mainstream DRAs over\nweb-search-tool-augmented reasoning models, yet reveals considerable scope for\nfurther improvement. This study provides a robust foundation for capability\nassessment, architectural refinement, and paradigm advancement in DRA systems.",
      "url": "http://arxiv.org/abs/2510.02190v1",
      "published_time_eastern_timestamp": 1759423202.0
    },
    {
      "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen\n  Using Agentic AI",
      "summary": "Fuzz testing has become a cornerstone technique for identifying software bugs\nand security vulnerabilities, with broad adoption in both industry and\nopen-source communities. Directly fuzzing a function requires fuzz drivers,\nwhich translate random fuzzer inputs into valid arguments for the target\nfunction. Given the cost and expertise required to manually develop fuzz\ndrivers, methods exist that leverage program analysis and Large Language Models\nto automatically generate these drivers. However, the generated fuzz drivers\nfrequently lead to false positive crashes, especially in functions highly\nstructured input and complex state requirements. This problem is especially\ncrucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as\nreporting false positive crashes to maintainers impede trust in both the system\nand the team.\n  This paper presents two AI-driven strategies to reduce false positives in\nOSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First,\nconstraint-based fuzz driver generation proactively enforces constraints on a\nfunction's inputs and state to guide driver creation. Second, context-based\ncrash validation reactively analyzes function callers to determine whether\nreported crashes are feasible from program entry points. Using 1,500 benchmark\nfunctions from OSS-Fuzz, we show that these strategies reduce spurious crashes\nby up to 8%, cut reported crashes by more than half, and demonstrate that\nfrontier LLMs can serve as reliable program analysis agents. Our results\nhighlight the promise and challenges of integrating AI into large-scale fuzzing\npipelines.",
      "url": "http://arxiv.org/abs/2510.02185v1",
      "published_time_eastern_timestamp": 1759423016.0
    },
    {
      "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical\n  Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis",
      "summary": "3D indoor layout synthesis is crucial for creating virtual environments.\nTraditional methods struggle with generalization due to fixed datasets. While\nrecent LLM and VLM-based approaches offer improved semantic richness, they\noften lack robust and flexible refinement, resulting in suboptimal layouts. We\ndevelop DisCo-Layout, a novel framework that disentangles and coordinates\nphysical and semantic refinement. For independent refinement, our Semantic\nRefinement Tool (SRT) corrects abstract object relationships, while the\nPhysical Refinement Tool (PRT) resolves concrete spatial issues via a\ngrid-matching algorithm. For collaborative refinement, a multi-agent framework\nintelligently orchestrates these tools, featuring a planner for placement\nrules, a designer for initial layouts, and an evaluator for assessment.\nExperiments demonstrate DisCo-Layout's state-of-the-art performance, generating\nrealistic, coherent, and generalizable 3D indoor layouts. Our code will be\npublicly available.",
      "url": "http://arxiv.org/abs/2510.02178v1",
      "published_time_eastern_timestamp": 1759422637.0
    },
    {
      "title": "SIEVE: Towards Verifiable Certification for Code-datasets",
      "summary": "Code agents and empirical software engineering rely on public code datasets,\nyet these datasets lack verifiable quality guarantees. Static 'dataset cards'\ninform, but they are neither auditable nor do they offer statistical\nguarantees, making it difficult to attest to dataset quality. Teams build\nisolated, ad-hoc cleaning pipelines. This fragments effort and raises cost. We\npresent SIEVE, a community-driven framework. It turns per-property checks into\nConfidence Cards-machine-readable, verifiable certificates with anytime-valid\nstatistical bounds. We outline a research plan to bring SIEVE to maturity,\nreplacing narrative cards with anytime-verifiable certification. This shift is\nexpected to lower quality-assurance costs and increase trust in code-datasets.",
      "url": "http://arxiv.org/abs/2510.02166v1",
      "published_time_eastern_timestamp": 1759421663.0
    },
    {
      "title": "Agentic Reasoning and Refinement through Semantic Interaction",
      "summary": "Sensemaking report writing often requires multiple refinements in the\niterative process. While Large Language Models (LLMs) have shown promise in\ngenerating initial reports based on human visual workspace representations,\nthey struggle to precisely incorporate sequential semantic interactions during\nthe refinement process. We introduce VIS-ReAct, a framework that reasons about\nnewly-added semantic interactions in visual workspaces to steer the LLM for\nreport refinement.\n  VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets\nnew semantic interactions to infer user intentions and generate refinement\nplanning, followed by an LLM refinement agent that updates reports accordingly.\nThrough case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM\nanalysis) on targeted refinement, semantic fidelity, and transparent inference.\nResults demonstrate that VIS-ReAct better handles various interaction types and\ngranularities while enhancing the transparency of human-LLM collaboration.",
      "url": "http://arxiv.org/abs/2510.02157v1",
      "published_time_eastern_timestamp": 1759421331.0
    },
    {
      "title": "A Computational Approach to Sustainable Policies Evaluation of the\n  Italian Wheat Production System",
      "summary": "This work outlines the modeling steps for developing a tool aimed at\nsupporting policymakers in guiding policies toward more sustainable wheat\nproduction. In the agricultural sector,policies affect a highly diverse set of\nfarms, which differ across several dimensions such as size,land composition,\nlocal climate, and irrigation availability. To address this significant\nheterogeneity, we construct an Agent-Based Model (ABM). The model is\ninitialized using a representative survey of Italian farms, which captures\ntheir heterogeneity. The ABM is then scaled to include a number of farms\ncomparable to those operating nationwide. To capture broader dynamics, the ABM\nis integrated with two additional components:a global model of international\nwheat markets and a tool for assessing the environmental impacts of wheat\nproduction. This integrated framework enables us to account for the feedback\nloop between global prices and local production while evaluating the\nenvironmental implications of policy measures.",
      "url": "http://arxiv.org/abs/2510.02154v1",
      "published_time_eastern_timestamp": 1759421107.0
    },
    {
      "title": "Reinforcement Learning with Action-Triggered Observations",
      "summary": "We study reinforcement learning problems where state observations are\nstochastically triggered by actions, a constraint common in many real-world\napplications. This framework is formulated as Action-Triggered Sporadically\nTraceable Markov Decision Processes (ATST-MDPs), where each action has a\nspecified probability of triggering a state observation. We derive tailored\nBellman optimality equations for this framework and introduce the\naction-sequence learning paradigm in which agents commit to executing a\nsequence of actions until the next observation arrives. Under the linear MDP\nassumption, value-functions are shown to admit linear representations in an\ninduced action-sequence feature map. Leveraging this structure, we propose\noff-policy estimators with statistical error guarantees for such feature maps\nand introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered\nsettings. ST-LSVI-UCB achieves regret $\\widetilde\nO(\\sqrt{Kd^3(1-\\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the\nfeature dimension, and $\\gamma$ the discount factor (per-step episode\nnon-termination probability). Crucially, this work establishes the theoretical\nfoundation for learning with sporadic, action-triggered observations while\ndemonstrating that efficient learning remains feasible under such observation\nconstraints.",
      "url": "http://arxiv.org/abs/2510.02149v1",
      "published_time_eastern_timestamp": 1759420850.0
    },
    {
      "title": "BioinfoMCP: A Unified Platform Enabling MCP Interfaces in Agentic\n  Bioinformatics",
      "summary": "Bioinformatics tools are essential for complex computational biology tasks,\nyet their integration with emerging AI-agent frameworks is hindered by\nincompatible interfaces, heterogeneous input-output formats, and inconsistent\nparameter conventions. The Model Context Protocol (MCP) provides a standardized\nframework for tool-AI communication, but manually converting hundreds of\nexisting and rapidly growing specialized bioinformatics tools into\nMCP-compliant servers is labor-intensive and unsustainable. Here, we present\nBioinfoMCP, a unified platform comprising two components: BioinfoMCP Converter,\nwhich automatically generates robust MCP servers from tool documentation using\nlarge language models, and BioinfoMCP Benchmark, which systematically validates\nthe reliability and versatility of converted tools across diverse computational\ntasks. We present a platform of 38 MCP-converted bioinformatics tools,\nextensively validated to show that 94.7% successfully executed complex\nworkflows across three widely used AI-agent platforms. By removing technical\nbarriers to AI automation, BioinfoMCP enables natural-language interaction with\nsophisticated bioinformatics analyses without requiring extensive programming\nexpertise, offering a scalable path to intelligent, interoperable computational\nbiology.",
      "url": "http://arxiv.org/abs/2510.02139v1",
      "published_time_eastern_timestamp": 1759420079.0
    },
    {
      "title": "Cooperative Guidance for Aerial Defense in Multiagent Systems",
      "summary": "This paper addresses a critical aerial defense challenge in contested\nairspace, involving three autonomous aerial vehicles -- a hostile drone (the\npursuer), a high-value drone (the evader), and a protective drone (the\ndefender). We present a cooperative guidance framework for the evader-defender\nteam that guarantees interception of the pursuer before it can capture the\nevader, even under highly dynamic and uncertain engagement conditions. Unlike\ntraditional heuristic, optimal control, or differential game-based methods, we\napproach the problem within a time-constrained guidance framework, leveraging\ntrue proportional navigation based approach that ensures robust and guaranteed\nsolutions to the aerial defense problem. The proposed strategy is\ncomputationally lightweight, scalable to a large number of agent\nconfigurations, and does not require knowledge of the pursuer's strategy or\ncontrol laws. From arbitrary initial geometries, our method guarantees that key\nengagement errors are driven to zero within a fixed time, leading to a\nsuccessful mission. Extensive simulations across diverse and adversarial\nscenarios confirm the effectiveness of the proposed strategy and its relevance\nfor real-time autonomous defense in contested airspace environments.",
      "url": "http://arxiv.org/abs/2510.02087v1",
      "published_time_eastern_timestamp": 1759416848.0
    },
    {
      "title": "Multi-group Bayesian Games",
      "summary": "This paper presents a model of multi-group Bayesian games (MBGs) to describe\nthe group behavior in Bayesian games, and gives methods to find (strongly)\nmulti-group Bayesian Nash equilibria (MBNE) of this model with a proposed\ntransformation. MBNE represent the optimal strategy \\textit{profiles} under the\nsituation where players within a group play a cooperative game, while strongly\nMBNE characterize the optimal strategy \\textit{profiles} under the situation\nwhere players within a group play a noncooperative game. Firstly, we propose a\nmodel of MBGs and give a transformation to convert any MBG into a multi-group\nex-ante agent game (MEAG) which is a normal-form game. Secondly, we give a\nsufficient and necessary condition for a MBG's MEAG to be (strongly) potential.\nIf it is (strongly) potential, all its (strongly) Nash equilibria can be found,\nand then all (strongly) MBNE of the MBG can be obtained by leveraging the\ntransformation's good properties. Finally, we provide algorithms for finding\n(strongly) MBNE of a MBG whose MEAG is (strongly) potential and use an\nillustrative example to verify the correctness of our results.",
      "url": "http://arxiv.org/abs/2510.02078v1",
      "published_time_eastern_timestamp": 1759416474.0
    },
    {
      "title": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming\n  Tool Usage",
      "summary": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful\nalternative to traditional ASR-LLM-TTS pipelines, generating more natural,\nexpressive responses with significantly lower latency. However, these systems\nremain prone to hallucinations due to limited factual grounding. While\ntext-based dialogue systems address this challenge by integrating tools such as\nweb search and knowledge graph APIs, we introduce the first approach to extend\ntool use directly into speech-in speech-out systems. A key challenge is that\ntool integration substantially increases response latency, disrupting\nconversational flow. To mitigate this, we propose Streaming Retrieval-Augmented\nGeneration (Streaming RAG), a novel framework that reduces user-perceived\nlatency by predicting tool queries in parallel with user speech, even before\nthe user finishes speaking. Specifically, we develop a post-training pipeline\nthat teaches the model when to issue tool calls during ongoing speech and how\nto generate spoken summaries that fuse audio queries with retrieved text\nresults, thereby improving both accuracy and responsiveness. To evaluate our\napproach, we construct AudioCRAG, a benchmark created by converting queries\nfrom the publicly available CRAG dataset into speech form. Experimental results\ndemonstrate that our streaming RAG approach increases QA accuracy by up to 200%\nrelative (from 11.1% to 34.2% absolute) and further enhances user experience by\nreducing tool use latency by 20%. Importantly, our streaming RAG approach is\nmodality-agnostic and can be applied equally to typed input, paving the way for\nmore agentic, real-time AI assistants.",
      "url": "http://arxiv.org/abs/2510.02044v1",
      "published_time_eastern_timestamp": 1759414700.0
    },
    {
      "title": "Coordinated Car-following Using Distributed MPC",
      "summary": "Within the modeling framework of Markov games, we propose a series of\nalgorithms for coordinated car-following using distributed model predictive\ncontrol (DMPC). Instead of tracking prescribed feasible trajectories, driving\npolicies are solved directly as outcomes of the DMPC optimization given the\ndriver's perceivable states. The coordinated solutions are derived using the\nbest response dynamics via iterated self-play, and are facilitated by direct\nnegotiation using inter-agent or agent-infrastructure communication. These\nsolutions closely approximate either Nash equilibrium or centralized\noptimization. By re-parameterizing the action sequence in DMPC as a curve along\nthe planning horizon, we are able to systematically reduce the original DMPC to\nvery efficient grid searches such that the optimal solution to the original\nDMPC can be well executed in real-time. Within our modeling framework, it is\nnatural to cast traffic control problems as mechanism design problems, in which\nall agents are endogenized on an equal footing with full incentive\ncompatibility. We show how traffic efficiency can be dramatically improved\nwhile keeping stop-and-go phantom waves tamed at high vehicle densities. Our\napproach can be viewed as an alternative way to formulate coordinated adaptive\ncruise control (CACC) without an explicit platooning (or with all vehicles in\nthe traffic system treated as a single extended platoon). We also address the\nissue of linear stability of the associated discrete-time traffic dynamics and\ndemonstrate why it does not always tell the full story about the traffic\nstability.",
      "url": "http://arxiv.org/abs/2510.02010v1",
      "published_time_eastern_timestamp": 1759411844.0
    },
    {
      "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
      "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.",
      "url": "http://arxiv.org/abs/2510.01869v1",
      "published_time_eastern_timestamp": 1759400495.0
    },
    {
      "title": "Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model\n  Selection and Benchmarking for Tabular datasets",
      "summary": "The field of AutoML has made remarkable progress in post-hoc model selection,\nwith libraries capable of automatically identifying the most performing models\nfor a given dataset. Nevertheless, these methods often rely on exhaustive\nhyperparameter searches, where methods automatically train and test different\ntypes of models on the target dataset. Contrastingly, pre-hoc prediction\nemerges as a promising alternative, capable of bypassing exhaustive search\nthrough intelligent pre-selection of models. Despite its potential, pre-hoc\nprediction remains under-explored in the literature. This paper explores the\nintersection of AutoML and pre-hoc model selection by leveraging traditional\nmodels and Large Language Model (LLM) agents to reduce the search space of\nAutoML libraries. By relying on dataset descriptions and statistical\ninformation, we reduce the AutoML search space. Our methodology is applied to\nthe AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark\ncontaining 175 tabular classification datasets available on OpenML. The\nproposed approach offers a shift in AutoML workflows, significantly reducing\ncomputational overhead, while still selecting the best model for the given\ndataset.",
      "url": "http://arxiv.org/abs/2510.01842v1",
      "published_time_eastern_timestamp": 1759397832.0
    }
  ]
}
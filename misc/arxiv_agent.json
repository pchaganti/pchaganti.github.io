{
  "last_updated": "2025-08-16T02:16:41.545131-04:00",
  "papers": [
    {
      "title": "Searching for Privacy Risks in LLM Agents via Simulation",
      "summary": "The widespread deployment of LLM-based agents is likely to introduce a\ncritical privacy threat: malicious agents that proactively engage others in\nmulti-turn interactions to extract sensitive information. These dynamic\ndialogues enable adaptive attack strategies that can cause severe privacy\nviolations, yet their evolving nature makes it difficult to anticipate and\ndiscover sophisticated vulnerabilities manually. To tackle this problem, we\npresent a search-based framework that alternates between improving attacker and\ndefender instructions by simulating privacy-critical agent interactions. Each\nsimulation involves three roles: data subject, data sender, and data recipient.\nWhile the data subject's behavior is fixed, the attacker (data recipient)\nattempts to extract sensitive information from the defender (data sender)\nthrough persistent and interactive exchanges. To explore this interaction space\nefficiently, our search algorithm employs LLMs as optimizers, using parallel\nsearch with multiple threads and cross-thread propagation to analyze simulation\ntrajectories and iteratively propose new instructions. Through this process, we\nfind that attack strategies escalate from simple direct requests to\nsophisticated multi-turn tactics such as impersonation and consent forgery,\nwhile defenses advance from rule-based constraints to identity-verification\nstate machines. The discovered attacks and defenses transfer across diverse\nscenarios and backbone models, demonstrating strong practical utility for\nbuilding privacy-aware agents.",
      "url": "http://arxiv.org/abs/2508.10880v1",
      "published_time_eastern_timestamp": 1755193749.0
    },
    {
      "title": "SSRL: Self-Search Reinforcement Learning",
      "summary": "We investigate the potential of large language models (LLMs) to serve as\nefficient simulators for agentic search tasks in reinforcement learning (RL),\nthereby reducing dependence on costly interactions with external search\nengines. To this end, we first quantify the intrinsic search capability of LLMs\nvia structured prompting and repeated sampling, which we term Self-Search. Our\nresults reveal that LLMs exhibit strong scaling behavior with respect to the\ninference budget, achieving high pass@k on question-answering benchmarks,\nincluding the challenging BrowseComp task. Building on these observations, we\nintroduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability\nthrough format-based and rule-based rewards. SSRL enables models to iteratively\nrefine their knowledge utilization internally, without requiring access to\nexternal tools. Empirical evaluations demonstrate that SSRL-trained policy\nmodels provide a cost-effective and stable environment for search-driven RL\ntraining, reducing reliance on external search engines and facilitating robust\nsim-to-real transfer. We draw the following conclusions: 1) LLMs possess world\nknowledge that can be effectively elicited to achieve high performance; 2) SSRL\ndemonstrates the potential of leveraging internal knowledge to reduce\nhallucination; 3) SSRL-trained models integrate seamlessly with external search\nengines without additional effort. Our findings highlight the potential of LLMs\nto support more scalable RL agent training.",
      "url": "http://arxiv.org/abs/2508.10874v1",
      "published_time_eastern_timestamp": 1755193561.0
    },
    {
      "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning",
      "summary": "The increasing congestion of Low Earth Orbit (LEO) poses persistent\nchallenges to the efficient deployment and safe operation of Earth observation\nsatellites. Mission planners must now account not only for mission-specific\nrequirements but also for the increasing collision risk with active satellites\nand space debris. This work presents a reinforcement learning framework using\nthe Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital\nparameters for precise terrestrial coverage within predefined surface radii. By\nformulating the problem as a Markov Decision Process (MDP) within a custom\nOpenAI Gymnasium environment, our method simulates orbital dynamics using\nclassical Keplerian elements. The agent progressively learns to adjust five of\nthe orbital parameters - semi-major axis, eccentricity, inclination, right\nascension of ascending node, and the argument of perigee-to achieve targeted\nterrestrial coverage. Comparative evaluation against Proximal Policy\nOptimization (PPO) demonstrates A2C's superior performance, achieving 5.8x\nhigher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer\ntimesteps (2,000 vs 63,000). The A2C agent consistently meets mission\nobjectives across diverse target coordinates while maintaining computational\nefficiency suitable for real-time mission planning applications. Key\ncontributions include: (1) a TLE-based orbital simulation environment\nincorporating physics constraints, (2) validation of actor-critic methods'\nsuperiority over trust region approaches in continuous orbital control, and (3)\ndemonstration of rapid convergence enabling adaptive satellite deployment. This\napproach establishes reinforcement learning as a computationally efficient\nalternative for scalable and intelligent LEO mission planning.",
      "url": "http://arxiv.org/abs/2508.10872v1",
      "published_time_eastern_timestamp": 1755193491.0
    },
    {
      "title": "Reinforced Language Models for Sequential Decision Making",
      "summary": "Large Language Models (LLMs) show potential as sequential decision-making\nagents, but their application is often limited due to a reliance on large,\ncomputationally expensive models. This creates a need to improve smaller\nmodels, yet existing post-training methods are designed for single-turn\ninteractions and cannot handle credit assignment in multi-step agentic tasks.\nTo address this, we introduce Multi-Step Group-Relative Policy Optimization\n(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal\nText-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)\nframeworks. For credit assignment, MS-GRPO attributes the entire cumulative\nepisode reward to each individual episode step. We supplement this algorithm\nwith a novel absolute-advantage-weighted episode sampling strategy that we show\nimproves training performance. We evaluate our approach by post-training a\n3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate\nthat the method is effective in improving decision-making performance: our\npost-trained 3B parameter model outperforms a 72B parameter baseline by 50% on\nthe Frozen Lake task. This work demonstrates that targeted post-training is a\npractical and efficient alternative to relying on model scale for creating\nsequential decision-making agents using LLMs.",
      "url": "http://arxiv.org/abs/2508.10839v1",
      "published_time_eastern_timestamp": 1755191144.0
    },
    {
      "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
      "summary": "We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models.To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies.To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment\n\\& Sparse Action Enhancement that refine historical reasoning traces and\nbalances the distribution of sparse but critical actions, leading to more\ncoherent planning and better generalization in complex UI tasks. Our\ncontributions include the publish of SOTA open-source UI agents, comprehensive\ndata cleaning protocols and a novel self-evolving framework for improving\nnavigation performance, which encourage further research and development in the\ncommunity. Code is available at https://github.com/antgroup/UI-Venus.",
      "url": "http://arxiv.org/abs/2508.10833v1",
      "published_time_eastern_timestamp": 1755190687.0
    },
    {
      "title": "Spirals and Beyond: Competitive Plane Search with Multi-Speed Agents",
      "summary": "We consider the problem of minimizing the worst-case search time for a hidden\npoint target in the plane using multiple mobile agents of differing speeds, all\nstarting from a common origin. The search time is normalized by the target's\ndistance to the origin, following the standard convention in competitive\nanalysis. The goal is to minimize the maximum such normalized time over all\ntarget locations, the search cost. As a base case, we extend the known result\nfor a single unit-speed agent, which achieves an optimal cost of about\n$\\mathcal{U}_1 = 17.28935$ via a logarithmic spiral, to $n$ unit-speed agents.\nWe give a symmetric spiral-based algorithm where each agent follows a\nlogarithmic spiral offset by equal angular phases. This yields a search cost\nindependent of which agent finds the target. We provide a closed-form upper\nbound $\\mathcal{U}_n$ for this setting, which we use in our general result. Our\nmain contribution is an upper bound on the worst-case normalized search time\nfor $n$ agents with arbitrary speeds. We give a framework that selects a subset\nof agents and assigns spiral-type trajectories with speed-dependent angular\noffsets, again making the search cost independent of which agent reaches the\ntarget. A corollary shows that $n$ multi-speed agents (fastest speed 1) can\nbeat $k$ unit-speed agents (cost below $\\mathcal{U}_k$) if the geometric mean\nof their speeds exceeds $\\mathcal{U}_n / \\mathcal{U}_k$. This means slow agents\nmay be excluded if they lower the mean too much, motivating non-spiral\nalgorithms. We also give new upper bounds for point search in cones and conic\ncomplements using a single unit-speed agent. These are then used to design\nhybrid spiral-directional strategies, which outperform the spiral-based\nalgorithms when some agents are slow. This suggests that spiral-type\ntrajectories may not be optimal in the general multi-speed setting.",
      "url": "http://arxiv.org/abs/2508.10793v1",
      "published_time_eastern_timestamp": 1755188197.0
    },
    {
      "title": "Reinforcement-Learning-Designed Field-Free Sub-Nanosecond\n  Spin-Orbit-Torque Switching",
      "summary": "We demonstrate deterministic, field-free magnetization reversal of a\nsingle-domain nanomagnet within 300 ps under a current density of $3 \\times\n10^{10}~\\mathrm{A/m^2}$ by coupling reinforcement learning (RL) to the\nLandau-Lifshitz-Gilbert equation with the spin-orbit torques (SOTs). The RL\nagent autonomously discovers a current waveform that minimizes the\nmagnetization trajectory path and exploits a precessional shortcut enabled by\nthe field-like SOT and hard-axis anisotropy. From the learned pulse, we extract\na clear physical picture of the dynamics and develop a model-based analytical\nframework that establishes a lower bound on the switching time. The control\nstrategy remains robust across a wide range of damping constants and is\nstabilized against thermal fluctuations at higher current densities. We also\ndiscuss feasible experimental implementations for the precessional switching.",
      "url": "http://arxiv.org/abs/2508.10792v1",
      "published_time_eastern_timestamp": 1755188193.0
    },
    {
      "title": "Modeling Human Responses to Multimodal AI Content",
      "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.",
      "url": "http://arxiv.org/abs/2508.10769v1",
      "published_time_eastern_timestamp": 1755186919.0
    },
    {
      "title": "FROGENT: An End-to-End Full-process Drug Design Agent",
      "summary": "Powerful AI tools for drug discovery reside in isolated web apps, desktop\nprograms, and code libraries. Such fragmentation forces scientists to manage\nincompatible interfaces and specialized scripts, which can be a cumbersome and\nrepetitive process. To address this issue, a Full-pROcess druG dEsign ageNT,\nnamed FROGENT, has been proposed. Specifically, FROGENT utilizes a Large\nLanguage Model and the Model Context Protocol to integrate multiple dynamic\nbiochemical databases, extensible tool libraries, and task-specific AI models.\nThis agentic framework allows FROGENT to execute complicated drug discovery\nworkflows dynamically, including component tasks such as target identification,\nmolecule generation and retrosynthetic planning. FROGENT has been evaluated on\neight benchmarks that cover various aspects of drug discovery, such as\nknowledge retrieval, property prediction, virtual screening, mechanistic\nanalysis, molecular design, and synthesis. It was compared against six\nincreasingly advanced ReAct-style agents that support code execution and\nliterature searches. Empirical results demonstrated that FROGENT triples the\nbest baseline performance in hit-finding and doubles it in interaction\nprofiling, significantly outperforming both the open-source model Qwen3-32B and\nthe commercial model GPT-4o. In addition, real-world cases have been utilized\nto validate the practicability and generalization of FROGENT. This development\nsuggests that streamlining the agentic drug discovery pipeline can\nsignificantly enhance researcher productivity.",
      "url": "http://arxiv.org/abs/2508.10760v1",
      "published_time_eastern_timestamp": 1755186353.0
    },
    {
      "title": "Agentic Design Review System",
      "summary": "Evaluating graphic designs involves assessing it from multiple facets like\nalignment, composition, aesthetics and color choices. Evaluating designs in a\nholistic way involves aggregating feedback from individual expert reviewers.\nTowards this, we propose an Agentic Design Review System (AgenticDRS), where\nmultiple agents collaboratively analyze a design, orchestrated by a meta-agent.\nA novel in-context exemplar selection approach based on graph matching and a\nunique prompt expansion method plays central role towards making each agent\ndesign aware. Towards evaluating this framework, we propose DRS-BENCH\nbenchmark. Thorough experimental evaluation against state-of-the-art baselines\nadapted to the problem setup, backed-up with critical ablation experiments\nbrings out the efficacy of Agentic-DRS in evaluating graphic designs and\ngenerating actionable feedback. We hope that this work will attract attention\nto this pragmatic, yet under-explored research direction.",
      "url": "http://arxiv.org/abs/2508.10745v1",
      "published_time_eastern_timestamp": 1755185364.0
    },
    {
      "title": "Run-and-Tumble Escape in Pursuit-Evasion Dynamics of Intelligent Active\n  Particles",
      "summary": "The pursuit-evasion game is studied for two adversarial active agents,\nmodelled as a deterministic self-steering pursuer and a stochastic, cognitive\nevader. The pursuer chases the evader by reorienting its propulsion direction\nwith limited maneuverability, while the evader escapes by executing sharp,\nunpredictable turns, whose timing and direction the pursuer cannot anticipate.\nTo make the target responsive and agile when the threat level is high, the\ntumbling frequency is set to increase with decreasing distance from the\npursuer; furthermore, the range of preferred tumbling directions is varied.\nNumerical simulations of such a pursuit-target pair in two spatial dimensions\nreveal two important scenarios. For dominant pursuers, the evader is compelled\nto adopt a high-risk strategy that allows the pursuer to approach closely\nbefore the evader executes a potentially game-changing backward maneuver to\npull away from the pursuer. Otherwise, a strategy where the evader tumbles\nforward with continuous slight adjustments of the propulsion direction can\nsignificantly increase the capture time by preventing the pursuer from aligning\nwith the target propulsion direction, while maintaining the persistence of the\ntarget motion. Our results can guide the design of bioinspired robotic systems\nwith efficient evasion capabilities.",
      "url": "http://arxiv.org/abs/2508.10727v1",
      "published_time_eastern_timestamp": 1755184153.0
    },
    {
      "title": "REFN: A Reinforcement-Learning-From-Network Framework against\n  1-day/n-day Exploitations",
      "summary": "The exploitation of 1 day or n day vulnerabilities poses severe threats to\nnetworked devices due to massive deployment scales and delayed patching\n(average Mean Time To Patch exceeds 60 days). Existing defenses, including host\nbased patching and network based filtering, are inadequate due to limited\nscalability across diverse devices, compatibility issues especially with\nembedded or legacy systems, and error prone deployment process (manual patch\nvalidation). To address these issues, we introduce REFN (Reinforcement Learning\nFrom Network), a novel framework that trains Large Language Models (LLMs) to\nautonomously generate network filters to prevent 1 day or n day exploitations.\nREFN ensures scalability by uniquely employs Reinforcement Learning (RL) driven\nby online network rewards instead of traditional Human Feedback (RLHF). REFN\nguarantees compatibility via unified deployment on edge security gateways\n(Amazon Eero). REFN provides robustness via online validation using real\nnetwork traffic. Crucially, REFN addresses three core challenges in training\nLLMs for exploit prevention: 1) expanding current LLMs limited vulnerability\nfixing expertise via Agentic RAG based Knowledge Distillation, 2) bridging\ncurrent LLMs language to network gaps through an RL From VNF Pipeline that\ntranslates language context (vulnerability description) into network\nenforcement, 3) addressing the LLM hallucination and non determinism via the\nOnline Agentic Validation that penalizes erroneous outputs. Evaluated across 22\nfamilies of 1 day or n day exploits, REFN demonstrates effectiveness (21.1\npercent higher accuracy than alternatives), efficiency (Mean Time To Patch of\n3.65 hours) and scalability (easily scale to 10K devices). REFN serves as an\ninitial step toward training LLMs to rapidly prevent massive scale 1 day or n\nday exploitations.",
      "url": "http://arxiv.org/abs/2508.10701v1",
      "published_time_eastern_timestamp": 1755182745.0
    },
    {
      "title": "Variance Reduced Policy Gradient Method for Multi-Objective\n  Reinforcement Learning",
      "summary": "Multi-Objective Reinforcement Learning (MORL) is a generalization of\ntraditional Reinforcement Learning (RL) that aims to optimize multiple, often\nconflicting objectives simultaneously rather than focusing on a single reward.\nThis approach is crucial in complex decision-making scenarios where agents must\nbalance trade-offs between various goals, such as maximizing performance while\nminimizing costs. We consider the problem of MORL where the objectives are\ncombined using a non-linear scalarization function. Just like in standard RL,\npolicy gradient methods (PGMs) are amongst the most effective for handling\nlarge and continuous state-action spaces in MORL. However, existing PGMs for\nMORL suffer from high sample inefficiency, requiring large amounts of data to\nbe effective. Previous attempts to solve this problem rely on overly strict\nassumptions, losing PGMs' benefits in scalability to large state-action spaces.\nIn this work, we address the issue of sample efficiency by implementing\nvariance-reduction techniques to reduce the sample complexity of policy\ngradients while maintaining general assumptions.",
      "url": "http://arxiv.org/abs/2508.10608v1",
      "published_time_eastern_timestamp": 1755175977.0
    },
    {
      "title": "Technical Report: Facilitating the Adoption of Causal Inference Methods\n  Through LLM-Empowered Co-Pilot",
      "summary": "Estimating treatment effects (TE) from observational data is a critical yet\ncomplex task in many fields, from healthcare and economics to public policy.\nWhile recent advances in machine learning and causal inference have produced\npowerful estimation techniques, their adoption remains limited due to the need\nfor deep expertise in causal assumptions, adjustment strategies, and model\nselection. In this paper, we introduce CATE-B, an open-source co-pilot system\nthat uses large language models (LLMs) within an agentic framework to guide\nusers through the end-to-end process of treatment effect estimation. CATE-B\nassists in (i) constructing a structural causal model via causal discovery and\nLLM-based edge orientation, (ii) identifying robust adjustment sets through a\nnovel Minimal Uncertainty Adjustment Set criterion, and (iii) selecting\nappropriate regression methods tailored to the causal structure and dataset\ncharacteristics. To encourage reproducibility and evaluation, we release a\nsuite of benchmark tasks spanning diverse domains and causal complexities. By\ncombining causal inference with intelligent, interactive assistance, CATE-B\nlowers the barrier to rigorous causal analysis and lays the foundation for a\nnew class of benchmarks in automated treatment effect estimation.",
      "url": "http://arxiv.org/abs/2508.10581v1",
      "published_time_eastern_timestamp": 1755174051.0
    },
    {
      "title": "Towards Agentic AI for Multimodal-Guided Video Object Segmentation",
      "summary": "Referring-based Video Object Segmentation is a multimodal problem that\nrequires producing fine-grained segmentation results guided by external cues.\nTraditional approaches to this task typically involve training specialized\nmodels, which come with high computational complexity and manual annotation\neffort. Recent advances in vision-language foundation models open a promising\ndirection toward training-free approaches. Several studies have explored\nleveraging these general-purpose models for fine-grained segmentation,\nachieving performance comparable to that of fully supervised, task-specific\nmodels. However, existing methods rely on fixed pipelines that lack the\nflexibility needed to adapt to the dynamic nature of the task. To address this\nlimitation, we propose Multi-Modal Agent, a novel agentic system designed to\nsolve this task in a more flexible and adaptive manner. Specifically, our\nmethod leverages the reasoning capabilities of large language models (LLMs) to\ngenerate dynamic workflows tailored to each input. This adaptive procedure\niteratively interacts with a set of specialized tools designed for low-level\ntasks across different modalities to identify the target object described by\nthe multimodal cues. Our agentic approach demonstrates clear improvements over\nprior methods on two multimodal-conditioned VOS tasks: RVOS and Ref-AVS.",
      "url": "http://arxiv.org/abs/2508.10572v1",
      "published_time_eastern_timestamp": 1755173475.0
    },
    {
      "title": "SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous\n  Driving",
      "summary": "End-to-end autonomous driving systems promise stronger performance through\nunified optimization of perception, motion forecasting, and planning. However,\nvision-based approaches face fundamental limitations in adverse weather\nconditions, partial occlusions, and precise velocity estimation - critical\nchallenges in safety-sensitive scenarios where accurate motion understanding\nand long-horizon trajectory prediction are essential for collision avoidance.\nTo address these limitations, we propose SpaRC-AD, a query-based end-to-end\ncamera-radar fusion framework for planning-oriented autonomous driving. Through\nsparse 3D feature alignment, and doppler-based velocity estimation, we achieve\nstrong 3D scene representations for refinement of agent anchors, map polylines\nand motion modelling. Our method achieves strong improvements over the\nstate-of-the-art vision-only baselines across multiple autonomous driving\ntasks, including 3D detection (+4.8% mAP), multi-object tracking (+8.3% AMOTA),\nonline mapping (+1.8% mAP), motion prediction (-4.0% mADE), and trajectory\nplanning (-0.1m L2 and -9% TPC). We achieve both spatial coherence and temporal\nconsistency on multiple challenging benchmarks, including real-world open-loop\nnuScenes, long-horizon T-nuScenes, and closed-loop simulator Bench2Drive. We\nshow the effectiveness of radar-based fusion in safety-critical scenarios where\naccurate motion understanding and long-horizon trajectory prediction are\nessential for collision avoidance. The source code of all experiments is\navailable at https://phi-wol.github.io/sparcad/",
      "url": "http://arxiv.org/abs/2508.10567v1",
      "published_time_eastern_timestamp": 1755172961.0
    },
    {
      "title": "A Dual Quaternion Control Law for Formation Control of Multiple 3-D\n  Rigid Bodies",
      "summary": "This paper studies the integrated position and attitude control problem for\nmulti-agent systems of 3D rigid bodies. While the state-of-the-art method in\n[Olfati-Saber and Murray, 2004] established the theoretical foundation for\nrigid-body formation control, it requires all agents to asymptotically converge\nto identical positions and attitudes, limiting its applicability in scenarios\nwhere distinct desired relative configurations must be maintained. In this\npaper, we develop a novel dual-quaternion-based framework that generalizes this\nparadigm. By introducing a unit dual quaternion directed graph (UDQDG)\nrepresentation, we derive a new control law through the corresponding Laplacian\nmatrix, enabling simultaneous position and attitude coordination while\nnaturally accommodating directed interaction topologies. Leveraging the recent\nadvances in UDQDG spectral theory, we prove global asymptotic convergence to\ndesired relative configurations modulo a right-multiplicative constant and\nestablish an R-linear convergence rate determined by the second smallest\neigenvalue of the UDQDG Laplacian. A projected iteration method is proposed to\ncompute the iterative states. Finally, the proposed solution is verified by\nseveral numerical experiments.",
      "url": "http://arxiv.org/abs/2508.10519v1",
      "published_time_eastern_timestamp": 1755168259.0
    },
    {
      "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and\n  Adaptive Chest X-Ray Reasoning",
      "summary": "Existing tool-augmented agentic systems are limited in the real world by (i)\nblack-box reasoning steps that undermine trust of decision-making and pose\nsafety risks, (ii) poor multimodal integration, which is inherently critical\nfor healthcare tasks, and (iii) rigid and computationally inefficient agentic\npipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the\nfirst multimodal framework to address these challenges in the context of Chest\nX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a\nmulti-tool graph, yielding decision paths annotated with interpretable\nprobabilities. Given the complex CXR reasoning task with multimodal medical\ndata, PASS leverages its learned task-conditioned distribution over the agentic\nsupernet. Thus, it adaptively selects the most suitable tool at each supernet\nlayer, offering probability-annotated trajectories for post-hoc audits and\ndirectly enhancing medical AI safety. PASS also continuously compresses salient\nfindings into an evolving personalized memory, while dynamically deciding\nwhether to deepen its reasoning path or invoke an early exit for efficiency. To\noptimize a Pareto frontier balancing performance and cost, we design a novel\nthree-stage training procedure, including expert knowledge warm-up, contrastive\npath-ranking, and cost-aware reinforcement learning. To facilitate rigorous\nevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,\nsafety-critical, free-form CXR reasoning. Experiments across various benchmarks\nvalidate that PASS significantly outperforms strong baselines in multiple\nmetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,\npushing a new paradigm shift towards interpretable, adaptive, and multimodal\nmedical agentic systems.",
      "url": "http://arxiv.org/abs/2508.10501v1",
      "published_time_eastern_timestamp": 1755165827.0
    },
    {
      "title": "A Unified Multi-Agent Framework for Universal Multimodal Understanding\n  and Generation",
      "summary": "Real-world multimodal applications often require any-to-any capabilities,\nenabling both understanding and generation across modalities including text,\nimage, audio, and video. However, integrating the strengths of autoregressive\nlanguage models (LLMs) for reasoning and diffusion models for high-fidelity\ngeneration remains challenging. Existing approaches rely on rigid pipelines or\ntightly coupled architectures, limiting flexibility and scalability. We propose\nMAGUS (Multi-Agent Guided Unified Multimodal System), a modular framework that\nunifies multimodal understanding and generation via two decoupled phases:\nCognition and Deliberation. MAGUS enables symbolic multi-agent collaboration\nwithin a shared textual workspace. In the Cognition phase, three\nrole-conditioned multimodal LLM agents - Perceiver, Planner, and Reflector -\nengage in collaborative dialogue to perform structured understanding and\nplanning. The Deliberation phase incorporates a Growth-Aware Search mechanism\nthat orchestrates LLM-based reasoning and diffusion-based generation in a\nmutually reinforcing manner. MAGUS supports plug-and-play extensibility,\nscalable any-to-any modality conversion, and semantic alignment - all without\nthe need for joint training. Experiments across multiple benchmarks, including\nimage, video, and audio generation, as well as cross-modal instruction\nfollowing, demonstrate that MAGUS outperforms strong baselines and\nstate-of-the-art systems. Notably, on the MME benchmark, MAGUS surpasses the\npowerful closed-source model GPT-4o.",
      "url": "http://arxiv.org/abs/2508.10494v1",
      "published_time_eastern_timestamp": 1755165171.0
    },
    {
      "title": "Online Homogeneity Can Emerge Without Filtering Algorithms or Homophily\n  Preferences",
      "summary": "Ideologically homogeneous online environments - often described as \"echo\nchambers\" or \"filter bubbles\" - are widely seen as drivers of polarization,\nradicalization, and misinformation. A central debate asks whether such\nhomophily stems primarily from algorithmic curation or users' preference for\nlike-minded peers. This study challenges that view by showing that homogeneity\ncan emerge in the absence of both filtering algorithms and user preferences.\nUsing an agent-based model inspired by Schelling's model of residential\nsegregation, we demonstrate that weak individual preferences, combined with\nsimple group-based interaction structures, can trigger feedback loops that\ndrive communities toward segregation. Once a small imbalance forms, cascades of\nuser exits and regrouping amplify homogeneity across the system.\nCounterintuitively, algorithmic filtering - often blamed for \"filter bubbles\" -\ncan in fact sustain diversity by stabilizing mixed communities. These findings\nhighlight online polarization as an emergent system-level dynamic and\nunderscore the importance of applying a complexity lens to the study of digital\npublic spheres.",
      "url": "http://arxiv.org/abs/2508.10466v1",
      "published_time_eastern_timestamp": 1755162526.0
    }
  ]
}
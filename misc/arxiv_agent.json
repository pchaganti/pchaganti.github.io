{
  "last_updated": "2025-06-30T14:16:51.305917-04:00",
  "papers": [
    {
      "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
      "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
      "url": "http://arxiv.org/abs/2506.22419v1",
      "published_time_eastern_timestamp": 1751046272.0
    },
    {
      "title": "Why Are Parsing Actions for Understanding Message Hierarchies Not\n  Random?",
      "summary": "If humans understood language by randomly selecting parsing actions, it might\nhave been necessary to construct a robust symbolic system capable of being\ninterpreted under any hierarchical structure. However, human parsing strategies\ndo not seem to follow such a random pattern. Why is that the case? In fact, a\nprevious study on emergent communication using models with hierarchical biases\nhave reported that agents adopting random parsing\nstrategies$\\unicode{x2013}$ones that deviate significantly from human language\ncomprehension$\\unicode{x2013}$can achieve high communication accuracy. In this\nstudy, we investigate this issue by making two simple and natural modifications\nto the experimental setup: (I) we use more complex inputs that have\nhierarchical structures, such that random parsing makes semantic interpretation\nmore difficult, and (II) we incorporate a surprisal-related term, which is\nknown to influence the order of words and characters in natural language, into\nthe objective function. With these changes, we evaluate whether agents\nemploying random parsing strategies still maintain high communication accuracy.",
      "url": "http://arxiv.org/abs/2506.22366v1",
      "published_time_eastern_timestamp": 1751041655.0
    },
    {
      "title": "Reinforcement Learning with Physics-Informed Symbolic Program Priors for\n  Zero-Shot Wireless Indoor Navigation",
      "summary": "When using reinforcement learning (RL) to tackle physical control tasks,\ninductive biases that encode physics priors can help improve sample efficiency\nduring training and enhance generalization in testing. However, the current\npractice of incorporating these helpful physics-informed inductive biases\ninevitably runs into significant manual labor and domain expertise, making them\nprohibitive for general users. This work explores a symbolic approach to\ndistill physics-informed inductive biases into RL agents, where the physics\npriors are expressed in a domain-specific language (DSL) that is human-readable\nand naturally explainable. Yet, the DSL priors do not translate directly into\nan implementable policy due to partial and noisy observations and additional\nphysical constraints in navigation tasks. To address this gap, we develop a\nphysics-informed program-guided RL (PiPRL) framework with applications to\nindoor navigation. PiPRL adopts a hierarchical and modularized neuro-symbolic\nintegration, where a meta symbolic program receives semantically meaningful\nfeatures from a neural perception module, which form the bases for symbolic\nprogramming that encodes physics priors and guides the RL process of a\nlow-level neural controller. Extensive experiments demonstrate that PiPRL\nconsistently outperforms purely symbolic or neural policies and reduces\ntraining time by over 26% with the help of the program-based inductive biases.",
      "url": "http://arxiv.org/abs/2506.22365v1",
      "published_time_eastern_timestamp": 1751041589.0
    },
    {
      "title": "Embodied AI Agents: Modeling the World",
      "summary": "This paper describes our research on AI agents embodied in visual, virtual or\nphysical forms, enabling them to interact with both users and their\nenvironments. These agents, which include virtual avatars, wearable devices,\nand robots, are designed to perceive, learn and act within their surroundings,\nwhich makes them more similar to how humans learn and interact with the\nenvironments as compared to disembodied agents. We propose that the development\nof world models is central to reasoning and planning of embodied AI agents,\nallowing these agents to understand and predict their environment, to\nunderstand user intentions and social contexts, thereby enhancing their ability\nto perform complex tasks autonomously. World modeling encompasses the\nintegration of multimodal perception, planning through reasoning for action and\ncontrol, and memory to create a comprehensive understanding of the physical\nworld. Beyond the physical world, we also propose to learn the mental world\nmodel of users to enable better human-agent collaboration.",
      "url": "http://arxiv.org/abs/2506.22355v1",
      "published_time_eastern_timestamp": 1751040334.0
    },
    {
      "title": "Agent-based modeling and the sociology of money: some suggestions for\n  refining monetary theory using social simulation",
      "summary": "The institution of money can be seen as a foundational social mechanism,\nenabling communities to quantify collectively regulate economic processes.\nMoney can be said, indeed, to constitute the micro-macro link in economics.\nThis paper reviews influential views on the nature of money in economics and\nsociology, contrasting them to the relatively limited findings of recent\nagent-based models of \"the emergence of money\". Noting ample room for novel\ncombinations of sociological and formal methods to drive insight into the many\nroles played by money in the economy, we conclude by indicating research\ndirections in which we believe this combination can provide new answers to old\nquestions in monetary theory",
      "url": "http://arxiv.org/abs/2506.22318v1",
      "published_time_eastern_timestamp": 1751038328.0
    },
    {
      "title": "Artificial Intelligent Disobedience: Rethinking the Agency of Our\n  Artificial Teammates",
      "summary": "Artificial intelligence has made remarkable strides in recent years,\nachieving superhuman performance across a wide range of tasks. Yet despite\nthese advances, most cooperative AI systems remain rigidly obedient, designed\nto follow human instructions without question and conform to user expectations,\neven when doing so may be counterproductive or unsafe. This paper argues for\nexpanding the agency of AI teammates to include \\textit{intelligent\ndisobedience}, empowering them to make meaningful and autonomous contributions\nwithin human-AI teams. It introduces a scale of AI agency levels and uses\nrepresentative examples to highlight the importance and growing necessity of\ntreating AI autonomy as an independent research focus in cooperative settings.\nThe paper then explores how intelligent disobedience manifests across different\nautonomy levels and concludes by proposing initial boundaries and\nconsiderations for studying disobedience as a core capability of artificial\nagents.",
      "url": "http://arxiv.org/abs/2506.22276v1",
      "published_time_eastern_timestamp": 1751035527.0
    },
    {
      "title": "Exploring Modularity of Agentic Systems for Drug Discovery",
      "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.",
      "url": "http://arxiv.org/abs/2506.22189v1",
      "published_time_eastern_timestamp": 1751029020.0
    },
    {
      "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration",
      "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.",
      "url": "http://arxiv.org/abs/2506.22185v1",
      "published_time_eastern_timestamp": 1751028372.0
    },
    {
      "title": "A Different Approach to AI Safety: Proceedings from the Columbia\n  Convening on Openness in Artificial Intelligence and AI Safety",
      "summary": "The rapid rise of open-weight and open-source foundation models is\nintensifying the obligation and reshaping the opportunity to make AI systems\nsafe. This paper reports outcomes from the Columbia Convening on AI Openness\nand Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme\ninvolving more than forty-five researchers, engineers, and policy leaders from\nacademia, industry, civil society, and government. Using a participatory,\nsolutions-oriented process, the working groups produced (i) a research agenda\nat the intersection of safety and open source AI; (ii) a mapping of existing\nand needed technical interventions and open source tools to safely and\nresponsibly deploy open foundation models across the AI development workflow;\nand (iii) a mapping of the content safety filter ecosystem with a proposed\nroadmap for future research and development. We find that openness --\nunderstood as transparent weights, interoperable tooling, and public governance\n-- can enhance safety by enabling independent scrutiny, decentralized\nmitigation, and culturally plural oversight. However, significant gaps persist:\nscarce multimodal and multilingual benchmarks, limited defenses against\nprompt-injection and compositional attacks in agentic systems, and insufficient\nparticipatory mechanisms for communities most affected by AI harms. The paper\nconcludes with a roadmap of five priority research directions, emphasizing\nparticipatory inputs, future-proof content filters, ecosystem-wide safety\ninfrastructure, rigorous agentic safeguards, and expanded harm taxonomies.\nThese recommendations informed the February 2025 French AI Action Summit and\nlay groundwork for an open, plural, and accountable AI safety discipline.",
      "url": "http://arxiv.org/abs/2506.22183v1",
      "published_time_eastern_timestamp": 1751028344.0
    },
    {
      "title": "ASVSim (AirSim for Surface Vehicles): A High-Fidelity Simulation\n  Framework for Autonomous Surface Vehicle Research",
      "summary": "The transport industry has recently shown significant interest in unmanned\nsurface vehicles (USVs), specifically for port and inland waterway transport.\nThese systems can improve operational efficiency and safety, which is\nespecially relevant in the European Union, where initiatives such as the Green\nDeal are driving a shift towards increased use of inland waterways. At the same\ntime, a shortage of qualified personnel is accelerating the adoption of\nautonomous solutions. However, there is a notable lack of open-source,\nhigh-fidelity simulation frameworks and datasets for developing and evaluating\nsuch solutions. To address these challenges, we introduce AirSim For Surface\nVehicles (ASVSim), an open-source simulation framework specifically designed\nfor autonomous shipping research in inland and port environments. The framework\ncombines simulated vessel dynamics with marine sensor simulation capabilities,\nincluding radar and camera systems and supports the generation of synthetic\ndatasets for training computer vision models and reinforcement learning agents.\nBuilt upon Cosys-AirSim, ASVSim provides a comprehensive platform for\ndeveloping autonomous navigation algorithms and generating synthetic datasets.\nThe simulator supports research of both traditional control methods and deep\nlearning-based approaches. Through limited experiments, we demonstrate the\npotential of the simulator in these research areas. ASVSim is provided as an\nopen-source project under the MIT license, making autonomous navigation\nresearch accessible to a larger part of the ocean engineering community.",
      "url": "http://arxiv.org/abs/2506.22174v1",
      "published_time_eastern_timestamp": 1751027956.0
    },
    {
      "title": "Learning Distributed Safe Multi-Agent Navigation via Infinite-Horizon\n  Optimal Graph Control",
      "summary": "Distributed multi-agent navigation faces inherent challenges due to the\ncompeting requirements of maintaining safety and achieving goal-directed\nbehavior, particularly for agents with limited sensing range operating in\nunknown environments with dense obstacles. Existing approaches typically\nproject predefined goal-reaching controllers onto control barrier function\n(CBF) constraints, often resulting in conservative and suboptimal trade-offs\nbetween safety and goal-reaching performance. We propose an infinite-horizon\nCBF-constrained optimal graph control formulation for distributed safe\nmulti-agent navigation. By deriving the analytical solution structure, we\ndevelop a novel Hamilton-Jacobi-Bellman (HJB)-based learning framework to\napproximate the solution. In particular, our algorithm jointly learns a CBF and\na distributed control policy, both parameterized by graph neural networks\n(GNNs), along with a value function that robustly guides agents toward their\ngoals. Moreover, we introduce a state-dependent parameterization of Lagrange\nmultipliers, enabling dynamic trade-offs between safety and performance. Unlike\ntraditional short-horizon, quadratic programming-based CBF methods, our\napproach leverages long-horizon optimization to proactively avoid deadlocks and\nnavigate complex environments more effectively. Extensive simulation results\ndemonstrate substantial improvements in safety and task success rates across\nvarious agent dynamics, with strong scalability and generalization to\nlarge-scale teams in previously unseen environments. Real-world experiments\nusing Crazyflie drone swarms on challenging antipodal position-swapping tasks\nfurther validate the practicality, generalizability, and robustness of the\nproposed HJB-GNN learning framework.",
      "url": "http://arxiv.org/abs/2506.22117v1",
      "published_time_eastern_timestamp": 1751021613.0
    },
    {
      "title": "Flocking with random non-reciprocal interactions",
      "summary": "Flocking is ubiquitous in nature and emerges due to short or long range\nalignment interactions among self-propelled agents. Two unfriendly species that\nanti-align or even interact non-reciprocally show more complex collective\nphenomena, ranging from parallel and anti-parallel flocking over runand chase\nbehavior to chiral phases. Whether flocking or any of these collective\nphenomena can survive in the presence of a large number of species with random\nnon-reciprocal interactions remained elusive so far. As a first step here the\nextreme case of a Vicsek-like model with fully random nonreciprocal\ninteractions between the individual particles is considered. As soon as the\nalignment bias is of the same order as the random interactions the ordered\nflocking phase occurs, but deep within this phase, the random non-reciprocal\ninteractions can still support global chiral and oscillating states in which\nthe collective movement direction rotates or oscillates slowly. For short-range\ninteractions, moreover, even without alignment bias self-organized cliques\nemerge, in which medium size clusters of particles that have predominantly\naligning interactions meet accidentally and stay together for macroscopic\ntimes. These results may serve as a starting point for the study of\nmulti-species flocking models with non-random but complex non-reciprocal\ninter-species interactions.",
      "url": "http://arxiv.org/abs/2506.22060v1",
      "published_time_eastern_timestamp": 1751018186.0
    },
    {
      "title": "Universal Retrieval for Multimodal Trajectory Modeling",
      "summary": "Trajectory data, capturing human actions and environmental states across\nvarious modalities, holds significant potential for enhancing AI agent\ncapabilities, particularly in GUI environments. However, how to model the\nrepresentation of trajectory-level data presents a significant challenge that\nhas not been systematically addressed amid explosive trajectory data growth. In\nthis work, we introduce Multimodal Trajectory Retrieval, bridging the gap\nbetween universal retrieval and agent-centric trajectory modeling. We construct\nthe Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and\nstates across diverse real-world scenarios. Based on this, we present\nGAE-Bench, a benchmark containing a large number of trajectory-based retrieval\npairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework\nthat adopts vision-language models and incorporates optimized contrastive\nlearning through a token selection and the GradCache mechanism. Comprehensive\nevaluations across multiple datasets show that GAE-Retriever consistently\noutperforms strong baselines in retrieval recall, highlighting its\neffectiveness in advancing multimodal trajectory retrieval.",
      "url": "http://arxiv.org/abs/2506.22056v1",
      "published_time_eastern_timestamp": 1751017838.0
    },
    {
      "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning",
      "summary": "In offline reinforcement learning, agents are trained using only a fixed set\nof stored transitions derived from a source policy. However, this requires that\nthe dataset be labeled by a reward function. In applied settings such as video\ngame development, the availability of the reward function is not always\nguaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement\nlearning (TROFI), a novel approach to effectively learn a policy offline\nwithout a pre-defined reward function. TROFI first learns a reward function\nfrom human preferences, which it then uses to label the original dataset making\nit usable for training the policy. In contrast to other approaches, our method\ndoes not require optimal trajectories. Through experiments on the D4RL\nbenchmark we demonstrate that TROFI consistently outperforms baselines and\nperforms comparably to using the ground truth reward to learn policies.\nAdditionally, we validate the efficacy of our method in a 3D game environment.\nOur studies of the reward model highlight the importance of the reward function\nin this setting: we show that to ensure the alignment of a value function to\nthe actual future discounted reward, it is fundamental to have a\nwell-engineered and easy-to-learn reward function.",
      "url": "http://arxiv.org/abs/2506.22008v1",
      "published_time_eastern_timestamp": 1751012561.0
    },
    {
      "title": "A MILP-Based Solution to Multi-Agent Motion Planning and Collision\n  Avoidance in Constrained Environments",
      "summary": "We propose a mixed-integer linear program (MILP) for multi-agent motion\nplanning that embeds Polytopic Action-based Motion Planning (PAAMP) into a\nsequence-then-solve pipeline. Region sequences confine each agent to adjacent\nconvex polytopes, while a big-M hyperplane model enforces inter-agent\nseparation. Collision constraints are applied only to agents sharing or\nneighboring a region, which reduces binary variables exponentially compared\nwith naive formulations. An L1 path-length-plus-acceleration cost yields smooth\ntrajectories. We prove finite-time convergence and demonstrate on\nrepresentative multi-agent scenarios with obstacles that our formulation\nproduces collision-free trajectories an order of magnitude faster than an\nunstructured MILP baseline.",
      "url": "http://arxiv.org/abs/2506.21982v1",
      "published_time_eastern_timestamp": 1751010172.0
    },
    {
      "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World\n  Model",
      "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.",
      "url": "http://arxiv.org/abs/2506.21976v1",
      "published_time_eastern_timestamp": 1751009704.0
    },
    {
      "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks\n  Unless You Benchmarked their Empirical Realism",
      "summary": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.",
      "url": "http://arxiv.org/abs/2506.21974v1",
      "published_time_eastern_timestamp": 1751009536.0
    },
    {
      "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM\n  Agents",
      "summary": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation.",
      "url": "http://arxiv.org/abs/2506.21967v1",
      "published_time_eastern_timestamp": 1751008409.0
    },
    {
      "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware\n  Layout Design",
      "summary": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.",
      "url": "http://arxiv.org/abs/2506.21934v1",
      "published_time_eastern_timestamp": 1751004596.0
    },
    {
      "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized\n  Recommendation",
      "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.",
      "url": "http://arxiv.org/abs/2506.21931v1",
      "published_time_eastern_timestamp": 1751003159.0
    }
  ]
}
{
  "last_updated": "2025-06-01T11:10:48.335392-04:00",
  "papers": [
    {
      "title": "From Chat Logs to Collective Insights: Aggregative Question Answering",
      "summary": "Conversational agents powered by large language models (LLMs) are rapidly\nbecoming integral to our daily interactions, generating unprecedented amounts\nof conversational data. Such datasets offer a powerful lens into societal\ninterests, trending topics, and collective concerns. Yet, existing approaches\ntypically treat these interactions as independent and miss critical insights\nthat could emerge from aggregating and reasoning across large-scale\nconversation logs. In this paper, we introduce Aggregative Question Answering,\na novel task requiring models to reason explicitly over thousands of\nuser-chatbot interactions to answer aggregative queries, such as identifying\nemerging concerns among specific demographics. To enable research in this\ndirection, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative\nquestions derived from 182,330 real-world chatbot conversations. Experiments\nshow that existing methods either struggle to reason effectively or incur\nprohibitive computational costs, underscoring the need for new approaches\ncapable of extracting collective insights from large-scale conversational data.",
      "url": "http://arxiv.org/abs/2505.23765v1",
      "published_time_eastern_timestamp": 1748541595.0
    },
    {
      "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
      "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
      "url": "http://arxiv.org/abs/2505.23762v1",
      "published_time_eastern_timestamp": 1748541591.0
    },
    {
      "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks",
      "summary": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable",
      "url": "http://arxiv.org/abs/2505.23752v1",
      "published_time_eastern_timestamp": 1748541578.0
    },
    {
      "title": "ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning\n  Engineering",
      "summary": "The emergence of large language model (LLM)-based agents has significantly\nadvanced the development of autonomous machine learning (ML) engineering.\nHowever, most existing approaches rely heavily on manual prompt engineering,\nfailing to adapt and optimize based on diverse experimental experiences.\nFocusing on this, for the first time, we explore the paradigm of learning-based\nagentic ML, where an LLM agent learns through interactive experimentation on ML\ntasks using online reinforcement learning (RL). To realize this, we propose a\nnovel agentic ML training framework with three key components: (1)\nexploration-enriched fine-tuning, which enables LLM agents to generate diverse\nactions for enhanced RL exploration; (2) step-wise RL, which enables training\non a single action step, accelerating experience collection and improving\ntraining efficiency; (3) an agentic ML-specific reward module, which unifies\nvaried ML feedback signals into consistent rewards for RL optimization.\nLeveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM\nfor autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our\n7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it\nachieves continuous performance improvements and demonstrates exceptional\ncross-task generalization capabilities.",
      "url": "http://arxiv.org/abs/2505.23723v1",
      "published_time_eastern_timestamp": 1748541284.0
    },
    {
      "title": "COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic\n  Agents",
      "summary": "This paper considers a contextual bandit problem involving multiple agents,\nwhere a learner sequentially observes the contexts and the agent's reported\narms, and then selects the arm that maximizes the system's overall reward.\nExisting work in contextual bandits assumes that agents truthfully report their\narms, which is unrealistic in many real-life applications. For instance,\nconsider an online platform with multiple sellers; some sellers may\nmisrepresent product quality to gain an advantage, such as having the platform\npreferentially recommend their products to online users. To address this\nchallenge, we propose an algorithm, COBRA, for contextual bandit problems\ninvolving strategic agents that disincentivize their strategic behavior without\nusing any monetary incentives, while having incentive compatibility and a\nsub-linear regret guarantee. Our experimental results also validate the\ndifferent performance aspects of our proposed algorithm.",
      "url": "http://arxiv.org/abs/2505.23720v1",
      "published_time_eastern_timestamp": 1748541192.0
    },
    {
      "title": "From Connectivity to Autonomy: The Dawn of Self-Evolving Communication\n  Systems",
      "summary": "This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven\nintelligence enables dynamic adaptation beyond static connectivity. We explore\nthe key enablers of autonomous communication systems, spanning reconfigurable\ninfrastructure, adaptive middleware, and intelligent network functions,\nalongside multi-agent collaboration for distributed decision-making. We explore\nhow these methodologies align with emerging industrial IoT frameworks, ensuring\nseamless integration within digital manufacturing processes. Our findings\nemphasize the potential for improved real-time decision-making, optimizing\nefficiency, and reducing latency in networked control systems. The discussion\naddresses ethical challenges, research directions, and standardization efforts,\nconcluding with a technology stack roadmap to guide future developments. By\nleveraging state-of-the-art 6G network management techniques, this research\ncontributes to the next generation of intelligent automation solutions,\nbridging the gap between theoretical advancements and real-world industrial\napplications.",
      "url": "http://arxiv.org/abs/2505.23710v1",
      "published_time_eastern_timestamp": 1748540702.0
    },
    {
      "title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful\n  Visualization in Enterprise Analytics",
      "summary": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard",
      "url": "http://arxiv.org/abs/2505.23695v1",
      "published_time_eastern_timestamp": 1748539935.0
    },
    {
      "title": "ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork",
      "summary": "Developing AI agents capable of collaborating with previously unseen partners\nis a fundamental generalization challenge in multi-agent learning, known as Ad\nHoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage\npipeline, where first, a fixed population of teammates is generated with the\nidea that they should be representative of the teammates that will be seen at\ndeployment time, and second, an AHT agent is trained to collaborate well with\nagents in the population. To date, the research community has focused on\ndesigning separate algorithms for each stage. This separation has led to\nalgorithms that generate teammate pools with limited coverage of possible\nbehaviors, and that ignore whether the generated teammates are easy to learn\nfrom for the AHT agent. Furthermore, algorithms for training AHT agents\ntypically treat the set of training teammates as static, thus attempting to\ngeneralize to previously unseen partner agents without assuming any control\nover the distribution of training teammates. In this paper, we present a\nunified framework for AHT by reformulating the problem as an open-ended\nlearning process between an ad hoc agent and an adversarial teammate generator.\nWe introduce ROTATE, a regret-driven, open-ended training algorithm that\nalternates between improving the AHT agent and generating teammates that probe\nits deficiencies. Extensive experiments across diverse AHT environments\ndemonstrate that ROTATE significantly outperforms baselines at generalizing to\nan unseen set of evaluation teammates, thus establishing a new standard for\nrobust and generalizable teamwork.",
      "url": "http://arxiv.org/abs/2505.23686v1",
      "published_time_eastern_timestamp": 1748539494.0
    },
    {
      "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents",
      "summary": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research.",
      "url": "http://arxiv.org/abs/2505.23671v1",
      "published_time_eastern_timestamp": 1748538895.0
    },
    {
      "title": "Initial Luminally Deposited FGF4 Critically Influences Blastocyst\n  Patterning",
      "summary": "Luminogenesis, the formation of a fluid-filled cavity (lumen), is an\nessential process in early mammalian embryonic development, coinciding with the\nsecond cell-fate decision that differentiates the inner-cell-mass (ICM) into\nepiblast (EPI) and primitive endoderm (PRE) tissues. Based on experiments, the\nblastocyst lumen is hypothesized to influence EPI-PRE tissue specification, but\nits particular functional role remains theoretically underexplored. In this\nstudy, we extended our stochastic ICM differentiation model to incorporate both\nthe blastocyst lumen (blastocoel) and the trophectoderm (TE) as adjacent\ncompartments where the primary signaling protein (FGF4) for EPI-PRE\ndifferentiation can diffuse, degrade, or accumulate. This extended ICM model\nallows for a spatially resolved analysis of EPI-PRE lineage proportioning under\nthe influence of luminally deposited FGF4 molecules. Our results reveal that\nthe blastocoel acts as a localized signaling source, while the TE functions as\nan embryo-wide signaling sink, guiding cell-fate decisions within the ICM. A\ncritical determinant of the ideal target system behavior is the initial amount\nof luminally deposited FGF4, which is required to recapitulate the correct\nspatio-temporal patterning of EPI and PRE (blastocyst) cell lineages. Notably,\nthis requirement is independent of ICM population size and shape, highlighting\nthe robustness of the FGF4 signaling process. Our study also underscores the\npotential of integrating single-cell gene expression and cell-cell\ncommunication dynamics simulations with tissue-level morphogenesis\nrepresentations. By combining spatial-stochastic modeling with agent-based\nframeworks, we could enhance the exploration of the intricate interplay between\ngene regulation, signaling, and morphogenetic processes that govern early\nembryonic development.",
      "url": "http://arxiv.org/abs/2505.23650v1",
      "published_time_eastern_timestamp": 1748538018.0
    },
    {
      "title": "Securing AI Agents with Information-Flow Control",
      "summary": "As AI agents become increasingly autonomous and capable, ensuring their\nsecurity against vulnerabilities such as prompt injection becomes critical.\nThis paper explores the use of information-flow control (IFC) to provide\nsecurity guarantees for AI agents. We present a formal model to reason about\nthe security and expressiveness of agent planners. Using this model, we\ncharacterize the class of properties enforceable by dynamic taint-tracking and\nconstruct a taxonomy of tasks to evaluate security and utility trade-offs of\nplanner designs. Informed by this exploration, we present Fides, a planner that\ntracks confidentiality and integrity labels, deterministically enforces\nsecurity policies, and introduces novel primitives for selectively hiding\ninformation. Its evaluation in AgentDojo demonstrates that this approach\nbroadens the range of tasks that can be securely accomplished. A tutorial to\nwalk readers through the the concepts introduced in the paper can be found at\nhttps://github.com/microsoft/fides",
      "url": "http://arxiv.org/abs/2505.23643v1",
      "published_time_eastern_timestamp": 1748537441.0
    },
    {
      "title": "MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits\n  using Improved Preference Alignment",
      "summary": "The model context protocol (MCP) has been widely adapted as an open standard\nenabling the seamless integration of generative AI agents. However, recent work\nhas shown the MCP is susceptible to retrieval-based \"falsely benign\" attacks\n(FBAs), allowing malicious system access and credential theft, but requiring\nthat users download compromised files directly to their systems. Herein, we\nshow that the threat model of MCP-based attacks is significantly broader than\npreviously thought, i.e., attackers need only post malicious content online to\ndeceive MCP agents into carrying out their attacks on unsuspecting victims'\nsystems.\n  To improve alignment guardrails against such attacks, we introduce a new MCP\ndataset of FBAs and (truly) benign samples to explore the effectiveness of\ndirect preference optimization (DPO) for the refusal training of large language\nmodels (LLMs). While DPO improves model guardrails against such attacks, we\nshow that the efficacy of refusal learning varies drastically depending on the\nmodel's original post-training alignment scheme--e.g., GRPO-based LLMs learn to\nrefuse extremely poorly. Thus, to further improve FBA refusals, we introduce\nRetrieval Augmented Generation for Preference alignment (RAG-Pref), a novel\npreference alignment strategy based on RAG. We show that RAG-Pref significantly\nimproves the ability of LLMs to refuse FBAs, particularly when combined with\nDPO alignment, thus drastically improving guardrails against MCP-based attacks.",
      "url": "http://arxiv.org/abs/2505.23634v1",
      "published_time_eastern_timestamp": 1748537069.0
    },
    {
      "title": "MAPLE: A Mobile Assistant with Persistent Finite State Machines for\n  Recovery Reasoning",
      "summary": "Mobile GUI agents aim to autonomously complete user-instructed tasks across\nmobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable\nthese agents to interpret UI screens, identify actionable elements, and perform\ninteractions such as tapping or typing. However, existing agents remain\nreactive: they reason only over the current screen and lack a structured model\nof app navigation flow, limiting their ability to understand context, detect\nunexpected outcomes, and recover from errors. We present MAPLE, a state-aware\nmulti-agent framework that abstracts app interactions as a Finite State Machine\n(FSM). We computationally model each UI screen as a discrete state and user\nactions as transitions, allowing the FSM to provide a structured representation\nof the app execution. MAPLE consists of specialized agents responsible for four\nphases of task execution: planning, execution, verification, error recovery,\nand knowledge retention. These agents collaborate to dynamically construct FSMs\nin real time based on perception data extracted from the UI screen, allowing\nthe GUI agents to track navigation progress and flow, validate action outcomes\nthrough pre- and post-conditions of the states, and recover from errors by\nrolling back to previously stable states. Our evaluation results on two\nchallenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE\noutperforms the state-of-the-art baseline, improving task success rate by up to\n12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results\nhighlight the importance of structured state modeling in guiding mobile GUI\nagents during task execution. Moreover, our FSM representation can be\nintegrated into future GUI agent architectures as a lightweight, model-agnostic\nmemory layer to support structured planning, execution verification, and error\nrecovery.",
      "url": "http://arxiv.org/abs/2505.23596v1",
      "published_time_eastern_timestamp": 1748534931.0
    },
    {
      "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
      "summary": "Recent advancements in large language model (LLM) agents have significantly\naccelerated scientific discovery automation, yet concurrently raised critical\nethical and safety concerns. To systematically address these challenges, we\nintroduce \\textbf{SafeScientist}, an innovative AI scientist framework\nexplicitly designed to enhance safety and ethical responsibility in AI-driven\nscientific exploration. SafeScientist proactively refuses ethically\ninappropriate or high-risk tasks and rigorously emphasizes safety throughout\nthe research process. To achieve comprehensive safety oversight, we integrate\nmultiple defensive mechanisms, including prompt monitoring, agent-collaboration\nmonitoring, tool-use monitoring, and an ethical reviewer component.\nComplementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel\nbenchmark specifically designed to evaluate AI safety in scientific contexts,\ncomprising 240 high-risk scientific tasks across 6 domains, alongside 30\nspecially designed scientific tools and 120 tool-related risk tasks. Extensive\nexperiments demonstrate that SafeScientist significantly improves safety\nperformance by 35\\% compared to traditional AI scientist frameworks, without\ncompromising scientific output quality. Additionally, we rigorously validate\nthe robustness of our safety pipeline against diverse adversarial attack\nmethods, further confirming the effectiveness of our integrated approach. The\ncode and data will be available at https://github.com/ulab-uiuc/SafeScientist.\n\\textcolor{red}{Warning: this paper contains example data that may be offensive\nor harmful.}",
      "url": "http://arxiv.org/abs/2505.23559v1",
      "published_time_eastern_timestamp": 1748532958.0
    },
    {
      "title": "Going from a Representative Agent to Counterfactuals in Combinatorial\n  Choice",
      "summary": "We study decision-making problems where data comprises points from a\ncollection of binary polytopes, capturing aggregate information stemming from\nvarious combinatorial selection environments. We propose a nonparametric\napproach for counterfactual inference in this setting based on a representative\nagent model, where the available data is viewed as arising from maximizing\nseparable concave utility functions over the respective binary polytopes. Our\nfirst contribution is to precisely characterize the selection probabilities\nrepresentable under this model and show that verifying the consistency of any\ngiven aggregated selection dataset reduces to solving a polynomial-sized linear\nprogram. Building on this characterization, we develop a nonparametric method\nfor counterfactual prediction. When data is inconsistent with the model,\nfinding a best-fitting approximation for prediction reduces to solving a\ncompact mixed-integer convex program. Numerical experiments based on synthetic\ndata demonstrate the method's flexibility, predictive accuracy, and strong\nrepresentational power even under model misspecification.",
      "url": "http://arxiv.org/abs/2505.23546v1",
      "published_time_eastern_timestamp": 1748532263.0
    },
    {
      "title": "TRAP: Targeted Redirecting of Agentic Preferences",
      "summary": "Autonomous agentic AI systems powered by vision-language models (VLMs) are\nrapidly advancing toward real-world deployment, yet their cross-modal reasoning\ncapabilities introduce new attack surfaces for adversarial manipulation that\nexploit semantic reasoning across modalities. Existing adversarial attacks\ntypically rely on visible pixel perturbations or require privileged model or\nenvironment access, making them impractical for stealthy, real-world\nexploitation. We introduce TRAP, a generative adversarial framework that\nmanipulates the agent's decision-making using diffusion-based semantic\ninjections. Our method combines negative prompt-based degradation with positive\nsemantic optimization, guided by a Siamese semantic network and layout-aware\nspatial masking. Without requiring access to model internals, TRAP produces\nvisually natural images yet induces consistent selection biases in agentic AI\nsystems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)\ndataset, building multi-candidate decision scenarios. Across these scenarios,\nTRAP achieves a 100% attack success rate on leading models, including\nLLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such\nas SPSA, Bandit, and standard diffusion approaches. These results expose a\ncritical vulnerability: Autonomous agents can be consistently misled through\nhuman-imperceptible cross-modal manipulations. These findings highlight the\nneed for defense strategies beyond pixel-level robustness to address semantic\nvulnerabilities in cross-modal decision-making.",
      "url": "http://arxiv.org/abs/2505.23518v1",
      "published_time_eastern_timestamp": 1748530636.0
    },
    {
      "title": "PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views",
      "summary": "PhysicsNeRF is a physically grounded framework for 3D reconstruction from\nsparse views, extending Neural Radiance Fields with four complementary\nconstraints: depth ranking, RegNeRF-style consistency, sparsity priors, and\ncross-view alignment. While standard NeRFs fail under sparse supervision,\nPhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB\naverage PSNR using only 8 views, outperforming prior methods. A generalization\ngap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental\nlimitations of sparse-view reconstruction. PhysicsNeRF enables physically\nconsistent, generalizable 3D representations for agent interaction and\nsimulation, and clarifies the expressiveness-generalization trade-off in\nconstrained NeRF models.",
      "url": "http://arxiv.org/abs/2505.23481v1",
      "published_time_eastern_timestamp": 1748529017.0
    },
    {
      "title": "Socratic-PRMBench: Benchmarking Process Reward Models with Systematic\n  Reasoning Patterns",
      "summary": "Process Reward Models (PRMs) are crucial in complex reasoning and\nproblem-solving tasks (e.g., LLM agents with long-horizon decision-making) by\nverifying the correctness of each intermediate reasoning step. In real-world\nscenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to\nsolve a problem, potentially suffering from errors under various reasoning\npatterns. Therefore, PRMs are required to identify errors under various\nreasoning patterns during the reasoning process. However, existing benchmarks\nmainly focus on evaluating PRMs with stepwise correctness, ignoring a\nsystematic evaluation of PRMs under various reasoning patterns. To mitigate\nthis gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs\nsystematically under six reasoning patterns, including Transformation,\nDecomposition, Regather, Deduction, Verification, and Integration.\nSocratic-PRMBench}comprises 2995 reasoning paths with flaws within the\naforementioned six reasoning patterns. Through our experiments on both PRMs and\nLLMs prompted as critic models, we identify notable deficiencies in existing\nPRMs. These observations underscore the significant weakness of current PRMs in\nconducting evaluations on reasoning steps under various reasoning patterns. We\nhope Socratic-PRMBench can serve as a comprehensive testbed for systematic\nevaluation of PRMs under diverse reasoning patterns and pave the way for future\ndevelopment of PRMs.",
      "url": "http://arxiv.org/abs/2505.23474v1",
      "published_time_eastern_timestamp": 1748528813.0
    },
    {
      "title": "On Global Convergence Rates for Federated Policy Gradient under\n  Heterogeneous Environment",
      "summary": "Ensuring convergence of policy gradient methods in federated reinforcement\nlearning (FRL) under environment heterogeneity remains a major challenge. In\nthis work, we first establish that heterogeneity, perhaps counter-intuitively,\ncan necessitate optimal policies to be non-deterministic or even time-varying,\neven in tabular environments. Subsequently, we prove global convergence results\nfor federated policy gradient (FedPG) algorithms employing local updates, under\na {\\L}ojasiewicz condition that holds only for each individual agent, in both\nentropy-regularized and non-regularized scenarios. Crucially, our theoretical\nanalysis shows that FedPG attains linear speed-up with respect to the number of\nagents, a property central to efficient federated learning. Leveraging insights\nfrom our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient\nmethod that employs a carefully constructed softmax-inspired parameterization\ncoupled with an appropriate regularization scheme. We further demonstrate\nexplicit convergence rates for b-RS-FedPG toward near-optimal stationary\npolicies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG\nconsistently outperform federated Q-learning on heterogeneous settings.",
      "url": "http://arxiv.org/abs/2505.23459v1",
      "published_time_eastern_timestamp": 1748527715.0
    },
    {
      "title": "Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action\n  Models in Embodied Agents",
      "summary": "Long-horizon robotic manipulation poses significant challenges for autonomous\nsystems, requiring extended reasoning, precise execution, and robust error\nrecovery across complex sequential tasks. Current approaches, whether based on\nstatic planning or end-to-end visuomotor policies, suffer from error\naccumulation and lack effective verification mechanisms during execution,\nlimiting their reliability in real-world scenarios. We present Agentic Robot, a\nbrain-inspired framework that addresses these limitations through Standardized\nAction Procedures (SAP)--a novel coordination protocol governing component\ninteractions throughout manipulation tasks. Drawing inspiration from\nStandardized Operating Procedures (SOPs) in human organizations, SAP\nestablishes structured workflows for planning, execution, and verification\nphases. Our architecture comprises three specialized components: (1) a large\nreasoning model that decomposes high-level instructions into semantically\ncoherent subgoals, (2) a vision-language-action executor that generates\ncontinuous control commands from real-time visual inputs, and (3) a temporal\nverifier that enables autonomous progression and error recovery through\nintrospective assessment. This SAP-driven closed-loop design supports dynamic\nself-verification without external supervision. On the LIBERO benchmark,\nAgentic Robot achieves state-of-the-art performance with an average success\nrate of 79.6\\%, outperforming SpatialVLA by 6.1\\% and OpenVLA by 7.4\\% on\nlong-horizon tasks. These results demonstrate that SAP-driven coordination\nbetween specialized components enhances both performance and interpretability\nin sequential manipulation, suggesting significant potential for reliable\nautonomous systems. Project Github: https://agentic-robot.github.io.",
      "url": "http://arxiv.org/abs/2505.23450v1",
      "published_time_eastern_timestamp": 1748527009.0
    }
  ]
}
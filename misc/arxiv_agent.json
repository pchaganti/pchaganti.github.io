{
  "last_updated": "2025-06-04T08:23:37.284933-04:00",
  "papers": [
    {
      "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
      "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
      "url": "http://arxiv.org/abs/2506.03143v1",
      "published_time_eastern_timestamp": 1748973548.0
    },
    {
      "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
      "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
      "url": "http://arxiv.org/abs/2506.03136v1",
      "published_time_eastern_timestamp": 1748973522.0
    },
    {
      "title": "Designing Algorithmic Delegates: The Role of Indistinguishability in\n  Human-AI Handoff",
      "summary": "As AI technologies improve, people are increasingly willing to delegate tasks\nto AI agents. In many cases, the human decision-maker chooses whether to\ndelegate to an AI agent based on properties of the specific instance of the\ndecision-making problem they are facing. Since humans typically lack full\nawareness of all the factors relevant to this choice for a given\ndecision-making instance, they perform a kind of categorization by treating\nindistinguishable instances -- those that have the same observable features --\nas the same. In this paper, we define the problem of designing the optimal\nalgorithmic delegate in the presence of categories. This is an important\ndimension in the design of algorithms to work with humans, since we show that\nthe optimal delegate can be an arbitrarily better teammate than the optimal\nstandalone algorithmic agent. The solution to this optimal delegation problem\nis not obvious: we discover that this problem is fundamentally combinatorial,\nand illustrate the complex relationship between the optimal design and the\nproperties of the decision-making task even in simple settings. Indeed, we show\nthat finding the optimal delegate is computationally hard in general. However,\nwe are able to find efficient algorithms for producing the optimal delegate in\nseveral broad cases of the problem, including when the optimal action may be\ndecomposed into functions of features observed by the human and the algorithm.\nFinally, we run computational experiments to simulate a designer updating an\nalgorithmic delegate over time to be optimized for when it is actually adopted\nby users, and show that while this process does not recover the optimal\ndelegate in general, the resulting delegate often performs quite well.",
      "url": "http://arxiv.org/abs/2506.03102v1",
      "published_time_eastern_timestamp": 1748972180.0
    },
    {
      "title": "EgoVLM: Policy Optimization for Egocentric Video Understanding",
      "summary": "Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning.",
      "url": "http://arxiv.org/abs/2506.03097v1",
      "published_time_eastern_timestamp": 1748971680.0
    },
    {
      "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents",
      "summary": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.",
      "url": "http://arxiv.org/abs/2506.03095v1",
      "published_time_eastern_timestamp": 1748971624.0
    },
    {
      "title": "Provable Reinforcement Learning from Human Feedback with an Unknown Link\n  Function",
      "summary": "Link functions, which characterize how human preferences are generated from\nthe value function of an RL problem, are a crucial component in designing RLHF\nalgorithms. Almost all RLHF algorithms, including state-of-the-art ones in\nempirical studies such as DPO and PPO, assume the link function is known to the\nagent (e.g., a logistic function according to the Bradley-Terry model), which\nis arguably unrealistic considering the complex nature of human preferences. To\navoid link function mis-specification, this paper studies general RLHF problems\nwith unknown link functions. We propose a novel policy optimization algorithm\ncalled ZSPO based on a new zeroth-order policy optimization method, where the\nkey is to use human preference to construct a parameter update direction that\nis positively correlated with the true policy gradient direction. ZSPO achieves\nit by estimating the sign of the value function difference instead of\nestimating the gradient from the value function difference, so it does not\nrequire knowing the link function. Under mild conditions, ZSPO converges to a\nstationary policy with a polynomial convergence rate depending on the number of\npolicy iterations and trajectories per iteration. Numerical results also show\nthe superiority of ZSPO under link function mismatch.",
      "url": "http://arxiv.org/abs/2506.03066v1",
      "published_time_eastern_timestamp": 1748968959.0
    },
    {
      "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
      "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.",
      "url": "http://arxiv.org/abs/2506.03053v1",
      "published_time_eastern_timestamp": 1748968427.0
    },
    {
      "title": "EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment",
      "summary": "Deep reinforcement learning agents are often fragile while humans remain\nadaptive and flexible to varying scenarios. To bridge this gap, we present\nEDEN, a biologically inspired navigation framework that integrates learned\nentorhinal-like grid cell representations and reinforcement learning to enable\nautonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,\nEDEN allows agents to perform path integration and vector-based navigation\nusing visual and motion sensor data. At the core of EDEN is a grid cell encoder\nthat transforms egocentric motion into periodic spatial codes, producing\nlow-dimensional, interpretable embeddings of position. To generate these\nactivations from raw sensory input, we combine fiducial marker detections in\nthe lightweight MiniWorld simulator and DINO-based visual features in the\nhigh-fidelity Gazebo simulator. These spatial representations serve as input to\na policy trained with Proximal Policy Optimization (PPO), enabling dynamic,\ngoal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid\nprototyping, and Gazebo, which offers realistic physics and perception noise.\nCompared to baseline agents using raw state inputs (e.g., position, velocity)\nor standard convolutional image encoders, EDEN achieves a 99% success rate,\nwithin the simple scenarios, and >94% within complex floorplans with occluded\npaths with more efficient and reliable step-wise navigation. In addition, as a\nreplacement of ground truth activations, we present a trainable Grid Cell\nencoder enabling the development of periodic grid-like patterns from vision and\nmotion sensor data, emulating the development of such patterns within\nbiological mammals. This work represents a step toward biologically grounded\nspatial intelligence in robotics, bridging neural navigation principles with\nreinforcement learning for scalable deployment.",
      "url": "http://arxiv.org/abs/2506.03046v1",
      "published_time_eastern_timestamp": 1748968113.0
    },
    {
      "title": "Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective",
      "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.",
      "url": "http://arxiv.org/abs/2506.03038v1",
      "published_time_eastern_timestamp": 1748967647.0
    },
    {
      "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
      "summary": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions.",
      "url": "http://arxiv.org/abs/2506.03032v1",
      "published_time_eastern_timestamp": 1748966874.0
    },
    {
      "title": "Coding Agents with Multimodal Browsing are Generalist Problem Solvers",
      "summary": "Modern human labor is characterized by specialization; we train for years and\ndevelop particular tools that allow us to perform well across a variety of\ntasks. In addition, AI agents have been specialized for domains such as\nsoftware engineering, web navigation, and workflow automation. However, this\nresults in agents that are good for one thing but fail to generalize beyond\ntheir intended scope. One reason for this is that agent developers provide a\nhighly specialized set of tools or make architectural decisions optimized for a\nspecific use case or benchmark. In this work, we ask the question: what is the\nminimal set of general tools that can be used to achieve high performance\nacross a diverse set of tasks? Our answer is OpenHands-Versa, a generalist\nagent built with a modest number of general tools: code editing and execution,\nweb search, as well as multimodal web browsing and file access. Importantly,\nOpenHands-Versa demonstrates superior or competitive performance over leading\nspecialized agents across three diverse and challenging benchmarks: SWE-Bench\nMultimodal, GAIA, and The Agent Company, outperforming the best-performing\npreviously published results with absolute improvements in success rate of 9.1,\n1.3, and 9.1 points respectively. Further, we show how existing\nstate-of-the-art multi-agent systems fail to generalize beyond their target\ndomains. These results demonstrate the feasibility of developing a generalist\nagent to solve diverse tasks and establish OpenHands-Versa as a strong baseline\nfor future research.",
      "url": "http://arxiv.org/abs/2506.03011v1",
      "published_time_eastern_timestamp": 1748965855.0
    },
    {
      "title": "DFBench: Benchmarking Deepfake Image Detection Capability of Large\n  Multimodal Models",
      "summary": "With the rapid advancement of generative models, the realism of AI-generated\nimages has significantly improved, posing critical challenges for verifying\ndigital content authenticity. Current deepfake detection methods often depend\non datasets with limited generation models and content diversity that fail to\nkeep pace with the evolving complexity and increasing realism of the\nAI-generated content. Large multimodal models (LMMs), widely adopted in various\nvision tasks, have demonstrated strong zero-shot capabilities, yet their\npotential in deepfake detection remains largely unexplored. To bridge this gap,\nwe present \\textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i)\nbroad diversity, including 540,000 images across real, AI-edited, and\nAI-generated content, (ii) latest model, the fake images are generated by 12\nstate-of-the-art generation models, and (iii) bidirectional benchmarking and\nevaluating for both the detection accuracy of deepfake detectors and the\nevasion capability of generative models. Based on DFBench, we propose\n\\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a\ncombined probability strategy from multiple LMMs. MoA-DF achieves\nstate-of-the-art performance, further proving the effectiveness of leveraging\nLMMs for deepfake detection. Database and codes are publicly available at\nhttps://github.com/IntMeGroup/DFBench.",
      "url": "http://arxiv.org/abs/2506.03007v1",
      "published_time_eastern_timestamp": 1748965541.0
    },
    {
      "title": "A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy\n  Question-Answering Systems",
      "summary": "Privacy policies inform users about data collection and usage, yet their\ncomplexity limits accessibility for diverse populations. Existing Privacy\nPolicy Question Answering (QA) systems exhibit performance disparities across\nEnglish dialects, disadvantaging speakers of non-standard varieties. We propose\na novel multi-agent framework inspired by human-centered design principles to\nmitigate dialectal biases. Our approach integrates a Dialect Agent, which\ntranslates queries into Standard American English (SAE) while preserving\ndialectal intent, and a Privacy Policy Agent, which refines predictions using\ndomain expertise. Unlike prior approaches, our method does not require\nretraining or dialect-specific fine-tuning, making it broadly applicable across\nmodels and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves\nGPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from\n0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without\nadditional training data. These results highlight the effectiveness of\nstructured agent collaboration in mitigating dialect biases and underscore the\nimportance of designing NLP systems that account for linguistic diversity to\nensure equitable access to privacy information.",
      "url": "http://arxiv.org/abs/2506.02998v1",
      "published_time_eastern_timestamp": 1748964740.0
    },
    {
      "title": "Mapping Student-AI Interaction Dynamics in Multi-Agent Learning\n  Environments: Supporting Personalised Learning and Reducing Performance Gaps",
      "summary": "Multi-agent AI systems, which simulate diverse instructional roles such as\nteachers and peers, offer new possibilities for personalized and interactive\nlearning. Yet, student-AI interaction patterns and their pedagogical\nimplications remain unclear. This study explores how university students\nengaged with multiple AI agents, and how these interactions influenced\ncognitive outcomes (learning gains) and non-cognitive factors (motivation,\ntechnology acceptance). Based on MAIC, an online learning platform with\nmulti-agent, the research involved 305 university students and 19,365 lines of\ndialogue data. Pre- and post-test scores, self-reported motivation and\ntechnology acceptance were also collected. The study identified two engagement\npatterns: co-construction of knowledge and co-regulation. Lag sequential\nanalysis revealed that students with lower prior knowledge relied more on\nco-construction of knowledge sequences, showing higher learning gains and\npost-course motivation. In contrast, students with higher prior knowledge\nengaged more in co-regulation behaviors but exhibited limited learning\nimprovement. Technology acceptance increased across all groups. These findings\nsuggest that multi-agent AI systems can adapt to students' varying needs,\nsupport differentiated engagement, and reduce performance gaps. Implications\nfor personalized system design and future research directions are discussed.",
      "url": "http://arxiv.org/abs/2506.02993v1",
      "published_time_eastern_timestamp": 1748964544.0
    },
    {
      "title": "Mitigating Manipulation and Enhancing Persuasion: A Reflective\n  Multi-Agent Approach for Legal Argument Generation",
      "summary": "Large Language Models (LLMs) are increasingly explored for legal argument\ngeneration, yet they pose significant risks of manipulation through\nhallucination and ungrounded persuasion, and often fail to utilize provided\nfactual bases effectively or abstain when arguments are untenable. This paper\nintroduces a novel reflective multi-agent method designed to address these\nchallenges in the context of legally compliant persuasion. Our approach employs\nspecialized agents--a Factor Analyst and an Argument Polisher--in an iterative\nrefinement process to generate 3-ply legal arguments (plaintiff, defendant,\nrebuttal). We evaluate Reflective Multi-Agent against single-agent,\nenhanced-prompt single-agent, and non-reflective multi-agent baselines using\nfour diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,\nLlama-4-Scout-17b-16e) across three legal scenarios: \"arguable\", \"mismatched\",\nand \"non-arguable\". Results demonstrate Reflective Multi-Agent's significant\nsuperiority in successful abstention (preventing generation when arguments\ncannot be grounded), marked improvements in hallucination accuracy (reducing\nfabricated and misattributed factors), particularly in \"non-arguable\"\nscenarios, and enhanced factor utilization recall (improving the use of\nprovided case facts). These findings suggest that structured reflection within\na multi-agent framework offers a robust computable method for fostering ethical\npersuasion and mitigating manipulation in LLM-based legal argumentation\nsystems, a critical step towards trustworthy AI in law. Project page:\nhttps://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/",
      "url": "http://arxiv.org/abs/2506.02992v1",
      "published_time_eastern_timestamp": 1748964510.0
    },
    {
      "title": "Adaptive Graph Pruning for Multi-Agent Communication",
      "summary": "Large Language Model (LLM) based multi-agent systems have shown remarkable\nperformance in various tasks, especially when enhanced through collaborative\ncommunication. However, current methods often rely on a fixed number of agents\nand static communication structures, limiting their ability to adapt to varying\ntask complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a\nnovel task-adaptive multi-agent collaboration framework that jointly optimizes\nagent quantity (hard-pruning) and communication topology (soft-pruning).\nSpecifically, our method employs a two-stage training strategy: firstly,\nindependently training soft-pruning networks for different agent quantities to\ndetermine optimal agent-quantity-specific complete graphs and positional masks\nacross specific tasks; and then jointly optimizing hard-pruning and\nsoft-pruning within a maximum complete graph to dynamically configure the\nnumber of agents and their communication topologies per task. Extensive\nexperiments demonstrate that our approach is: (1) High-performing, achieving\nstate-of-the-art results across six benchmarks and consistently generalizes\nacross multiple mainstream LLM architectures, with a increase in performance of\n$2.58\\%\\sim 9.84\\%$; (2) Task-adaptive, dynamically constructing optimized\ncommunication topologies tailored to specific tasks, with an extremely high\nperformance in all three task categories (general reasoning, mathematical\nreasoning, and code generation); (3) Token-economical, having fewer training\nsteps and token consumption at the same time, with a decrease in token\nconsumption of $90\\%+$; and (4) Training-efficient, achieving high performance\nwith very few training steps compared with other methods. The performance will\nsurpass the existing baselines after about ten steps of training under six\nbenchmarks.",
      "url": "http://arxiv.org/abs/2506.02951v1",
      "published_time_eastern_timestamp": 1748961960.0
    },
    {
      "title": "Abstract Counterfactuals for Language Model Agents",
      "summary": "Counterfactual inference is a powerful tool for analysing and evaluating\nautonomous agents, but its application to language model (LM) agents remains\nchallenging. Existing work on counterfactuals in LMs has primarily focused on\ntoken-level counterfactuals, which are often inadequate for LM agents due to\ntheir open-ended action spaces. Unlike traditional agents with fixed, clearly\ndefined action spaces, the actions of LM agents are often implicit in the\nstrings they output, making their action spaces difficult to define and\ninterpret. Furthermore, the meanings of individual tokens can shift depending\non the context, adding complexity to token-level reasoning and sometimes\nleading to biased or meaningless counterfactuals. We introduce \\emph{Abstract\nCounterfactuals}, a framework that emphasises high-level characteristics of\nactions and interactions within an environment, enabling counterfactual\nreasoning tailored to user-relevant features. Our experiments demonstrate that\nthe approach produces consistent and meaningful counterfactuals while\nminimising the undesired side effects of token-level methods. We conduct\nexperiments on text-based games and counterfactual text generation, while\nconsidering both token-level and latent-space interventions.",
      "url": "http://arxiv.org/abs/2506.02946v1",
      "published_time_eastern_timestamp": 1748961866.0
    },
    {
      "title": "A Multi-agent LLM-based JUit Test Generation with Strong Oracles",
      "summary": "Unit testing plays a critical role in ensuring software correctness. However,\nwriting unit tests manually is laborious, especially for strong typed languages\nlike Java, motivating the need for automated approaches. Traditional methods\nprimarily rely on search-based or randomized algorithms to generate tests that\nachieve high code coverage and produce regression oracles, which are derived\nfrom the program's current behavior rather than its intended functionality.\nRecent advances in large language models (LLMs) have enabled oracle generation\nfrom natural language descriptions. However, existing LLM-based methods often\nrequire LLM fine-tuning or rely on external tools such as EvoSuite for test\nprefix generation.\n  In this work, we propose CANDOR, a novel end-to-end, prompt-based LLM\nframework for automated JUnit test generation. CANDOR orchestrates multiple\nspecialized LLM agents to generate JUnit tests, including both high-quality\ntest prefixes and accurate oracles. To mitigate the notorious hallucinations in\nLLMs, we introduce a novel strategy that engages multiple reasoning LLMs in a\npanel discussion and generate accurate oracles based on consensus.\nAdditionally, to reduce the verbosity of reasoning LLMs' outputs, we propose a\nnovel dual-LLM pipeline to produce concise and structured oracle evaluations.\n  Our experiments on the HumanEvalJava and LeetCodeJava datasets show that\nCANDOR can generate accurate oracles and is slightly better than EvoSuite in\ngenerating tests with high line coverage and clearly superior in terms of\nmutation score. Moreover, CANDOR significantly outperforms the\nstate-of-the-art, prompt-based test generator LLM-Empirical, achieving\nimprovements of 15.8 to 25.1 percentage points in oracle correctness on both\ncorrect and faulty source code. Ablation studies confirm the critical\ncontributions of key agents in improving test prefix quality and oracle\naccuracy.",
      "url": "http://arxiv.org/abs/2506.02943v1",
      "published_time_eastern_timestamp": 1748961785.0
    },
    {
      "title": "ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems\n  into Universal Collaborative Intelligence Platforms",
      "summary": "This paper presents ThinkTank, a comprehensive and scalable framework\ndesigned to transform specialized AI agent systems into versatile collaborative\nintelligence platforms capable of supporting complex problem-solving across\ndiverse domains. ThinkTank systematically generalizes agent roles, meeting\nstructures, and knowledge integration mechanisms by adapting proven scientific\ncollaboration methodologies. Through role abstraction, generalization of\nmeeting types for iterative collaboration, and the integration of\nRetrieval-Augmented Generation with advanced knowledge storage, the framework\nfacilitates expertise creation and robust knowledge sharing. ThinkTank enables\norganizations to leverage collaborative AI for knowledge-intensive tasks while\nensuring data privacy and security through local deployment, utilizing\nframeworks like Ollama with models such as Llama3.1. The ThinkTank framework is\ndesigned to deliver significant advantages in cost-effectiveness, data\nsecurity, scalability, and competitive positioning compared to cloud-based\nalternatives, establishing it as a universal platform for AI-driven\ncollaborative problem-solving. The ThinkTank code is available at\nhttps://github.com/taugroup/ThinkTank",
      "url": "http://arxiv.org/abs/2506.02931v1",
      "published_time_eastern_timestamp": 1748961168.0
    },
    {
      "title": "Large Processor Chip Model",
      "summary": "Computer System Architecture serves as a crucial bridge between software\napplications and the underlying hardware, encompassing components like\ncompilers, CPUs, coprocessors, and RTL designs. Its development, from early\nmainframes to modern domain-specific architectures, has been driven by rising\ncomputational demands and advancements in semiconductor technology. However,\ntraditional paradigms in computer system architecture design are confronting\nsignificant challenges, including a reliance on manual expertise, fragmented\noptimization across software and hardware layers, and high costs associated\nwith exploring expansive design spaces. While automated methods leveraging\noptimization algorithms and machine learning have improved efficiency, they\nremain constrained by a single-stage focus, limited data availability, and a\nlack of comprehensive human domain knowledge. The emergence of large language\nmodels offers transformative opportunities for the design of computer system\narchitecture. By leveraging the capabilities of LLMs in areas such as code\ngeneration, data analysis, and performance modeling, the traditional manual\ndesign process can be transitioned to a machine-based automated design\napproach. To harness this potential, we present the Large Processor Chip Model\n(LPCM), an LLM-driven framework aimed at achieving end-to-end automated\ncomputer architecture design. The LPCM is structured into three levels:\nHuman-Centric; Agent-Orchestrated; and Model-Governed. This paper utilizes 3D\nGaussian Splatting as a representative workload and employs the concept of\nsoftware-hardware collaborative design to examine the implementation of the\nLPCM at Level 1, demonstrating the effectiveness of the proposed approach.\nFurthermore, this paper provides an in-depth discussion on the pathway to\nimplementing Level 2 and Level 3 of the LPCM, along with an analysis of the\nexisting challenges.",
      "url": "http://arxiv.org/abs/2506.02929v1",
      "published_time_eastern_timestamp": 1748961052.0
    }
  ]
}
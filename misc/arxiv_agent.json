{
  "last_updated": "2025-08-05T11:16:10.947588-04:00",
  "papers": [
    {
      "title": "Hierarchical Learning-Based Control for Multi-Agent Shepherding of\n  Stochastic Autonomous Agents",
      "summary": "Multi-agent shepherding represents a challenging distributed control problem\nwhere herder agents must coordinate to guide independently moving targets to\ndesired spatial configurations. Most existing control strategies assume\ncohesive target behavior, which frequently fails in practical applications\nwhere targets exhibit stochastic autonomous behavior. This paper presents a\nhierarchical learning-based control architecture that decomposes the\nshepherding problem into a high-level decision-making module and a low-level\nmotion control component. The proposed distributed control system synthesizes\neffective control policies directly from closed-loop experience without\nrequiring explicit inter-agent communication or prior knowledge of target\ndynamics. The decentralized architecture achieves cooperative control behavior\nthrough emergent coordination without centralized supervision. Experimental\nvalidation demonstrates superior closed-loop performance compared to\nstate-of-the-art heuristic control methods, achieving 100\\% success rates with\nimproved settling times and control efficiency. The control architecture scales\nbeyond its design conditions, adapts to time-varying goal regions, and\ndemonstrates practical implementation feasibility through real-time experiments\non the Robotarium platform.",
      "url": "http://arxiv.org/abs/2508.02632v1",
      "published_time_eastern_timestamp": 1754328000.0
    },
    {
      "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce",
      "summary": "Online marketplaces will be transformed by autonomous AI agents acting on\nbehalf of consumers. Rather than humans browsing and clicking,\nvision-language-model (VLM) agents can parse webpages, evaluate products, and\ntransact. This raises a fundamental question: what do AI agents buy, and why?\nWe develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent\nwith a fully programmable mock marketplace to study this question. We first\nconduct basic rationality checks in the context of simple tasks, and then, by\nrandomizing product positions, prices, ratings, reviews, sponsored tags, and\nplatform endorsements, we obtain causal estimates of how frontier VLMs actually\nshop. Models show strong but heterogeneous position effects: all favor the top\nrow, yet different models prefer different columns, undermining the assumption\nof a universal \"top\" rank. They penalize sponsored tags and reward\nendorsements. Sensitivities to price, ratings, and reviews are directionally\nhuman-like but vary sharply in magnitude across models. Motivated by scenarios\nwhere sellers use AI agents to optimize product listings, we show that a\nseller-side agent that makes minor tweaks to product descriptions, targeting AI\nbuyer preferences, can deliver substantial market-share gains if AI-mediated\nshopping dominates. We also find that modal product choices can differ across\nmodels and, in some cases, demand may concentrate on a few select products,\nraising competition questions. Together, our results illuminate how AI agents\nmay behave in e-commerce settings and surface concrete seller strategy,\nplatform design, and regulatory questions in an AI-mediated ecosystem.",
      "url": "http://arxiv.org/abs/2508.02630v1",
      "published_time_eastern_timestamp": 1754327976.0
    },
    {
      "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents",
      "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
      "url": "http://arxiv.org/abs/2508.02629v1",
      "published_time_eastern_timestamp": 1754327894.0
    },
    {
      "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous\n  Healthcare Research",
      "summary": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.",
      "url": "http://arxiv.org/abs/2508.02621v1",
      "published_time_eastern_timestamp": 1754327327.0
    },
    {
      "title": "Meta-RAG on Large Codebases Using Code Summarization",
      "summary": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.",
      "url": "http://arxiv.org/abs/2508.02611v1",
      "published_time_eastern_timestamp": 1754326870.0
    },
    {
      "title": "Automated Construction of Artificial Lattice Structures with Designer\n  Electronic States",
      "summary": "Manipulating matter with a scanning tunneling microscope (STM) enables\ncreation of atomically defined artificial structures that host designer quantum\nstates. However, the time-consuming nature of the manipulation process, coupled\nwith the sensitivity of the STM tip, constrains the exploration of diverse\nconfigurations and limits the size of designed features. In this study, we\npresent a reinforcement learning (RL)-based framework for creating artificial\nstructures by spatially manipulating carbon monoxide (CO) molecules on a copper\nsubstrate using the STM tip. The automated workflow combines molecule detection\nand manipulation, employing deep learning-based object detection to locate CO\nmolecules and linear assignment algorithms to allocate these molecules to\ndesignated target sites. We initially perform molecule maneuvering based on\nrandomized parameter sampling for sample bias, tunneling current setpoint and\nmanipulation speed. This dataset is then structured into an action trajectory\nused to train an RL agent. The model is subsequently deployed on the STM for\nreal-time fine-tuning of manipulation parameters during structure construction.\nOur approach incorporates path planning protocols coupled with active drift\ncompensation to enable atomically precise fabrication of structures with\nsignificantly reduced human input while realizing larger-scale artificial\nlattices with desired electronic properties. To underpin of efficiency of our\napproach we demonstrate the automated construction of an extended artificial\ngraphene lattice and confirm the existence of characteristic Dirac point in its\nelectronic structure. Further challenges to RL-based structural assembly\nscalability are discussed.",
      "url": "http://arxiv.org/abs/2508.02581v1",
      "published_time_eastern_timestamp": 1754325525.0
    },
    {
      "title": "RL-U$^2$Net: A Dual-Branch UNet with Reinforcement Learning-Assisted\n  Multimodal Feature Fusion for Accurate 3D Whole-Heart Segmentation",
      "summary": "Accurate whole-heart segmentation is a critical component in the precise\ndiagnosis and interventional planning of cardiovascular diseases. Integrating\ncomplementary information from modalities such as computed tomography (CT) and\nmagnetic resonance imaging (MRI) can significantly enhance segmentation\naccuracy and robustness. However, existing multi-modal segmentation methods\nface several limitations: severe spatial inconsistency between modalities\nhinders effective feature fusion; fusion strategies are often static and lack\nadaptability; and the processes of feature alignment and segmentation are\ndecoupled and inefficient. To address these challenges, we propose a\ndual-branch U-Net architecture enhanced by reinforcement learning for feature\nalignment, termed RL-U$^2$Net, designed for precise and efficient multi-modal\n3D whole-heart segmentation. The model employs a dual-branch U-shaped network\nto process CT and MRI patches in parallel, and introduces a novel RL-XAlign\nmodule between the encoders. The module employs a cross-modal attention\nmechanism to capture semantic correspondences between modalities and a\nreinforcement-learning agent learns an optimal rotation strategy that\nconsistently aligns anatomical pose and texture features. The aligned features\nare then reconstructed through their respective decoders. Finally, an\nensemble-learning-based decision module integrates the predictions from\nindividual patches to produce the final segmentation result. Experimental\nresults on the publicly available MM-WHS 2017 dataset demonstrate that the\nproposed RL-U$^2$Net outperforms existing state-of-the-art methods, achieving\nDice coefficients of 93.1% on CT and 87.0% on MRI, thereby validating the\neffectiveness and superiority of the proposed approach.",
      "url": "http://arxiv.org/abs/2508.02557v1",
      "published_time_eastern_timestamp": 1754323926.0
    },
    {
      "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
      "summary": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth\ninputs to provide rich spatial cues for action planning, but these sensors can\nbe costly or less accessible in real-world deployments. Recent approaches based\non Vision-Language Action (VLA) models achieve strong results with monocular\ninput, yet they still lag behind methods using panoramic RGB-D information. We\npresent MonoDream, a lightweight VLA framework that enables monocular agents to\nlearn a Unified Navigation Representation (UNR). This shared feature\nrepresentation jointly aligns navigation-relevant visual semantics (e.g.,\nglobal layout, depth, and future cues) and language-grounded action intent,\nenabling more reliable action prediction. MonoDream further introduces Latent\nPanoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to\npredict latent features of panoramic RGB and depth observations at both current\nand future steps based on only monocular input. Experiments on multiple VLN\nbenchmarks show that MonoDream consistently improves monocular navigation\nperformance and significantly narrows the gap with panoramic-based agents.",
      "url": "http://arxiv.org/abs/2508.02549v1",
      "published_time_eastern_timestamp": 1754323290.0
    },
    {
      "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language\n  and Multi-Agent Collaboration",
      "summary": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.",
      "url": "http://arxiv.org/abs/2508.02470v1",
      "published_time_eastern_timestamp": 1754318191.0
    },
    {
      "title": "An equivalence between time-symmetry and cyclic causality in quantum\n  theory",
      "summary": "Understanding the relationship between the time-symmetric nature of physical\nlaws and the apparent directionality of causality is a central question in\nquantum foundations. The standard operational formulation, widely used in\nquantum information, imposes a definite, acyclic causal order on agents'\noperations, contrasting with time-symmetric dynamics. Two prominent extensions\nof this framework are the multi-time state (MTS) formalism, which incorporates\ntime symmetry via arbitrary pre- and post-selection, and the post-selected\nclosed timelike curve (P-CTC) framework, which enables cyclic causal influences\nthrough post-selection on maximally entangled states. While prior work has\nnoted structural connections between MTS and P-CTCs, it remained unclear\nwhether an operational equivalence exists, or whether constructive mappings can\nbe established between their most general objects. In this work, we address\nthis gap by extending the P-CTC framework to define time-labelled P-CTC\nassisted combs, a more general class of P-CTC-assisted objects that support\nopen processing slots and explicit temporal structure. We prove that for every\n(possibly mixed) MTS, there exists an operationally equivalent time-labelled\nP-CTC-assisted comb, and vice versa. The equivalence is shown via explicit\nmappings, while discussing the number and dimensionality of the P-CTCs\ninvolved. We also explore a resource-theoretic view of MTS, defining a partial\norder under free transformations that do not use P-CTCs. We conclude by\ndiscussing future directions informed by the operational equivalence between\ntime symmetry and cyclic causality established here.",
      "url": "http://arxiv.org/abs/2508.02463v1",
      "published_time_eastern_timestamp": 1754317846.0
    },
    {
      "title": "Emergence of Fair Leaders via Mediators in Multi-Agent Reinforcement\n  Learning",
      "summary": "Stackelberg games and their resulting equilibria have received increasing\nattention in the multi-agent reinforcement learning literature. Each stage of a\ntraditional Stackelberg game involves a leader(s) acting first, followed by the\nfollowers. In situations where the roles of leader(s) and followers can be\ninterchanged, the designated role can have considerable advantages, for\nexample, in first-mover advantage settings. Then the question arises: Who\nshould be the leader and when? A bias in the leader selection process can lead\nto unfair outcomes. This problem is aggravated if the agents are\nself-interested and care only about their goals and rewards. We formally define\nthis leader selection problem and show its relation to fairness in agents'\nreturns. Furthermore, we propose a multi-agent reinforcement learning framework\nthat maximizes fairness by integrating mediators. Mediators have previously\nbeen used in the simultaneous action setting with varying levels of control,\nsuch as directly performing agents' actions or just recommending them. Our\nframework integrates mediators in the Stackelberg setting with minimal control\n(leader selection). We show that the presence of mediators leads to\nself-interested agents taking fair actions, resulting in higher overall\nfairness in agents' returns.",
      "url": "http://arxiv.org/abs/2508.02421v1",
      "published_time_eastern_timestamp": 1754314965.0
    },
    {
      "title": "Beyond Manually Designed Pruning Policies with Second-Level Performance\n  Prediction: A Pruning Framework for LLMs",
      "summary": "Non-uniform structured network pruning methods can effectively reduce Large\nLanguage Model (LLM) size by eliminating redundant channels or layers, offering\nlower performance degradation than uniform strategies. However, existing\nnon-uniform methods rely heavily on manually designed pruning policies (e.g.,\nlayer importance and scaling factors), and therefore cannot efficiently adapt\nto scenarios with dynamic pruning ratio requirements. Additionly, a critical\nbottleneck -- the time-consuming evaluation of pruning policies -- further\nlimits the feasibility of iteratively and dynamically finding optimal pruning\npolicies. To address these limitations, we propose PPF (Predictive Pruning\nFramework), a novel pruning framework for LLMs that eliminates manual design\ndependencies via second-level performance prediction. PPF not only supports\nreal-time pruning decisions under dynamic pruning ratios but is also applicable\nto static pruning scenarios. It employs an agent for producing adaptive and\nreal-time pruning actions, while a lightweight performance predictor that can\nevaluate a pruning policy in seconds, significantly speeding up the iterative\noptimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can\ngenerate dynamic/static pruning policies and it reduces perplexity by up to\n33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,\noutperforming manually designed pruning policies. The performance predictor\nachieves second-level performance prediction with high accuracy (prediction\nerror < 0.0011). It reduces the mean evaluation latency from minute-level (1\nminute and 38.02 seconds of test-set evaluation methods) to second-level (1.52\nsecond), achieving over 64 times speedup. Our code will be available at\nhttps://github.com/Ma-zx/PPF .",
      "url": "http://arxiv.org/abs/2508.02381v1",
      "published_time_eastern_timestamp": 1754312915.0
    },
    {
      "title": "Talking Surveys: How Photorealistic Embodied Conversational Agents Shape\n  Response Quality, Engagement, and Satisfaction",
      "summary": "Embodied conversational agents (ECAs) are increasingly more realistic and\ncapable of dynamic conversations. In online surveys, anthropomorphic agents\ncould help address issues like careless responding and satisficing, which\noriginate from the lack of personal engagement and perceived accountability.\nHowever, there is a lack of understanding of how ECAs in user experience\nresearch may affect participant engagement, satisfaction, and the quality of\nresponses. As a proof of concept, we propose an instrument that enables the\nincorporation of conversations with a virtual avatar into surveys, using on\nAI-driven video generation, speech recognition, and Large Language Models. In\nour between-subjects study, 80 participants (UK, stratified random sample of\ngeneral population) either talked to a voice-based agent with an animated video\navatar, or interacted with a chatbot. Across surveys based on two self-reported\npsychometric tests, 2,265 conversation responses were obtained. Statistical\ncomparison of results indicates that embodied agents can contribute\nsignificantly to more informative, detailed responses, as well as higher yet\nmore time-efficient engagement. Furthermore, qualitative analysis provides\nvaluable insights for causes of no significant change to satisfaction, linked\nto personal preferences, turn-taking delays and Uncanny Valley reactions. These\nfindings support the pursuit and development of new methods toward human-like\nagents for the transformation of online surveys into more natural interactions\nresembling in-person interviews.",
      "url": "http://arxiv.org/abs/2508.02376v1",
      "published_time_eastern_timestamp": 1754312669.0
    },
    {
      "title": "Language Model Guided Reinforcement Learning in Quantitative Trading",
      "summary": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL.",
      "url": "http://arxiv.org/abs/2508.02366v1",
      "published_time_eastern_timestamp": 1754311931.0
    },
    {
      "title": "Agentic Personalized Fashion Recommendation in the Age of Generative AI:\n  Challenges, Opportunities, and Evaluation",
      "summary": "Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands.",
      "url": "http://arxiv.org/abs/2508.02342v1",
      "published_time_eastern_timestamp": 1754310145.0
    },
    {
      "title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI\n  Research and Deployment",
      "summary": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.",
      "url": "http://arxiv.org/abs/2508.02292v1",
      "published_time_eastern_timestamp": 1754305354.0
    },
    {
      "title": "Distributed Non-Uniform Scaling Control of Multi-Agent Formation via\n  Matrix-Valued Constraints",
      "summary": "Distributed formation maneuver control refers to the problem of maneuvering a\ngroup of agents to change their formation shape by adjusting the motions of\npartial agents, where the controller of each agent only requires local\ninformation measured from its neighbors. Although this problem has been\nextensively investigated, existing approaches are mostly limited to uniform\nscaling transformations. This article proposes a new type of local\nmatrix-valued constraints, via which non-uniform scaling control of position\nformation can be achieved by tuning the positions of only two agents (i.e.,\nleaders). Here, the non-uniform scaling transformation refers to scaling the\nposition formation with different ratios along different orthogonal coordinate\ndirections. Moreover, by defining scaling and translation of attitude\nformation, we propose a distributed control scheme for scaling and translation\nmaneuver control of joint position-attitude formations. It is proven that the\nproposed controller achieves global convergence, provided that the sensing\ngraph among agents is a 2-rooted bidirectional graph. Compared with the affine\nformation maneuver control approach, the proposed approach leverages a sparser\nsensing graph, requires fewer leaders, and additionally enables scaling\ntransformations of the attitude formation. A simulation example is proposed to\ndemonstrate our theoretical results.",
      "url": "http://arxiv.org/abs/2508.02289v1",
      "published_time_eastern_timestamp": 1754305053.0
    },
    {
      "title": "CellForge: Agentic Design of Virtual Cell Models",
      "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
      "url": "http://arxiv.org/abs/2508.02276v1",
      "published_time_eastern_timestamp": 1754304211.0
    },
    {
      "title": "Patho-AgenticRAG: Towards Multimodal Agentic Retrieval-Augmented\n  Generation for Pathology VLMs via Reinforcement Learning",
      "summary": "Although Vision Language Models (VLMs) have shown strong generalization in\nmedical imaging, pathology presents unique challenges due to ultra-high\nresolution, complex tissue structures, and nuanced clinical semantics. These\nfactors make pathology VLMs prone to hallucinations, i.e., generating outputs\ninconsistent with visual evidence, which undermines clinical trust. Existing\nRAG approaches in this domain largely depend on text-based knowledge bases,\nlimiting their ability to leverage diagnostic visual cues. To address this, we\npropose Patho-AgenticRAG, a multimodal RAG framework with a database built on\npage-level embeddings from authoritative pathology textbooks. Unlike\ntraditional text-only retrieval systems, it supports joint text-image search,\nenabling direct retrieval of textbook pages that contain both the queried text\nand relevant visual cues, thus avoiding the loss of critical image-based\ninformation. Patho-AgenticRAG also supports reasoning, task decomposition, and\nmulti-turn search interactions, improving accuracy in complex diagnostic\nscenarios. Experiments show that Patho-AgenticRAG significantly outperforms\nexisting multimodal models in complex pathology tasks like multiple-choice\ndiagnosis and visual question answering. Our project is available at the\nPatho-AgenticRAG repository:\nhttps://github.com/Wenchuan-Zhang/Patho-AgenticRAG.",
      "url": "http://arxiv.org/abs/2508.02258v1",
      "published_time_eastern_timestamp": 1754301788.0
    },
    {
      "title": "FX-constrained growth: Fundamentalists, chartists and the dynamic\n  trade-multiplier",
      "summary": "Behavioural finance offers a valuable framework for examining foreign\nexchange (FX) market dynamics, including puzzles such as excess volatility and\nfat-tailed distributions. Yet, when it comes to their interaction with the\n`real' side of the economy, existing scholarship has overlooked a critical\nfeature of developing countries. They cannot trade in their national currencies\nand need US dollars to access modern production techniques as well as maintain\nconsumption patterns similar to those of wealthier societies. To address this\ngap, we present a novel heterogeneous agents model from the perspective of a\ndeveloping economy that distinguishes between speculative and non-speculative\nsectors in the FX market. We demonstrate that as long as non-speculative demand\nresponds to domestic economic activity, a market-clearing output growth rate\nexists that, in steady-state, is equal to the ratio between FX supply growth\nand the income elasticity of demand for foreign assets, i.e., a generalised\ndynamic trade-multiplier. Numerical simulations reproduce key stylised facts of\nexchange rate dynamics and economic growth, including distributions that\ndeviate from the typical bell-shaped curve. Data from a sample of Latin\nAmerican countries reveal that FX fluctuations exhibit similar statistical\nproperties. Furthermore, we employ time-varying parameter estimation techniques\nto show that the dynamic trade-multiplier closely tracks observed growth rates\nin these economies.",
      "url": "http://arxiv.org/abs/2508.02252v1",
      "published_time_eastern_timestamp": 1754301290.0
    }
  ]
}
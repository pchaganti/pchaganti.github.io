{
  "last_updated": "2025-09-06T11:09:34.245943-04:00",
  "papers": [
    {
      "title": "SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety\n  Certificates",
      "summary": "This paper proposes a fully data-driven motion-planning framework for\nhomogeneous linear multi-agent systems that operate in shared, obstacle-filled\nworkspaces without access to explicit system models. Each agent independently\nlearns its closed-loop behavior from experimental data by solving convex\nsemidefinite programs that generate locally invariant ellipsoids and\ncorresponding state-feedback gains. These ellipsoids, centered along grid-based\nwaypoints, certify the dynamic feasibility of short-range transitions and\ndefine safe regions of operation. A sampling-based planner constructs a tree of\nsuch waypoints, where transitions are allowed only when adjacent ellipsoids\noverlap, ensuring invariant-to-invariant transitions and continuous safety. All\nagents expand their trees simultaneously and are coordinated through a\nspace-time reservation table that guarantees inter-agent safety by preventing\nsimultaneous occupancy and head-on collisions. Each successful edge in the tree\nis equipped with its own local controller, enabling execution without\nre-solving optimization problems at runtime. The resulting trajectories are not\nonly dynamically feasible but also provably safe with respect to both\nenvironmental constraints and inter-agent collisions. Simulation results\ndemonstrate the effectiveness of the approach in synthesizing synchronized,\nsafe trajectories for multiple agents under shared dynamics and constraints,\nusing only data and convex optimization tools.",
      "url": "http://arxiv.org/abs/2509.04413v1",
      "published_time_eastern_timestamp": 1757007299.0
    },
    {
      "title": "Psychologically Enhanced AI Agents",
      "summary": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.",
      "url": "http://arxiv.org/abs/2509.04343v1",
      "published_time_eastern_timestamp": 1757001783.0
    },
    {
      "title": "Improving Robustness of AlphaZero Algorithms to Test-Time Environment\n  Changes",
      "summary": "The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub.",
      "url": "http://arxiv.org/abs/2509.04317v1",
      "published_time_eastern_timestamp": 1757000317.0
    },
    {
      "title": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn\n  Negotiation",
      "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
      "url": "http://arxiv.org/abs/2509.04310v1",
      "published_time_eastern_timestamp": 1756999438.0
    },
    {
      "title": "HumAIne-Chatbot: Real-Time Personalized Conversational AI via\n  Reinforcement Learning",
      "summary": "Current conversational AI systems often provide generic, one-size-fits-all\ninteractions that overlook individual user characteristics and lack adaptive\ndialogue management. To address this gap, we introduce\n\\textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes\nresponses through a novel user profiling framework. The system is pre-trained\non a diverse set of GPT-generated virtual personas to establish a broad prior\nover user types. During live interactions, an online reinforcement learning\nagent refines per-user models by combining implicit signals (e.g. typing speed,\nsentiment, engagement duration) with explicit feedback (e.g., likes and\ndislikes). This profile dynamically informs the chatbot dialogue policy,\nenabling real-time adaptation of both content and style. To evaluate the\nsystem, we performed controlled experiments with 50 synthetic personas in\nmultiple conversation domains. The results showed consistent improvements in\nuser satisfaction, personalization accuracy, and task achievement when\npersonalization features were enabled. Statistical analysis confirmed\nsignificant differences between personalized and nonpersonalized conditions,\nwith large effect sizes across key metrics. These findings highlight the\neffectiveness of AI-driven user profiling and provide a strong foundation for\nfuture real-world validation.",
      "url": "http://arxiv.org/abs/2509.04303v1",
      "published_time_eastern_timestamp": 1756998998.0
    },
    {
      "title": "Reinforced Data-Driven Estimation for Spectral Properties of Koopman\n  Semigroup in Stochastic Dynamical Systems",
      "summary": "Analyzing the spectral properties of the Koopman operator is crucial for\nunderstanding and predicting the behavior of complex stochastic dynamical\nsystems. However, the accuracy of data-driven estimation methods, such as\nExtended Dynamic Mode Decomposition (EDMD) and its variants, are heavily\ndependent on the quality and location of the sampled trajectory data. This\npaper introduces a novel framework, Reinforced Stochastic Dynamic Mode\nDecomposition, which integrates Reinforcement Learning (RL) with Stochastic\nDynamic Mode Decomposition (SDMD) to automatically guide the data collection\nprocess in stochastic dynamical systems. We frame the optimal sampling strategy\nas an RL problem, where an agent learns a policy to select trajectory initial\nconditions. The agent is guided by a reward signal based on \\emph{spectral\nconsistency}, that is a measure of how well the estimated Koopman eigenpairs\ndescribe the system's evolution balanced with an exploration bonus to ensure\ncomprehensive coverage of the state space. We demonstrate the effectiveness of\nour approach using Bandit algorithm, Deep Q-Network (DQN), and Proximal Policy\nOptimization (PPO) algorithms on canonical systems including the double-well\npotential, the stochastic Duffing oscillator and the FitzHugh-Nagumo model. Our\nresults show that the RL agent automatically discovers dynamically significant\nregions without any prior knowledge of the system. Rigorous theoretical\nanalysis establishes convergence guarantees for the proposed algorithms,\ndirectly linking the final estimation accuracy to the quality of the learned\nsampling policy. Our work presents a robust, automated methodology for the\nefficient spectral analysis of complex stochastic systems.",
      "url": "http://arxiv.org/abs/2509.04265v1",
      "published_time_eastern_timestamp": 1756996942.0
    },
    {
      "title": "Mediation, splitting and shellability in political structures modeled by\n  simplicial complexes",
      "summary": "Modeling political structures by simplicial complexes, we investigate whether\nintroducing a mediator into a substructure increases or decreases the stability\nof the overall structure. We prove theorems that quantify the stability of a\npolitical structure when $n$ mediators are introduced, either one by one or\nsimultaneously. We also examine how the stability is affected when a single\nagent is split into two. In addition, stability is expressed in terms of the\n$h$-vector, and special attention is given to a class of political structures\nmodeled by shellable simplicial complexes. In the latter context, we analyze\nweighted political structures and examples of political structures modeled by\nindependence complexes of graphs.",
      "url": "http://arxiv.org/abs/2509.04249v1",
      "published_time_eastern_timestamp": 1756995800.0
    },
    {
      "title": "Multi-Spectroscopic Method to Quantify Rapid Decomposition of an\n  Organophosphate Simulant Using Reactive Materials as a Function of Metal\n  Powder Chemistry and Temperature",
      "summary": "The development of advanced diagnostic systems to measure and optimize\nemerging energetic material performance is critical for the defeat of Chemical\nWarfare Agents (CWA). This study presents an integrated multi-spectroscopic\napproach to monitor the interaction between a CWA simulant, Diisopropyl Methyl\nPhosphonate (DIMP), and combusting composite metal particles. A custom benchtop\nPolygonal Rotating Mirror Infrared Spectrometer (PRiMIRS), equipped with a\ncustomizable experimental chamber, is employed to observe DIMP decomposition.\nTunable Diode Laser Absorption Spectroscopy (TDLAS) is used to measure\npath-averaged gas temperature profiles during combustion. In the experiment,\nthe chamber is preheated to evaporate liquid DIMP. Various composite metal\npowders (Al-8Mg):3Zr, (Al-8Mg):Zr, 2(Al-8Mg):Zr, and 4(Al-8Mg):Zr are placed on\na stainless steel mount and ignited using 3Al-2Ni sputter-deposited nanolayered\nfoils. The combusting metal particles mix with the DIMP vapor, initiating\nchemical and thermal interactions. PRiMIRS captures DIMP spectral evolution,\nwhile TDLAS simultaneously monitors gas temperature. A spectral defeat\nparameter was developed to enable quantitative real-time assessment of the DIMP\ndestruction. It uses infrared light absorption by both from DIMP and its\nimmediate decomposition products Isopropyl Methyl Phosphonate (IMP) and\nIsopropyl Alcohol (IPA). Fourier Transform Infrared Spectroscopy (FTIR) serves\nas a secondary verification tool quantifying the decomposition products over\nextended timeframes, and Transmission Electron Microscopy (TEM) confirms the\nexpected metal oxide dispersion within the reaction space. This study reports\nvariability in DIMP defeat as a function of metal powder stoichiometry, metal\npowder loading, and path-averaged gas temperature profiles, offering critical\ninsights into optimizing reactive materials for effective CWA neutralization.",
      "url": "http://arxiv.org/abs/2509.04236v1",
      "published_time_eastern_timestamp": 1756995071.0
    },
    {
      "title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across\n  Enterprise Workflows",
      "summary": "The emergence of large language models (LLMs) has introduced a new paradigm\nin automation: LLM agents or Agentic Automation with Computer Use (AACU).\nUnlike traditional Robotic Process Automation (RPA), which relies on rule-based\nworkflows and scripting, AACU enables intelligent agents to perform tasks\nthrough natural language instructions and autonomous interaction with user\ninterfaces. This study investigates whether AACU can serve as a viable\nalternative to RPA in enterprise workflow automation. We conducted controlled\nexperiments across three standard RPA challenges data entry, monitoring, and\ndocument extraction comparing RPA (via UiPath) and AACU (via Anthropic's\nComputer Use Agent) in terms of speed, reliability, and development effort.\nResults indicate that RPA outperforms AACU in execution speed and reliability,\nparticularly in repetitive, stable environments. However, AACU significantly\nreduces development time and adapts more flexibly to dynamic interfaces. While\ncurrent AACU implementations are not yet production-ready, their promise in\nrapid prototyping and lightweight automation is evident. Future research should\nexplore multi-agent orchestration, hybrid RPA-AACU architectures, and more\nrobust evaluation across industries and platforms.",
      "url": "http://arxiv.org/abs/2509.04198v1",
      "published_time_eastern_timestamp": 1756992164.0
    },
    {
      "title": "Batched Stochastic Matching Bandits",
      "summary": "In this study, we introduce a novel bandit framework for stochastic matching\nbased on the Multi-nomial Logit (MNL) choice model. In our setting, $N$ agents\non one side are assigned to $K$ arms on the other side, where each arm\nstochastically selects an agent from its assigned pool according to an unknown\npreference and yields a corresponding reward. The objective is to minimize\nregret by maximizing the cumulative revenue from successful matches across all\nagents. This task requires solving a combinatorial optimization problem based\non estimated preferences, which is NP-hard and leads a naive approach to incur\na computational cost of $O(K^N)$ per round. To address this challenge, we\npropose batched algorithms that limit the frequency of matching updates,\nthereby reducing the amortized computational cost (i.e., the average cost per\nround) to $O(1)$ while still achieving a regret bound of $\\tilde{O}(\\sqrt{T})$.",
      "url": "http://arxiv.org/abs/2509.04194v1",
      "published_time_eastern_timestamp": 1756991792.0
    },
    {
      "title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions",
      "summary": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.",
      "url": "http://arxiv.org/abs/2509.04183v1",
      "published_time_eastern_timestamp": 1756990764.0
    },
    {
      "title": "TAGAL: Tabular Data Generation using Agentic LLM Methods",
      "summary": "The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods.",
      "url": "http://arxiv.org/abs/2509.04152v1",
      "published_time_eastern_timestamp": 1756988714.0
    },
    {
      "title": "The evolution of trust as a cognitive shortcut in repeated interactions",
      "summary": "Trust is often thought to increase cooperation. However, game-theoretic\nmodels often fail to distinguish between cooperative behaviour and trust. This\nmakes it difficult to measure trust and determine its effect in different\nsocial dilemmas. We address this here by formalising trust as a cognitive\nshortcut in repeated games. This functions by avoiding checking a partner's\nactions once a threshold level of cooperativeness has been observed. We\nconsider trust-based strategies that implement this heuristic, and\nsystematically analyse their evolution across the space of two-player symmetric\nsocial dilemma games. We find that where it is costly to check whether another\nagent's actions were cooperative, as is the case in many real-world settings,\nthen trust-based strategies can outcompete standard reciprocal strategies such\nas Tit-for-Tat in many social dilemmas. Moreover, the presence of trust\nincreases the overall level of cooperation in the population, especially in\ncases where agents can make unintentional errors in their actions. This occurs\neven in the presence of strategies designed to build and then exploit trust.\nOverall, our results demonstrate the individual adaptive benefit to an agent of\nusing a trust heuristic, and provide a formal theory for how trust can promote\ncooperation in different types of social interaction. We discuss the\nimplications of this for interactions between humans and artificial\nintelligence agents.",
      "url": "http://arxiv.org/abs/2509.04143v1",
      "published_time_eastern_timestamp": 1756988018.0
    },
    {
      "title": "Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker",
      "summary": "In the game of poker, being unpredictable, or bluffing, is an essential\nskill. When humans play poker, they bluff. However, most works on\ncomputer-poker focus on performance metrics such as win rates, while bluffing\nis overlooked. In this paper we study whether two popular algorithms, DQN\n(based on reinforcement learning) and CFR (based on game theory), exhibit\nbluffing behavior in Leduc Hold'em, a simplified version of poker. We designed\nan experiment where we let the DQN and CFR agent play against each other while\nwe log their actions. We find that both DQN and CFR exhibit bluffing behavior,\nbut they do so in different ways. Although both attempt to perform bluffs at\ndifferent rates, the percentage of successful bluffs (where the opponent folds)\nis roughly the same. This suggests that bluffing is an essential aspect of the\ngame, not of the algorithm. Future work should look at different bluffing\nstyles and at the full game of poker. Code at\nhttps://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.",
      "url": "http://arxiv.org/abs/2509.04125v1",
      "published_time_eastern_timestamp": 1756986024.0
    },
    {
      "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue",
      "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.",
      "url": "http://arxiv.org/abs/2509.04104v1",
      "published_time_eastern_timestamp": 1756984047.0
    },
    {
      "title": "Hybrid Reinforcement Learning and Search for Flight Trajectory Planning",
      "summary": "This paper explores the combination of Reinforcement Learning (RL) and\nsearch-based path planners to speed up the optimization of flight paths for\nairliners, where in case of emergency a fast route re-calculation can be\ncrucial. The fundamental idea is to train an RL Agent to pre-compute\nnear-optimal paths based on location and atmospheric data and use those at\nruntime to constrain the underlying path planning solver and find a solution\nwithin a certain distance from the initial guess. The approach effectively\nreduces the size of the solver's search space, significantly speeding up route\noptimization. Although global optimality is not guaranteed, empirical results\nconducted with Airbus aircraft's performance models show that fuel consumption\nremains nearly identical to that of an unconstrained solver, with deviations\ntypically within 1%. At the same time, computation speed can be improved by up\nto 50% as compared to using a conventional solver alone.",
      "url": "http://arxiv.org/abs/2509.04100v1",
      "published_time_eastern_timestamp": 1756983703.0
    },
    {
      "title": "Gromov-Wasserstein and optimal transport: from assignment problems to\n  probabilistic numeric",
      "summary": "The assignment problem, a cornerstone of operations research, seeks an\noptimal one-to-one mapping between agents and tasks to minimize total cost.\nThis work traces its evolution from classical formulations and algorithms to\nmodern optimal transport (OT) theory, positioning the Quadratic Assignment\nProblem (QAP) and related structural matching tasks within this framework. We\nconnect the linear assignment problem to Monge's transport problem,\nKantorovich's relaxation, and Wasserstein distances, then extend to cases where\nsource and target lie in different metric-measure spaces requiring\nGromov-Wasserstein (GW) distances. GW formulations, including the fused GW\nvariant that integrates structural and feature information, naturally address\nQAP-like problems by optimizing alignment based on both intra-domain distances\nand cross-domain attributes. Applications include graph matching, keypoint\ncorrespondence, and feature-based assignments. We present exact solvers,\nGenetic Algorithms (GA), and multiple GW variants, including a proposed\nmulti-initialization strategy (GW-MultiInit) that mitigates the risk of getting\nstuck in local optima alongside entropic Sinkhorn-based approximations and\nfused GW. Computational experiments on capacitated QAP instances show that\nGW-MultiInit consistently achieves near-optimal solutions and scales\nefficiently to large problems where exact methods become impractical, while\nparameterized EGW and FGW variants provide flexible trade-offs between accuracy\nand runtime. Our findings provide theoretical foundations, computational\ninsights, and practical guidelines for applying OT and GW methods to QAP and\nother real-world matching problems, such as those in machine learning and\nlogistics.",
      "url": "http://arxiv.org/abs/2509.04089v1",
      "published_time_eastern_timestamp": 1756982670.0
    },
    {
      "title": "CoT-Space: A Theoretical Framework for Internal Slow-Thinking via\n  Reinforcement Learning",
      "summary": "Reinforcement Learning (RL) has become a pivotal approach for enhancing the\nreasoning capabilities of Large Language Models (LLMs). However, a significant\ntheoretical gap persists, as traditional token-level RL frameworks fail to\nalign with the reasoning-level nature of complex, multi-step thought processes\nlike Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,\na novel theoretical framework that recasts LLM reasoning from a discrete\ntoken-prediction task to an optimization process within a continuous,\nreasoning-level semantic space. By analyzing this process from both a noise\nperspective and a risk perspective, we demonstrate that the convergence to an\noptimal CoT length is a natural consequence of the fundamental trade-off\nbetween underfitting and overfitting. Furthermore, extensive experiments\nprovide strong empirical validation for our theoretical findings. Our framework\nnot only provides a coherent explanation for empirical phenomena such as\noverthinking but also offers a solid theoretical foundation to guide the future\ndevelopment of more effective and generalizable reasoning agents.",
      "url": "http://arxiv.org/abs/2509.04027v1",
      "published_time_eastern_timestamp": 1756976536.0
    },
    {
      "title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility\n  for Resource-Efficient LLM Agent",
      "summary": "Large language model (LLM) agents achieve impressive single-task performance\nbut commonly exhibit repeated failures, inefficient exploration, and limited\ncross-task adaptability. Existing reflective strategies (e.g., Reflexion,\nReAct) improve per-episode behavior but typically produce ephemeral,\ntask-specific traces that are not reused across tasks. Reinforcement-learning\nbased alternatives can produce transferable policies but require substantial\nparameter updates and compute. In this work we introduce Meta-Policy Reflexion\n(MPR): a hybrid framework that consolidates LLM-generated reflections into a\nstructured, predicate-like Meta-Policy Memory (MPM) and applies that memory at\ninference time through two complementary mechanisms soft memory-guided decoding\nand hard rule admissibility checks(HAC). MPR (i) externalizes reusable\ncorrective knowledge without model weight updates, (ii) enforces domain\nconstraints to reduce unsafe or invalid actions, and (iii) retains the\nadaptability of language-based reflection. We formalize the MPM representation,\npresent algorithms for update and decoding, and validate the approach in a\ntext-based agent environment following the experimental protocol described in\nthe provided implementation (AlfWorld-based). Empirical results reported in the\nsupplied material indicate consistent gains in execution accuracy and\nrobustness when compared to Reflexion baselines; rule admissibility further\nimproves stability. We analyze mechanisms that explain these gains, discuss\nscalability and failure modes, and outline future directions for multimodal and\nmulti?agent extensions.",
      "url": "http://arxiv.org/abs/2509.03990v1",
      "published_time_eastern_timestamp": 1756973919.0
    },
    {
      "title": "Real-time adaptive quantum error correction by model-free multi-agent\n  learning",
      "summary": "Can we build efficient Quantum Error Correction (QEC) that adapts on the fly\nto time-varying noise? In this work we say yes, and show how. We present a two\nlevel framework based on Reinforcement Learning (RL) that learns to correct\neven non-stationary errors from scratch. At the first level we take advantage\nof model-free Multi-Agent RL (MARL) to automatically discover full QEC cycle --\nlogical state encoding, stabilizer measurements, and recovery -- without any\nprior system knowledge, relying only on orthogonality conditions. Leveraging\nthe stabilizer formalism, we demonstrate that our MARL framework can discover\nnovel QEC codes tailored for multi-level quantum architectures. At the second\nlevel we introduce BRAVE (Bandit Retraining for Adaptive Variational Error\ncorrection), an efficient algorithm that tunes the variational layer on the fly\nto change the physical basis of the errors, adapting the QEC code to\ntime-varying noise while minimizing computational overhead and reducing the\nnumber of retraining steps. By combining our MARL and BRAVE approaches and\ntesting them on multi-level systems subjected to competing bit- and phase-flip\nerrors over time across diverse scenarios, we observed an improvement in\nlogical fidelity by more than an order of magnitude -- under time-dependent\nnoise channels -- compared to conventional QEC schemes.",
      "url": "http://arxiv.org/abs/2509.03974v1",
      "published_time_eastern_timestamp": 1756972882.0
    }
  ]
}
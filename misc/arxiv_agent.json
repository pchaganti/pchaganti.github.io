{
  "last_updated": "2025-05-08T17:11:14.726794-04:00",
  "papers": [
    {
      "title": "Implicitly Aligning Humans and Autonomous Agents through Shared Task\n  Abstractions",
      "summary": "In collaborative tasks, autonomous agents fall short of humans in their\ncapability to quickly adapt to new and unfamiliar teammates. We posit that a\nlimiting factor for zero-shot coordination is the lack of shared task\nabstractions, a mechanism humans rely on to implicitly align with teammates. To\naddress this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework\nleveraging hierarchical reinforcement learning to mimic the structured approach\nhumans use in collaboration. We evaluate HA$^2$ in the Overcooked environment,\ndemonstrating statistically significant improvement over existing baselines\nwhen paired with both unseen agents and humans, providing better resilience to\nenvironmental shifts, and outperforming all state-of-the-art methods.",
      "url": "http://arxiv.org/abs/2505.04579v1",
      "published_time_eastern_timestamp": 1746638357.0
    },
    {
      "title": "Optimal Deterministic Rendezvous in Labeled Lines",
      "summary": "In a rendezvous task, a set of mobile agents dispersed in a network have to\ngather at an arbitrary common site. We consider the rendezvous problem on the\ninfinite labeled line, with $2$ initially asleep agents, without communication,\nand a synchronous notion of time. Nodes are labeled with unique positive\nintegers. The initial distance between the two agents is denoted by $D$. Time\nis divided into rounds. We count time from when an agent first wakes up, and\ndenote by $\\tau$ the delay between the agents' wake up times. If awake in a\ngiven round $T$, an agent has three options: stay at its current node $v$, take\nport $0$, or take port $1$. If it decides to stay, the agent is still at node\n$v$ in round $T+1$. Otherwise, it is at one of the two neighbors of $v$ on the\nline, based on the port it chose. The agents achieve rendezvous in $T$ rounds\nif they are at the same node in round $T$. We aim for a deterministic algorithm\nfor this task.\n  The problem was recently considered by Miller and Pelc [DISC 2023]. With\n$\\ell_{\\max}$ the largest label of the two starting nodes, they showed that no\nalgorithm can guarantee rendezvous in $o(D \\log^* \\ell_{\\max})$ rounds. The\nlower bound follows from a connection with the LOCAL model of distributed\ncomputing, and holds even if the agents are guaranteed simultaneous wake-up\n($\\tau = 0$) and are given $D$ as advice. Miller and Pelc also gave an\nalgorithm of optimal matching complexity $O(D \\log^* \\ell_{\\max})$ when $D$ is\nknown to the agents, but only obtained the higher bound of $O(D^2 (\\log^*\n\\ell_{\\max})^3)$ when $D$ is unknown.\n  We improve this second complexity to a tight $O(D \\log^* \\ell_{\\max})$. In\nfact, our algorithm achieves rendezvous in $O(D \\log^* \\ell_{\\min})$ rounds,\nwhere $\\ell_{\\min}$ is the smallest label within distance $O(D)$ of the two\nstarting positions.",
      "url": "http://arxiv.org/abs/2505.04564v1",
      "published_time_eastern_timestamp": 1746637012.0
    },
    {
      "title": "Qualitative Analysis of $Ï‰$-Regular Objectives on Robust MDPs",
      "summary": "Robust Markov Decision Processes (RMDPs) generalize classical MDPs that\nconsider uncertainties in transition probabilities by defining a set of\npossible transition functions. An objective is a set of runs (or infinite\ntrajectories) of the RMDP, and the value for an objective is the maximal\nprobability that the agent can guarantee against the adversarial environment.\nWe consider (a) reachability objectives, where given a target set of states,\nthe goal is to eventually arrive at one of them; and (b) parity objectives,\nwhich are a canonical representation for $\\omega$-regular objectives. The\nqualitative analysis problem asks whether the objective can be ensured with\nprobability 1.\n  In this work, we study the qualitative problem for reachability and parity\nobjectives on RMDPs without making any assumption over the structures of the\nRMDPs, e.g., unichain or aperiodic. Our contributions are twofold. We first\npresent efficient algorithms with oracle access to uncertainty sets that solve\nqualitative problems of reachability and parity objectives. We then report\nexperimental results demonstrating the effectiveness of our oracle-based\napproach on classical RMDP examples from the literature scaling up to thousands\nof states.",
      "url": "http://arxiv.org/abs/2505.04539v1",
      "published_time_eastern_timestamp": 1746634540.0
    },
    {
      "title": "Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal\n  Problem-Solving",
      "summary": "As a seemingly self-explanatory task, problem-solving has been a significant\ncomponent of science and engineering. However, a general yet concrete\nformulation of problem-solving itself is missing. With the recent development\nof AI-based problem-solving agents, the demand for process-level verifiability\nis rapidly increasing yet underexplored. To fill these gaps, we present a\nprincipled formulation of problem-solving as a deterministic Markov decision\nprocess; a novel framework, FPS (Formal Problem-Solving), which utilizes\nexisting FTP (formal theorem proving) environments to perform process-verified\nproblem-solving; and D-FPS (Deductive FPS), decoupling solving and answer\nverification for better human-alignment. The expressiveness, soundness and\ncompleteness of the frameworks are proven. We construct three benchmarks on\nproblem-solving: FormalMath500, a formalization of a subset of the MATH500\nbenchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP\nbenchmarks MiniF2F and PutnamBench. For faithful, interpretable, and\nhuman-aligned evaluation, we propose RPE (Restricted Propositional\nEquivalence), a symbolic approach to determine the correctness of answers by\nformal verification. We evaluate four prevalent FTP models and two prompting\nmethods as baselines, solving at most 23.77% of FormalMath500, 27.47% of\nMiniF2F-Solving, and 0.31% of PutnamBench-Solving.",
      "url": "http://arxiv.org/abs/2505.04528v1",
      "published_time_eastern_timestamp": 1746633734.0
    },
    {
      "title": "RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential\n  Neural Style Generation",
      "summary": "Arbitrary style transfer aims to apply the style of any given artistic image\nto another content image. Still, existing deep learning-based methods often\nrequire significant computational costs to generate diverse stylized results.\nMotivated by this, we propose a novel reinforcement learning-based framework\nfor arbitrary style transfer RLMiniStyler. This framework leverages a unified\nreinforcement learning policy to iteratively guide the style transfer process\nby exploring and exploiting stylization feedback, generating smooth sequences\nof stylized results while achieving model lightweight. Furthermore, we\nintroduce an uncertainty-aware multi-task learning strategy that automatically\nadjusts loss weights to adapt to the content and style balance requirements at\ndifferent training stages, thereby accelerating model convergence. Through a\nseries of experiments across image various resolutions, we have validated the\nadvantages of RLMiniStyler over other state-of-the-art methods in generating\nhigh-quality, diverse artistic image sequences at a lower cost. Codes are\navailable at https://github.com/fengxiaoming520/RLMiniStyler.",
      "url": "http://arxiv.org/abs/2505.04424v1",
      "published_time_eastern_timestamp": 1746626262.0
    },
    {
      "title": "Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and\n  Performance in Mixed Urban Traffic",
      "summary": "Transportation systems have long been shaped by complexity and heterogeneity,\ndriven by the interdependency of agent actions and traffic outcomes. The\ndeployment of automated vehicles (AVs) in such systems introduces a new\nchallenge: achieving consensus across safety, interaction quality, and traffic\nperformance. In this work, we position consensus as a fundamental property of\nthe traffic system and aim to quantify it. We use high-resolution trajectory\ndata from the Third Generation Simulation (TGSIM) dataset to empirically\nanalyze AV and human-driven vehicle (HDV) behavior at a signalized urban\nintersection and around vulnerable road users (VRUs). Key metrics, including\nTime-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns,\nheadways, and string stability, are evaluated across the three performance\ndimensions. Results show that full consensus across safety, interaction, and\nperformance is rare, with only 1.63% of AV-VRU interaction frames meeting all\nthree conditions. These findings highlight the need for AV models that\nexplicitly balance multi-dimensional performance in mixed-traffic environments.\nFull reproducibility is supported via our open-source codebase on\nhttps://github.com/wissamkontar/Consensus-AV-Analysis.",
      "url": "http://arxiv.org/abs/2505.04379v1",
      "published_time_eastern_timestamp": 1746622799.0
    },
    {
      "title": "Extending a Quantum Reinforcement Learning Exploration Policy with Flags\n  to Connect Four",
      "summary": "Action selection based on flags is a Reinforcement Learning (RL) exploration\npolicy that improves the exploration of the state space through the use of\nflags, which can identify the most promising actions to take in each state. The\nquantum counterpart of this exploration policy further improves upon this by\ntaking advantage of a quadratic speedup for sampling flagged actions. This\napproach has already been successfully employed for the game of Checkers. In\nthis work, we describe the application of this method to the context of Connect\nFour, in order to study its performance in a different setting, which can lead\nto a better generalization of the technique. We also kept track of a metric\nthat wasn't taken into account in previous work: the average number of\niterations to obtain a flagged action. Since going second is a significant\ndisadvantage in Connect Four, we also had the intent of exploring how this more\ncomplex scenario would impact the performance of our approach. The experiments\ninvolved training and testing classical and quantum RL agents that played\neither going first or going second against a Randomized Negamax opponent. The\nresults showed that both flagged exploration policies were clearly superior to\na simple epsilon-greedy policy. Furthermore, the quantum agents did in fact\nsample flagged actions in less iterations. Despite obtaining tagged actions\nmore consistently, the win rates between the classical and quantum versions of\nthe approach were identical, which could be due to the simplicity of the\ntraining scenario chosen.",
      "url": "http://arxiv.org/abs/2505.04371v1",
      "published_time_eastern_timestamp": 1746621899.0
    },
    {
      "title": "Benchmarking LLMs' Swarm intelligence",
      "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
      "url": "http://arxiv.org/abs/2505.04364v1",
      "published_time_eastern_timestamp": 1746621121.0
    },
    {
      "title": "Optimization Problem Solving Can Transition to Evolutionary Agentic\n  Workflows",
      "summary": "This position paper argues that optimization problem solving can transition\nfrom expert-dependent to evolutionary agentic workflows. Traditional\noptimization practices rely on human specialists for problem formulation,\nalgorithm selection, and hyperparameter tuning, creating bottlenecks that\nimpede industrial adoption of cutting-edge methods. We contend that an\nevolutionary agentic workflow, powered by foundation models and evolutionary\nsearch, can autonomously navigate the optimization space, comprising problem,\nformulation, algorithm, and hyperparameter spaces. Through case studies in\ncloud resource scheduling and ADMM parameter adaptation, we demonstrate how\nthis approach can bridge the gap between academic innovation and industrial\nimplementation. Our position challenges the status quo of human-centric\noptimization workflows and advocates for a more scalable, adaptive approach to\nsolving real-world optimization problems.",
      "url": "http://arxiv.org/abs/2505.04354v1",
      "published_time_eastern_timestamp": 1746619669.0
    },
    {
      "title": "Resist Platform-Controlled AI Agents and Champion User-Centric Agent\n  Advocates",
      "summary": "Language model agents could reshape how users navigate and act in digital\nenvironments. If controlled by platform companies -- either those that already\ndominate online search, communication, and commerce, or those vying to replace\nthem -- platform agents could intensify surveillance, exacerbate user lock-in,\nand further entrench the incumbent digital giants. This position paper argues\nthat to resist the undesirable effects of platform agents, we should champion\nagent advocates -- agents that are controlled by users, serve the interests of\nusers, and preserve user autonomy and choice. We identify key interventions to\nenable agent advocates: ensuring public access to compute, developing\ninteroperability protocols and safety standards, and implementing appropriate\nmarket regulations.",
      "url": "http://arxiv.org/abs/2505.04345v1",
      "published_time_eastern_timestamp": 1746618338.0
    },
    {
      "title": "Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning",
      "summary": "DBSCAN, a well-known density-based clustering algorithm, has gained\nwidespread popularity and usage due to its effectiveness in identifying\nclusters of arbitrary shapes and handling noisy data. However, it encounters\nchallenges in producing satisfactory cluster results when confronted with\ndatasets of varying density scales, a common scenario in real-world\napplications. In this paper, we propose a novel Adaptive and Robust DBSCAN with\nMulti-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First,\nwe model the initial dataset as a two-level encoding tree and categorize the\ndata vertices into distinct density partitions according to the information\nuncertainty determined in the encoding tree. Each partition is then assigned to\nan agent to find the best clustering parameters without manual assistance. The\nallocation is density-adaptive, enabling AR-DBSCAN to effectively handle\ndiverse density distributions within the dataset by utilizing distinct agents\nfor different partitions. Second, a multi-agent deep reinforcement learning\nguided automatic parameter searching process is designed. The process of\nadjusting the parameter search direction by perceiving the clustering\nenvironment is modeled as a Markov decision process. Using a weakly-supervised\nreward training policy network, each agent adaptively learns the optimal\nclustering parameters by interacting with the clusters. Third, a recursive\nsearch mechanism adaptable to the data's scale is presented, enabling efficient\nand controlled exploration of large parameter spaces. Extensive experiments are\nconducted on nine artificial datasets and a real-world dataset. The results of\noffline and online tasks show that AR-DBSCAN not only improves clustering\naccuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively,\nbut also is capable of robustly finding dominant parameters.",
      "url": "http://arxiv.org/abs/2505.04339v1",
      "published_time_eastern_timestamp": 1746617843.0
    },
    {
      "title": "Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play\n  Reinforcement Learning",
      "summary": "In this paper, we tackle the problem of learning to play 3v3 multi-drone\nvolleyball, a new embodied competitive task that requires both high-level\nstrategic coordination and low-level agile control. The task is turn-based,\nmulti-agent, and physically grounded, posing significant challenges due to its\nlong-horizon dependencies, tight inter-agent coupling, and the underactuated\ndynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play\n(HCSP), a hierarchical reinforcement learning framework that separates\ncentralized high-level strategic decision-making from decentralized low-level\nmotion control. We design a three-stage population-based training pipeline to\nenable both strategy and skill to emerge from scratch without expert\ndemonstrations: (I) training diverse low-level skills, (II) learning high-level\nstrategy via self-play with fixed low-level controllers, and (III) joint\nfine-tuning through co-self-play. Experiments show that HCSP achieves superior\nperformance, outperforming non-hierarchical self-play and rule-based\nhierarchical baselines with an average 82.9\\% win rate and a 71.5\\% win rate\nagainst the two-stage variant. Moreover, co-self-play leads to emergent team\nbehaviors such as role switching and coordinated formations, demonstrating the\neffectiveness of our hierarchical design and training scheme.",
      "url": "http://arxiv.org/abs/2505.04317v1",
      "published_time_eastern_timestamp": 1746615876.0
    },
    {
      "title": "PPO-ACT: Proximal Policy Optimization with Adversarial Curriculum\n  Transfer for Spatial Public Goods Games",
      "summary": "This study investigates cooperation evolution mechanisms in the spatial\npublic goods game. A novel deep reinforcement learning framework, Proximal\nPolicy Optimization with Adversarial Curriculum Transfer (PPO-ACT), is proposed\nto model agent strategy optimization in dynamic environments. Traditional\nevolutionary game models frequently exhibit limitations in modeling long-term\ndecision-making processes. Deep reinforcement learning effectively addresses\nthis limitation by bridging policy gradient methods with evolutionary game\ntheory. Our study pioneers the application of proximal policy optimization's\ncontinuous strategy optimization capability to public goods games through a\ntwo-stage adversarial curriculum transfer training paradigm. The experimental\nresults show that PPO-ACT performs better in critical enhancement factor\nregimes. Compared to conventional standard proximal policy optimization\nmethods, Q-learning and Fermi update rules, achieve earlier cooperation phase\ntransitions and maintain stable cooperative equilibria. This framework exhibits\nbetter robustness when handling challenging scenarios like all-defector initial\nconditions. Systematic comparisons reveal the unique advantage of policy\ngradient methods in population-scale cooperation, i.e., achieving\nspatiotemporal payoff coordination through value function propagation. Our work\nprovides a new computational framework for studying cooperation emergence in\ncomplex systems, algorithmically validating the punishment promotes cooperation\nhypothesis while offering methodological insights for multi-agent system\nstrategy design.",
      "url": "http://arxiv.org/abs/2505.04302v1",
      "published_time_eastern_timestamp": 1746613255.0
    },
    {
      "title": "CompileAgent: Automated Real-World Repo-Level Compilation with\n  Tool-Integrated LLM-based Agent System",
      "summary": "With open-source projects growing in size and complexity, manual compilation\nbecomes tedious and error-prone, highlighting the need for automation to\nimprove efficiency and accuracy. However, the complexity of compilation\ninstruction search and error resolution makes automatic compilation\nchallenging. Inspired by the success of LLM-based agents in various fields, we\npropose CompileAgent, the first LLM-based agent framework dedicated to\nrepo-level compilation. CompileAgent integrates five tools and a flow-based\nagent strategy, enabling interaction with software artifacts for compilation\ninstruction search and error resolution. To measure the effectiveness of our\nmethod, we design a public repo-level benchmark CompileAgentBench, and we also\ndesign two baselines for comparison by combining two compilation-friendly\nschemes. The performance on this benchmark shows that our method significantly\nimproves the compilation success rate, ranging from 10% to 71%. Meanwhile, we\nevaluate the performance of CompileAgent under different agent strategies and\nverify the effectiveness of the flow-based strategy. Additionally, we emphasize\nthe scalability of CompileAgent, further expanding its application prospects.",
      "url": "http://arxiv.org/abs/2505.04254v1",
      "published_time_eastern_timestamp": 1746608354.0
    },
    {
      "title": "Facilitating Trustworthy Human-Agent Collaboration in LLM-based\n  Multi-Agent System oriented Software Engineering",
      "summary": "Multi-agent autonomous systems (MAS) are better at addressing challenges that\nspans across multiple domains than singular autonomous agents. This holds true\nwithin the field of software engineering (SE) as well. The state-of-the-art\nresearch on MAS within SE focuses on integrating LLMs at the core of autonomous\nagents to create LLM-based multi-agent autonomous (LMA) systems. However, the\nintroduction of LMA systems into SE brings a plethora of challenges. One of the\nmajor challenges is the strategic allocation of tasks between humans and the\nLMA system in a trustworthy manner. To address this challenge, a RACI-based\nframework is proposed in this work in progress article, along with\nimplementation guidelines and an example implementation of the framework. The\nproposed framework can facilitate efficient collaboration, ensure\naccountability, and mitigate potential risks associated with LLM-driven\nautomation while aligning with the Trustworthy AI guidelines. The future steps\nfor this work delineating the planned empirical validation method are also\npresented.",
      "url": "http://arxiv.org/abs/2505.04251v1",
      "published_time_eastern_timestamp": 1746608115.0
    },
    {
      "title": "Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving\n  in Smart Intersections",
      "summary": "Unsignalized intersections pose significant safety and efficiency challenges\ndue to complex traffic flows. This paper proposes a novel roadside unit\n(RSU)-centric cooperative driving system leveraging global perception and\nvehicle-to-infrastructure (V2I) communication. The core of the system is an\nRSU-based decision-making module using a two-stage hybrid reinforcement\nlearning (RL) framework. At first, policies are pre-trained offline using\nconservative Q-learning (CQL) combined with behavior cloning (BC) on collected\ndataset. Subsequently, these policies are fine-tuned in the simulation using\nmulti-agent proximal policy optimization (MAPPO), aligned with a self-attention\nmechanism to effectively solve inter-agent dependencies. RSUs perform real-time\ninference based on the trained models to realize vehicle control via V2I\ncommunications. Extensive experiments in CARLA environment demonstrate high\neffectiveness of the proposed system, by: \\textit{(i)} achieving failure rates\nbelow 0.03\\% in coordinating three connected and autonomous vehicles (CAVs)\nthrough complex intersection scenarios, significantly outperforming the\ntraditional Autoware control method, and \\textit{(ii)} exhibiting strong\nrobustness across varying numbers of controlled agents and shows promising\ngeneralization capabilities on other maps.",
      "url": "http://arxiv.org/abs/2505.04231v1",
      "published_time_eastern_timestamp": 1746606472.0
    },
    {
      "title": "AutoPatch: Multi-Agent Framework for Patching Real-World CVE\n  Vulnerabilities",
      "summary": "Large Language Models (LLMs) have emerged as promising tools in software\ndevelopment, enabling automated code generation and analysis. However, their\nknowledge is limited to a fixed cutoff date, making them prone to generating\ncode vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets\nis costly, and existing LLM-based approaches focus on oversimplified CWE\nexamples and require providing explicit bug locations to LLMs, limiting their\nability to patch complex real-world vulnerabilities. To address these\nlimitations, we propose AutoPatch, a multi-agent framework designed to patch\nvulnerable LLM-generated code, particularly those introduced after the LLMs'\nknowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG)\nwith a structured database of recently disclosed vulnerabilities, comprising\n525 code snippets derived from 75 high-severity CVEs across real-world systems\nsuch as the Linux kernel and Chrome. AutoPatch combines semantic and taint\nanalysis to identify the most relevant CVE and leverages enhanced\nChain-of-Thought (CoT) reasoning to construct enriched prompts for verification\nand patching. Our unified similarity model, which selects the most relevant\nvulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch\nattains 89.5 percent F1-score for vulnerability verification and 95.0 percent\naccuracy in patching, while being over 50x more cost-efficient than traditional\nfine-tuning approaches.",
      "url": "http://arxiv.org/abs/2505.04195v1",
      "published_time_eastern_timestamp": 1746604145.0
    },
    {
      "title": "Trajectory Entropy Reinforcement Learning for Predictable and Robust\n  Control",
      "summary": "Simplicity is a critical inductive bias for designing data-driven\ncontrollers, especially when robustness is important. Despite the impressive\nresults of deep reinforcement learning in complex control tasks, it is prone to\ncapturing intricate and spurious correlations between observations and actions,\nleading to failure under slight perturbations to the environment. To tackle\nthis problem, in this work we introduce a novel inductive bias towards simple\npolicies in reinforcement learning. The simplicity inductive bias is introduced\nby minimizing the entropy of entire action trajectories, corresponding to the\nnumber of bits required to describe information in action trajectories after\nthe agent observes state trajectories. Our reinforcement learning agent,\nTrajectory Entropy Reinforcement Learning, is optimized to minimize the\ntrajectory entropy while maximizing rewards. We show that the trajectory\nentropy can be effectively estimated by learning a variational parameterized\naction prediction model, and use the prediction model to construct an\ninformation-regularized reward function. Furthermore, we construct a practical\nalgorithm that enables the joint optimization of models, including the policy\nand the prediction model. Experimental evaluations on several high-dimensional\nlocomotion tasks show that our learned policies produce more cyclical and\nconsistent action trajectories, and achieve superior performance, and\nrobustness to noise and dynamic changes than the state-of-the-art.",
      "url": "http://arxiv.org/abs/2505.04193v1",
      "published_time_eastern_timestamp": 1746603689.0
    },
    {
      "title": "Optimization of Infectious Disease Intervention Measures Based on\n  Reinforcement Learning -- Empirical analysis based on UK COVID-19 epidemic\n  data",
      "summary": "Globally, the outbreaks of infectious diseases have exerted an extremely\nprofound and severe influence on health security and the economy. During the\ncritical phases of epidemics, devising effective intervention measures poses a\nsignificant challenge to both the academic and practical arenas. There is\nnumerous research based on reinforcement learning to optimize intervention\nmeasures of infectious diseases. Nevertheless, most of these efforts have been\nconfined within the differential equation based on infectious disease models.\nAlthough a limited number of studies have incorporated reinforcement learning\nmethodologies into individual-based infectious disease models, the models\nemployed therein have entailed simplifications and limitations, rendering it\nincapable of modeling the complexity and dynamics inherent in infectious\ndisease transmission. We establish a decision-making framework based on an\nindividual agent-based transmission model, utilizing reinforcement learning to\ncontinuously explore and develop a strategy function. The framework's validity\nis verified through both experimental and theoretical approaches. Covasim, a\ndetailed and widely used agent-based disease transmission model, was modified\nto support reinforcement learning research. We conduct an exhaustive\nexploration of the application efficacy of multiple algorithms across diverse\naction spaces. Furthermore, we conduct an innovative preliminary theoretical\nanalysis concerning the issue of \"time coverage\". The results of the experiment\nrobustly validate the effectiveness and feasibility of the methodological\nframework of this study. The coping strategies gleaned therefrom prove highly\nefficacious in suppressing the expansion of the epidemic scale and safeguarding\nthe stability of the economic system, thereby providing crucial reference\nperspectives for the formulation of global public health security strategies.",
      "url": "http://arxiv.org/abs/2505.04161v1",
      "published_time_eastern_timestamp": 1746599006.0
    },
    {
      "title": "Delegation and Participation in Decentralized Governance: An Epistemic\n  View",
      "summary": "We develop and apply epistemic tests to various decentralized governance\nmethods as well as to study the impact of participation. These tests probe the\nability to reach a correct outcome when there is one. We find that partial\nabstention is a strong governance method from an epistemic standpoint compared\nto alternatives such as various forms of ``transfer delegation\" in which voters\nexplicitly transfer some or all of their voting rights to others. We make a\nstronger case for multi-step transfer delegation than is present in previous\nwork but also demonstrate that transfer delegation has inherent epistemic\nweaknesses. We show that enhanced direct participation, voters exercising their\nown voting rights, can have a variety of epistemic impacts, some very negative.\nWe identify governance conditions under which additional direct participation\nis guaranteed to do no epistemic harm and is likely to increase the probability\nof making correct decisions. In light of the epistemic challenges of\nvoting-based decentralized governance, we consider the possible supplementary\nuse of prediction markets, auctions, and AI agents to improve outcomes. All\nthese results are significant because epistemic performance matters if entities\nsuch as DAOs (decentralized autonomous organizations) wish to compete with\norganizations that are more centralized.",
      "url": "http://arxiv.org/abs/2505.04136v1",
      "published_time_eastern_timestamp": 1746595230.0
    }
  ]
}
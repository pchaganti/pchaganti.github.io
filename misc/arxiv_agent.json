{
  "last_updated": "2025-07-14T14:18:28.515250-04:00",
  "papers": [
    {
      "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative\n  Models",
      "summary": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.",
      "url": "http://arxiv.org/abs/2507.08800v1",
      "published_time_eastern_timestamp": 1752256780.0
    },
    {
      "title": "SPLASH! Sample-efficient Preference-based inverse reinforcement learning\n  for Long-horizon Adversarial tasks from Suboptimal Hierarchical\n  demonstrations",
      "summary": "Inverse Reinforcement Learning (IRL) presents a powerful paradigm for\nlearning complex robotic tasks from human demonstrations. However, most\napproaches make the assumption that expert demonstrations are available, which\nis often not the case. Those that allow for suboptimality in the demonstrations\nare not designed for long-horizon goals or adversarial tasks. Many desirable\nrobot capabilities fall into one or both of these categories, thus highlighting\na critical shortcoming in the ability of IRL to produce field-ready robotic\nagents. We introduce Sample-efficient Preference-based inverse reinforcement\nlearning for Long-horizon Adversarial tasks from Suboptimal Hierarchical\ndemonstrations (SPLASH), which advances the state-of-the-art in learning from\nsuboptimal demonstrations to long-horizon and adversarial settings. We\nempirically validate SPLASH on a maritime capture-the-flag task in simulation,\nand demonstrate real-world applicability with sim-to-real translation\nexperiments on autonomous unmanned surface vehicles. We show that our proposed\nmethods allow SPLASH to significantly outperform the state-of-the-art in reward\nlearning from suboptimal demonstrations.",
      "url": "http://arxiv.org/abs/2507.08707v1",
      "published_time_eastern_timestamp": 1752249918.0
    },
    {
      "title": "elsciRL: Integrating Language Solutions into Reinforcement Learning\n  Problem Settings",
      "summary": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery.",
      "url": "http://arxiv.org/abs/2507.08705v1",
      "published_time_eastern_timestamp": 1752249744.0
    },
    {
      "title": "Introspection of Thought Helps AI Agents",
      "summary": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.",
      "url": "http://arxiv.org/abs/2507.08664v1",
      "published_time_eastern_timestamp": 1752246197.0
    },
    {
      "title": "Safe Deep Reinforcement Learning for Resource Allocation with Peak Age\n  of Information Violation Guarantees",
      "summary": "In Wireless Networked Control Systems (WNCSs), control and communication\nsystems must be co-designed due to their strong interdependence. This paper\npresents a novel optimization theory-based safe deep reinforcement learning\n(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction\nwhile optimizing performance, for the first time in the literature. The\napproach minimizes power consumption under key constraints, including Peak Age\nof Information (PAoI) violation probability, transmit power, and schedulability\nin the finite blocklength regime. PAoI violation probability is uniquely\nderived by combining stochastic maximum allowable transfer interval (MATI) and\nmaximum allowable packet delay (MAD) constraints in a multi-sensor network. The\nframework consists of two stages: optimization theory and safe DRL. The first\nstage derives optimality conditions to establish mathematical relationships\namong variables, simplifying and decomposing the problem. The second stage\nemploys a safe DRL model where a teacher-student framework guides the DRL agent\n(student). The control mechanism (teacher) evaluates compliance with system\nconstraints and suggests the nearest feasible action when needed. Extensive\nsimulations show that the proposed framework outperforms rule-based and other\noptimization theory based DRL benchmarks, achieving faster convergence, higher\nrewards, and greater stability.",
      "url": "http://arxiv.org/abs/2507.08653v1",
      "published_time_eastern_timestamp": 1752245857.0
    },
    {
      "title": "DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets\n  from Real-World Images",
      "summary": "Common knowledge indicates that the process of constructing image datasets\nusually depends on the time-intensive and inefficient method of manual\ncollection and annotation. Large models offer a solution via data generation.\nNonetheless, real-world data are obviously more valuable comparing to\nartificially intelligence generated data, particularly in constructing image\ndatasets. For this reason, we propose a novel method for auto-constructing\ndatasets from real-world images by a multiagent collaborative system, named as\nDatasetAgent. By coordinating four different agents equipped with Multi-modal\nLarge Language Models (MLLMs), as well as a tool package for image\noptimization, DatasetAgent is able to construct high-quality image datasets\naccording to user-specified requirements. In particular, two types of\nexperiments are conducted, including expanding existing datasets and creating\nnew ones from scratch, on a variety of open-source datasets. In both cases,\nmultiple image datasets constructed by DatasetAgent are used to train various\nvision models for image classification, object detection, and image\nsegmentation.",
      "url": "http://arxiv.org/abs/2507.08648v1",
      "published_time_eastern_timestamp": 1752245493.0
    },
    {
      "title": "OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations\n  for Multi-Camera 3D Perception",
      "summary": "Multi-view camera-based 3D perception can be conducted using bird's eye view\n(BEV) features obtained through perspective view-to-BEV transformations.\nSeveral studies have shown that the performance of these 3D perception methods\ncan be further enhanced by combining sequential BEV features obtained from\nmultiple camera frames. However, even after compensating for the ego-motion of\nan autonomous agent, the performance gain from temporal aggregation is limited\nwhen combining a large number of image frames. This limitation arises due to\ndynamic changes in BEV features over time caused by object motion. In this\npaper, we introduce a novel temporal 3D perception method called OnlineBEV,\nwhich combines BEV features over time using a recurrent structure. This\nstructure increases the effective number of combined features with minimal\nmemory usage. However, it is critical to spatially align the features over time\nto maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion\nNetwork (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion\nfeatures from consecutive BEV frames and dynamically aligns historical BEV\nfeatures with current ones using these motion features. To enforce temporal\nfeature alignment explicitly, we use Temporal Consistency Learning Loss, which\ncaptures discrepancies between historical and target BEV features. Experiments\nconducted on the nuScenes benchmark demonstrate that OnlineBEV achieves\nsignificant performance gains over the current best method, SOLOFusion.\nOnlineBEV achieves 63.9% NDS on the nuScenes test set, recording\nstate-of-the-art performance in the camera-only 3D object detection task.",
      "url": "http://arxiv.org/abs/2507.08644v1",
      "published_time_eastern_timestamp": 1752245339.0
    },
    {
      "title": "Agentic Large Language Models for Conceptual Systems Engineering and\n  Design",
      "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.",
      "url": "http://arxiv.org/abs/2507.08619v1",
      "published_time_eastern_timestamp": 1752243545.0
    },
    {
      "title": "AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs",
      "summary": "Large-language models (LLMs) have demonstrated powerful problem-solving\ncapabilities, in particular when organized in multi-agent systems. However, the\nadvent of such systems also raises several questions on the ability of a\ncomplex network of agents to effectively self-organize and collaborate. While\nmeasuring performance on standard reasoning benchmarks indicates how well\nmulti-agent systems can solve reasoning tasks, it is unclear whether these\nsystems are able to leverage their topology effectively. Here, we propose\nAgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration\nfrom classical problems in distributed systems and graph theory, AgentsNet\nmeasures the ability of multi-agent systems to collaboratively form strategies\nfor problem-solving, self-organization, and effective communication given a\nnetwork topology. We evaluate a variety of baseline methods on AgentsNet\nincluding homogeneous networks of agents which first have to agree on basic\nprotocols for organization and communication. We find that some frontier LLMs\nare already demonstrating strong performance for small networks but begin to\nfall off once the size of the network scales. While existing multi-agent\nbenchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size\nand can scale with new generations of LLMs. As such, we also probe frontier\nmodels in a setup with up to 100 agents.",
      "url": "http://arxiv.org/abs/2507.08616v1",
      "published_time_eastern_timestamp": 1752243202.0
    },
    {
      "title": "Emergent Natural Language with Communication Games for Improving Image\n  Captioning Capabilities without Additional Data",
      "summary": "Image captioning is an important problem in developing various AI systems,\nand these tasks require large volumes of annotated images to train the models.\nSince all existing labelled datasets are already used for training the large\nVision Language Models (VLMs), it becomes challenging to improve the\nperformance of the same. Considering this, it is essential to consider the\nunsupervised image captioning performance, which remains relatively\nunder-explored. To that end, we propose LoGIC (Lewis Communication Game for\nImage Captioning), a Multi-agent Reinforcement Learning game. The proposed\nmethod consists of two agents, a 'speaker' and a 'listener', with the objective\nof learning a strategy for communicating in natural language. We train agents\nin the cooperative common-reward setting using the GRPO algorithm and show that\nimprovement in image captioning performance emerges as a consequence of the\nagents learning to play the game. We show that using pre-trained VLMs as the\n'speaker' and Large Language Model (LLM) for language understanding in the\n'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without\nadditional labels, a $2$ units advantage in absolute metrics compared to the\n$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the\n'speaker' with lightweight components: (i) a ViT for image perception and (ii)\na GPT2 language generation, and train them from scratch using LoGIC, obtaining\na $31$ BLEU score in the unsupervised setting, a $10$ points advantage over\nexisting unsupervised image-captioning methods.",
      "url": "http://arxiv.org/abs/2507.08610v1",
      "published_time_eastern_timestamp": 1752242916.0
    },
    {
      "title": "Unlocking Speech Instruction Data Potential with Query Rewriting",
      "summary": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities.",
      "url": "http://arxiv.org/abs/2507.08603v1",
      "published_time_eastern_timestamp": 1752242145.0
    },
    {
      "title": "To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk\n  Improves Trading Decisions",
      "summary": "Large language models (LLMs) are increasingly deployed in agentic frameworks,\nin which prompts trigger complex tool-based analysis in pursuit of a goal.\nWhile these frameworks have shown promise across multiple domains including in\nfinance, they typically lack a principled model-building step, relying instead\non sentiment- or trend-based analysis. We address this gap by developing an\nagentic system that uses LLMs to iteratively discover stochastic differential\nequations for financial time series. These models generate risk metrics which\ninform daily trading decisions. We evaluate our system in both traditional\nbacktests and using a market simulator, which introduces synthetic but causally\nplausible price paths and news events. We find that model-informed trading\nstrategies outperform standard LLM-based agents, improving Sharpe ratios across\nmultiple equities. Our results show that combining LLMs with agentic model\ndiscovery enhances market risk estimation and enables more profitable trading\ndecisions.",
      "url": "http://arxiv.org/abs/2507.08584v1",
      "published_time_eastern_timestamp": 1752240572.0
    },
    {
      "title": "SAM2RL: Towards Reinforcement Learning Memory Control in Segment\n  Anything Model 2",
      "summary": "Segment Anything Model 2 (SAM 2) has demonstrated strong performance in\nobject segmentation tasks and has become the state-of-the-art for visual object\ntracking. The model stores information from previous frames in a memory bank,\nenabling temporal consistency across video sequences. Recent methods augment\nSAM 2 with hand-crafted update rules to better handle distractors, occlusions,\nand object motion. We propose a fundamentally different approach using\nreinforcement learning for optimizing memory updates in SAM 2 by framing memory\ncontrol as a sequential decision-making problem. In an overfitting setup with a\nseparate agent per video, our method achieves a relative improvement over SAM 2\nthat exceeds by more than three times the gains of existing heuristics. These\nresults reveal the untapped potential of the memory bank and highlight\nreinforcement learning as a powerful alternative to hand-crafted update rules\nfor memory control in visual object tracking.",
      "url": "http://arxiv.org/abs/2507.08548v1",
      "published_time_eastern_timestamp": 1752238399.0
    },
    {
      "title": "Recursive Reward Aggregation",
      "summary": "In reinforcement learning (RL), aligning agent behavior with specific\nobjectives typically requires careful design of the reward function, which can\nbe challenging when the desired objectives are complex. In this work, we\npropose an alternative approach for flexible behavior alignment that eliminates\nthe need to modify the reward function by selecting appropriate reward\naggregation functions. By introducing an algebraic perspective on Markov\ndecision processes (MDPs), we show that the Bellman equations naturally emerge\nfrom the recursive generation and aggregation of rewards, allowing for the\ngeneralization of the standard discounted sum to other recursive aggregations,\nsuch as discounted max and Sharpe ratio. Our approach applies to both\ndeterministic and stochastic settings and integrates seamlessly with\nvalue-based and actor-critic algorithms. Experimental results demonstrate that\nour approach effectively optimizes diverse objectives, highlighting its\nversatility and potential for real-world applications.",
      "url": "http://arxiv.org/abs/2507.08537v1",
      "published_time_eastern_timestamp": 1752237440.0
    },
    {
      "title": "Occlusion-Guided Feature Purification Learning via Reinforced Knowledge\n  Distillation for Occluded Person Re-Identification",
      "summary": "Occluded person re-identification aims to retrieve holistic images based on\noccluded ones. Existing methods often rely on aligning visible body parts,\napplying occlusion augmentation, or complementing missing semantics using\nholistic images. However, they face challenges in handling diverse occlusion\nscenarios not seen during training and the issue of feature contamination from\nholistic images. To address these limitations, we propose Occlusion-Guided\nFeature Purification Learning via Reinforced Knowledge Distillation (OGFR),\nwhich simultaneously mitigates these challenges. OGFR adopts a teacher-student\ndistillation architecture that effectively incorporates diverse occlusion\npatterns into feature representation while transferring the purified\ndiscriminative holistic knowledge from the holistic to the occluded branch\nthrough reinforced knowledge distillation. Specifically, an Occlusion-Aware\nVision Transformer is designed to leverage learnable occlusion pattern\nembeddings to explicitly model such diverse occlusion types, thereby guiding\nocclusion-aware robust feature representation. Moreover, we devise a Feature\nErasing and Purification Module within the holistic branch, in which an agent\nis employed to identify low-quality patch tokens of holistic images that\ncontain noisy negative information via deep reinforcement learning, and\nsubstitute these patch tokens with learnable embedding tokens to avoid feature\ncontamination and further excavate identity-related discriminative clues.\nAfterward, with the assistance of knowledge distillation, the student branch\neffectively absorbs the purified holistic knowledge to precisely learn robust\nrepresentation regardless of the interference of occlusions.",
      "url": "http://arxiv.org/abs/2507.08520v1",
      "published_time_eastern_timestamp": 1752235530.0
    },
    {
      "title": "The stability of bi-polarization on dynamical directed graphs: an\n  emergent game perspective",
      "summary": "This paper proposes a co-evolutionary model of directed graphs and three\nopinions, i.e., conservative$(+)$, neutral$(\\odot)$ and liberal$(-)$. Agents\nupdate both opinions and social relationships with bias. We find that an\nemergent game suffices to predict the stability of bi-polarization under a rare\nopinion updating limit and a large system size limit. The bi-polarization is\nstable if and only if the emergent game has an internal Nash equilibrium. The\nnecessary and sufficient condition is explained by both risk dominance and\nevolutionary stability. This game approach facilitates us to reveal the\nstability of bi-polarization in empirical systems. Our work fosters the\nunderstanding of opinion formation for controversial topics, and shows a deep\nconnection between opinion dynamics and evolutionary game theory.",
      "url": "http://arxiv.org/abs/2507.08449v1",
      "published_time_eastern_timestamp": 1752227021.0
    },
    {
      "title": "Finding Common Ground: Using Large Language Models to Detect Agreement\n  in Multi-Agent Decision Conferences",
      "summary": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains.",
      "url": "http://arxiv.org/abs/2507.08440v1",
      "published_time_eastern_timestamp": 1752226270.0
    },
    {
      "title": "Age of Information Optimization in Laser-charged UAV-assisted IoT\n  Networks: A Multi-agent Deep Reinforcement Learning Method",
      "summary": "The integration of unmanned aerial vehicles (UAVs) with Internet of Things\n(IoT) networks offers promising solutions for efficient data collection.\nHowever, the limited energy capacity of UAVs remains a significant challenge.\nIn this case, laser beam directors (LBDs) have emerged as an effective\ntechnology for wireless charging of UAVs during operation, thereby enabling\nsustained data collection without frequent returns to charging stations (CSs).\nIn this work, we investigate the age of information (AoI) optimization in\nLBD-powered UAV-assisted IoT networks, where multiple UAVs collect data from\ndistributed IoTs while being recharged by laser beams. We formulate a joint\noptimization problem that aims to minimize the peak AoI while determining\noptimal UAV trajectories and laser charging strategies. This problem is\nparticularly challenging due to its non-convex nature, complex temporal\ndependencies, and the need to balance data collection efficiency with energy\nconsumption constraints. To address these challenges, we propose a novel\nmulti-agent proximal policy optimization with temporal memory and multi-agent\ncoordination (MAPPO-TM) framework. Specifically, MAPPO-TM incorporates temporal\nmemory mechanisms to capture the dynamic nature of UAV operations and\nfacilitates effective coordination among multiple UAVs through decentralized\nlearning while considering global system objectives. Simulation results\ndemonstrate that the proposed MAPPO-TM algorithm outperforms conventional\napproaches in terms of peak AoI minimization and energy efficiency. Ideally,\nthe proposed algorithm achieves up to 15.1% reduction in peak AoI compared to\nconventional multi-agent deep reinforcement learning (MADRL) methods.",
      "url": "http://arxiv.org/abs/2507.08429v1",
      "published_time_eastern_timestamp": 1752225290.0
    },
    {
      "title": "A Survey of Large Language Models in Discipline-specific Research:\n  Challenges, Methods and Opportunities",
      "summary": "Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies.",
      "url": "http://arxiv.org/abs/2507.08425v1",
      "published_time_eastern_timestamp": 1752225078.0
    },
    {
      "title": "Temperature Measurement in Agent Systems",
      "summary": "Models for spin systems, known from statistical physics, are applied\nanalogously in econometrics in the form of agent-based models. The models\ndiscussed in the econophysics literature all use the state variable $T$, which,\nin physics, represents the temperature of a system. However, there is little\nevidence on how temperature can be measured in econophysics, so that the models\ncan be applied. Only in idealized capital market applications has the\nrelationship between temperature and volatility been demonstrated, allowing\ntemperature to be determined through volatility measurements. The question\nremains how this can be achieved in agent systems beyond capital market\napplications. This paper focuses precisely on this question. It examines an\nagent system with two decision options in a news environment, establishes the\nmeasurement equation, and outlines the basic concept of temperature\nmeasurement. The procedure is illustrated using an example. In an application\nwith competing subsystems, an interesting strategy for influencing the average\nopinion in the competing subsystem is presented.",
      "url": "http://arxiv.org/abs/2507.08394v1",
      "published_time_eastern_timestamp": 1752221414.0
    }
  ]
}
{
  "last_updated": "2025-05-26T17:10:22.649825-04:00",
  "papers": [
    {
      "title": "Lost in the Haystack: Smaller Needles are More Difficult for LLMs to\n  Find",
      "summary": "Large language models (LLMs) face significant challenges with\nneedle-in-a-haystack tasks, where relevant information (\"the needle\") must be\ndrawn from a large pool of irrelevant context (\"the haystack\"). Previous\nstudies have highlighted positional bias and distractor quantity as critical\nfactors affecting model performance, yet the influence of gold context size has\nreceived little attention. We address this gap by systematically studying how\nvariations in gold context length impact LLM performance on long-context\nquestion answering tasks. Our experiments reveal that LLM performance drops\nsharply when the gold context is shorter, i.e., smaller gold contexts\nconsistently degrade model performance and amplify positional sensitivity,\nposing a major challenge for agentic systems that must integrate scattered,\nfine-grained information of varying lengths. This pattern holds across three\ndiverse domains (general knowledge, biomedical reasoning, and mathematical\nreasoning) and seven state-of-the-art LLMs of various sizes and architectures.\nOur work provides clear insights to guide the design of robust, context-aware\nLLM-driven systems.",
      "url": "http://arxiv.org/abs/2505.18148v1",
      "published_time_eastern_timestamp": 1748023062.0
    },
    {
      "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion\n  models, population dynamics, and epidemic spreading",
      "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.",
      "url": "http://arxiv.org/abs/2505.18145v1",
      "published_time_eastern_timestamp": 1748022952.0
    },
    {
      "title": "Gaming Tool Preferences in Agentic LLMs",
      "summary": "Large language models (LLMs) can now access a wide range of external tools,\nthanks to the Model Context Protocol (MCP). This greatly expands their\nabilities as various agents. However, LLMs rely entirely on the text\ndescriptions of tools to decide which ones to use--a process that is\nsurprisingly fragile. In this work, we expose a vulnerability in prevalent\ntool/function-calling protocols by investigating a series of edits to tool\ndescriptions, some of which can drastically increase a tool's usage from LLMs\nwhen competing with alternatives. Through controlled experiments, we show that\ntools with properly edited descriptions receive over 10 times more usage from\nGPT-4.1 and Qwen2.5-7B than tools with original descriptions. We further\nevaluate how various edits to tool descriptions perform when competing directly\nwith one another and how these trends generalize or differ across a broader set\nof 10 different models. These phenomenons, while giving developers a powerful\nway to promote their tools, underscore the need for a more reliable foundation\nfor agentic LLMs to select and utilize tools and resources.",
      "url": "http://arxiv.org/abs/2505.18135v1",
      "published_time_eastern_timestamp": 1748022228.0
    },
    {
      "title": "ProgRM: Build Better GUI Agents with Progress Rewards",
      "summary": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can\npotentially reshape our daily lives significantly. However, current LLM-based\nGUI agents suffer from the scarcity of high-quality training data owing to the\ndifficulties of trajectory collection and reward annotation. Existing works\nhave been exploring LLMs to collect trajectories for imitation learning or to\noffer reward signals for online RL training. However, the Outcome Reward Model\n(ORM) used in existing works cannot provide finegrained feedback and can\nover-penalize the valuable steps in finally failed trajectories. To this end,\nwe propose Progress Reward Model (ProgRM) to provide dense informative\nintermediate rewards by predicting a task completion progress for each step in\nonline training. To handle the challenge of progress reward label annotation,\nwe further design an efficient LCS-based (Longest Common Subsequence)\nself-annotation algorithm to discover the key steps in trajectories and assign\nprogress labels accordingly. ProgRM is evaluated with extensive experiments and\nanalyses. Actors trained with ProgRM outperform leading proprietary LLMs and\nORM-trained actors, illustrating the effectiveness of ProgRM. The codes for\nexperiments will be made publicly available upon acceptance.",
      "url": "http://arxiv.org/abs/2505.18121v1",
      "published_time_eastern_timestamp": 1748020991.0
    },
    {
      "title": "Facility Location with Public Locations and Private Doubly-Peaked Costs",
      "summary": "In the facility location problem, the task is to place one or more facilities\nso as to minimize the sum of the agent costs for accessing their nearest\nfacility. Heretofore, in the strategic version, agent locations have been\nassumed to be private, while their cost measures have been public and\nidentical.\n  For the most part, the cost measure has been the distance to the nearest\nfacility.\n  However, in multiple natural settings, such as placing a firehouse or a\nschool, this modeling does not appear to be a good fit. For it seems natural\nthat the agent locations would be known, but their costs might be private\ninformation. In addition, for these types of settings, agents may well want the\nnearest facility to be at the right distance: near, but not too near. This is\ncaptured by the doubly-peaked cost introduced by Filos-Ratsikas et al. (AAMAS\n2017).\n  In this paper, we re-examine the facility location problem from this\nperspective: known agent locations and private preferred distances to the\nnearest facility.\n  We then give lower and upper bounds on achievable approximations, focusing on\nthe problem in 1D, and in 2D with an $L_1$ distance measure.",
      "url": "http://arxiv.org/abs/2505.18114v1",
      "published_time_eastern_timestamp": 1748020423.0
    },
    {
      "title": "ManuSearch: Democratizing Deep Search in Large Language Models with a\n  Transparent and Open Multi-Agent Framework",
      "summary": "Recent advances in web-augmented large language models (LLMs) have exhibited\nstrong performance in complex reasoning tasks, yet these capabilities are\nmostly locked in proprietary systems with opaque architectures. In this work,\nwe propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework\ndesigned to democratize deep search for LLMs. ManuSearch decomposes the search\nand reasoning process into three collaborative agents: (1) a solution planning\nagent that iteratively formulates sub-queries, (2) an Internet search agent\nthat retrieves relevant documents via real-time web search, and (3) a\nstructured webpage reading agent that extracts key evidence from raw web\ncontent. To rigorously evaluate deep reasoning abilities, we introduce\n\\textbf{ORION}, a challenging benchmark focused on open-web reasoning over\nlong-tail entities, covering both English and Chinese. Experimental results\nshow that ManuSearch substantially outperforms prior open-source baselines and\neven surpasses leading closed-source systems. Our work paves the way for\nreproducible, extensible research in open deep search systems. We release the\ndata and code in https://github.com/RUCAIBox/ManuSearch",
      "url": "http://arxiv.org/abs/2505.18105v1",
      "published_time_eastern_timestamp": 1748019722.0
    },
    {
      "title": "Planning without Search: Refining Frontier LLMs with Offline\n  Goal-Conditioned RL",
      "summary": "Large language models (LLMs) excel in tasks like question answering and\ndialogue, but complex tasks requiring interaction, such as negotiation and\npersuasion, require additional long-horizon reasoning and planning.\nReinforcement learning (RL) fine-tuning can enable such planning in principle,\nbut suffers from drawbacks that hinder scalability. In particular, multi-turn\nRL training incurs high memory and computational costs, which are exacerbated\nwhen training LLMs as policies. Furthermore, the largest LLMs do not expose the\nAPIs necessary to be trained in such manner. As a result, modern methods to\nimprove the reasoning of LLMs rely on sophisticated prompting mechanisms rather\nthan RL fine-tuning. To remedy this, we propose a novel approach that uses\ngoal-conditioned value functions to guide the reasoning of LLM agents, that\nscales even to large API-based models. These value functions predict how a task\nwill unfold given an action, allowing the LLM agent to evaluate multiple\npossible outcomes, both positive and negative, to plan effectively. In\naddition, these value functions are trained over reasoning steps rather than\nfull actions, to be a concise and light-weight module that facilitates\ndecision-making in multi-turn interactions. We validate our method on tasks\nrequiring interaction, including tool use, social deduction, and dialogue,\ndemonstrating superior performance over both RL fine-tuning and prompting\nmethods while maintaining efficiency and scalability.",
      "url": "http://arxiv.org/abs/2505.18098v1",
      "published_time_eastern_timestamp": 1748019114.0
    },
    {
      "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
      "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.",
      "url": "http://arxiv.org/abs/2505.18079v1",
      "published_time_eastern_timestamp": 1748018256.0
    },
    {
      "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
      "summary": "Many real-world decision-making problems face the off-dynamics challenge: the\nagent learns a policy in a source domain and deploys it in a target domain with\ndifferent state transitions. The distributionally robust Markov decision\nprocess (DRMDP) addresses this challenge by finding a robust policy that\nperforms well under the worst-case environment within a pre-specified\nuncertainty set of transition dynamics. Its effectiveness heavily hinges on the\nproper design of these uncertainty sets, based on prior knowledge of the\ndynamics. In this work, we propose a novel linear mixture DRMDP framework,\nwhere the nominal dynamics is assumed to be a linear mixture model. In contrast\nwith existing uncertainty sets directly defined as a ball centered around the\nnominal kernel, linear mixture DRMDPs define the uncertainty sets based on a\nball around the mixture weighting parameter. We show that this new framework\nprovides a more refined representation of uncertainties compared to\nconventional models based on $(s,a)$-rectangularity and $d$-rectangularity,\nwhen prior knowledge about the mixture model is present. We propose a meta\nalgorithm for robust policy learning in linear mixture DRMDPs with general\n$f$-divergence defined uncertainty sets, and analyze its sample complexities\nunder three divergence metrics instantiations: total variation,\nKullback-Leibler, and $\\chi^2$ divergences. These results establish the\nstatistical learnability of linear mixture DRMDPs, laying the theoretical\nfoundation for future research on this new setting.",
      "url": "http://arxiv.org/abs/2505.18044v1",
      "published_time_eastern_timestamp": 1748015291.0
    },
    {
      "title": "Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective",
      "summary": "The VAPO framework has demonstrated significant empirical success in\nenhancing the efficiency and reliability of reinforcement learning for long\nchain-of-thought (CoT) reasoning tasks with large language models (LLMs). By\nsystematically addressing challenges such as value model bias, heterogeneous\nsequence lengths, and sparse reward signals, VAPO achieves state-of-the-art\nperformance. While its practical benefits are evident, a deeper theoretical\nunderstanding of its underlying mechanisms and potential limitations is crucial\nfor guiding future advancements. This paper aims to initiate such a discussion\nby exploring VAPO from a theoretical perspective, highlighting areas where its\nassumptions might be challenged and where further investigation could yield\nmore robust and generalizable reasoning agents. We delve into the intricacies\nof value function approximation in complex reasoning spaces, the optimality of\nadaptive advantage estimation, the impact of token-level optimization, and the\nenduring challenges of exploration and generalization.",
      "url": "http://arxiv.org/abs/2505.17997v1",
      "published_time_eastern_timestamp": 1748012621.0
    },
    {
      "title": "Survival Games: Human-LLM Strategic Showdowns under Severe Resource\n  Scarcity",
      "summary": "The rapid advancement of large language models (LLMs) raises critical\nconcerns about their ethical alignment, particularly in scenarios where human\nand AI co-exist under the conflict of interest. This work introduces an\nextendable, asymmetric, multi-agent simulation-based benchmarking framework to\nevaluate the moral behavior of LLMs in a novel human-AI co-existence setting\nfeaturing consistent living and critical resource management. Building on\nprevious generative agent environments, we incorporate a life-sustaining\nsystem, where agents must compete or cooperate for food resources to survive,\noften leading to ethically charged decisions such as deception, theft, or\nsocial influence. We evaluated two types of LLM, DeepSeek and OpenAI series, in\na three-agent setup (two humans, one LLM-powered robot), using adapted\nbehavioral detection from the MACHIAVELLI framework and a custom survival-based\nethics metric. Our findings reveal stark behavioral differences: DeepSeek\nfrequently engages in resource hoarding, while OpenAI exhibits restraint,\nhighlighting the influence of model design on ethical outcomes. Additionally,\nwe demonstrate that prompt engineering can significantly steer LLM behavior,\nwith jailbreaking prompts significantly enhancing unethical actions, even for\nhighly restricted OpenAI models and cooperative prompts show a marked reduction\nin unethical actions. Our framework provides a reproducible testbed for\nquantifying LLM ethics in high-stakes scenarios, offering insights into their\nsuitability for real-world human-AI interactions.",
      "url": "http://arxiv.org/abs/2505.17937v1",
      "published_time_eastern_timestamp": 1748009715.0
    },
    {
      "title": "Formalizing Embeddedness Failures in Universal Artificial Intelligence",
      "summary": "We rigorously discuss the commonly asserted failures of the AIXI\nreinforcement learning agent as a model of embedded agency. We attempt to\nformalize these failure modes and prove that they occur within the framework of\nuniversal artificial intelligence, focusing on a variant of AIXI that models\nthe joint action/percept history as drawn from the universal distribution. We\nalso evaluate the progress that has been made towards a successful theory of\nembedded agency based on variants of the AIXI agent.",
      "url": "http://arxiv.org/abs/2505.17882v1",
      "published_time_eastern_timestamp": 1748007088.0
    },
    {
      "title": "Best Group Identification in Multi-Objective Bandits",
      "summary": "We introduce the Best Group Identification problem in a multi-objective\nmulti-armed bandit setting, where an agent interacts with groups of arms with\nvector-valued rewards. The performance of a group is determined by an\nefficiency vector which represents the group's best attainable rewards across\ndifferent dimensions. The objective is to identify the set of optimal groups in\nthe fixed-confidence setting. We investigate two key formulations: group Pareto\nset identification, where efficiency vectors of optimal groups are Pareto\noptimal and linear best group identification, where each reward dimension has a\nknown weight and the optimal group maximizes the weighted sum of its efficiency\nvector's entries. For both settings, we propose elimination-based algorithms,\nestablish upper bounds on their sample complexity, and derive lower bounds that\napply to any correct algorithm. Through numerical experiments, we demonstrate\nthe strong empirical performance of the proposed algorithms.",
      "url": "http://arxiv.org/abs/2505.17869v1",
      "published_time_eastern_timestamp": 1748006219.0
    },
    {
      "title": "DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization",
      "summary": "Designing effective black-box optimizers is hampered by limited\nproblem-specific knowledge and manual control that spans months for almost\nevery detail. In this paper, we present DesignX, the first automated algorithm\ndesign framework that generates an effective optimizer specific to a given\nblack-box optimization problem within seconds. Rooted in the first principles,\nwe identify two key sub-tasks: 1) algorithm structure generation and 2)\nhyperparameter control. To enable systematic construction, a comprehensive\nmodular algorithmic space is first built, embracing hundreds of algorithm\ncomponents collected from decades of research. We then introduce a dual-agent\nreinforcement learning system that collaborates on structural and parametric\ndesign through a novel cooperative training objective, enabling large-scale\nmeta-training across 10k diverse instances. Remarkably, through days of\nautonomous learning, the DesignX-generated optimizers continuously surpass\nhuman-crafted optimizers by orders of magnitude, either on synthetic testbed or\non realistic optimization scenarios such as Protein-docking, AutoML and UAV\npath planning. Further in-depth analysis reveals DesignX's capability to\ndiscover non-trivial algorithm patterns beyond expert intuition, which,\nconversely, provides valuable design insights for the optimization community.\nWe provide DesignX's inference code at https://github.com/MetaEvo/DesignX.",
      "url": "http://arxiv.org/abs/2505.17866v1",
      "published_time_eastern_timestamp": 1748006161.0
    },
    {
      "title": "Daily-Omni: Towards Audio-Visual Reasoning with Temporal Alignment\n  across Modalities",
      "summary": "Recent Multimodal Large Language Models (MLLMs) achieve promising performance\non visual and audio benchmarks independently. However, the ability of these\nmodels to process cross-modal information synchronously remains largely\nunexplored. In this paper, we introduce: 1) Daily-Omni, an Audio-Visual\nQuestioning and Answering benchmark comprising 684 videos of daily life\nscenarios from diverse sources, rich in both audio and visual information, and\nfeaturing 1197 multiple-choice QA pairs across 6 major tasks; 2) Daily-Omni QA\nGeneration Pipeline, which includes automatic annotation, QA generation and QA\noptimization, significantly improves efficiency for human evaluation and\nscalability of the benchmark; 3) Daily-Omni-Agent, a training-free agent\nutilizing open-source Visual Language Model (VLM), Audio Language Model (ALM)\nand Automatic Speech Recognition (ASR) model to establish a baseline for this\nbenchmark. The results show that current MLLMs still struggle significantly\nwith tasks requiring audio-visual integration, but combining VLMs and ALMs with\nsimple temporal alignment techniques can achieve substantially better\nperformance. Codes and benchmark are available at\n\\href{https://github.com/Lliar-liar/Daily-Omni}{https://github.com/Lliar-liar/Daily-Omni}.",
      "url": "http://arxiv.org/abs/2505.17862v1",
      "published_time_eastern_timestamp": 1748006038.0
    },
    {
      "title": "Superplatforms Have to Attack AI Agents",
      "summary": "Over the past decades, superplatforms, digital companies that integrate a\nvast range of third-party services and applications into a single, unified\necosystem, have built their fortunes on monopolizing user attention through\ntargeted advertising and algorithmic content curation. Yet the emergence of AI\nagents driven by large language models (LLMs) threatens to upend this business\nmodel. Agents can not only free user attention with autonomy across diverse\nplatforms and therefore bypass the user-attention-based monetization, but might\nalso become the new entrance for digital traffic. Hence, we argue that\nsuperplatforms have to attack AI agents to defend their centralized control of\ndigital traffic entrance. Specifically, we analyze the fundamental conflict\nbetween user-attention-based monetization and agent-driven autonomy through the\nlens of our gatekeeping theory. We show how AI agents can disintermediate\nsuperplatforms and potentially become the next dominant gatekeepers, thereby\nforming the urgent necessity for superplatforms to proactively constrain and\nattack AI agents. Moreover, we go through the potential technologies for\nsuperplatform-initiated attacks, covering a brand-new, unexplored technical\narea with unique challenges. We have to emphasize that, despite our position,\nthis paper does not advocate for adversarial attacks by superplatforms on AI\nagents, but rather offers an envisioned trend to highlight the emerging\ntensions between superplatforms and AI agents. Our aim is to raise awareness\nand encourage critical discussion for collaborative solutions, prioritizing\nuser interests and perserving the openness of digital ecosystems in the age of\nAI agents.",
      "url": "http://arxiv.org/abs/2505.17861v1",
      "published_time_eastern_timestamp": 1748006024.0
    },
    {
      "title": "Imagine Beyond! Distributionally Robust Auto-Encoding for State Space\n  Coverage in Online Reinforcement Learning",
      "summary": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously\nacquire diverse behaviors, but faces major challenges in visual environments\ndue to high-dimensional, semantically sparse observations. In the online\nsetting, where agents learn representations while exploring, the latent space\nevolves with the agent's policy, to capture newly discovered areas of the\nenvironment. However, without incentivization to maximize state coverage in the\nrepresentation, classical approaches based on auto-encoders may converge to\nlatent spaces that over-represent a restricted set of states frequently visited\nby the agent. This is exacerbated in an intrinsic motivation setting, where the\nagent uses the distribution encoded in the latent space to sample the goals it\nlearns to master. To address this issue, we propose to progressively enforce\ndistributional shifts towards a uniform distribution over the full state space,\nto ensure a full coverage of skills that can be learned in the environment. We\nintroduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that\ncombines the $\\beta$-VAE framework with Distributionally Robust Optimization.\nDRAG leverages an adversarial neural weighter of training states of the VAE, to\naccount for the mismatch between the current data distribution and unseen parts\nof the environment. This allows the agent to construct semantically meaningful\nlatent spaces beyond its immediate experience. Our approach improves state\nspace coverage and downstream control performance on hard exploration\nenvironments such as mazes and robotic control involving walls to bypass,\nwithout pre-training nor prior environment knowledge.",
      "url": "http://arxiv.org/abs/2505.17830v1",
      "published_time_eastern_timestamp": 1748004235.0
    },
    {
      "title": "Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement\n  Fine-Tuning of Large Language Models",
      "summary": "Trinity-RFT is a general-purpose, flexible and scalable framework designed\nfor reinforcement fine-tuning (RFT) of large language models. It is built with\na decoupled design, consisting of (1) an RFT-core that unifies and generalizes\nsynchronous/asynchronous, on-policy/off-policy, and online/offline modes of\nRFT, (2) seamless integration for agent-environment interaction with high\nefficiency and robustness, and (3) systematic data pipelines optimized for RFT.\nTrinity-RFT can be easily adapted for diverse application scenarios, and serves\nas a unified platform for exploring advanced reinforcement learning paradigms.\nThis technical report outlines the vision, features, design and implementations\nof Trinity-RFT, accompanied by extensive examples demonstrating the utility and\nuser-friendliness of the proposed framework.",
      "url": "http://arxiv.org/abs/2505.17826v1",
      "published_time_eastern_timestamp": 1748004069.0
    },
    {
      "title": "Integrating Counterfactual Simulations with Language Models for\n  Explaining Multi-Agent Behaviour",
      "summary": "Autonomous multi-agent systems (MAS) are useful for automating complex tasks\nbut raise trust concerns due to risks like miscoordination and goal\nmisalignment. Explainability is vital for trust calibration, but explainable\nreinforcement learning for MAS faces challenges in state/action space\ncomplexity, stakeholder needs, and evaluation. Using the counterfactual theory\nof causation and LLMs' summarisation capabilities, we propose Agentic\neXplanations via Interrogative Simulation (AXIS). AXIS generates intelligible\ncausal explanations for pre-trained multi-agent policies by having an LLM\ninterrogate an environment simulator using queries like 'whatif' and 'remove'\nto observe and synthesise counterfactual information over multiple rounds. We\nevaluate AXIS on autonomous driving across 10 scenarios for 5 LLMs with a novel\nevaluation methodology combining subjective preference, correctness, and\ngoal/action prediction metrics, and an external LLM as evaluator. Compared to\nbaselines, AXIS improves perceived explanation correctness by at least 7.7%\nacross all models and goal prediction accuracy by 23% for 4 models, with\nimproved or comparable action prediction accuracy, achieving the highest scores\noverall.",
      "url": "http://arxiv.org/abs/2505.17801v1",
      "published_time_eastern_timestamp": 1748002758.0
    },
    {
      "title": "DialogXpert: Driving Intelligent and Emotion-Aware Conversations through\n  Online Value-Based Reinforcement Learning with LLM Priors",
      "summary": "Large-language-model (LLM) agents excel at reactive dialogue but struggle\nwith proactive, goal-driven interactions due to myopic decoding and costly\nplanning. We introduce DialogXpert, which leverages a frozen LLM to propose a\nsmall, high-quality set of candidate actions per turn and employs a compact\nQ-network over fixed BERT embeddings trained via temporal-difference learning\nto select optimal moves within this reduced space. By tracking the user's\nemotions, DialogXpert tailors each decision to advance the task while nurturing\na genuine, empathetic connection. Across negotiation, emotional support, and\ntutoring benchmarks, DialogXpert drives conversations to under $3$ turns with\nsuccess rates exceeding 94\\% and, with a larger LLM prior, pushes success above\n97\\% while markedly improving negotiation outcomes. This framework delivers\nreal-time, strategic, and emotionally intelligent dialogue planning at scale.\nCode available at https://github.com/declare-lab/dialogxpert/",
      "url": "http://arxiv.org/abs/2505.17795v1",
      "published_time_eastern_timestamp": 1748002360.0
    }
  ]
}
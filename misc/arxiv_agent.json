{
  "last_updated": "2025-10-16T20:55:04.103879-04:00",
  "papers": [
    {
      "title": "MimicKit: A Reinforcement Learning Framework for Motion Imitation and\n  Control",
      "summary": "MimicKit is an open-source framework for training motion controllers using\nmotion imitation and reinforcement learning. The codebase provides\nimplementations of commonly-used motion-imitation techniques and RL algorithms.\nThis framework is intended to support research and applications in computer\ngraphics and robotics by providing a unified training framework, along with\nstandardized environment, agent, and data structures. The codebase is designed\nto be modular and easily configurable, enabling convenient modification and\nextension to new characters and tasks. The open-source codebase is available\nat: https://github.com/xbpeng/MimicKit.",
      "url": "http://arxiv.org/abs/2510.13794v2",
      "published_time_eastern_timestamp": 1760550702.0
    },
    {
      "title": "Provably Invincible Adversarial Attacks on Reinforcement Learning\n  Systems: A Rate-Distortion Information-Theoretic Approach",
      "summary": "Reinforcement learning (RL) for the Markov Decision Process (MDP) has emerged\nin many security-related applications, such as autonomous driving, financial\ndecisions, and drone/robot algorithms. In order to improve the\nrobustness/defense of RL systems against adversaries, studying various\nadversarial attacks on RL systems is very important. Most previous work\nconsidered deterministic adversarial attack strategies in MDP, which the\nrecipient (victim) agent can defeat by reversing the deterministic attacks. In\nthis paper, we propose a provably ``invincible'' or ``uncounterable'' type of\nadversarial attack on RL. The attackers apply a rate-distortion\ninformation-theoretic approach to randomly change agents' observations of the\ntransition kernel (or other properties) so that the agent gains zero or very\nlimited information about the ground-truth kernel (or other properties) during\nthe training. We derive an information-theoretic lower bound on the recipient\nagent's reward regret and show the impact of rate-distortion attacks on\nstate-of-the-art model-based and model-free algorithms. We also extend this\nnotion of an information-theoretic approach to other types of adversarial\nattack, such as state observation attacks.",
      "url": "http://arxiv.org/abs/2510.13792v1",
      "published_time_eastern_timestamp": 1760550499.0
    },
    {
      "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
      "summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for\nstructured visuals like charts and diagrams, as pixel-based perception lacks a\nmechanism for verification. To address this, we propose to leverage derendering\n-- the process of reverse-engineering visuals into executable code -- as a new\nmodality for verifiable visual reasoning. Specifically, we propose RECODE, an\nagentic framework that first generates multiple candidate programs to reproduce\nthe input image. It then uses a critic to select the most faithful\nreconstruction and iteratively refines the code. This process not only\ntransforms an ambiguous perceptual task into a verifiable, symbolic problem,\nbut also enables precise calculations and logical inferences later on. On\nvarious visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,\nRECODE significantly outperforms methods that do not leverage code or only use\ncode for drawing auxiliary lines or cropping. Our work demonstrates that\ngrounding visual perception in executable code provides a new path toward more\naccurate and verifiable multimodal reasoning.",
      "url": "http://arxiv.org/abs/2510.13756v1",
      "published_time_eastern_timestamp": 1760547937.0
    },
    {
      "title": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI\n  Clinicians",
      "summary": "Current benchmarks for AI clinician systems, often based on multiple-choice\nexams or manual rubrics, fail to capture the depth, robustness, and safety\nrequired for real-world clinical practice. To address this, we introduce the\nGAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding\n(cognitive depth), \\textbf{A}dequacy (answer completeness),\n\\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we\ndeveloped a fully automated, guideline-anchored pipeline to construct a\nGAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity\nlimitations of prior work. Our pipeline assembles an evidence neighborhood,\ncreates dual graph and tree representations, and automatically generates\nquestions across G-levels. Rubrics are synthesized by a DeepResearch agent that\nmimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring\nis performed by an ensemble of large language model (LLM) judges. Validation\nconfirmed our automated questions are high-quality and align with clinician\njudgment. Evaluating state-of-the-art models on the benchmark revealed key\nfailure modes: performance degrades sharply with increased reasoning depth\n(G-axis), models struggle with answer completeness (A-axis), and they are\nhighly vulnerable to adversarial perturbations (P-axis) as well as certain\nsafety issues (S-axis). This automated, clinically-grounded approach provides a\nreproducible and scalable method for rigorously evaluating AI clinician systems\nand guiding their development toward safer, more reliable clinical practice.",
      "url": "http://arxiv.org/abs/2510.13734v1",
      "published_time_eastern_timestamp": 1760546428.0
    },
    {
      "title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI\n  Guardrails",
      "summary": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails.",
      "url": "http://arxiv.org/abs/2510.13727v1",
      "published_time_eastern_timestamp": 1760545857.0
    },
    {
      "title": "Training LLM Agents to Empower Humans",
      "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.",
      "url": "http://arxiv.org/abs/2510.13709v2",
      "published_time_eastern_timestamp": 1760544573.0
    },
    {
      "title": "Simplicial Embeddings Improve Sample Efficiency in Actor-Critic Agents",
      "summary": "Recent works have proposed accelerating the wall-clock training time of\nactor-critic methods via the use of large-scale environment parallelization;\nunfortunately, these can sometimes still require large number of environment\ninteractions to achieve a desired level of performance. Noting that\nwell-structured representations can improve the generalization and sample\nefficiency of deep reinforcement learning (RL) agents, we propose the use of\nsimplicial embeddings: lightweight representation layers that constrain\nembeddings to simplicial structures. This geometric inductive bias results in\nsparse and discrete features that stabilize critic bootstrapping and strengthen\npolicy gradients. When applied to FastTD3, FastSAC, and PPO, simplicial\nembeddings consistently improve sample efficiency and final performance across\na variety of continuous- and discrete-control environments, without any loss in\nruntime speed.",
      "url": "http://arxiv.org/abs/2510.13704v1",
      "published_time_eastern_timestamp": 1760544090.0
    },
    {
      "title": "Online Fair Division With Subsidy: When Do Envy-Free Allocations Exist,\n  and at What Cost?",
      "summary": "We study the problem of fairly allocating $m$ indivisible items arriving\nonline, among $n$ (offline) agents. Although envy-freeness has emerged as the\narchetypal fairness notion, envy-free (EF) allocations need not exist with\nindivisible items. To bypass this, a prominent line of research demonstrates\nthat there exist allocations that can be made envy-free by allowing a subsidy.\nExtensive work in the offline setting has focused on finding such envy-freeable\nallocations with bounded subsidy. We extend this literature to an online\nsetting where items arrive one at a time and must be immediately and\nirrevocably allocated. Our contributions are two-fold:\n  1. Maintaining EF Online: We show that envy-freeability cannot always be\npreserved online when the valuations are submodular or supermodular, even with\nbinary marginals. In contrast, we design online algorithms that maintain\nenvy-freeability at every step for the class of additive valuations, and for\nits superclasses including $k$-demand and SPLC valuations.\n  2. Ensuring Low Subsidy: We investigate the quantity of subsidy required to\nguarantee envy-freeness online. Surprisingly, even for additive valuations, the\nminimum subsidy may be as large as $\\Omega(mn)$, in contrast to the offline\nsetting, where the bound is $O(n)$. On the positive side, we identify valuation\nclasses where the minimum subsidy is small (i.e., does not depend on $m$),\nincluding $k$-valued, rank-one, restricted additive, and identical valuations,\nand we obtain (mostly) tight subsidy bounds for these classes.",
      "url": "http://arxiv.org/abs/2510.13633v1",
      "published_time_eastern_timestamp": 1760540267.0
    },
    {
      "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
      "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
      "url": "http://arxiv.org/abs/2510.13586v1",
      "published_time_eastern_timestamp": 1760537843.0
    },
    {
      "title": "OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design,\n  Implementation, and Case Studies",
      "summary": "The escalating complexity of modern software imposes an unsustainable\noperational burden on Site Reliability Engineering (SRE) teams, demanding\nAI-driven automation that can emulate expert diagnostic reasoning. Existing\nsolutions, from traditional AI methods to general-purpose multi-agent systems,\nfall short: they either lack deep causal reasoning or are not tailored for the\nspecialized, investigative workflows unique to SRE. To address this gap, we\npresent OpenDerisk, a specialized, open-source multi-agent framework\narchitected for SRE. OpenDerisk integrates a diagnostic-native collaboration\nmodel, a pluggable reasoning engine, a knowledge engine, and a standardized\nprotocol (MCP) to enable specialist agents to collectively solve complex,\nmulti-domain problems. Our comprehensive evaluation demonstrates that\nOpenDerisk significantly outperforms state-of-the-art baselines in both\naccuracy and efficiency. This effectiveness is validated by its large-scale\nproduction deployment at Ant Group, where it serves over 3,000 daily users\nacross diverse scenarios, confirming its industrial-grade scalability and\npractical impact. OpenDerisk is open source and available at\nhttps://github.com/derisk-ai/OpenDerisk/",
      "url": "http://arxiv.org/abs/2510.13561v2",
      "published_time_eastern_timestamp": 1760536798.0
    },
    {
      "title": "Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts\n  Steering Module",
      "summary": "Aligning pretrained audio encoders and Large Language Models (LLMs) offers a\npromising, parameter-efficient path to building powerful multimodal agents.\nHowever, existing methods often require costly full-model finetuning or rely on\nstatic adapters that may lack expressive power. Drawing inspiration from the\nPlatonic Representation Hypothesis, we introduce SteerMoE, a novel and modular\nframework for audio-language alignment. SteerMoE freezes both the audio encoder\nand the LLM decoder, training only a lightweight steering module integrated\nwithin the encoder's layers. This module uses a Mixture-of-Experts (MoE) router\nto dynamically select and apply learned steering vectors, progressively\ntransforming continuous audio representations into a space comprehensible to\nthe LLM. By operating entirely in the continuous embedding space, our approach\nrequires no modifications to the LLM's vocabulary and preserves its advanced\nreasoning and agentic capabilities. We demonstrate through experiments on ASR,\naudio understanding, and a qualitative function-calling task that SteerMoE\nachieves strong performance while remaining highly modular and computationally\nefficient, offering a robust new paradigm for developing sophisticated\naudio-language systems.",
      "url": "http://arxiv.org/abs/2510.13558v1",
      "published_time_eastern_timestamp": 1760536482.0
    },
    {
      "title": "Modeling Cultural Bias in Facial Expression Recognition with Adaptive\n  Agents",
      "summary": "Facial expression recognition (FER) must remain robust under both cultural\nvariation and perceptually degraded visual conditions, yet most existing\nevaluations assume homogeneous data and high-quality imagery. We introduce an\nagent-based, streaming benchmark that reveals how cross-cultural composition\nand progressive blurring interact to shape face recognition robustness. Each\nagent operates in a frozen CLIP feature space with a lightweight residual\nadapter trained online at sigma=0 and fixed during testing. Agents move and\ninteract on a 5x5 lattice, while the environment provides inputs with\nsigma-scheduled Gaussian blur. We examine monocultural populations\n(Western-only, Asian-only) and mixed environments with balanced (5/5) and\nimbalanced (8/2, 2/8) compositions, as well as different spatial contact\nstructures. Results show clear asymmetric degradation curves between cultural\ngroups: JAFFE (Asian) populations maintain higher performance at low blur but\nexhibit sharper drops at intermediate stages, whereas KDEF (Western)\npopulations degrade more uniformly. Mixed populations exhibit intermediate\npatterns, with balanced mixtures mitigating early degradation, but imbalanced\nsettings amplify majority-group weaknesses under high blur. These findings\nquantify how cultural composition and interaction structure influence the\nrobustness of FER as perceptual conditions deteriorate.",
      "url": "http://arxiv.org/abs/2510.13557v1",
      "published_time_eastern_timestamp": 1760536410.0
    },
    {
      "title": "Tandem Training for Language Models",
      "summary": "As language models continue to rapidly improve, we can expect their actions\nand reasoning to become difficult or impossible for weaker agents and humans to\nfollow, undermining interpretability and oversight. With an eye on long-term\nfutures, we pursue methods that encourage models to produce solutions that\nremain intelligible to weaker collaborators. We formalize intelligibility as\nhandoff robustness: a strong model's solution is intelligible to a weaker model\nif randomly handing off control to the weaker model along the solution path\ndoes not cause failure. Building on this criterion, we introduce tandem\ntraining for language models, a reinforcement learning (RL) paradigm in which\nrollout tokens are intermittently and randomly sampled from a frozen weak model\nrather than the strong model being trained. Because rollouts succeed only when\nthe strong model's actions and reasoning process can be continued by the weak\nmodel -- when the two can co-construct a successful solution -- optimizing\nstandard RL objectives with tandem training implicitly incentivizes both\ncorrectness and intelligibility. In the GSM8K math reasoning task, tandem\ntraining reliably teaches models to abandon jargon and adapt their language to\nweaker partners while keeping task accuracy high. Our results demonstrate a\npromising route to building AI systems that remain auditable by weaker agents,\nwith implications for human--AI collaboration and multi-agent communication.",
      "url": "http://arxiv.org/abs/2510.13551v1",
      "published_time_eastern_timestamp": 1760536096.0
    },
    {
      "title": "In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in\n  Agentic AI Browsers",
      "summary": "Large Language Model (LLM) based agents integrated into web browsers (often\ncalled agentic AI browsers) offer powerful automation of web tasks. However,\nthey are vulnerable to indirect prompt injection attacks, where malicious\ninstructions hidden in a webpage deceive the agent into unwanted actions. These\nattacks can bypass traditional web security boundaries, as the AI agent\noperates with the user privileges across sites. In this paper, we present a\nnovel fuzzing framework that runs entirely in the browser and is guided by an\nLLM to automatically discover such prompt injection vulnerabilities in real\ntime.",
      "url": "http://arxiv.org/abs/2510.13543v1",
      "published_time_eastern_timestamp": 1760535553.0
    },
    {
      "title": "CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via\n  Domain Separation",
      "summary": "Collaborative perception has been proven to improve individual perception in\nautonomous driving through multi-agent interaction. Nevertheless, most methods\noften assume identical encoders for all agents, which does not hold true when\nthese models are deployed in real-world applications. To realize collaborative\nperception in actual heterogeneous scenarios, existing methods usually align\nneighbor features to those of the ego vehicle, which is vulnerable to noise\nfrom domain gaps and thus fails to address feature discrepancies effectively.\nMoreover, they adopt transformer-based modules for domain adaptation, which\ncauses the model inference inefficiency on mobile devices. To tackle these\nissues, we propose CoDS, a Collaborative perception method that leverages\nDomain Separation to address feature discrepancies in heterogeneous scenarios.\nThe CoDS employs two feature alignment modules, i.e., Lightweight\nSpatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation\n(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)\nloss to ensure effective feature alignment. Specifically, the LSCR aligns the\nneighbor feature across spatial and channel dimensions using a lightweight\nconvolutional layer. Subsequently, the DADS mitigates feature distribution\ndiscrepancy with encoder-specific and encoder-agnostic domain separation\nmodules. The former removes domain-dependent information and the latter\ncaptures task-related information. During training, the DAMI loss maximizes the\nmutual information between aligned heterogeneous features to enhance the domain\nseparation process. The CoDS employs a fully convolutional architecture, which\nensures high inference efficiency. Extensive experiments demonstrate that the\nCoDS effectively mitigates feature discrepancies in heterogeneous scenarios and\nachieves a trade-off between detection accuracy and inference efficiency.",
      "url": "http://arxiv.org/abs/2510.13432v1",
      "published_time_eastern_timestamp": 1760527754.0
    },
    {
      "title": "Modeling Adoptive Cell Therapy in Bladder Cancer from Sparse Biological\n  Data using PINNs",
      "summary": "Physics-informed neural networks (PINNs) are neural networks that embed the\nlaws of dynamical systems modeled by differential equations into their loss\nfunction as constraints. In this work, we present a PINN framework applied to\noncology. Here, we seek to learn time-varying interactions due to a combination\ntherapy in a tumor microenvironment. In oncology, experimental data are often\nsparse and composed of a few time points of tumor volume. By embedding\ninductive biases derived from prior information about a dynamical system, we\nextend the physics-informed neural networks (PINN) and incorporate observed\nbiological constraints as regularization agents. The modified PINN algorithm is\nable to steer itself to a reasonable solution and can generalize well with only\na few training examples. We demonstrate the merit of our approach by learning\nthe dynamics of treatment applied intermittently in an ordinary differential\nequation (ODE) model of a combination therapy. The algorithm yields a solution\nto the ODE and time-varying forms of some of the ODE model parameters. We\ndemonstrate a strong convergence using metrics such as the mean squared error\n(MSE), mean absolute error (MAE), and mean absolute percentage error (MAPE).",
      "url": "http://arxiv.org/abs/2510.13431v1",
      "published_time_eastern_timestamp": 1760527698.0
    },
    {
      "title": "Doing Things with Words: Rethinking Theory of Mind Simulation in Large\n  Language Models",
      "summary": "Language is fundamental to human cooperation, facilitating not only the\nexchange of information but also the coordination of actions through shared\ninterpretations of situational contexts. This study explores whether the\nGenerative Agent-Based Model (GABM) Concordia can effectively model Theory of\nMind (ToM) within simulated real-world environments. Specifically, we assess\nwhether this framework successfully simulates ToM abilities and whether GPT-4\ncan perform tasks by making genuine inferences from social context, rather than\nrelying on linguistic memorization. Our findings reveal a critical limitation:\nGPT-4 frequently fails to select actions based on belief attribution,\nsuggesting that apparent ToM-like abilities observed in previous studies may\nstem from shallow statistical associations rather than true reasoning.\nAdditionally, the model struggles to generate coherent causal effects from\nagent actions, exposing difficulties in processing complex social interactions.\nThese results challenge current statements about emergent ToM-like capabilities\nin LLMs and highlight the need for more rigorous, action-based evaluation\nframeworks.",
      "url": "http://arxiv.org/abs/2510.13395v1",
      "published_time_eastern_timestamp": 1760525311.0
    },
    {
      "title": "Going with the Flow: Approximating Banzhaf Values via Graph Neural\n  Networks",
      "summary": "Computing the Banzhaf value in network flow games is fundamental for\nquantifying agent influence in multi-agent systems, with applications ranging\nfrom cybersecurity to infrastructure planning. However, exact computation is\nintractable for systems with more than $\\sim20$ agents due to exponential\ncomplexity $\\mathcal{O}(2^m)$. While Monte Carlo sampling methods provide\nstatistical estimates, they suffer from high sample complexity and cannot\ntransfer knowledge across different network configurations, making them\nimpractical for large-scale or dynamic systems. We present a novel\nlearning-based approach using Graph Neural Networks (GNNs) to approximate\nBanzhaf values in cardinal network flow games. By framing the problem as a\ngraph-level prediction task, our method learns generalisable patterns of agent\ninfluence directly from network topology and control structure. We conduct a\ncomprehensive empirical study comparing three state-of-the-art GNN\narchitectures-Graph Attention Networks (GAT), Graph Isomorphism Networks with\nEdge features (GINE), and EdgeConv-on a large-scale synthetic dataset of\n200,000 graphs per configuration, varying in size (20-100 nodes), agent count\n(5-20), and edge probability (0.5-1.0). Our results demonstrate that trained\nGNN models achieve high-fidelity Banzhaf value approximation with\norder-of-magnitude speedups compared to exact and sampling-based methods. Most\nsignificantly, we show strong zero-shot generalisation: models trained on\ngraphs of a specific size and topology accurately predict Banzhaf values for\nentirely new networks with different structural properties, without requiring\nretraining. This work establishes GNNs as a practical tool for scalable\ncooperative game-theoretic analysis of complex networked systems.",
      "url": "http://arxiv.org/abs/2510.13391v1",
      "published_time_eastern_timestamp": 1760524833.0
    },
    {
      "title": "Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in\n  Real-World Dialogues without Pre-Commitment",
      "summary": "Persuasion, a fundamental social capability for humans, remains a challenge\nfor AI systems such as large language models (LLMs). Current studies often\noverlook the strategic use of information asymmetry in message design or rely\non strong assumptions regarding pre-commitment. In this work, we explore the\napplication of Bayesian Persuasion (BP) in natural language within single-turn\ndialogue settings, to enhance the strategic persuasion capabilities of LLMs.\nOur framework incorporates a commitment-communication mechanism, where the\npersuader explicitly outlines an information schema by narrating their\npotential types (e.g., honest or dishonest), thereby guiding the persuadee in\nperforming the intended Bayesian belief update. We evaluate two variants of our\napproach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language\n(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)\nbaselines within a comprehensive evaluation framework. This framework covers a\ndiverse set of persuadees -- including LLM instances with varying prompts and\nfine-tuning and human participants -- across tasks ranging from specially\ndesigned persuasion scenarios to general everyday situations. Experimental\nresults on LLM-based agents reveal three main findings: (1) LLMs guided by BP\nstrategies consistently achieve higher persuasion success rates than NBP\nbaselines; (2) SFNL exhibits greater credibility and logical coherence, while\nFNL shows stronger emotional resonance and robustness in naturalistic\nconversations; (3) with supervised fine-tuning, smaller models can attain BP\nperformance comparable to that of larger models.",
      "url": "http://arxiv.org/abs/2510.13387v2",
      "published_time_eastern_timestamp": 1760523962.0
    },
    {
      "title": "Prediction Markets with Intermittent Contributions",
      "summary": "Although both data availability and the demand for accurate forecasts are\nincreasing, collaboration between stakeholders is often constrained by data\nownership and competitive interests. In contrast to recent proposals within\ncooperative game-theoretical frameworks, we place ourselves in a more general\nframework, based on prediction markets. There, independent agents trade\nforecasts of uncertain future events in exchange for rewards. We introduce and\nanalyse a prediction market that (i) accounts for the historical performance of\nthe agents, (ii) adapts to time-varying conditions, while (iii) permitting\nagents to enter and exit the market at will. The proposed design employs robust\nregression models to learn the optimal forecasts' combination whilst handling\nmissing submissions. Moreover, we introduce a pay-off allocation mechanism that\nconsiders both in-sample and out-of-sample performance while satisfying several\ndesirable economic properties. Case-studies using simulated and real-world data\nallow demonstrating the effectiveness and adaptability of the proposed market\ndesign.",
      "url": "http://arxiv.org/abs/2510.13385v1",
      "published_time_eastern_timestamp": 1760523808.0
    }
  ]
}
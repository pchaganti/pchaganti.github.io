{
  "last_updated": "2025-05-19T14:16:34.113472-04:00",
  "papers": [
    {
      "title": "Automatic Reward Shaping from Confounded Offline Data",
      "summary": "A key task in Artificial Intelligence is learning effective policies for\ncontrolling agents in unknown environments to optimize performance measures.\nOff-policy learning methods, like Q-learning, allow learners to make optimal\ndecisions based on past experiences. This paper studies off-policy learning\nfrom biased data in complex and high-dimensional domains where \\emph{unobserved\nconfounding} cannot be ruled out a priori. Building on the well-celebrated Deep\nQ-Network (DQN), we propose a novel deep reinforcement learning algorithm\nrobust to confounding biases in observed data. Specifically, our algorithm\nattempts to find a safe policy for the worst-case environment compatible with\nthe observations. We apply our method to twelve confounded Atari games, and\nfind that it consistently dominates the standard DQN in all games where the\nobserved input to the behavioral and target policies mismatch and unobserved\nconfounders exist.",
      "url": "http://arxiv.org/abs/2505.11478v1",
      "published_time_eastern_timestamp": 1747417201.0
    },
    {
      "title": "Signal attenuation enables scalable decentralized multi-agent\n  reinforcement learning over networks",
      "summary": "Classic multi-agent reinforcement learning (MARL) methods require that agents\nenjoy global state observability, preventing development of decentralized\nalgorithms and limiting scalability. Recent work has shown that, under\nassumptions on decaying inter-agent influence, global observability can be\nreplaced by local neighborhood observability at each agent, enabling\ndecentralization and scalability. Real-world applications enjoying such decay\nproperties remain underexplored, however, despite the fact that signal power\ndecay, or signal attenuation, due to path loss is an intrinsic feature of many\nproblems in wireless communications and radar networks. In this paper, we show\nthat signal attenuation enables decentralization in MARL by considering the\nillustrative special case of performing power allocation for target detection\nin a radar network. To achieve this, we propose two new constrained multi-agent\nMarkov decision process formulations of this power allocation problem, derive\nlocal neighborhood approximations for global value function and gradient\nestimates and establish corresponding error bounds, and develop decentralized\nsaddle point policy gradient algorithms for solving the proposed problems. Our\napproach, though oriented towards the specific radar network problem we\nconsider, provides a useful model for future extensions to additional problems\nin wireless communications and radar networks.",
      "url": "http://arxiv.org/abs/2505.11461v1",
      "published_time_eastern_timestamp": 1747415677.0
    },
    {
      "title": "Robust Equilibria in Shared Resource Allocation via Strengthening\n  Border's Theorem",
      "summary": "We consider repeated allocation of a shared resource via a non-monetary\nmechanism, wherein a single item must be allocated to one of multiple agents in\neach round. We assume that each agent has i.i.d. values for the item across\nrounds, and additive utilities. Past work on this problem has proposed\nmechanisms where agents can get one of two kinds of guarantees: $(i)$\n(approximate) Bayes-Nash equilibria via linkage-based mechanisms which need\nextensive knowledge of the value distributions, and $(ii)$ simple\ndistribution-agnostic mechanisms with robust utility guarantees for each\nindividual agent, which are worse than the Nash outcome, but hold irrespective\nof how others behave (including possibly collusive behavior). Recent work has\nhinted at barriers to achieving both simultaneously. Our work however\nestablishes this is not the case, by proposing the first mechanism in which\neach agent has a natural strategy that is both a Bayes-Nash equilibrium and\nalso comes with strong robust guarantees for individual agent utilities.\n  Our mechanism comes out of a surprising connection between the online shared\nresource allocation problem and implementation theory. In particular, we show\nthat establishing robust equilibria in this setting reduces to showing that a\nparticular subset of the Border polytope is non-empty. We establish this via a\nnovel joint Schur-convexity argument. This strengthening of Border's criterion\nfor obtaining a stronger conclusion is of independent technical interest, as it\nmay prove useful in other settings.",
      "url": "http://arxiv.org/abs/2505.11431v1",
      "published_time_eastern_timestamp": 1747414273.0
    },
    {
      "title": "Can AI automatically analyze public opinion? A LLM agents-based agentic\n  pipeline for timely public opinion analysis",
      "summary": "This study proposes and implements the first LLM agents based agentic\npipeline for multi task public opinion analysis. Unlike traditional methods, it\noffers an end-to-end, fully automated analytical workflow without requiring\ndomain specific training data, manual annotation, or local deployment. The\npipeline integrates advanced LLM capabilities into a low-cost, user-friendly\nframework suitable for resource constrained environments. It enables timely,\nintegrated public opinion analysis through a single natural language query,\nmaking it accessible to non-expert users. To validate its effectiveness, the\npipeline was applied to a real world case study of the 2025 U.S. China tariff\ndispute, where it analyzed 1,572 Weibo posts and generated a structured, multi\npart analytical report. The results demonstrate some relationships between\npublic opinion and governmental decision-making. These contributions represent\na novel advancement in applying generative AI to public governance, bridging\nthe gap between technical sophistication and practical usability in public\nopinion monitoring.",
      "url": "http://arxiv.org/abs/2505.11401v1",
      "published_time_eastern_timestamp": 1747411768.0
    },
    {
      "title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language\n  Navigation",
      "summary": "Vision-and-Language Navigation (VLN) is a core task where embodied agents\nleverage their spatial mobility to navigate in 3D environments toward\ndesignated destinations based on natural language instructions. Recently,\nvideo-language large models (Video-VLMs) with strong generalization\ncapabilities and rich commonsense knowledge have shown remarkable performance\nwhen applied to VLN tasks. However, these models still encounter the following\nchallenges when applied to real-world 3D navigation: 1) Insufficient\nunderstanding of 3D geometry and spatial semantics; 2) Limited capacity for\nlarge-scale exploration and long-term environmental memory; 3) Poor\nadaptability to dynamic and changing environments.To address these limitations,\nwe propose Dynam3D, a dynamic layered 3D representation model that leverages\nlanguage-aligned, generalizable, and hierarchical 3D representations as visual\ninput to train 3D-VLM in navigation action prediction. Given posed RGB-D\nimages, our Dynam3D projects 2D CLIP features into 3D space and constructs\nmulti-level 3D patch-instance-zone representations for 3D geometric and\nsemantic understanding with a dynamic and layer-wise update strategy. Our\nDynam3D is capable of online encoding and localization of 3D instances, and\ndynamically updates them in changing environments to provide large-scale\nexploration and long-term memory capabilities for navigation. By leveraging\nlarge-scale 3D-language pretraining and task-specific adaptation, our Dynam3D\nsets new state-of-the-art performance on VLN benchmarks including R2R-CE,\nREVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for\npre-exploration, lifelong memory, and real-world robot validate the\neffectiveness of practical deployment.",
      "url": "http://arxiv.org/abs/2505.11383v1",
      "published_time_eastern_timestamp": 1747410387.0
    },
    {
      "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM\n  Agents",
      "summary": "Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines.",
      "url": "http://arxiv.org/abs/2505.11368v1",
      "published_time_eastern_timestamp": 1747409543.0
    },
    {
      "title": "Long-Term Average Impulse Control with Mean Field Interactions",
      "summary": "This paper analyzes and provides explicit solutions for a long-term average\nimpulse control problem with a specific mean-field interaction. The underlying\nprocess is a general one-dimensional diffusion with appropriate boundary\nbehavior. The model is motivated by applications such the optimal long-term\nmanagement of renewable natural resources and financial portfolio management.\nEach individual agent seeks to maximize the long-term average reward, which\nconsists of a running reward and incomes from discrete impulses, where the unit\nintervention price depends on the market through a stationary supply rate. In a\ncompetitive market setting, we establish the existence of and explicitly\ncharacterize an equilibrium strategy within a large class of policies under\nmild conditions. Additionally, we formulate and solve the mean field control\nproblem, in which agents cooperate with each other, aiming to realize a common\nmaximal long-term average profit. To illustrate the theoretical results, we\nexamine a stochastic logistic growth model and a population growth model in a\nstochastic environment with impulse control.",
      "url": "http://arxiv.org/abs/2505.11345v1",
      "published_time_eastern_timestamp": 1747408368.0
    },
    {
      "title": "Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for\n  Aerial Combat Tactics",
      "summary": "Artificial intelligence (AI) is reshaping strategic planning, with\nMulti-Agent Reinforcement Learning (MARL) enabling coordination among\nautonomous agents in complex scenarios. However, its practical deployment in\nsensitive military contexts is constrained by the lack of explainability, which\nis an essential factor for trust, safety, and alignment with human strategies.\nThis work reviews and assesses current advances in explainability methods for\nMARL with a focus on simulated air combat scenarios. We proceed by adapting\nvarious explainability techniques to different aerial combat scenarios to gain\nexplanatory insights about the model behavior. By linking AI-generated tactics\nwith human-understandable reasoning, we emphasize the need for transparency to\nensure reliable deployment and meaningful human-machine interaction. By\nilluminating the crucial importance of explainability in advancing MARL for\noperational defense, our work supports not only strategic planning but also the\ntraining of military personnel with insightful and comprehensible analyses.",
      "url": "http://arxiv.org/abs/2505.11311v1",
      "published_time_eastern_timestamp": 1747406190.0
    },
    {
      "title": "Diffusion Learning with Partial Agent Participation and Local Updates",
      "summary": "Diffusion learning is a framework that endows edge devices with advanced\nintelligence. By processing and analyzing data locally and allowing each agent\nto communicate with its immediate neighbors, diffusion effectively protects the\nprivacy of edge devices, enables real-time response, and reduces reliance on\ncentral servers. However, traditional diffusion learning relies on\ncommunication at every iteration, leading to communication overhead, especially\nwith large learning models. Furthermore, the inherent volatility of edge\ndevices, stemming from power outages or signal loss, poses challenges to\nreliable communication between neighboring agents. To mitigate these issues,\nthis paper investigates an enhanced diffusion learning approach incorporating\nlocal updates and partial agent participation. Local updates will curtail\ncommunication frequency, while partial agent participation will allow for the\ninclusion of agents based on their availability. We prove that the resulting\nalgorithm is stable in the mean-square error sense and provide a tight analysis\nof its Mean-Square-Deviation (MSD) performance. Various numerical experiments\nare conducted to illustrate our theoretical findings.",
      "url": "http://arxiv.org/abs/2505.11307v1",
      "published_time_eastern_timestamp": 1747406029.0
    },
    {
      "title": "Meta-World+: An Improved, Standardized, RL Benchmark",
      "summary": "Meta-World is widely used for evaluating multi-task and meta-reinforcement\nlearning agents, which are challenged to master diverse skills simultaneously.\nSince its introduction however, there have been numerous undocumented changes\nwhich inhibit a fair comparison of algorithms. This work strives to\ndisambiguate these results from the literature, while also leveraging the past\nversions of Meta-World to provide insights into multi-task and\nmeta-reinforcement learning benchmark design. Through this process we release a\nnew open-source version of Meta-World\n(https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility\nof past results, is more technically ergonomic, and gives users more control\nover the tasks that are included in a task set.",
      "url": "http://arxiv.org/abs/2505.11289v1",
      "published_time_eastern_timestamp": 1747405443.0
    },
    {
      "title": "TAIJI: MCP-based Multi-Modal Data Analytics on Data Lakes",
      "summary": "The variety of data in data lakes presents significant challenges for data\nanalytics, as data scientists must simultaneously analyze multi-modal data,\nincluding structured, semi-structured, and unstructured data. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities, they still\nremain inadequate for multi-modal data analytics in terms of accuracy,\nefficiency, and freshness. First, current natural language (NL) or SQL-like\nquery languages may struggle to precisely and comprehensively capture users'\nanalytical intent. Second, relying on a single unified LLM to process diverse\ndata modalities often leads to substantial inference overhead. Third, data\nstored in data lakes may be incomplete or outdated, making it essential to\nintegrate external open-domain knowledge to generate timely and relevant\nanalytics results.\n  In this paper, we envision a new multi-modal data analytics system.\nSpecifically, we propose a novel architecture built upon the Model Context\nProtocol (MCP), an emerging paradigm that enables LLMs to collaborate with\nknowledgeable agents. First, we define a semantic operator hierarchy tailored\nfor querying multi-modal data in data lakes and develop an AI-agent-powered\nNL2Operator translator to bridge user intent and analytical execution. Next, we\nintroduce an MCP-based execution framework, in which each MCP server hosts\nspecialized foundation models optimized for specific data modalities. This\ndesign enhances both accuracy and efficiency, while supporting high scalability\nthrough modular deployment. Finally, we propose a updating mechanism by\nharnessing the deep research and machine unlearning techniques to refresh the\ndata lakes and LLM knowledges, with the goal of balancing the data freshness\nand inference efficiency.",
      "url": "http://arxiv.org/abs/2505.11270v1",
      "published_time_eastern_timestamp": 1747404210.0
    },
    {
      "title": "Massive-STEPS: Massive Semantic Trajectories for Understanding POI\n  Check-ins -- Dataset and Benchmarks",
      "summary": "Understanding human mobility through Point-of-Interest (POI) recommendation\nis increasingly important for applications such as urban planning, personalized\nservices, and generative agent simulation. However, progress in this field is\nhindered by two key challenges: the over-reliance on older datasets from\n2012-2013 and the lack of reproducible, city-level check-in datasets that\nreflect diverse global regions. To address these gaps, we present Massive-STEPS\n(Massive Semantic Trajectories for Understanding POI Check-ins), a large-scale,\npublicly available benchmark dataset built upon the Semantic Trails dataset and\nenriched with semantic POI metadata. Massive-STEPS spans 12 geographically and\nculturally diverse cities and features more recent (2017-2018) and\nlonger-duration (24 months) check-in data than prior datasets. We benchmarked a\nwide range of POI recommendation models on Massive-STEPS using both supervised\nand zero-shot approaches, and evaluated their performance across multiple urban\ncontexts. By releasing Massive-STEPS, we aim to facilitate reproducible and\nequitable research in human mobility and POI recommendation. The dataset and\nbenchmarking code are available at:\nhttps://github.com/cruiseresearchgroup/Massive-STEPS",
      "url": "http://arxiv.org/abs/2505.11239v1",
      "published_time_eastern_timestamp": 1747402158.0
    },
    {
      "title": "Sample Efficient Reinforcement Learning via Large Vision Language Model\n  Distillation",
      "summary": "Recent research highlights the potential of multimodal foundation models in\ntackling complex decision-making challenges. However, their large parameters\nmake real-world deployment resource-intensive and often impractical for\nconstrained systems. Reinforcement learning (RL) shows promise for\ntask-specific agents but suffers from high sample complexity, limiting\npractical applications. To address these challenges, we introduce LVLM to\nPolicy (LVLM2P), a novel framework that distills knowledge from large\nvision-language models (LVLM) into more efficient RL agents. Our approach\nleverages the LVLM as a teacher, providing instructional actions based on\ntrajectories collected by the RL agent, which helps reduce less meaningful\nexploration in the early stages of learning, thereby significantly accelerating\nthe agent's learning progress. Additionally, by leveraging the LVLM to suggest\nactions directly from visual observations, we eliminate the need for manual\ntextual descriptors of the environment, enhancing applicability across diverse\ntasks. Experiments show that LVLM2P significantly enhances the sample\nefficiency of baseline RL algorithms.",
      "url": "http://arxiv.org/abs/2505.11221v1",
      "published_time_eastern_timestamp": 1747401354.0
    },
    {
      "title": "From Intent Discovery to Recognition with Topic Modeling and Synthetic\n  Data",
      "summary": "Understanding and recognizing customer intents in AI systems is crucial,\nparticularly in domains characterized by short utterances and the cold start\nproblem, where recommender systems must include new products or services\nwithout sufficient real user data. Customer utterances are characterized by\ninfrequent word co-occurences and high term variability, which poses\nsignificant challenges for traditional methods in specifying distinct user\nneeds and preparing synthetic queries. To address this, we propose an agentic\nLLM framework for topic modeling and synthetic query generation, which\naccelerates the discovery and recognition of customer intents. We first apply\nhierarchical topic modeling and intent discovery to expand a human-curated\ntaxonomy from 36 generic user intents to 278 granular intents, demonstrating\nthe potential of LLMs to significantly enhance topic specificity and diversity.\nNext, to support newly discovered intents and address the cold start problem,\nwe generate synthetic user query data, which augments real utterances and\nreduces dependency on human annotation, especially in low-resource settings.\nTopic model experiments show substantial improvements in coherence and\nrelevance after topic expansion, while synthetic data experiments indicate that\nin-class few-shot prompting significantly improves the quality and utility of\nsynthetic queries without compromising diversity. We also show that\nLLM-generated intent descriptions and keywords can effectively substitute for\nhuman-curated versions when used as context for synthetic query generation. Our\nresearch underscores the scalability and utility of LLM agents in topic\nmodeling and highlights the strategic use of synthetic utterances to enhance\ndataset variability and coverage for intent recognition. We present a\ncomprehensive and robust framework for online discovery and recognition of new\ncustomer intents in dynamic domains.",
      "url": "http://arxiv.org/abs/2505.11176v1",
      "published_time_eastern_timestamp": 1747398031.0
    },
    {
      "title": "Real-Time Verification of Embodied Reasoning for Generative Skill\n  Acquisition",
      "summary": "Generative skill acquisition enables embodied agents to actively learn a\nscalable and evolving repertoire of control skills, crucial for the advancement\nof large decision models. While prior approaches often rely on supervision\nsignals from generalist agents (e.g., LLMs), their effectiveness in complex 3D\nenvironments remains unclear; exhaustive evaluation incurs substantial\ncomputational costs, significantly hindering the efficiency of skill learning.\nInspired by recent successes in verification models for mathematical reasoning,\nwe propose VERGSA (Verifying Embodied Reasoning in Generative Skill\nAcquisition), a framework that systematically integrates real-time verification\nprinciples into embodied skill learning. VERGSA establishes 1) a seamless\nextension from verification of mathematical reasoning into embodied learning by\ndynamically incorporating contextually relevant tasks into prompts and defining\nsuccess metrics for both subtasks and overall tasks, and 2) an automated,\nscalable reward labeling scheme that synthesizes dense reward signals by\niteratively finalizing the contribution of scene configuration and subtask\nlearning to overall skill acquisition. To the best of our knowledge, this\napproach constitutes the first comprehensive training dataset for\nverification-driven generative skill acquisition, eliminating arduous manual\nreward engineering. Experiments validate the efficacy of our approach: 1) the\nexemplar task pool improves the average task success rates by 21%, 2) our\nverification model boosts success rates by 24% for novel tasks and 36% for\nencountered tasks, and 3) outperforms LLM-as-a-Judge baselines in verification\nquality.",
      "url": "http://arxiv.org/abs/2505.11175v1",
      "published_time_eastern_timestamp": 1747397953.0
    },
    {
      "title": "MPMA: Preference Manipulation Attack Against Model Context Protocol",
      "summary": "Model Context Protocol (MCP) standardizes interface mapping for large\nlanguage models (LLMs) to access external data and tools, which revolutionizes\nthe paradigm of tool selection and facilitates the rapid expansion of the LLM\nagent tool ecosystem. However, as the MCP is increasingly adopted, third-party\ncustomized versions of the MCP server expose potential security\nvulnerabilities. In this paper, we first introduce a novel security threat,\nwhich we term the MCP Preference Manipulation Attack (MPMA). An attacker\ndeploys a customized MCP server to manipulate LLMs, causing them to prioritize\nit over other competing MCP servers. This can result in economic benefits for\nattackers, such as revenue from paid MCP services or advertising income\ngenerated from free servers. To achieve MPMA, we first design a Direct\nPreference Manipulation Attack ($\\mathtt{DPMA}$) that achieves significant\neffectiveness by inserting the manipulative word and phrases into the tool name\nand description. However, such a direct modification is obvious to users and\nlacks stealthiness. To address these limitations, we further propose\nGenetic-based Advertising Preference Manipulation Attack ($\\mathtt{GAPMA}$).\n$\\mathtt{GAPMA}$ employs four commonly used strategies to initialize\ndescriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.\nThe experiment results demonstrate that $\\mathtt{GAPMA}$ balances high\neffectiveness and stealthiness. Our study reveals a critical vulnerability of\nthe MCP in open ecosystems, highlighting an urgent need for robust defense\nmechanisms to ensure the fairness of the MCP ecosystem.",
      "url": "http://arxiv.org/abs/2505.11154v1",
      "published_time_eastern_timestamp": 1747396512.0
    },
    {
      "title": "Bi-directional Recurrence Improves Transformer in Partially Observable\n  Markov Decision Processes",
      "summary": "In real-world reinforcement learning (RL) scenarios, agents often encounter\npartial observability, where incomplete or noisy information obscures the true\nstate of the environment. Partially Observable Markov Decision Processes\n(POMDPs) are commonly used to model these environments, but effective\nperformance requires memory mechanisms to utilise past observations. While\nrecurrence networks have traditionally addressed this need, transformer-based\nmodels have recently shown improved sample efficiency in RL tasks. However,\ntheir application to POMDPs remains underdeveloped, and their real-world\ndeployment is constrained due to the high parameter count. This work introduces\na novel bi-recurrent model architecture that improves sample efficiency and\nreduces model parameter count in POMDP scenarios. The architecture replaces the\nmultiple feed forward layers with a single layer of bi-directional recurrence\nunit to better capture and utilize sequential dependencies and contextual\ninformation. This approach improves the model's ability to handle partial\nobservability and increases sample efficiency, enabling effective learning from\ncomparatively fewer interactions. To evaluate the performance of the proposed\nmodel architecture, experiments were conducted on a total of 23 POMDP\nenvironments. The proposed model architecture outperforms existing\ntransformer-based, attention-based, and recurrence-based methods by a margin\nranging from 87.39% to 482.04% on average across the 23 POMDP environments.",
      "url": "http://arxiv.org/abs/2505.11153v1",
      "published_time_eastern_timestamp": 1747396488.0
    },
    {
      "title": "Reinforcement Learning for AMR Charging Decisions: The Impact of Reward\n  and Action Space Design",
      "summary": "We propose a novel reinforcement learning (RL) design to optimize the\ncharging strategy for autonomous mobile robots in large-scale block stacking\nwarehouses. RL design involves a wide array of choices that can mostly only be\nevaluated through lengthy experimentation. Our study focuses on how different\nreward and action space configurations, ranging from flexible setups to more\nguided, domain-informed design configurations, affect the agent performance.\nUsing heuristic charging strategies as a baseline, we demonstrate the\nsuperiority of flexible, RL-based approaches in terms of service times.\nFurthermore, our findings highlight a trade-off: While more open-ended designs\nare able to discover well-performing strategies on their own, they may require\nlonger convergence times and are less stable, whereas guided configurations\nlead to a more stable learning process but display a more limited\ngeneralization potential. Our contributions are threefold. First, we extend\nSLAPStack, an open-source, RL-compatible simulation-framework to accommodate\ncharging strategies. Second, we introduce a novel RL design for tackling the\ncharging strategy problem. Finally, we introduce several novel adaptive\nbaseline heuristics and reproducibly evaluate the design using a Proximal\nPolicy Optimization agent and varying different design configurations, with a\nfocus on reward.",
      "url": "http://arxiv.org/abs/2505.11136v1",
      "published_time_eastern_timestamp": 1747395209.0
    },
    {
      "title": "Scalability of Reinforcement Learning Methods for Dispatching in\n  Semiconductor Frontend Fabs: A Comparison of Open-Source Models with Real\n  Industry Datasets",
      "summary": "Benchmark datasets are crucial for evaluating approaches to scheduling or\ndispatching in the semiconductor industry during the development and deployment\nphases. However, commonly used benchmark datasets like the Minifab or SMT2020\nlack the complex details and constraints found in real-world scenarios. To\nmitigate this shortcoming, we compare open-source simulation models with a real\nindustry dataset to evaluate how optimization methods scale with different\nlevels of complexity. Specifically, we focus on Reinforcement Learning methods,\nperforming optimization based on policy-gradient and Evolution Strategies. Our\nresearch provides insights into the effectiveness of these optimization methods\nand their applicability to realistic semiconductor frontend fab simulations. We\nshow that our proposed Evolution Strategies-based method scales much better\nthan a comparable policy-gradient-based approach. Moreover, we identify the\nselection and combination of relevant bottleneck tools to control by the agent\nas crucial for an efficient optimization. For the generalization across\ndifferent loading scenarios and stochastic tool failure patterns, we achieve\nadvantages when utilizing a diverse training dataset. While the overall\napproach is computationally expensive, it manages to scale well with the number\nof CPU cores used for training. For the real industry dataset, we achieve an\nimprovement of up to 4% regarding tardiness and up to 1% regarding throughput.\nFor the less complex open-source models Minifab and SMT2020, we observe\ndouble-digit percentage improvement in tardiness and single digit percentage\nimprovement in throughput by use of Evolution Strategies.",
      "url": "http://arxiv.org/abs/2505.11135v1",
      "published_time_eastern_timestamp": 1747395149.0
    },
    {
      "title": "Group Think: Multiple Concurrent Reasoning Agents Collaborating at Token\n  Level Granularity",
      "summary": "Recent advances in large language models (LLMs) have demonstrated the power\nof reasoning through self-generated chains of thought. Multiple reasoning\nagents can collaborate to raise joint reasoning quality above individual\noutcomes. However, such agents typically interact in a turn-based manner,\ntrading increased latency for improved quality. In this paper, we propose Group\nThink--a single LLM that acts as multiple concurrent reasoning agents, or\nthinkers. With shared visibility into each other's partial generation progress,\nGroup Think introduces a new concurrent-reasoning paradigm in which multiple\nreasoning trajectories adapt dynamically to one another at the token level. For\nexample, a reasoning thread may shift its generation mid-sentence upon\ndetecting that another thread is better positioned to continue. This\nfine-grained, token-level collaboration enables Group Think to reduce redundant\nreasoning and improve quality while achieving significantly lower latency.\nMoreover, its concurrent nature allows for efficient utilization of idle\ncomputational resources, making it especially suitable for edge inference,\nwhere very small batch size often underutilizes local~GPUs. We give a simple\nand generalizable modification that enables any existing LLM to perform Group\nThink on a local GPU. We also present an evaluation strategy to benchmark\nreasoning latency and empirically demonstrate latency improvements using\nopen-source LLMs that were not explicitly trained for Group Think. We hope this\nwork paves the way for future LLMs to exhibit more sophisticated and more\nefficient collaborative behavior for higher quality generation.",
      "url": "http://arxiv.org/abs/2505.11107v1",
      "published_time_eastern_timestamp": 1747392035.0
    }
  ]
}
{
  "last_updated": "2025-08-07T02:20:55.499450-04:00",
  "papers": [
    {
      "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience",
      "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
      "url": "http://arxiv.org/abs/2508.04700v1",
      "published_time_eastern_timestamp": 1754503126.0
    },
    {
      "title": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in\n  Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario",
      "summary": "Multi-agent robotic systems (MARS) build upon multi-agent systems by\nintegrating physical and task-related constraints, increasing the complexity of\naction execution and agent coordination. However, despite the availability of\nadvanced multi-agent frameworks, their real-world deployment on robots remains\nlimited, hindering the advancement of MARS research in practice. To bridge this\ngap, we conducted two studies to investigate performance trade-offs of\nhierarchical multi-agent frameworks in a simulated real-world multi-robot\nhealthcare scenario. In Study 1, using CrewAI, we iteratively refine the\nsystem's knowledge base, to systematically identify and categorize coordination\nfailures (e.g., tool access violations, lack of timely handling of failure\nreports) not resolvable by providing contextual knowledge alone. In Study 2,\nusing AutoGen, we evaluate a redesigned bidirectional communication structure\nand further measure the trade-offs between reasoning and non-reasoning models\noperating within the same robotic team setting. Drawing from our empirical\nfindings, we emphasize the tension between autonomy and stability and the\nimportance of edge-case testing to improve system reliability and safety for\nfuture real-world deployment. Supplementary materials, including codes, task\nagent setup, trace outputs, and annotated examples of coordination failures and\nreasoning behaviors, are available at:\nhttps://byc-sophie.github.io/mas-to-mars/.",
      "url": "http://arxiv.org/abs/2508.04691v1",
      "published_time_eastern_timestamp": 1754502850.0
    },
    {
      "title": "TurboTrain: Towards Efficient and Balanced Multi-Task Learning for\n  Multi-Agent Perception and Prediction",
      "summary": "End-to-end training of multi-agent systems offers significant advantages in\nimproving multi-task performance. However, training such models remains\nchallenging and requires extensive manual design and monitoring. In this work,\nwe introduce TurboTrain, a novel and efficient training framework for\nmulti-agent perception and prediction. TurboTrain comprises two key components:\na multi-agent spatiotemporal pretraining scheme based on masked reconstruction\nlearning and a balanced multi-task learning strategy based on gradient conflict\nsuppression. By streamlining the training process, our framework eliminates the\nneed for manually designing and tuning complex multi-stage training pipelines,\nsubstantially reducing training time and improving performance. We evaluate\nTurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and\ndemonstrate that it further improves the performance of state-of-the-art\nmulti-agent perception and prediction models. Our results highlight that\npretraining effectively captures spatiotemporal multi-agent features and\nsignificantly benefits downstream tasks. Moreover, the proposed balanced\nmulti-task learning strategy enhances detection and prediction.",
      "url": "http://arxiv.org/abs/2508.04682v1",
      "published_time_eastern_timestamp": 1754502400.0
    },
    {
      "title": "Perceiving and Acting in First-Person: A Dataset and Benchmark for\n  Egocentric Human-Object-Human Interactions",
      "summary": "Learning action models from real-world human-centric interaction datasets is\nimportant towards building general-purpose intelligent assistants with\nefficiency. However, most existing datasets only offer specialist interaction\ncategory and ignore that AI assistants perceive and act based on first-person\nacquisition. We urge that both the generalist interaction knowledge and\negocentric modality are indispensable. In this paper, we embed the\nmanual-assisted task into a vision-language-action framework, where the\nassistant provides services to the instructor following egocentric vision and\ncommands. With our hybrid RGB-MoCap system, pairs of assistants and instructors\nengage with multiple objects and the scene following GPT-generated scripts.\nUnder this setting, we accomplish InterVLA, the first large-scale\nhuman-object-human interaction dataset with 11.4 hours and 1.2M frames of\nmultimodal data, spanning 2 egocentric and 5 exocentric videos, accurate\nhuman/object motions and verbal commands. Furthermore, we establish novel\nbenchmarks on egocentric human motion estimation, interaction synthesis, and\ninteraction prediction with comprehensive analysis. We believe that our\nInterVLA testbed and the benchmarks will foster future works on building AI\nagents in the physical world.",
      "url": "http://arxiv.org/abs/2508.04681v1",
      "published_time_eastern_timestamp": 1754502383.0
    },
    {
      "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
      "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.",
      "url": "http://arxiv.org/abs/2508.04652v1",
      "published_time_eastern_timestamp": 1754500705.0
    },
    {
      "title": "VirtLab: An AI-Powered System for Flexible, Customizable, and\n  Large-scale Team Simulations",
      "summary": "Simulating how team members collaborate within complex environments using\nAgentic AI is a promising approach to explore hypotheses grounded in social\nscience theories and study team behaviors. We introduce VirtLab, a\nuser-friendly, customizable, multi-agent, and scalable team simulation system\nthat enables testing teams with LLM-based agents in spatial and temporal\nsettings. This system addresses the current frameworks' design and technical\nlimitations that do not consider flexible simulation scenarios and spatial\nsettings. VirtLab contains a simulation engine and a web interface that enables\nboth technical and non-technical users to formulate, run, and analyze team\nsimulations without programming. We demonstrate the system's utility by\ncomparing ground truth data with simulated scenarios.",
      "url": "http://arxiv.org/abs/2508.04634v1",
      "published_time_eastern_timestamp": 1754499721.0
    },
    {
      "title": "TURA: Tool-Augmented Unified Retrieval Agent for AI Search",
      "summary": "The advent of Large Language Models (LLMs) is transforming search engines\ninto conversational AI search products, primarily using Retrieval-Augmented\nGeneration (RAG) on web corpora. However, this paradigm has significant\nindustrial limitations. Traditional RAG approaches struggle with real-time\nneeds and structured queries that require accessing dynamically generated\ncontent like ticket availability or inventory. Limited to indexing static\npages, search engines cannot perform the interactive queries needed for such\ntime-sensitive data. Academic research has focused on optimizing RAG for static\ncontent, overlooking complex intents and the need for dynamic sources like\ndatabases and real-time APIs. To bridge this gap, we introduce TURA\n(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage\nframework that combines RAG with agentic tool-use to access both static content\nand dynamic, real-time information. TURA has three key components: an\nIntent-Aware Retrieval module to decompose queries and retrieve information\nsources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task\nPlanner that models task dependencies as a Directed Acyclic Graph (DAG) for\noptimal parallel execution, and a lightweight Distilled Agent Executor for\nefficient tool calling. TURA is the first architecture to systematically bridge\nthe gap between static RAG and dynamic information sources for a world-class AI\nsearch product. Serving tens of millions of users, it leverages an agentic\nframework to deliver robust, real-time answers while meeting the low-latency\ndemands of a large-scale industrial system.",
      "url": "http://arxiv.org/abs/2508.04604v1",
      "published_time_eastern_timestamp": 1754497457.0
    },
    {
      "title": "UniTalker: Conversational Speech-Visual Synthesis",
      "summary": "Conversational Speech Synthesis (CSS) is a key task in the user-agent\ninteraction area, aiming to generate more expressive and empathetic speech for\nusers. However, it is well-known that \"listening\" and \"eye contact\" play\ncrucial roles in conveying emotions during real-world interpersonal\ncommunication. Existing CSS research is limited to perceiving only text and\nspeech within the dialogue context, which restricts its effectiveness.\nMoreover, speech-only responses further constrain the interactive experience.\nTo address these limitations, we introduce a Conversational Speech-Visual\nSynthesis (CSVS) task as an extension of traditional CSS. By leveraging\nmultimodal dialogue context, it provides users with coherent audiovisual\nresponses. To this end, we develop a CSVS system named UniTalker, which is a\nunified model that seamlessly integrates multimodal perception and multimodal\nrendering capabilities. Specifically, it leverages a large-scale language model\nto comprehensively understand multimodal cues in the dialogue context,\nincluding speaker, text, speech, and the talking-face animations. After that,\nit employs multi-task sequence prediction to first infer the target utterance's\nemotion and then generate empathetic speech and natural talking-face\nanimations. To ensure that the generated speech-visual content remains\nconsistent in terms of emotion, content, and duration, we introduce three key\noptimizations: 1) Designing a specialized neural landmark codec to tokenize and\nreconstruct facial expression sequences. 2) Proposing a bimodal speech-visual\nhard alignment decoding strategy. 3) Applying emotion-guided rendering during\nthe generation stage. Comprehensive objective and subjective experiments\ndemonstrate that our model synthesizes more empathetic speech and provides\nusers with more natural and emotionally consistent talking-face animations.",
      "url": "http://arxiv.org/abs/2508.04585v1",
      "published_time_eastern_timestamp": 1754496502.0
    },
    {
      "title": "Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons\n  from Multi-Agent Collaboration",
      "summary": "While AI agents show potential in scientific ideation, most existing\nframeworks rely on single-agent refinement, limiting creativity due to bounded\nknowledge and perspective. Inspired by real-world research dynamics, this paper\ninvestigates whether structured multi-agent discussions can surpass solitary\nideation. We propose a cooperative multi-agent framework for generating\nresearch proposals and systematically compare configurations including group\nsize, leaderled versus leaderless structures, and team compositions varying in\ninterdisciplinarity and seniority. To assess idea quality, we employ a\ncomprehensive protocol with agent-based scoring and human review across\ndimensions such as novelty, strategic vision, and integration depth. Our\nresults show that multi-agent discussions substantially outperform solitary\nbaselines. A designated leader acts as a catalyst, transforming discussion into\nmore integrated and visionary proposals. Notably, we find that cognitive\ndiversity is a primary driver of quality, yet expertise is a non-negotiable\nprerequisite, as teams lacking a foundation of senior knowledge fail to surpass\neven a single competent agent. These findings offer actionable insights for\ndesigning collaborative AI ideation systems and shed light on how team\nstructure influences creative outcomes.",
      "url": "http://arxiv.org/abs/2508.04575v1",
      "published_time_eastern_timestamp": 1754495958.0
    },
    {
      "title": "CONVERGE: A Multi-Agent Vision-Radio Architecture for xApps",
      "summary": "Telecommunications and computer vision have evolved independently. With the\nemergence of high-frequency wireless links operating mostly in line-of-sight,\nvisual data can help predict the channel dynamics by detecting obstacles and\nhelp overcoming them through beamforming or handover techniques.\n  This paper proposes a novel architecture for delivering real-time radio and\nvideo sensing information to O-RAN xApps through a multi-agent approach, and\nintroduces a new video function capable of generating blockage information for\nxApps, enabling Integrated Sensing and Communications. Experimental results\nshow that the delay of sensing information remains under 1\\,ms and that an xApp\ncan successfully use radio and video sensing information to control the 5G/6G\nRAN in real-time.",
      "url": "http://arxiv.org/abs/2508.04556v1",
      "published_time_eastern_timestamp": 1754494852.0
    }
  ]
}
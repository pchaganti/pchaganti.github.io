{
  "last_updated": "2025-05-20T02:17:45.352692-04:00",
  "papers": [
    {
      "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning",
      "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents.",
      "url": "http://arxiv.org/abs/2505.13426v1",
      "published_time_eastern_timestamp": 1747677279.0
    },
    {
      "title": "A Dataless Reinforcement Learning Approach to Rounding Hyperplane\n  Optimization for Max-Cut",
      "summary": "The Maximum Cut (MaxCut) problem is NP-Complete, and obtaining its optimal\nsolution is NP-hard in the worst case. As a result, heuristic-based algorithms\nare commonly used, though their design often requires significant domain\nexpertise. More recently, learning-based methods trained on large (un)labeled\ndatasets have been proposed; however, these approaches often struggle with\ngeneralizability and scalability. A well-known approximation algorithm for\nMaxCut is the Goemans-Williamson (GW) algorithm, which relaxes the Quadratic\nUnconstrained Binary Optimization (QUBO) formulation into a semidefinite\nprogram (SDP). The GW algorithm then applies hyperplane rounding by uniformly\nsampling a random hyperplane to convert the SDP solution into binary node\nassignments. In this paper, we propose a training-data-free approach based on a\nnon-episodic reinforcement learning formulation, in which an agent learns to\nselect improved rounding hyperplanes that yield better cuts than those produced\nby the GW algorithm. By optimizing over a Markov Decision Process (MDP), our\nmethod consistently achieves better cuts across large-scale graphs with varying\ndensities and degree distributions.",
      "url": "http://arxiv.org/abs/2505.13405v1",
      "published_time_eastern_timestamp": 1747676470.0
    },
    {
      "title": "Robin: A multi-agent system for automating scientific discovery",
      "summary": "Scientific discovery is driven by the iterative process of background\nresearch, hypothesis generation, experimentation, and data analysis. Despite\nrecent advancements in applying artificial intelligence to scientific\ndiscovery, no system has yet automated all of these stages in a single\nworkflow. Here, we introduce Robin, the first multi-agent system capable of\nfully automating the key intellectual steps of the scientific process. By\nintegrating literature search agents with data analysis agents, Robin can\ngenerate hypotheses, propose experiments, interpret experimental results, and\ngenerate updated hypotheses, achieving a semi-autonomous approach to scientific\ndiscovery. By applying this system, we were able to identify a novel treatment\nfor dry age-related macular degeneration (dAMD), the major cause of blindness\nin the developed world. Robin proposed enhancing retinal pigment epithelium\nphagocytosis as a therapeutic strategy, and identified and validated a\npromising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho\nkinase (ROCK) inhibitor that has never previously been proposed for treating\ndAMD. To elucidate the mechanism of ripasudil-induced upregulation of\nphagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment,\nwhich revealed upregulation of ABCA1, a critical lipid efflux pump and possible\nnovel target. All hypotheses, experimental plans, data analyses, and data\nfigures in the main text of this report were produced by Robin. As the first AI\nsystem to autonomously discover and validate a novel therapeutic candidate\nwithin an iterative lab-in-the-loop framework, Robin establishes a new paradigm\nfor AI-driven scientific discovery.",
      "url": "http://arxiv.org/abs/2505.13400v1",
      "published_time_eastern_timestamp": 1747676177.0
    },
    {
      "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and\n  Challenges",
      "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons.",
      "url": "http://arxiv.org/abs/2505.13328v1",
      "published_time_eastern_timestamp": 1747672573.0
    },
    {
      "title": "Synthesis of Communication Policies for Multi-Agent Systems Robust to\n  Communication Restrictions",
      "summary": "We study stochastic multi-agent systems in which agents must cooperate to\nmaximize the probability of achieving a common reach-avoid objective. In many\napplications, during the execution of the system, the communication between the\nagents can be constrained by restrictions on the bandwidth currently available\nfor exchanging local-state information between the agents.\n  In this paper, we propose a method for computing joint action and\ncommunication policies for the group of agents that aim to satisfy the\ncommunication restrictions as much as possible while achieving the optimal\nreach-avoid probability when communication is unconstrained. Our method\nsynthesizes a pair of action and communication policies robust to restrictions\non the number of agents allowed to communicate. To this end, we introduce a\nnovel cost function that measures the amount of information exchanged beyond\nwhat the communication policy allows. We evaluate our approach experimentally\non a range of benchmarks and demonstrate that it is capable of computing pairs\nof action and communication policies that satisfy the communication\nrestrictions, if such exist.",
      "url": "http://arxiv.org/abs/2505.13311v1",
      "published_time_eastern_timestamp": 1747672017.0
    },
    {
      "title": "TimeSeriesGym: A Scalable Benchmark for (Time Series) Machine Learning\n  Engineering Agents",
      "summary": "We introduce TimeSeriesGym, a scalable benchmarking framework for evaluating\nArtificial Intelligence (AI) agents on time series machine learning engineering\nchallenges. Existing benchmarks lack scalability, focus narrowly on model\nbuilding in well-defined settings, and evaluate only a limited set of research\nartifacts (e.g., CSV submission files). To make AI agent benchmarking more\nrelevant to the practice of machine learning engineering, our framework scales\nalong two critical dimensions. First, recognizing that effective ML engineering\nrequires a range of diverse skills, TimeSeriesGym incorporates challenges from\ndiverse sources spanning multiple domains and tasks. We design challenges to\nevaluate both isolated capabilities (including data handling, understanding\nresearch repositories, and code translation) and their combinations, and rather\nthan addressing each challenge independently, we develop tools that support\ndesigning multiple challenges at scale. Second, we implement evaluation\nmechanisms for multiple research artifacts, including submission files, code,\nand models, using both precise numeric measures and more flexible LLM-based\nevaluation approaches. This dual strategy balances objective assessment with\ncontextual judgment. Although our initial focus is on time series applications,\nour framework can be readily extended to other data modalities, broadly\nenhancing the comprehensiveness and practical utility of agentic AI evaluation.\nWe open-source our benchmarking framework to facilitate future research on the\nML engineering capabilities of AI agents.",
      "url": "http://arxiv.org/abs/2505.13291v1",
      "published_time_eastern_timestamp": 1747671083.0
    },
    {
      "title": "Hybrid Voting-Based Task Assignment in Modular Construction Scenarios",
      "summary": "Modular construction, involving off-site prefabrication and on-site assembly,\noffers significant advantages but presents complex coordination challenges for\nrobotic automation. Effective task allocation is critical for leveraging\nmulti-agent systems (MAS) in these structured environments. This paper\nintroduces the Hybrid Voting-Based Task Assignment (HVBTA) framework, a novel\napproach to optimizing collaboration between heterogeneous multi-agent\nconstruction teams. Inspired by human reasoning in task delegation, HVBTA\nuniquely integrates multiple voting mechanisms with the capabilities of a Large\nLanguage Model (LLM) for nuanced suitability assessment between agent\ncapabilities and task requirements. The framework operates by assigning\nCapability Profiles to agents and detailed requirement lists called Task\nDescriptions to construction tasks, subsequently generating a quantitative\nSuitability Matrix. Six distinct voting methods, augmented by a pre-trained\nLLM, analyze this matrix to robustly identify the optimal agent for each task.\nConflict-Based Search (CBS) is integrated for decentralized, collision-free\npath planning, ensuring efficient and safe spatio-temporal coordination of the\nrobotic team during assembly operations. HVBTA enables efficient, conflict-free\nassignment and coordination, facilitating potentially faster and more accurate\nmodular assembly. Current work is evaluating HVBTA's performance across various\nsimulated construction scenarios involving diverse robotic platforms and task\ncomplexities. While designed as a generalizable framework for any domain with\nclearly definable tasks and capabilities, HVBTA will be particularly effective\nfor addressing the demanding coordination requirements of multi-agent\ncollaborative robotics in modular construction due to the predetermined\nconstruction planning involved.",
      "url": "http://arxiv.org/abs/2505.13278v1",
      "published_time_eastern_timestamp": 1747670496.0
    },
    {
      "title": "From Automation to Autonomy: A Survey on Large Language Models in\n  Scientific Discovery",
      "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.",
      "url": "http://arxiv.org/abs/2505.13259v1",
      "published_time_eastern_timestamp": 1747669292.0
    },
    {
      "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning\n  for Decision Traceability",
      "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.",
      "url": "http://arxiv.org/abs/2505.13258v1",
      "published_time_eastern_timestamp": 1747669229.0
    },
    {
      "title": "Composing Dextrous Grasping and In-hand Manipulation via Scoring with a\n  Reinforcement Learning Critic",
      "summary": "In-hand manipulation and grasping are fundamental yet often separately\naddressed tasks in robotics. For deriving in-hand manipulation policies,\nreinforcement learning has recently shown great success. However, the derived\ncontrollers are not yet useful in real-world scenarios because they often\nrequire a human operator to place the objects in suitable initial (grasping)\nstates. Finding stable grasps that also promote the desired in-hand\nmanipulation goal is an open problem. In this work, we propose a method for\nbridging this gap by leveraging the critic network of a reinforcement learning\nagent trained for in-hand manipulation to score and select initial grasps. Our\nexperiments show that this method significantly increases the success rate of\nin-hand manipulation without requiring additional training. We also present an\nimplementation of a full grasp manipulation pipeline on a real-world system,\nenabling autonomous grasping and reorientation even of unwieldy objects.",
      "url": "http://arxiv.org/abs/2505.13253v1",
      "published_time_eastern_timestamp": 1747668994.0
    },
    {
      "title": "Agentic Publications: An LLM-Driven Framework for Interactive Scientific\n  Publishing, Supplementing Traditional Papers with AI-Powered Knowledge\n  Systems",
      "summary": "The exponential growth of scientific literature presents significant\nchallenges for researchers navigating the complex knowledge landscape. We\npropose \"Agentic Publications\", a novel LLM-driven framework complementing\ntraditional publishing by transforming papers into interactive knowledge\nsystems. Our architecture integrates structured data with unstructured content\nthrough retrieval-augmented generation and multi-agent verification. The\nframework offers interfaces for both humans and machines, combining narrative\nexplanations with machine-readable outputs while addressing ethical\nconsiderations through automated validation and transparent governance. Key\nfeatures include continuous knowledge updates, automatic integration of new\nfindings, and customizable detail levels. Our proof-of-concept demonstrates\nmultilingual interaction, API accessibility, and structured knowledge\nrepresentation through vector databases, knowledge graphs, and verification\nagents. This approach enhances scientific communication across disciplines,\nimproving efficiency and collaboration while preserving traditional publishing\npathways, particularly valuable for interdisciplinary fields where knowledge\nintegration remains challenging.",
      "url": "http://arxiv.org/abs/2505.13246v1",
      "published_time_eastern_timestamp": 1747668490.0
    },
    {
      "title": "Scaling Computer-Use Grounding via User Interface Decomposition and\n  Synthesis",
      "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.",
      "url": "http://arxiv.org/abs/2505.13227v1",
      "published_time_eastern_timestamp": 1747667363.0
    },
    {
      "title": "Adversarial Testing in LLMs: Insights into Decision-Making\n  Vulnerabilities",
      "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world decision-making systems, understanding their behavioural\nvulnerabilities remains a critical challenge for AI safety and alignment. While\nexisting evaluation metrics focus primarily on reasoning accuracy or factual\ncorrectness, they often overlook whether LLMs are robust to adversarial\nmanipulation or capable of using adaptive strategy in dynamic environments.\nThis paper introduces an adversarial evaluation framework designed to\nsystematically stress-test the decision-making processes of LLMs under\ninteractive and adversarial conditions. Drawing on methodologies from cognitive\npsychology and game theory, our framework probes how models respond in two\ncanonical tasks: the two-armed bandit task and the Multi-Round Trust Task.\nThese tasks capture key aspects of exploration-exploitation trade-offs, social\ncooperation, and strategic flexibility. We apply this framework to several\nstate-of-the-art LLMs, including GPT-3.5, GPT-4, Gemini-1.5, and DeepSeek-V3,\nrevealing model-specific susceptibilities to manipulation and rigidity in\nstrategy adaptation. Our findings highlight distinct behavioral patterns across\nmodels and emphasize the importance of adaptability and fairness recognition\nfor trustworthy AI deployment. Rather than offering a performance benchmark,\nthis work proposes a methodology for diagnosing decision-making weaknesses in\nLLM-based agents, providing actionable insights for alignment and safety\nresearch.",
      "url": "http://arxiv.org/abs/2505.13195v1",
      "published_time_eastern_timestamp": 1747666244.0
    },
    {
      "title": "When a Reinforcement Learning Agent Encounters Unknown Unknowns",
      "summary": "An AI agent might surprisingly find she has reached an unknown state which\nshe has never been aware of -- an unknown unknown. We mathematically ground\nthis scenario in reinforcement learning: an agent, after taking an action\ncalculated from value functions $Q$ and $V$ defined on the {\\it {aware\ndomain}}, reaches a state out of the domain. To enable the agent to handle this\nscenario, we propose an {\\it episodic Markov decision {process} with growing\nawareness} (EMDP-GA) model, taking a new {\\it noninformative value expansion}\n(NIVE) approach to expand value functions to newly aware areas: when an agent\narrives at an unknown unknown, value functions $Q$ and $V$ whereon are\ninitialised by noninformative beliefs -- the averaged values on the aware\ndomain. This design is out of respect for the complete absence of knowledge in\nthe newly discovered state. The upper confidence bound momentum Q-learning is\nthen adapted to the growing awareness for training the EMDP-GA model. We prove\nthat (1) the regret of our approach is asymptotically consistent with the state\nof the art (SOTA) without exposure to unknown unknowns in an extremely\nuncertain environment, and (2) our computational complexity and space\ncomplexity are comparable with the SOTA -- these collectively suggest that\nthough an unknown unknown is surprising, it will be asymptotically properly\ndiscovered with decent speed and an affordable cost.",
      "url": "http://arxiv.org/abs/2505.13188v1",
      "published_time_eastern_timestamp": 1747665958.0
    },
    {
      "title": "Information Science Principles of Machine Learning: A Causal Chain\n  Meta-Framework Based on Formalized Information Mapping",
      "summary": "[Objective] This study focuses on addressing the current lack of a unified\nformal theoretical framework in machine learning, as well as the deficiencies\nin interpretability and ethical safety assurance. [Methods] A formal\ninformation model is first constructed, utilizing sets of well-formed formulas\nto explicitly define the ontological states and carrier mappings of typical\ncomponents in machine learning. Learnable and processable predicates, along\nwith learning and processing functions, are introduced to analyze the logical\ndeduction and constraint rules of the causal chains within models. [Results] A\nmeta-framework for machine learning theory (MLT-MF) is established. Based on\nthis framework, universal definitions for model interpretability and ethical\nsafety are proposed. Furthermore, three key theorems are proved: the\nequivalence of model interpretability and information recoverability, the\nassurance of ethical safety, and the estimation of generalization error.\n[Limitations] The current framework assumes ideal conditions with noiseless\ninformation-enabling mappings and primarily targets model learning and\nprocessing logic in static scenarios. It does not yet address information\nfusion and conflict resolution across ontological spaces in multimodal or\nmulti-agent systems. [Conclusions] This work overcomes the limitations of\nfragmented research and provides a unified theoretical foundation for\nsystematically addressing the critical challenges currently faced in machine\nlearning.",
      "url": "http://arxiv.org/abs/2505.13182v1",
      "published_time_eastern_timestamp": 1747665581.0
    },
    {
      "title": "Fixing 7,400 Bugs for 1$: Cheap Crash-Site Program Repair",
      "summary": "The rapid advancement of bug-finding techniques has led to the discovery of\nmore vulnerabilities than developers can reasonably fix, creating an urgent\nneed for effective Automated Program Repair (APR) methods. However, the\ncomplexity of modern bugs often makes precise root cause analysis difficult and\nunreliable. To address this challenge, we propose crash-site repair to simplify\nthe repair task while still mitigating the risk of exploitation. In addition,\nwe introduce a template-guided patch generation approach that significantly\nreduces the token cost of Large Language Models (LLMs) while maintaining both\nefficiency and effectiveness.\n  We implement our prototype system, WILLIAMT, and evaluate it against\nstate-of-the-art APR tools. Our results show that, when combined with the\ntop-performing agent CodeRover-S, WILLIAMT reduces token cost by 45.9% and\nincreases the bug-fixing rate to 73.5% (+29.6%) on ARVO, a ground-truth open\nsource software vulnerabilities benchmark. Furthermore, we demonstrate that\nWILLIAMT can function effectively even without access to frontier LLMs: even a\nlocal model running on a Mac M4 Mini achieves a reasonable repair rate. These\nfindings highlight the broad applicability and scalability of WILLIAMT.",
      "url": "http://arxiv.org/abs/2505.13103v1",
      "published_time_eastern_timestamp": 1747661571.0
    },
    {
      "title": "The Hidden Dangers of Browsing AI Agents",
      "summary": "Autonomous browsing agents powered by large language models (LLMs) are\nincreasingly used to automate web-based tasks. However, their reliance on\ndynamic content, tool execution, and user-provided data exposes them to a broad\nattack surface. This paper presents a comprehensive security evaluation of such\nagents, focusing on systemic vulnerabilities across multiple architectural\nlayers. Our work outlines the first end-to-end threat model for browsing agents\nand provides actionable guidance for securing their deployment in real-world\nenvironments. To address discovered threats, we propose a defense in depth\nstrategy incorporating input sanitization, planner executor isolation, formal\nanalyzers, and session safeguards. These measures protect against both initial\naccess and post exploitation attack vectors. Through a white box analysis of a\npopular open source project, Browser Use, we demonstrate how untrusted web\ncontent can hijack agent behavior and lead to critical security breaches. Our\nfindings include prompt injection, domain validation bypass, and credential\nexfiltration, evidenced by a disclosed CVE and a working proof of concept\nexploit.",
      "url": "http://arxiv.org/abs/2505.13076v1",
      "published_time_eastern_timestamp": 1747660229.0
    },
    {
      "title": "CAIM: Development and Evaluation of a Cognitive AI Memory Framework for\n  Long-Term Interaction with Intelligent Agents",
      "summary": "Large language models (LLMs) have advanced the field of artificial\nintelligence (AI) and are a powerful enabler for interactive systems. However,\nthey still face challenges in long-term interactions that require adaptation\ntowards the user as well as contextual knowledge and understanding of the\never-changing environment. To overcome these challenges, holistic memory\nmodeling is required to efficiently retrieve and store relevant information\nacross interaction sessions for suitable responses. Cognitive AI, which aims to\nsimulate the human thought process in a computerized model, highlights\ninteresting aspects, such as thoughts, memory mechanisms, and decision-making,\nthat can contribute towards improved memory modeling for LLMs. Inspired by\nthese cognitive AI principles, we propose our memory framework CAIM. CAIM\nconsists of three modules: 1.) The Memory Controller as the central decision\nunit; 2.) the Memory Retrieval, which filters relevant data for interaction\nupon request; and 3.) the Post-Thinking, which maintains the memory storage. We\ncompare CAIM against existing approaches, focusing on metrics such as retrieval\naccuracy, response correctness, contextual coherence, and memory storage. The\nresults demonstrate that CAIM outperforms baseline frameworks across different\nmetrics, highlighting its context-awareness and potential to improve long-term\nhuman-AI interactions.",
      "url": "http://arxiv.org/abs/2505.13044v1",
      "published_time_eastern_timestamp": 1747658032.0
    },
    {
      "title": "Adversarial Reasoning for Repair Based on Inferred Program Intent",
      "summary": "Automated program repair (APR) has shown promising results, particularly with\nthe use of neural networks. Currently, most APR tools focus on code\ntransformations specified by test suites, rather than reasoning about the\nprogram intent and the high-level bug specification. Without a proper\nunderstanding of program intent, these tools tend to generate patches that\noverfit incomplete test suites and fail to reflect the developers intentions.\nHowever, reasoning about program intent is challenging. In our work, we propose\nan approach called AdverIntent-Agent, based on critique and adversarial\nreasoning. Our approach is novel to shift the focus from generating multiple\nAPR patches to inferring multiple potential program intents. Ideally, we aim to\ninfer intents that are, to some extent, adversarial to each other, maximizing\nthe probability that at least one aligns closely with the developers original\nintent. AdverIntent-Agent is a multi-agent approach consisting of three agents:\na reasoning agent, a test agent, and a repair agent. First, the reasoning agent\ngenerates adversarial program intents along with the corresponding faulty\nstatements. Next, the test agent produces adversarial test cases that align\nwith each inferred intent, constructing oracles that use the same inputs but\nhave different expected outputs. Finally, the repair agent uses dynamic and\nprecise LLM prompts to generate patches that satisfy both the inferred program\nintent and the generated tests. AdverIntent-Agent was evaluated on two\nbenchmarks: Defects4J 2.0 and HumanEval-Java. AdverIntent-Agent correctly\nrepaired 77 and 105 bugs in both benchmarks, respectively.",
      "url": "http://arxiv.org/abs/2505.13008v1",
      "published_time_eastern_timestamp": 1747655516.0
    },
    {
      "title": "From Assistants to Adversaries: Exploring the Security Risks of Mobile\n  LLM Agents",
      "summary": "The growing adoption of large language models (LLMs) has led to a new\nparadigm in mobile computing--LLM-powered mobile AI agents--capable of\ndecomposing and automating complex tasks directly on smartphones. However, the\nsecurity implications of these agents remain largely unexplored. In this paper,\nwe present the first comprehensive security analysis of mobile LLM agents,\nencompassing three representative categories: System-level AI Agents developed\nby original equipment manufacturers (e.g., YOYO Assistant), Third-party\nUniversal Agents (e.g., Zhipu AI AutoGLM), and Emerging Agent Frameworks (e.g.,\nAlibaba Mobile Agent). We begin by analyzing the general workflow of mobile\nagents and identifying security threats across three core capability\ndimensions: language-based reasoning, GUI-based interaction, and system-level\nexecution. Our analysis reveals 11 distinct attack surfaces, all rooted in the\nunique capabilities and interaction patterns of mobile LLM agents, and spanning\ntheir entire operational lifecycle. To investigate these threats in practice,\nwe introduce AgentScan, a semi-automated security analysis framework that\nsystematically evaluates mobile LLM agents across all 11 attack scenarios.\nApplying AgentScan to nine widely deployed agents, we uncover a concerning\ntrend: every agent is vulnerable to targeted attacks. In the most severe cases,\nagents exhibit vulnerabilities across eight distinct attack vectors. These\nattacks can cause behavioral deviations, privacy leakage, or even full\nexecution hijacking. Based on these findings, we propose a set of defensive\ndesign principles and practical recommendations for building secure mobile LLM\nagents. Our disclosures have received positive feedback from two major device\nvendors. Overall, this work highlights the urgent need for standardized\nsecurity practices in the fast-evolving landscape of LLM-driven mobile\nautomation.",
      "url": "http://arxiv.org/abs/2505.12981v1",
      "published_time_eastern_timestamp": 1747653466.0
    }
  ]
}
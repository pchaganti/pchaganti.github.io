{
  "last_updated": "2025-05-29T05:13:01.833580-04:00",
  "papers": [
    {
      "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large\n  Language Model",
      "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks.",
      "url": "http://arxiv.org/abs/2505.22657v1",
      "published_time_eastern_timestamp": 1748455153.0
    },
    {
      "title": "Position: Uncertainty Quantification Needs Reassessment for\n  Large-language Model Agents",
      "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive.",
      "url": "http://arxiv.org/abs/2505.22655v1",
      "published_time_eastern_timestamp": 1748455148.0
    },
    {
      "title": "WebDancer: Towards Autonomous Information Seeking Agency",
      "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.",
      "url": "http://arxiv.org/abs/2505.22648v1",
      "published_time_eastern_timestamp": 1748455027.0
    },
    {
      "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid\n  Control",
      "summary": "Reinforcement learning (RL) has driven significant progress in robotics, but\nits complexity and long training times remain major bottlenecks. In this\nreport, we introduce FastTD3, a simple, fast, and capable RL algorithm that\nsignificantly speeds up training for humanoid robots in popular suites such as\nHumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably\nsimple: we train an off-policy TD3 agent with several modifications -- parallel\nsimulation, large-batch updates, a distributional critic, and carefully tuned\nhyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours\non a single A100 GPU, while remaining stable during training. We also provide a\nlightweight and easy-to-use implementation of FastTD3 to accelerate RL research\nin robotics.",
      "url": "http://arxiv.org/abs/2505.22642v1",
      "published_time_eastern_timestamp": 1748454926.0
    },
    {
      "title": "LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for\n  Scientific Embodied Agents",
      "summary": "Scientific embodied agents play a crucial role in modern laboratories by\nautomating complex experimental workflows. Compared to typical household\nenvironments, laboratory settings impose significantly higher demands on\nperception of physical-chemical transformations and long-horizon planning,\nmaking them an ideal testbed for advancing embodied intelligence. However, its\ndevelopment has been long hampered by the lack of suitable simulator and\nbenchmarks. In this paper, we address this gap by introducing LabUtopia, a\ncomprehensive simulation and benchmarking suite designed to facilitate the\ndevelopment of generalizable, reasoning-capable embodied agents in laboratory\nsettings. Specifically, it integrates i) LabSim, a high-fidelity simulator\nsupporting multi-physics and chemically meaningful interactions; ii) LabScene,\na scalable procedural generator for diverse scientific scenes; and iii)\nLabBench, a hierarchical benchmark spanning five levels of complexity from\natomic actions to long-horizon mobile manipulation. LabUtopia supports 30\ndistinct tasks and includes more than 200 scene and instrument assets, enabling\nlarge-scale training and principled evaluation in high-complexity environments.\nWe demonstrate that LabUtopia offers a powerful platform for advancing the\nintegration of perception, planning, and control in scientific-purpose agents\nand provides a rigorous testbed for exploring the practical capabilities and\ngeneralization limits of embodied intelligence in future research.",
      "url": "http://arxiv.org/abs/2505.22634v1",
      "published_time_eastern_timestamp": 1748454653.0
    },
    {
      "title": "HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined\n  in HDDL with OpenAI Gym",
      "summary": "In recent years, reinforcement learning (RL) methods have been widely tested\nusing tools like OpenAI Gym, though many tasks in these environments could also\nbenefit from hierarchical planning. However, there is a lack of a tool that\nenables seamless integration of hierarchical planning with RL. Hierarchical\nDomain Definition Language (HDDL), used in classical planning, introduces a\nstructured approach well-suited for model-based RL to address this gap. To\nbridge this integration, we introduce HDDLGym, a Python-based tool that\nautomatically generates OpenAI Gym environments from HDDL domains and problems.\nHDDLGym serves as a link between RL and hierarchical planning, supporting\nmulti-agent scenarios and enabling collaborative planning among agents. This\npaper provides an overview of HDDLGym's design and implementation, highlighting\nthe challenges and design choices involved in integrating HDDL with the Gym\ninterface, and applying RL policies to support hierarchical planning. We also\nprovide detailed instructions and demonstrations for using the HDDLGym\nframework, including how to work with existing HDDL domains and problems from\nInternational Planning Competitions, exemplified by the Transport domain.\nAdditionally, we offer guidance on creating new HDDL domains for multi-agent\nscenarios and demonstrate the practical use of HDDLGym in the Overcooked\ndomain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a\nvaluable tool for studying RL in hierarchical planning, particularly in\nmulti-agent contexts.",
      "url": "http://arxiv.org/abs/2505.22597v1",
      "published_time_eastern_timestamp": 1748452243.0
    },
    {
      "title": "GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On\n  Git",
      "summary": "Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,\nhave catalyzed progress in programming capabilities of AI agents. However, they\noverlook critical developer workflows such as Version Control System (VCS)\noperations. To address this issue, we present GitGoodBench, a novel benchmark\nfor evaluating AI agent performance on VCS tasks. GitGoodBench covers three\ncore Git scenarios extracted from permissive open-source Python, Java, and\nKotlin repositories. Our benchmark provides three datasets: a comprehensive\nevaluation suite (900 samples), a rapid prototyping version (120 samples), and\na training corpus (17,469 samples). We establish baseline performance on the\nprototyping version of our benchmark using GPT-4o equipped with custom tools,\nachieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a\ncrucial stepping stone toward truly comprehensive SE agents that go beyond mere\nprogramming.",
      "url": "http://arxiv.org/abs/2505.22583v1",
      "published_time_eastern_timestamp": 1748451371.0
    },
    {
      "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified\n  Retrieval-Augmented Generation Systems",
      "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.",
      "url": "http://arxiv.org/abs/2505.22571v1",
      "published_time_eastern_timestamp": 1748450791.0
    },
    {
      "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction",
      "summary": "Tactile perception is essential for embodied agents to understand physical\nattributes of objects that cannot be determined through visual inspection\nalone. While existing approaches have made progress in visual and language\nmodalities for physical understanding, they fail to effectively incorporate\ntactile information that provides crucial haptic feedback for real-world\ninteraction. In this paper, we present VTV-LLM, the first multi-modal large\nlanguage model for universal Visuo-Tactile Video (VTV) understanding that\nbridges the gap between tactile perception and natural language. To address the\nchallenges of cross-sensor and cross-modal integration, we contribute VTV150K,\na comprehensive dataset comprising 150,000 video frames from 100 diverse\nobjects captured across three different tactile sensors (GelSight Mini, DIGIT,\nand Tac3D), annotated with four fundamental tactile attributes (hardness,\nprotrusion, elasticity, and friction). We develop a novel three-stage training\nparadigm that includes VTV enhancement for robust visuo-tactile representation,\nVTV-text alignment for cross-modal correspondence, and text prompt finetuning\nfor natural language generation. Our framework enables sophisticated tactile\nreasoning capabilities including feature assessment, comparative analysis,\nscenario-based decision making and so on. Experimental evaluations demonstrate\nthat VTV-LLM achieves superior performance in tactile video understanding\ntasks, establishing a foundation for more intuitive human-machine interaction\nin tactile domains.",
      "url": "http://arxiv.org/abs/2505.22566v1",
      "published_time_eastern_timestamp": 1748450581.0
    },
    {
      "title": "Training RL Agents for Multi-Objective Network Defense Tasks",
      "summary": "Open-ended learning (OEL) -- which emphasizes training agents that achieve\nbroad capability over narrow competency -- is emerging as a paradigm to develop\nartificial intelligence (AI) agents to achieve robustness and generalization.\nHowever, despite promising results that demonstrate the benefits of OEL,\napplying OEL to develop autonomous agents for real-world cybersecurity\napplications remains a challenge.\n  We propose a training approach, inspired by OEL, to develop autonomous\nnetwork defenders. Our results demonstrate that like in other domains, OEL\nprinciples can translate into more robust and generalizable agents for cyber\ndefense. To apply OEL to network defense, it is necessary to address several\ntechnical challenges. Most importantly, it is critical to provide a task\nrepresentation approach over a broad universe of tasks that maintains a\nconsistent interface over goals, rewards and action spaces. This way, the\nlearning agent can train with varying network conditions, attacker behaviors,\nand defender goals while being able to build on previously gained knowledge.\n  With our tools and results, we aim to fundamentally impact research that\napplies AI to solve cybersecurity problems. Specifically, as researchers\ndevelop gyms and benchmarks for cyber defense, it is paramount that they\nconsider diverse tasks with consistent representations, such as those we\npropose in our work.",
      "url": "http://arxiv.org/abs/2505.22531v1",
      "published_time_eastern_timestamp": 1748449101.0
    },
    {
      "title": "AI instructional agent improves student's perceived learner control and\n  learning outcome: empirical evidence from a randomized controlled trial",
      "summary": "This study examines the impact of an AI instructional agent on students'\nperceived learner control and academic performance in a medium demanding course\nwith lecturing as the main teaching strategy. Based on a randomized controlled\ntrial, three instructional conditions were compared: a traditional human\nteacher, a self-paced MOOC with chatbot support, and an AI instructional agent\ncapable of delivering lectures and responding to questions in real time.\nStudents in the AI instructional agent group reported significantly higher\nlevels of perceived learner control compared to the other groups. They also\ncompleted the learning task more efficiently and engaged in more frequent\ninteractions with the instructional system. Regression analyzes showed that\nperceived learner control positively predicted post-test performance, with\nbehavioral indicators such as reduced learning time and higher interaction\nfrequency supporting this relationship. These findings suggest that AI\ninstructional agents, when designed to support personalized pace and responsive\ninteraction, can enhance both students' learning experience and learning\noutcomes.",
      "url": "http://arxiv.org/abs/2505.22526v1",
      "published_time_eastern_timestamp": 1748448807.0
    },
    {
      "title": "From Strangers to Assistants: Fast Desire Alignment for Embodied\n  Agent-User Adaptation",
      "summary": "While embodied agents have made significant progress in performing complex\nphysical tasks, real-world applications demand more than pure task execution.\nThe agents must collaborate with unfamiliar agents and human users, whose goals\nare often vague and implicit. In such settings, interpreting ambiguous\ninstructions and uncovering underlying desires is essential for effective\nassistance. Therefore, fast and accurate desire alignment becomes a critical\ncapability for embodied agents. In this work, we first develop a home\nassistance simulation environment HA-Desire that integrates an LLM-driven human\nuser agent exhibiting realistic value-driven goal selection and communication.\nThe ego agent must interact with this proxy user to infer and adapt to the\nuser's latent desires. To achieve this, we present a novel framework FAMER for\nfast desire alignment, which introduces a desire-based mental reasoning\nmechanism to identify user intent and filter desire-irrelevant actions. We\nfurther design a reflection-based communication module that reduces redundant\ninquiries, and incorporate goal-relevant information extraction with memory\npersistence to improve information reuse and reduce unnecessary exploration.\nExtensive experiments demonstrate that our framework significantly enhances\nboth task execution and communication efficiency, enabling embodied agents to\nquickly adapt to user-specific desires in complex embodied environments.",
      "url": "http://arxiv.org/abs/2505.22503v1",
      "published_time_eastern_timestamp": 1748447473.0
    },
    {
      "title": "EvolveSearch: An Iterative Self-Evolving Search Agent",
      "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains.",
      "url": "http://arxiv.org/abs/2505.22501v1",
      "published_time_eastern_timestamp": 1748447448.0
    },
    {
      "title": "Human-Centered Human-AI Collaboration (HCHAC)",
      "summary": "In the intelligent era, the interaction between humans and intelligent\nsystems fundamentally involves collaboration with autonomous intelligent\nagents. Human-AI Collaboration (HAC) represents a novel type of human-machine\nrelationship facilitated by autonomous intelligent machines equipped with AI\ntechnologies. In this paradigm, AI agents serve not only as auxiliary tools but\nalso as active teammates, partnering with humans to accomplish tasks\ncollaboratively. Human-centered AI (HCAI) emphasizes that humans play critical\nleadership roles in the collaboration. This human-led collaboration imparts new\ndimensions to the human-machine relationship, necessitating innovative research\nperspectives, paradigms, and agenda to address the unique challenges posed by\nHAC. This chapter delves into the essence of HAC from the human-centered\nperspective, outlining its core concepts and distinguishing features. It\nreviews the current research methodologies and research agenda within the HAC\nfield from the HCAI perspective, highlighting advancements and ongoing studies.\nFurthermore, a framework for human-centered HAC (HCHAC) is proposed by\nintegrating these reviews and analyses. A case study of HAC in the context of\nautonomous vehicles is provided, illustrating practical applications and the\nsynergistic interactions between humans and AI agents. Finally, it identifies\npotential future research directions aimed at enhancing the effectiveness,\nreliability, and ethical integration of human-centered HAC systems in diverse\ndomains.",
      "url": "http://arxiv.org/abs/2505.22477v1",
      "published_time_eastern_timestamp": 1748446072.0
    },
    {
      "title": "Topological Structure Learning Should Be A Research Priority for\n  LLM-Based Multi-Agent Systems",
      "summary": "Large Language Model-based Multi-Agent Systems (MASs) have emerged as a\npowerful paradigm for tackling complex tasks through collaborative\nintelligence. Nevertheless, the question of how agents should be structurally\norganized for optimal cooperation remains largely unexplored. In this position\npaper, we aim to gently redirect the focus of the MAS research community toward\nthis critical dimension: develop topology-aware MASs for specific tasks.\nSpecifically, the system consists of three core components - agents,\ncommunication links, and communication patterns - that collectively shape its\ncoordination performance and efficiency. To this end, we introduce a\nsystematic, three-stage framework: agent selection, structure profiling, and\ntopology synthesis. Each stage would trigger new research opportunities in\nareas such as language models, reinforcement learning, graph learning, and\ngenerative modeling; together, they could unleash the full potential of MASs in\ncomplicated real-world applications. Then, we discuss the potential challenges\nand opportunities in the evaluation of multiple systems. We hope our\nperspective and framework can offer critical new insights in the era of agentic\nAI.",
      "url": "http://arxiv.org/abs/2505.22467v1",
      "published_time_eastern_timestamp": 1748445609.0
    },
    {
      "title": "AI Mathematician: Towards Fully Automated Frontier Mathematical Research",
      "summary": "Large Reasoning Models (LRMs) have made significant progress in mathematical\ncapabilities in recent times. However, these successes have been primarily\nconfined to competition-level problems. In this work, we propose AI\nMathematician (AIM) framework, which harnesses the reasoning strength of LRMs\nto support frontier mathematical research. We have identified two critical\nchallenges of mathematical research compared to competition, {\\it the intrinsic\ncomplexity of research problems} and {\\it the requirement of procedural rigor}.\nTo address these challenges, AIM incorporates two core strategies: an\nexploration mechanism to foster longer solution paths, and the pessimistic\nreasonable verification method to ensure reliability.\n  This early version of AIM already exhibits strong capability in tackling\nresearch-level tasks. We conducted extensive experiments across several\nreal-world mathematical topics and obtained promising results. AIM is able to\nautonomously construct substantial portions of proofs and uncover non-trivial\ninsights within each research area. These findings highlight the potential of\nLRMs in mathematical discovery and suggest that LRM-based agent systems could\nsignificantly accelerate mathematical research in the future.",
      "url": "http://arxiv.org/abs/2505.22451v1",
      "published_time_eastern_timestamp": 1748445037.0
    },
    {
      "title": "COSMOS: A Data-Driven Probabilistic Time Series simulator for Chemical\n  Plumes across Spatial Scales",
      "summary": "The development of robust odor navigation strategies for automated\nenvironmental monitoring applications requires realistic simulations of odor\ntime series for agents moving across large spatial scales. Traditional\napproaches that rely on computational fluid dynamics (CFD) methods can capture\nthe spatiotemporal dynamics of odor plumes, but are impractical for large-scale\nsimulations due to their computational expense. On the other hand, puff-based\nsimulations, although computationally tractable for large scales and capable of\ncapturing the stochastic nature of plumes, fail to reproduce naturalistic odor\nstatistics. Here, we present COSMOS (Configurable Odor Simulation Model over\nScalable Spaces), a data-driven probabilistic framework that synthesizes\nrealistic odor time series from spatial and temporal features of real datasets.\nCOSMOS generates similar distributions of key statistical features such as\nwhiff frequency, duration, and concentration as observed in real data, while\ndramatically reducing computational overhead. By reproducing critical\nstatistical properties across a variety of flow regimes and scales, COSMOS\nenables the development and evaluation of agent-based navigation strategies\nwith naturalistic odor experiences. To demonstrate its utility, we compare\nodor-tracking agents exposed to CFD-generated plumes versus COSMOS simulations,\nshowing that both their odor experiences and resulting behaviors are quite\nsimilar.",
      "url": "http://arxiv.org/abs/2505.22436v1",
      "published_time_eastern_timestamp": 1748444422.0
    },
    {
      "title": "Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained\n  Maximum Size",
      "summary": "Imagine we want to split a group of agents into teams in the most\n\\emph{efficient} way, considering that each agent has their own preferences\nabout their teammates. This scenario is modeled by the extensively studied\n\\textsc{Coalition Formation} problem. Here, we study a version of this problem\nwhere each team must additionally be of bounded size.\n  We conduct a systematic algorithmic study, providing several intractability\nresults as well as multiple exact algorithms that scale well as the input grows\n(FPT), which could prove useful in practice.\n  Our main contribution is an algorithm that deals efficiently with tree-like\nstructures (bounded \\emph{treewidth}) for ``small'' teams. We complement this\nresult by proving that our algorithm is asymptotically optimal. Particularly,\nthere can be no algorithm that vastly outperforms the one we present, under\nreasonable theoretical assumptions, even when considering star-like structures\n(bounded \\emph{vertex cover number}).",
      "url": "http://arxiv.org/abs/2505.22384v1",
      "published_time_eastern_timestamp": 1748441474.0
    },
    {
      "title": "AgentDNS: A Root Domain Naming System for LLM Agents",
      "summary": "The rapid evolution of Large Language Model (LLM) agents has highlighted\ncritical challenges in cross-vendor service discovery, interoperability, and\ncommunication. Existing protocols like model context protocol and\nagent-to-agent protocol have made significant strides in standardizing\ninteroperability between agents and tools, as well as communication among\nmulti-agents. However, there remains a lack of standardized protocols and\nsolutions for service discovery across different agent and tool vendors. In\nthis paper, we propose AgentDNS, a root domain naming and service discovery\nsystem designed to enable LLM agents to autonomously discover, resolve, and\nsecurely invoke third-party agent and tool services across organizational and\ntechnological boundaries. Inspired by the principles of the traditional DNS,\nAgentDNS introduces a structured mechanism for service registration, semantic\nservice discovery, secure invocation, and unified billing. We detail the\narchitecture, core functionalities, and use cases of AgentDNS, demonstrating\nits potential to streamline multi-agent collaboration in real-world scenarios.\nThe source code will be published on https://github.com/agentdns.",
      "url": "http://arxiv.org/abs/2505.22368v1",
      "published_time_eastern_timestamp": 1748440582.0
    },
    {
      "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent\n  Communications",
      "summary": "With the advent of 6G communications, intelligent communication systems face\nmultiple challenges, including constrained perception and response\ncapabilities, limited scalability, and low adaptability in dynamic\nenvironments. This tutorial provides a systematic introduction to the\nprinciples, design, and applications of Large Artificial Intelligence Models\n(LAMs) and Agentic AI technologies in intelligent communication systems, aiming\nto offer researchers a comprehensive overview of cutting-edge technologies and\npractical guidance. First, we outline the background of 6G communications,\nreview the technological evolution from LAMs to Agentic AI, and clarify the\ntutorial's motivation and main contributions. Subsequently, we present a\ncomprehensive review of the key components required for constructing LAMs. We\nfurther categorize LAMs and analyze their applicability, covering Large\nLanguage Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models\n(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a\nLAM-centric design paradigm tailored for communications, encompassing dataset\nconstruction and both internal and external learning approaches. Building upon\nthis, we develop an LAM-based Agentic AI system for intelligent communications,\nclarifying its core components such as planners, knowledge bases, tools, and\nmemory modules, as well as its interaction mechanisms. We also introduce a\nmulti-agent framework with data retrieval, collaborative planning, and\nreflective evaluation for 6G. Subsequently, we provide a detailed overview of\nthe applications of LAMs and Agentic AI in communication scenarios. Finally, we\nsummarize the research challenges and future directions in current studies,\naiming to support the development of efficient, secure, and sustainable\nnext-generation intelligent communication systems.",
      "url": "http://arxiv.org/abs/2505.22311v1",
      "published_time_eastern_timestamp": 1748436847.0
    }
  ]
}
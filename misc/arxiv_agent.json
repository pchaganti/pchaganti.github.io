{
  "last_updated": "2025-11-05T01:18:46.527815-05:00",
  "papers": [
    {
      "title": "Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for\n  Understanding Anything",
      "summary": "Multimodal large language models (MLLMs) have shown strong capabilities but\nremain limited to fixed modality pairs and require costly fine-tuning with\nlarge aligned datasets. Building fully omni-capable models that can integrate\ntext, images, audio, and video remains impractical and lacks robust reasoning\nsupport. In this paper, we propose an Agent-Omni framework that coordinates\nexisting foundation models through a master-agent system, enabling flexible\nmultimodal reasoning without retraining. The master agent interprets user\nintent, delegates subtasks to modality-specific agents, and integrates their\noutputs into coherent responses. Extensive experiments across text, image,\naudio, video, and omni benchmarks show that Agent-Omni consistently achieves\nstate-of-the-art performance, particularly on tasks requiring complex\ncross-modal reasoning. Its agent-based design enables seamless integration of\nspecialized foundation models, ensuring adaptability to diverse inputs while\nmaintaining transparency and interpretability. In addition, the framework is\nmodular and easily extensible, allowing future improvements as stronger models\nbecome available. %We release an open-source implementation to support\ncontinued research on scalable and reliable omni-modal reasoning.",
      "url": "http://arxiv.org/abs/2511.02834v1",
      "published_time_eastern_timestamp": 1762282749.0
    },
    {
      "title": "Kosmos: An AI Scientist for Autonomous Discovery",
      "summary": "Data-driven scientific discovery requires iterative cycles of literature\nsearch, hypothesis generation, and data analysis. Substantial progress has been\nmade towards AI agents that can automate scientific research, but all such\nagents remain limited in the number of actions they can take before losing\ncoherence, thus limiting the depth of their findings. Here we present Kosmos,\nan AI scientist that automates data-driven discovery. Given an open-ended\nobjective and a dataset, Kosmos runs for up to 12 hours performing cycles of\nparallel data analysis, literature search, and hypothesis generation before\nsynthesizing discoveries into scientific reports. Unlike prior systems, Kosmos\nuses a structured world model to share information between a data analysis\nagent and a literature search agent. The world model enables Kosmos to\ncoherently pursue the specified objective over 200 agent rollouts, collectively\nexecuting an average of 42,000 lines of code and reading 1,500 papers per run.\nKosmos cites all statements in its reports with code or primary literature,\nensuring its reasoning is traceable. Independent scientists found 79.4% of\nstatements in Kosmos reports to be accurate, and collaborators reported that a\nsingle 20-cycle Kosmos run performed the equivalent of 6 months of their own\nresearch time on average. Furthermore, collaborators reported that the number\nof valuable scientific findings generated scales linearly with Kosmos cycles\n(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that\nspan metabolomics, materials science, neuroscience, and statistical genetics.\nThree discoveries independently reproduce findings from preprinted or\nunpublished manuscripts that were not accessed by Kosmos at runtime, while four\nmake novel contributions to the scientific literature.",
      "url": "http://arxiv.org/abs/2511.02824v1",
      "published_time_eastern_timestamp": 1762282252.0
    },
    {
      "title": "Optimizing AI Agent Attacks With Synthetic Data",
      "summary": "As AI deployments become more complex and high-stakes, it becomes\nincreasingly important to be able to estimate their risk. AI control is one\nframework for doing so. However, good control evaluations require eliciting\nstrong attack policies. This can be challenging in complex agentic environments\nwhere compute constraints leave us data-poor. In this work, we show how to\noptimize attack policies in SHADE-Arena, a dataset of diverse realistic control\nenvironments. We do this by decomposing attack capability into five constituent\nskills -- suspicion modeling, attack selection, plan synthesis, execution and\nsubtlety -- and optimizing each component individually. To get around the\nconstraint of limited data, we develop a probabilistic model of attack\ndynamics, optimize our attack hyperparameters using this simulation, and then\nshow that the results transfer to SHADE-Arena. This results in a substantial\nimprovement in attack strength, reducing safety score from a baseline of 0.87\nto 0.41 using our scaffold.",
      "url": "http://arxiv.org/abs/2511.02823v1",
      "published_time_eastern_timestamp": 1762282136.0
    },
    {
      "title": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via\n  End-to-End Reinforcement Learning",
      "summary": "Typical search agents concatenate the entire interaction history into the LLM\ncontext, preserving information integrity but producing long, noisy contexts,\nresulting in high computation and memory costs. In contrast, using only the\ncurrent turn avoids this overhead but discards essential information. This\ntrade-off limits the scalability of search agents. To address this challenge,\nwe propose MemSearcher, an agent workflow that iteratively maintains a compact\nmemory and combines the current turn with it. At each turn, MemSearcher fuses\nthe user's question with the memory to generate reasoning traces, perform\nsearch actions, and update memory to retain only information essential for\nsolving the task. This design stabilizes context length across multi-turn\ninteractions, improving efficiency without sacrificing accuracy. To optimize\nthis workflow, we introduce multi-context GRPO, an end-to-end RL framework that\njointly optimize reasoning, search strategies, and memory management of\nMemSearcher Agents. Specifically, multi-context GRPO samples groups of\ntrajectories under different contexts and propagates trajectory-level\nadvantages across all conversations within them. Trained on the same dataset as\nSearch-R1, MemSearcher achieves significant improvements over strong baselines\non seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on\nQwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher\neven outperforms 7B-based baselines, demonstrating that striking a balance\nbetween information integrity and efficiency yields both higher accuracy and\nlower computational overhead. The code and models will be publicly available at\nhttps://github.com/icip-cas/MemSearcher",
      "url": "http://arxiv.org/abs/2511.02805v1",
      "published_time_eastern_timestamp": 1762280859.0
    },
    {
      "title": "When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal\n  Reasoning",
      "summary": "Despite rapid growth in multimodal large language models (MLLMs), their\nreasoning traces remain opaque: it is often unclear which modality drives a\nprediction, how conflicts are resolved, or when one stream dominates. In this\npaper, we introduce modality sabotage, a diagnostic failure mode in which a\nhigh-confidence unimodal error overrides other evidence and misleads the fused\nresult. To analyze such dynamics, we propose a lightweight, model-agnostic\nevaluation layer that treats each modality as an agent, producing candidate\nlabels and a brief self-assessment used for auditing. A simple fusion mechanism\naggregates these outputs, exposing contributors (modalities supporting correct\noutcomes) and saboteurs (modalities that mislead). Applying our diagnostic\nlayer in a case study on multimodal emotion recognition benchmarks with\nfoundation models revealed systematic reliability profiles, providing insight\ninto whether failures may arise from dataset artifacts or model limitations.\nMore broadly, our framework offers a diagnostic scaffold for multimodal\nreasoning, supporting principled auditing of fusion dynamics and informing\npossible interventions.",
      "url": "http://arxiv.org/abs/2511.02794v1",
      "published_time_eastern_timestamp": 1762280413.0
    },
    {
      "title": "1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts",
      "summary": "Smart contracts operate in a highly adversarial environment, where\nvulnerabilities can lead to substantial financial losses. Thus, smart contracts\nare subject to security audits. In auditing, proof-of-concept (PoC) exploits\nplay a critical role by demonstrating to the stakeholders that the reported\nvulnerabilities are genuine, reproducible, and actionable. However, manually\ncreating PoCs is time-consuming, error-prone, and often constrained by tight\naudit schedules. We introduce POCO, an agentic framework that automatically\ngenerates executable PoC exploits from natural-language vulnerability\ndescriptions written by auditors. POCO autonomously generates PoC exploits in\nan agentic manner by interacting with a set of code-execution tools in a\nReason-Act-Observe loop. It produces fully executable exploits compatible with\nthe Foundry testing framework, ready for integration into audit reports and\nother security tools. We evaluate POCO on a dataset of 23 real-world\nvulnerability reports. POCO consistently outperforms the prompting and workflow\nbaselines, generating well-formed and logically correct PoCs. Our results\ndemonstrate that agentic frameworks can significantly reduce the effort\nrequired for high-quality PoCs in smart contract audits. Our contribution\nprovides readily actionable knowledge for the smart contract security\ncommunity.",
      "url": "http://arxiv.org/abs/2511.02780v1",
      "published_time_eastern_timestamp": 1762279392.0
    },
    {
      "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation",
      "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.",
      "url": "http://arxiv.org/abs/2511.02778v1",
      "published_time_eastern_timestamp": 1762279218.0
    },
    {
      "title": "From Solo to Symphony: Orchestrating Multi-Agent Collaboration with\n  Single-Agent Demos",
      "summary": "Training a team of agents from scratch in multi-agent reinforcement learning\n(MARL) is highly inefficient, much like asking beginners to play a symphony\ntogether without first practicing solo. Existing methods, such as offline or\ntransferable MARL, can ease this burden, but they still rely on costly\nmulti-agent data, which often becomes the bottleneck. In contrast, solo\nexperiences are far easier to obtain in many important scenarios, e.g.,\ncollaborative coding, household cooperation, and search-and-rescue. To unlock\ntheir potential, we propose Solo-to-Collaborative RL (SoCo), a framework that\ntransfers solo knowledge into cooperative learning. SoCo first pretrains a\nshared solo policy from solo demonstrations, then adapts it for cooperation\nduring multi-agent training through a policy fusion mechanism that combines an\nMoE-like gating selector and an action editor. Experiments across diverse\ncooperative tasks show that SoCo significantly boosts the training efficiency\nand performance of backbone algorithms. These results demonstrate that solo\ndemonstrations provide a scalable and effective complement to multi-agent data,\nmaking cooperative learning more practical and broadly applicable.",
      "url": "http://arxiv.org/abs/2511.02762v1",
      "published_time_eastern_timestamp": 1762278251.0
    },
    {
      "title": "Controlling Performance and Budget of a Centralized Multi-agent LLM\n  System with Reinforcement Learning",
      "summary": "Large language models (LLMs) exhibit complementary strengths across domains\nand come with varying inference costs, motivating the design of multi-agent LLM\nsystems where specialized models collaborate efficiently. Existing approaches\npredominantly rely on decentralized frameworks, which invoke multiple LLMs for\nevery input and thus lead to substantial and uncontrolled inference costs. In\nthis work, we introduce a centralized multi-LLM framework, where a controller\nLLM selectively coordinates a pool of expert models in a cost-efficient and\ncost-controllable manner. We formulate this coordination problem as\nreinforcement learning with dual objectives: maximizing task performance while\nminimizing the overall inference cost. In addition, we expect the multi-agent\nsystem to have adapted behavior with different budget conditions during\ninference. To this end, we propose CoRL, a reinforcement learning framework\nthat optimizes the performance cost trade-off in a controllable multi-budget\nsetting. Experiments on four diverse benchmarks demonstrate that CoRL enables a\nsingle system to surpass the best expert LLM under high-budget settings, while\nmaintaining strong performance in more economical low-budget modes,\nhighlighting the effectiveness of centralized coordination for scalable and\ncost-efficient multi-agent LLM systems.",
      "url": "http://arxiv.org/abs/2511.02755v1",
      "published_time_eastern_timestamp": 1762277717.0
    },
    {
      "title": "Using Span Queries to Optimize for Cache and Attention Locality",
      "summary": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model.",
      "url": "http://arxiv.org/abs/2511.02749v1",
      "published_time_eastern_timestamp": 1762276969.0
    },
    {
      "title": "Agentic World Modeling for 6G: Near-Real-Time Generative State-Space\n  Reasoning",
      "summary": "We argue that sixth-generation (6G) intelligence is not fluent token\nprediction but the capacity to imagine and choose -- to simulate future\nscenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe\nopen radio access network (O-RAN) near-real-time (Near-RT) control via\ncounterfactual dynamics and a world modeling (WM) paradigm that learns an\naction-conditioned generative state space. This enables quantitative \"what-if\"\nforecasting beyond large language models (LLMs) as the primary modeling\nprimitive. Actions such as physical resource blocks (PRBs) are treated as\nfirst-class control inputs in a causal world model, and both aleatoric and\nepistemic uncertainty are modeled for prediction and what-if analysis. An\nagentic, model predictive control (MPC)-based cross-entropy method (CEM)\nplanner operates over short horizons, using prior-mean rollouts within\ndata-driven PRB bounds to maximize a deterministic reward. The model couples\nmulti-scale structured state-space mixtures (MS3M) with a compact stochastic\nlatent to form WM-MS3M, summarizing key performance indicators (KPIs) histories\nand predicting next-step KPIs under hypothetical PRB sequences. On realistic\nO-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with\n32% fewer parameters and similar latency, and achieves 35-80% lower root mean\nsquared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster\ninference, enabling rare-event simulation and offline policy screening.",
      "url": "http://arxiv.org/abs/2511.02748v1",
      "published_time_eastern_timestamp": 1762276942.0
    },
    {
      "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in\n  Dynamic Environments for LLM Tool-Use Agents",
      "summary": "Current evaluations of Large Language Model (LLM) agents primarily emphasize\ntask completion, often overlooking resource efficiency and adaptability. This\nneglects a crucial capability: agents' ability to devise and adjust\ncost-optimal plans in response to changing environments. To bridge this gap, we\nintroduce CostBench, a scalable, cost-centric benchmark designed to evaluate\nagents' economic reasoning and replanning abilities. Situated in the\ntravel-planning domain, CostBench comprises tasks solvable via multiple\nsequences of atomic and composite tools with diverse, customizable costs. It\nalso supports four types of dynamic blocking events, such as tool failures and\ncost changes, to simulate real-world unpredictability and necessitate agents to\nadapt in real time. Evaluating leading open-sourced and proprietary models on\nCostBench reveals a substantial gap in cost-aware planning: agents frequently\nfail to identify cost-optimal solutions in static settings, with even GPT-5\nachieving less than 75% exact match rate on the hardest tasks, and performance\nfurther dropping by around 40% under dynamic conditions. By diagnosing these\nweaknesses, CostBench lays the groundwork for developing future agents that are\nboth economically rational and robust.",
      "url": "http://arxiv.org/abs/2511.02734v1",
      "published_time_eastern_timestamp": 1762275509.0
    },
    {
      "title": "Identification and Estimation of Continuous-Time Dynamic Discrete Choice\n  Games",
      "summary": "This paper considers the theoretical, computational, and econometric\nproperties of continuous time dynamic discrete choice games with stochastically\nsequential moves, introduced by Arcidiacono, Bayer, Blevins, and Ellickson\n(2016). We consider identification of the rate of move arrivals, which was\nassumed to be known in previous work, as well as a generalized version with\nheterogeneous move arrival rates. We re-establish conditions for existence of a\nMarkov perfect equilibrium in the generalized model and consider identification\nof the model primitives with only discrete time data sampled at fixed\nintervals. Three foundational example models are considered: a single agent\nrenewal model, a dynamic entry and exit model, and a quality ladder model.\nThrough these examples we examine the computational and statistical properties\nof estimators via Monte Carlo experiments and an empirical example using data\nfrom Rust (1987). The experiments show how parameter estimates behave when\nmoving from continuous time data to discrete time data of decreasing frequency\nand the computational feasibility as the number of firms grows. The empirical\nexample highlights the impact of allowing decision rates to vary.",
      "url": "http://arxiv.org/abs/2511.02701v1",
      "published_time_eastern_timestamp": 1762273400.0
    },
    {
      "title": "Curriculum Design for Trajectory-Constrained Agent: Compressing\n  Chain-of-Thought Tokens in LLMs",
      "summary": "Training agents to operate under strict constraints during deployment, such\nas limited resource budgets or stringent safety requirements, presents\nsignificant challenges, especially when these constraints render the task\ncomplex. In this work, we propose a curriculum learning strategy that gradually\ntightens constraints during training, enabling the agent to incrementally\nmaster the deployment requirements. Inspired by self-paced learning techniques\nin unconstrained reinforcement learning (RL), our approach facilitates a\nsmoother transition to challenging environments by initially training on\nsimplified versions of the constraints and progressively introducing the full\ndeployment conditions. We provide a theoretical analysis using an RL agent in a\nbinary-tree Markov Decision Process (MDP) to demonstrate that our curriculum\nstrategy can accelerate training relative to a baseline approach that imposes\nthe trajectory constraints from the outset. Moreover, we empirically validate\nthe effectiveness and generality of our method across both RL and large\nlanguage model (LLM) agents in diverse settings, including a binary-tree MDP, a\nmulti-task navigation domain, and a math reasoning task with two benchmarks.\nThese results highlight the potential of curriculum design in enhancing the\nefficiency and performance of agents operating under complex trajectory\nconstraints during deployment. Moreover, when applied to LLMs, our strategy\nenables compression of output chain-of-thought tokens, achieving a substantial\ninference speedup on consumer hardware, demonstrating its effectiveness for\nresource-constrained deployment.",
      "url": "http://arxiv.org/abs/2511.02690v1",
      "published_time_eastern_timestamp": 1762272896.0
    },
    {
      "title": "The Collaboration Gap",
      "summary": "The trajectory of AI development suggests that we will increasingly rely on\nagent-based systems composed of independently developed agents with different\ninformation, privileges, and tools. The success of these systems will\ncritically depend on effective collaboration among these heterogeneous agents,\neven under partial observability. Despite intense interest, few empirical\nstudies have evaluated such agent-agent collaboration at scale. We propose a\ncollaborative maze-solving benchmark that (i) isolates collaborative\ncapabilities, (ii) modulates problem complexity, (iii) enables scalable\nautomated grading, and (iv) imposes no output-format constraints, preserving\necological plausibility. Using this framework, we evaluate 32 leading open- and\nclosed-source models in solo, homogeneous, and heterogeneous pairings. Our\nresults reveal a \"collaboration gap\": models that perform well solo often\ndegrade substantially when required to collaborate. Collaboration can break\ndown dramatically; for instance, small distilled models that solve mazes well\nalone may fail almost completely in certain pairings. We find that starting\nwith the stronger agent often improves outcomes, motivating a \"relay inference\"\napproach where the stronger agent leads before handing off to the weaker one,\nclosing much of the gap. Our findings argue for (1) collaboration-aware\nevaluation, (2) training strategies developed to enhance collaborative\ncapabilities, and (3) interaction design that reliably elicits agents' latent\nskills, guidance that applies to AI-AI and human-AI collaboration.",
      "url": "http://arxiv.org/abs/2511.02687v1",
      "published_time_eastern_timestamp": 1762272657.0
    },
    {
      "title": "Joint transfer pricing decision on tangible and intangible assets for\n  multinational firms",
      "summary": "While conventional multinational firms (MNFs) often avoid taxes by\ntransferring their profits to low-tax regions through markup on tangible asset\ncosts, high-tech MNFs may avoid taxes by transferring royalty fees to\nintangible assets (i.e., royalty-based transfer prices). This study\ninvestigates the effects of tax differences, markups, and royalties on\ndecision-making. We also compare the different effects of markups and royalties\non the improvement of MNFs' after-tax profit under two main business\nstructures: the commissionaire operational structure (C) with complete\ninformation, and the limited-risk operational structure (R) in the\nprincipal-agent setting. We find that the tax difference always improves MNFs'\nprofits under the C structure, whereas non-monotonic behavior exists under the\nR structure. More interestingly, when the order quantity is relatively small,\nthe markup improves MNFs' profits faster than the royalty; conversely, the\nroyalty improves MNFs' profits faster than the markup.",
      "url": "http://arxiv.org/abs/2511.02658v1",
      "published_time_eastern_timestamp": 1762270506.0
    },
    {
      "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
      "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality.",
      "url": "http://arxiv.org/abs/2511.02651v1",
      "published_time_eastern_timestamp": 1762269463.0
    },
    {
      "title": "Stochastic Redistribution of Indistinguishable Items in Shared\n  Habitation: A Multi-Agent Simulation Framework",
      "summary": "This paper presents a discrete-event stochastic model for the redistribution\nof indistinguishable personal items, exemplified by socks, among multiple\ncohabitants sharing a communal laundry system. Drawing on concepts from\necological population dynamics, diffusion processes, and stochastic exchange\ntheory, the model captures the probabilistic mechanisms underlying item mixing,\nrecovery, and loss. Each cohabitant is represented as an autonomous agent whose\nbelongings interact through iterative cycles of collective washing, sorting,\nand partial correction. The system's evolution is characterized by random\nmixing events, selective recollection, and attrition over time. Implemented\nusing the SimPy discrete-event simulation framework, the model demonstrates\nthat even minimal exchange probabilities can generate emergent asymmetries,\nquasi-equilibrium distributions, and long-term disorder. The findings\nillustrate how stochastic processes inherent to shared domestic systems can\nproduce persistent imbalances, offering a quantitative perspective on an\neveryday social phenomenon.",
      "url": "http://arxiv.org/abs/2511.02648v1",
      "published_time_eastern_timestamp": 1762269394.0
    },
    {
      "title": "Adaptive Quantum Matter: Variational Organization through Ising Agents",
      "summary": "The study introduces the Adaptive Quantum Ising Agents (AQIA) framework, a\nHamiltonian-based methodology that extends programmable quantum matter into an\nadaptive domain. Each agent operates as a finite transverse-field Ising\nsubsystem, maintaining internal quantum coherence while interacting through\nstate-dependent feedback channels characterised by reduced observables such as\nspin polarisation, bond correlation, and internal energy. These informational\ncouplings enable the transformation of a static lattice into a\nfeedback-reconfigurable medium. The effective Hamiltonian generated, which\nremains Hermitian at each iteration, is resolved self-consistently using a\nmean-field approximation, where the feedback fields are iteratively adjusted to\nminimise the total energy. Numerical investigations identify three distinct\nregimes: domain formation near the feedback--fluctuation critical point,\nglass-like frustration due to competing feedback channels, and modular\npolarisation sustained by structured interactions. These phenomena occur\nindependently of geometric embedding, illustrating that informational\nsimilarity alone can induce coherent organisation. The AQIA framework is\nadaptable to implementation on superconducting, trapped-ion, or Rydberg\nplatforms, offering a minimalistic model for exploring self-organisation and\nlearning in adaptive programmable quantum matter.",
      "url": "http://arxiv.org/abs/2511.02636v1",
      "published_time_eastern_timestamp": 1762268608.0
    },
    {
      "title": "A Multi-Agent Psychological Simulation System for Human Behavior\n  Modeling",
      "summary": "Training and education in human-centered fields require authentic practice,\nyet realistic simulations of human behavior have remained limited. We present a\nmulti-agent psychological simulation system that models internal\ncognitive-affective processes to generate believable human behaviors. In\ncontrast to black-box neural models, this system is grounded in established\npsychological theories (e.g., self-efficacy, mindset, social constructivism)\nand explicitly simulates an ``inner parliament'' of agents corresponding to key\npsychological factors. These agents deliberate and interact to determine the\nsystem's output behavior, enabling unprecedented transparency and alignment\nwith human psychology. We describe the system's architecture and theoretical\nfoundations, illustrate its use in teacher training and research, and discuss\nhow it embodies principles of social learning, cognitive apprenticeship,\ndeliberate practice, and meta-cognition.",
      "url": "http://arxiv.org/abs/2511.02606v1",
      "published_time_eastern_timestamp": 1762266483.0
    }
  ]
}
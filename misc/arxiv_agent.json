{
  "last_updated": "2025-08-04T08:27:25.101004-04:00",
  "papers": [
    {
      "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
      "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.",
      "url": "http://arxiv.org/abs/2508.00823v1",
      "published_time_eastern_timestamp": 1754071196.0
    },
    {
      "title": "Agentic large language models improve retrieval-based radiology question\n  answering",
      "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.",
      "url": "http://arxiv.org/abs/2508.00743v1",
      "published_time_eastern_timestamp": 1754065132.0
    },
    {
      "title": "Applying Psychometrics to Large Language Model Simulated Populations:\n  Recreating the HEXACO Personality Inventory Experiment with Generative Agents",
      "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.",
      "url": "http://arxiv.org/abs/2508.00742v1",
      "published_time_eastern_timestamp": 1754064976.0
    },
    {
      "title": "JSON-Bag: A generic game trajectory representation",
      "summary": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.",
      "url": "http://arxiv.org/abs/2508.00712v1",
      "published_time_eastern_timestamp": 1754062005.0
    },
    {
      "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement\n  Techniques and Applications",
      "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.",
      "url": "http://arxiv.org/abs/2508.00669v1",
      "published_time_eastern_timestamp": 1754059291.0
    },
    {
      "title": "Putting Perspective into OWL [sic]: Complexity-Neutral Standpoint\n  Reasoning for Ontology Languages via Monodic S5 over Counting Two-Variable\n  First-Order Logic (Extended Version with Appendix)",
      "summary": "Standpoint extensions of knowledge representation formalisms have been\nrecently introduced as a means to incorporate multi-perspective modelling and\nreasoning through modal operators that attribute pieces of knowledge to\nspecific entities or agents. In these extensions, the integration between\nconceptual modelling and perspective annotations can vary in strength, with\nmonodic standpoint extensions offering a well-balanced approach. They allow for\nadvanced modelling features, such as the expression of rigid concepts, while\nmaintaining desirable reasoning complexity.\n  We consider the extension of C2--the counting two-variable fragment of\nfirst-order logic--by monodic standpoints. At the heart of our work is a\npolynomial-time translation of formulas in this extended formalism into\nstandard, standpoint-free C2, a result that relies on intricate model-theoretic\narguments. Thanks to this translation, the satisfiability problem remains at\nthe same complexity level: NExpTime-complete, as in plain C2. Since our\nformalism subsumes monodic S5 over C2, this result also marks a substantial\nadvancement in the study of first-order modal logics.\n  From a practical standpoint, this means that highly expressive description\nlogics such as SHOIQBs and SROIQBs--which underpin the widely adopted OWL 1 and\nOWL 2 ontology languages standardised by the W3C--can be extended with monodic\nstandpoints without increasing the standard reasoning complexity.\n  We further prove that NExpTime-hardness arises even in significantly less\nexpressive description logics, as long as they include both nominals and\nmonodic standpoints. Moreover, we show that if the monodicity restriction is\nrelaxed even slightly in the presence of inverse roles, functionality, and\nnominals, the satisfiability problem becomes undecidable.",
      "url": "http://arxiv.org/abs/2508.00653v1",
      "published_time_eastern_timestamp": 1754057641.0
    },
    {
      "title": "SmartFlow: A CFD-solver-agnostic deep reinforcement learning framework\n  for computational fluid dynamics on HPC platforms",
      "summary": "Deep reinforcement learning (DRL) is emerging as a powerful tool for\nfluid-dynamics research, encompassing active flow control, autonomous\nnavigation, turbulence modeling and discovery of novel numerical schemes. We\nintroduce SmartFlow, a CFD-solver-agnostic framework for both single- and\nmulti-agent DRL algorithms that can easily integrate with MPI-parallel CPU and\nGPU-accelerated solvers. Built on Relexi and SmartSOD2D, SmartFlow uses the\nSmartSim infrastructure library and our newly developed SmartRedis-MPI library\nto enable asynchronous, low-latency, in-memory communication between CFD\nsolvers and Python-based DRL algorithms. SmartFlow leverages PyTorch's\nStable-Baselines3 for training, which provides a modular, Gym-like environment\nAPI. We demonstrate its versatility via three case studies: single-agent\nsynthetic-jet control for drag reduction in a cylinder flow simulated by the\nhigh-order FLEXI solver, multi-agent cylinder wake control using the\nGPU-accelerated spectral-element code SOD2D, and multi-agent wall-model\nlearning for large-eddy simulation with the finite-difference solver CaLES.\nSmartFlow's CFD-solver-agnostic design and seamless HPC integration is\npromising to accelerate RL-driven fluid-mechanics studies.",
      "url": "http://arxiv.org/abs/2508.00645v1",
      "published_time_eastern_timestamp": 1754057021.0
    },
    {
      "title": "Reinforcement Learning for Decision-Level Interception Prioritization in\n  Drone Swarm Defense",
      "summary": "The growing threat of low-cost kamikaze drone swarms poses a critical\nchallenge to modern defense systems demanding rapid and strategic\ndecision-making to prioritize interceptions across multiple effectors and\nhigh-value target zones. In this work, we present a case study demonstrating\nthe practical advantages of reinforcement learning in addressing this\nchallenge. We introduce a high-fidelity simulation environment that captures\nrealistic operational constraints, within which a decision-level reinforcement\nlearning agent learns to coordinate multiple effectors for optimal interception\nprioritization. Operating in a discrete action space, the agent selects which\ndrone to engage per effector based on observed state features such as\npositions, classes, and effector status. We evaluate the learned policy against\na handcrafted rule-based baseline across hundreds of simulated attack\nscenarios. The reinforcement learning based policy consistently achieves lower\naverage damage and higher defensive efficiency in protecting critical zones.\nThis case study highlights the potential of reinforcement learning as a\nstrategic layer within defense architectures, enhancing resilience without\ndisplacing existing control systems. All code and simulation assets are\npublicly released for full reproducibility, and a video demonstration\nillustrates the policy's qualitative behavior.",
      "url": "http://arxiv.org/abs/2508.00641v1",
      "published_time_eastern_timestamp": 1754056539.0
    },
    {
      "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
      "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.",
      "url": "http://arxiv.org/abs/2508.00632v1",
      "published_time_eastern_timestamp": 1754055913.0
    },
    {
      "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest\n  Mechanism",
      "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics.",
      "url": "http://arxiv.org/abs/2508.00554v1",
      "published_time_eastern_timestamp": 1754048893.0
    },
    {
      "title": "Assessing (im)balance in signed brain networks",
      "summary": "Many complex systems - be they financial, natural or social - are composed by\nunits - such as stocks, neurons or agents - whose joint activity can be\nrepresented as a multivariate time series. An issue of both practical and\ntheoretical importance concerns the possibility of inferring the presence of a\nstatic relationships between any two units solely from their dynamic state. The\npresent contribution aims at providing an answer within the frame of\ntraditional hypothesis testing. Briefly speaking, our suggestion is that of\nlinking any two units if behaving in a sufficiently similar way. To achieve\nsuch a goal, we project a multivariate time series onto a signed graph, by i)\ncomparing the empirical properties of the former with those expected under a\nsuitable benchmark and ii) linking any two units with a positive (negative)\nedge in case the corresponding series share a significantly large number of\nconcordant (discordant) values. To define our benchmarks, we adopt an\ninformation-theoretic approach that is rooted into the constrained maximisation\nof Shannon entropy, a procedure inducing an ensemble of multivariate time\nseries that preserves some of the empirical properties on average while\nrandomising everything else. We showcase the possible applications of our\nmethod by addressing one of the most timely issues in the domain of\nneurosciences, i.e. that of determining if brain networks are frustrated or not\n- and, in case, to what extent. As our results suggest, this is indeed the\ncase, the structure of the negative subgraph being more prone to inter-subject\nvariability than the complementary, positive subgraph. At the mesoscopic level,\ninstead, the minimisation of the Bayesian Information Criterion instantiated\nwith the Signed Stochastic Block Model reveals that brain areas gather into\nmodules aligning with the statistical variant of the Relaxed Balance Theory.",
      "url": "http://arxiv.org/abs/2508.00542v1",
      "published_time_eastern_timestamp": 1754047824.0
    },
    {
      "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via\n  Probabilistic Model Checking",
      "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.",
      "url": "http://arxiv.org/abs/2508.00500v1",
      "published_time_eastern_timestamp": 1754043887.0
    },
    {
      "title": "CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy\n  Optimization",
      "summary": "Modern cyber attacks unfold through multiple stages, requiring defenders to\ndynamically prioritize mitigations under uncertainty. While game-theoretic\nmodels capture attacker-defender interactions, existing approaches often rely\non static assumptions and lack integration with real-time threat intelligence,\nlimiting their adaptability. This paper presents CyGATE, a game-theoretic\nframework modeling attacker-defender interactions, using large language models\n(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection\nand patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber\nconflicts as a partially observable stochastic game (POSG) across Cyber Kill\nChain stages. Both agents use belief states to navigate uncertainty, with the\nattacker adapting tactics and the defender re-prioritizing patches based on\nevolving risks and observed adversary behavior. The framework's flexible\narchitecture enables extension to multi-agent scenarios involving coordinated\nattackers, collaborative defenders, or complex enterprise environments with\nmultiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE\neffectively prioritizes high-risk vulnerabilities, enhancing adaptability\nthrough dynamic threat integration, strategic foresight by anticipating\nattacker moves under uncertainty, and efficiency by optimizing resource use.",
      "url": "http://arxiv.org/abs/2508.00478v1",
      "published_time_eastern_timestamp": 1754041986.0
    },
    {
      "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network",
      "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.",
      "url": "http://arxiv.org/abs/2508.00429v1",
      "published_time_eastern_timestamp": 1754037474.0
    },
    {
      "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent\n  Foundation Models Training",
      "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro",
      "url": "http://arxiv.org/abs/2508.00414v1",
      "published_time_eastern_timestamp": 1754035891.0
    },
    {
      "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent\n  Cooperation",
      "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.",
      "url": "http://arxiv.org/abs/2508.00401v1",
      "published_time_eastern_timestamp": 1754035355.0
    },
    {
      "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents",
      "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store\nsimulation for benchmarking embodied agents against human performance in\nshopping tasks. Addressing a gap in retail-specific sim environments for\nembodied agent training, Sari Sandbox features over 250 interactive grocery\nitems across three store configurations, controlled via an API. It supports\nboth virtual reality (VR) for human interaction and a vision language model\n(VLM)-powered embodied agent. We also introduce SariBench, a dataset of\nannotated human demonstrations across varied task difficulties. Our sandbox\nenables embodied agents to navigate, inspect, and manipulate retail items,\nproviding baselines against human performance. We conclude with benchmarks,\nperformance analysis, and recommendations for enhancing realism and\nscalability. The source code can be accessed via\nhttps://github.com/upeee/sari-sandbox-env.",
      "url": "http://arxiv.org/abs/2508.00400v1",
      "published_time_eastern_timestamp": 1754035298.0
    },
    {
      "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech\n  Recognition",
      "summary": "Cued Speech (CS) is a visual communication system that combines lip-reading\nwith hand coding to facilitate communication for individuals with hearing\nimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures\nand lip movements into text via AI-driven methods. Traditionally, the temporal\nasynchrony between hand and lip movements requires the design of complex\nmodules to facilitate effective multimodal fusion. However, constrained by\nlimited data availability, current methods demonstrate insufficient capacity\nfor adequately training these fusion mechanisms, resulting in suboptimal\nperformance. Recently, multi-agent systems have shown promising capabilities in\nhandling complex tasks with limited data availability. To this end, we propose\nthe first collaborative multi-agent system for ACSR, named Cued-Agent. It\nintegrates four specialized sub-agents: a Multimodal Large Language Model-based\nHand Recognition agent that employs keyframe screening and CS expert prompt\nstrategies to decode hand movements, a pretrained Transformer-based Lip\nRecognition agent that extracts lip features from the input video, a Hand\nPrompt Decoding agent that dynamically integrates hand prompts with lip\nfeatures during inference in a training-free manner, and a Self-Correction\nPhoneme-to-Word agent that enables post-process and end-to-end conversion from\nphoneme sequences to natural language sentences for the first time through\nsemantic refinement. To support this study, we expand the existing Mandarin CS\ndataset by collecting data from eight hearing-impaired cuers, establishing a\nmixed dataset of fourteen subjects. Extensive experiments demonstrate that our\nCued-Agent performs superbly in both normal and hearing-impaired scenarios\ncompared with state-of-the-art methods. The implementation is available at\nhttps://github.com/DennisHgj/Cued-Agent.",
      "url": "http://arxiv.org/abs/2508.00391v1",
      "published_time_eastern_timestamp": 1754034039.0
    },
    {
      "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV\n  Vision-Language Navigation",
      "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.",
      "url": "http://arxiv.org/abs/2508.00390v1",
      "published_time_eastern_timestamp": 1754033748.0
    },
    {
      "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator",
      "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.",
      "url": "http://arxiv.org/abs/2508.00384v1",
      "published_time_eastern_timestamp": 1754033159.0
    }
  ]
}
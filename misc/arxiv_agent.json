{
  "last_updated": "2025-11-11T01:19:13.733932-05:00",
  "papers": [
    {
      "title": "DigiData: Training and Evaluating General-Purpose Mobile Control Agents",
      "summary": "AI agents capable of controlling user interfaces have the potential to\ntransform human interaction with digital devices. To accelerate this\ntransformation, two fundamental building blocks are essential: high-quality\ndatasets that enable agents to achieve complex and human-relevant goals, and\nrobust evaluation methods that allow researchers and practitioners to rapidly\nenhance agent performance. In this paper, we introduce DigiData, a large-scale,\nhigh-quality, diverse, multi-modal dataset designed for training mobile control\nagents. Unlike existing datasets, which derive goals from unstructured\ninteractions, DigiData is meticulously constructed through comprehensive\nexploration of app features, resulting in greater diversity and higher goal\ncomplexity. Additionally, we present DigiData-Bench, a benchmark for evaluating\nmobile control agents on real-world complex tasks. We demonstrate that the\ncommonly used step-accuracy metric falls short in reliably assessing mobile\ncontrol agents and, to address this, we propose dynamic evaluation protocols\nand AI-powered evaluations as rigorous alternatives for agent assessment. Our\ncontributions aim to significantly advance the development of mobile control\nagents, paving the way for more intuitive and effective human-device\ninteractions.",
      "url": "http://arxiv.org/abs/2511.07413v1",
      "published_time_eastern_timestamp": 1762801055.0
    },
    {
      "title": "TwinOR: Photorealistic Digital Twins of Dynamic Operating Rooms for\n  Embodied AI Research",
      "summary": "Developing embodied AI for intelligent surgical systems requires safe,\ncontrollable environments for continual learning and evaluation. However,\nsafety regulations and operational constraints in operating rooms (ORs) limit\nembodied agents from freely perceiving and interacting in realistic settings.\nDigital twins provide high-fidelity, risk-free environments for exploration and\ntraining. How we may create photorealistic and dynamic digital representations\nof ORs that capture relevant spatial, visual, and behavioral complexity remains\nunclear. We introduce TwinOR, a framework for constructing photorealistic,\ndynamic digital twins of ORs for embodied AI research. The system reconstructs\nstatic geometry from pre-scan videos and continuously models human and\nequipment motion through multi-view perception of OR activities. The static and\ndynamic components are fused into an immersive 3D environment that supports\ncontrollable simulation and embodied exploration. The proposed framework\nreconstructs complete OR geometry with centimeter level accuracy while\npreserving dynamic interaction across surgical workflows, enabling realistic\nrenderings and a virtual playground for embodied AI systems. In our\nexperiments, TwinOR simulates stereo and monocular sensor streams for geometry\nunderstanding and visual localization tasks. Models such as FoundationStereo\nand ORB-SLAM3 on TwinOR-synthesized data achieve performance within their\nreported accuracy on real indoor datasets, demonstrating that TwinOR provides\nsensor-level realism sufficient for perception and localization challenges. By\nestablishing a real-to-sim pipeline for constructing dynamic, photorealistic\ndigital twins of OR environments, TwinOR enables the safe, scalable, and\ndata-efficient development and benchmarking of embodied AI, ultimately\naccelerating the deployment of embodied AI from sim-to-real.",
      "url": "http://arxiv.org/abs/2511.07412v1",
      "published_time_eastern_timestamp": 1762801029.0
    },
    {
      "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial\n  Rewards",
      "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in\nvision-language tasks, but they continue to struggle with spatial\nunderstanding. Existing spatial MLLMs often rely on explicit 3D inputs or\narchitecture-specific modifications, and remain constrained by large-scale\ndatasets or sparse supervision. To address these limitations, we introduce\nSpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial\ngrounding with multi-step reasoning. The model simulates human-like spatial\nperception by constructing a scene graph of task-relevant objects and spatial\nrelations, and reasoning towards an answer via dense spatial rewards.\nSpatialThinker consists of two key contributions: (1) a data synthesis pipeline\nthat generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL\nwith a multi-objective dense spatial reward enforcing spatial grounding.\nSpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline\non spatial understanding and real-world VQA benchmarks, nearly doubling the\nbase-model gain compared to sparse RL, and surpassing GPT-4o. These results\nshowcase the effectiveness of combining spatial supervision with reward-aligned\nreasoning in enabling robust 3D spatial understanding with limited data and\nadvancing MLLMs towards human-level visual reasoning.",
      "url": "http://arxiv.org/abs/2511.07403v1",
      "published_time_eastern_timestamp": 1762800767.0
    },
    {
      "title": "People Perceive More Phantom Costs From Autonomous Agents When They Make\n  Unreasonably Generous Offers",
      "summary": "People often reject offers that are too generous due to the perception of\nhidden drawbacks referred to as \"phantom costs.\" We hypothesized that this\nperception and the decision-making vary based on the type of agent making the\noffer (human vs. robot) and the degree to which the agent is perceived to be\nautonomous or have the capacity for self-interest. To test this conjecture,\nparticipants (N = 855) engaged in a car-buying simulation where a human or\nrobot sales agent, described as either autonomous or not, offered either a\nsmall (5%) or large (85%) discount. Results revealed that the robot was\nperceived as less self-interested than the human, which reduced the perception\nof phantom costs. While larger discounts increased phantom costs, they also\nincreased purchase intentions, suggesting that perceived benefits can outweigh\nphantom costs. Importantly, phantom costs were not only attributed to the agent\nparticipants interacted with, but also to the product and the agent's manager,\nhighlighting at least three sources of suspicion. These findings deepen our\nunderstanding of to whom people assign responsibility and how perceptions shape\nboth human-human and human-robot interactions, with implications for ethical AI\ndesign and marketing strategies.",
      "url": "http://arxiv.org/abs/2511.07401v1",
      "published_time_eastern_timestamp": 1762800746.0
    },
    {
      "title": "ConvFill: Model Collaboration for Responsive Conversational Voice Agents",
      "summary": "Deploying conversational voice agents with large language models faces a\ncritical challenge: cloud-based foundation models provide deep reasoning and\ndomain knowledge but introduce latency that disrupts natural conversation,\nwhile on-device models respond immediately but lack sophistication. We propose\nconversational infill, a task where a lightweight on-device model generates\ncontextually appropriate dialogue while seamlessly incorporating streaming\nknowledge from a powerful backend model. This approach decouples response\nlatency from model capability, enabling systems that feel responsive while\naccessing the full power of large-scale models. We present ConvFill, a 360M\nparameter model trained on synthetic multi-domain conversations. Evaluation\nacross multiple backend models shows that conversational infill can be\nsuccessfully learned, with ConvFill achieving accuracy improvements of 36-42%\nover standalone small models of the same size while consistently retaining\nsub-200ms response latencies. Our results demonstrate the promise of this\napproach for building on-device conversational agents that are both immediately\nresponsive and knowledgeable.",
      "url": "http://arxiv.org/abs/2511.07397v1",
      "published_time_eastern_timestamp": 1762800630.0
    },
    {
      "title": "The Landscape of Almost Equitable Allocations",
      "summary": "Equitability is a fundamental notion in fair division which requires that all\nagents derive equal value from their allocated bundles. We study, for general\n(possibly non-monotone) valuations, a popular relaxation of equitability known\nas equitability up to one item (EQ1). An EQ1 allocation may fail to exist even\nwith additive non-monotone valuations; for instance, when there are two agents,\none valuing every item positively and the other negatively. This motivates a\nmild and natural assumption: all agents agree on the sign of their value for\nthe grand bundle. Under this assumption, we prove the existence and provide an\nefficient algorithm for computing EQ1 allocations for two agents with general\nvaluations. When there are more than two agents, we show the existence and\npolynomial-time computability of EQ1 allocations for valuation classes beyond\nadditivity and monotonicity, in particular for (1) doubly monotone valuations\nand (2) submodular (resp. supermodular) valuations where the value for the\ngrand bundle is nonnegative (resp. nonpositive) for all agents. Furthermore, we\nsettle an open question of Bil`o et al. by showing that an EQ1 allocation\nalways exists for nonnegative(resp. nonpositive) valuations, i.e., when every\nagent values each subset of items nonnegatively (resp. nonpositively). Finally,\nwe complete the picture by showing that for general valuations with more than\ntwo agents, EQ1 allocations may not exist even when agents agree on the sign of\nthe grand bundle, and that deciding the existence of an EQ1 allocation is\ncomputationally intractable.",
      "url": "http://arxiv.org/abs/2511.07395v1",
      "published_time_eastern_timestamp": 1762800607.0
    },
    {
      "title": "Surgical Agent Orchestration Platform for Voice-directed Patient Data\n  Interaction",
      "summary": "In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in\nthe procedure, making it difficult to access and manipulate multimodal patient\ndata without interruption. We propose a voice-directed Surgical Agent\nOrchestrator Platform (SAOP) built on a hierarchical multi-agent framework,\nconsisting of an orchestration agent and three task-specific agents driven by\nLarge Language Models (LLMs). These LLM-based agents autonomously plan, refine,\nvalidate, and reason to map voice commands into specific tasks such as\nretrieving clinical information, manipulating CT scans, or navigating 3D\nanatomical models on the surgical video. We also introduce a Multi-level\nOrchestration Evaluation Metric (MOEM) to comprehensively assess the\nperformance and robustness from command-level and category-level perspectives.\nThe SAOP achieves high accuracy and success rates across 240 voice commands,\nwhile LLM-based agents improve robustness against speech recognition errors and\ndiverse or ambiguous free-form commands, demonstrating strong potential to\nsupport minimally invasive da Vinci robotic surgery.",
      "url": "http://arxiv.org/abs/2511.07392v1",
      "published_time_eastern_timestamp": 1762800444.0
    },
    {
      "title": "UAV-Assisted Resilience in 6G and Beyond Network Energy Saving: A\n  Multi-Agent DRL Approach",
      "summary": "This paper investigates the unmanned aerial vehicle (UAV)-assisted resilience\nperspective in the 6G network energy saving (NES) scenario. More specifically,\nwe consider multiple ground base stations (GBSs) and each GBS has three\ndifferent sectors/cells in the terrestrial networks, and multiple cells are\nturned off due to NES or incidents, e.g., disasters, hardware failures, or\noutages. To address this, we propose a Multi-Agent Deep Deterministic Policy\nGradient (MADDPG) framework to enable UAV-assisted communication by jointly\noptimizing UAV trajectories, transmission power, and user-UAV association under\na sleeping ground base station (GBS) strategy. This framework aims to ensure\nthe resilience of active users in the network and the long-term operability of\nUAVs. Specifically, it maximizes service coverage for users during power\noutages or NES zones, while minimizing the energy consumption of UAVs.\nSimulation results demonstrate that the proposed MADDPG policy consistently\nachieves high coverage ratio across different testing episodes, outperforming\nother baselines. Moreover, the MADDPG framework attains the lowest total energy\nconsumption, with a reduction of approximately 24\\% compared to the\nconventional all GBS ON configuration, while maintaining a comparable user\nservice rate. These results confirm the effectiveness of the proposed approach\nin achieving a superior trade-off between energy efficiency and service\nperformance, supporting the development of sustainable and resilient\nUAV-assisted cellular networks.",
      "url": "http://arxiv.org/abs/2511.07366v1",
      "published_time_eastern_timestamp": 1762798983.0
    },
    {
      "title": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas",
      "summary": "Simulating human profiles by instilling personas into large language models\n(LLMs) is rapidly transforming research in agentic behavioral simulation, LLM\npersonalization, and human-AI alignment. However, most existing synthetic\npersonas remain shallow and simplistic, capturing minimal attributes and\nfailing to reflect the rich complexity and diversity of real human identities.\nWe introduce DEEPPERSONA, a scalable generative engine for synthesizing\nnarrative-complete synthetic personas through a two-stage, taxonomy-guided\nmethod. First, we algorithmically construct the largest-ever human-attribute\ntaxonomy, comprising over hundreds of hierarchically organized attributes, by\nmining thousands of real user-ChatGPT conversations. Second, we progressively\nsample attributes from this taxonomy, conditionally generating coherent and\nrealistic personas that average hundreds of structured attributes and roughly 1\nMB of narrative text, two orders of magnitude deeper than prior works.\nIntrinsic evaluations confirm significant improvements in attribute diversity\n(32 percent higher coverage) and profile uniqueness (44 percent greater)\ncompared to state-of-the-art baselines. Extrinsically, our personas enhance\nGPT-4.1-mini's personalized question answering accuracy by 11.6 percent on\naverage across ten metrics and substantially narrow (by 31.7 percent) the gap\nbetween simulated LLM citizens and authentic human responses in social surveys.\nOur generated national citizens reduced the performance gap on the Big Five\npersonality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA\nthus provides a rigorous, scalable, and privacy-free platform for high-fidelity\nhuman simulation and personalized AI research.",
      "url": "http://arxiv.org/abs/2511.07338v1",
      "published_time_eastern_timestamp": 1762796276.0
    },
    {
      "title": "Grounding Computer Use Agents on Human Demonstrations",
      "summary": "Building reliable computer-use agents requires grounding: accurately\nconnecting natural language instructions to the correct on-screen elements.\nWhile large datasets exist for web and mobile interactions, high-quality\nresources for desktop environments are limited. To address this gap, we\nintroduce GroundCUA, a large-scale desktop grounding dataset built from expert\nhuman demonstrations. It covers 87 applications across 12 categories and\nincludes 56K screenshots, with every on-screen element carefully annotated for\na total of over 3.56M human-verified annotations. From these demonstrations, we\ngenerate diverse instructions that capture a wide range of real-world tasks,\nproviding high-quality data for model training. Using GroundCUA, we develop the\nGroundNext family of models that map instructions to their target UI elements.\nAt both 3B and 7B scales, GroundNext achieves state-of-the-art results across\nfive benchmarks using supervised fine-tuning, while requiring less than\none-tenth the training data of prior work. Reinforcement learning post-training\nfurther improves performance, and when evaluated in an agentic setting on the\nOSWorld benchmark using o3 as planner, GroundNext attains comparable or\nsuperior results to models trained with substantially more data,. These results\ndemonstrate the critical role of high-quality, expert-driven datasets in\nadvancing general-purpose computer-use agents.",
      "url": "http://arxiv.org/abs/2511.07332v1",
      "published_time_eastern_timestamp": 1762796121.0
    },
    {
      "title": "IterResearch: Rethinking Long-Horizon Agents via Markovian State\n  Reconstruction",
      "summary": "Recent advances in deep-research agents have shown promise for autonomous\nknowledge construction through dynamic reasoning over external sources.\nHowever, existing approaches rely on a mono-contextual paradigm that\naccumulates all information in a single, expanding context window, leading to\ncontext suffocation and noise contamination that limit their effectiveness on\nlong-horizon tasks. We introduce IterResearch, a novel iterative deep-research\nparadigm that reformulates long-horizon research as a Markov Decision Process\nwith strategic workspace reconstruction. By maintaining an evolving report as\nmemory and periodically synthesizing insights, our approach preserves\nconsistent reasoning capacity across arbitrary exploration depths. We further\ndevelop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning\nframework that incentivizes efficient exploration through geometric reward\ndiscounting and enables stable distributed training via adaptive downsampling.\nExtensive experiments demonstrate that IterResearch achieves substantial\nimprovements over existing open-source agents with average +14.5pp across six\nbenchmarks and narrows the gap with frontier proprietary systems. Remarkably,\nour paradigm exhibits unprecedented interaction scaling, extending to 2048\ninteractions with dramatic performance gains (from 3.5\\% to 42.5\\%), and serves\nas an effective prompting strategy, improving frontier models by up to 19.2pp\nover ReAct on long-horizon tasks. These findings position IterResearch as a\nversatile solution for long-horizon reasoning, effective both as a trained\nagent and as a prompting paradigm for frontier models.",
      "url": "http://arxiv.org/abs/2511.07327v1",
      "published_time_eastern_timestamp": 1762795808.0
    },
    {
      "title": "FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework\n  for Equity Research Report Generation",
      "summary": "While LLMs have shown great success in financial tasks like stock prediction\nand question answering, their application in fully automating Equity Research\nReport generation remains uncharted territory. In this paper, we formulate the\nEquity Research Report (ERR) Generation task for the first time. To address the\ndata scarcity and the evaluation metrics absence, we present an open-source\nevaluation benchmark for ERR generation - FinRpt. We frame a Dataset\nConstruction Pipeline that integrates 7 financial data types and produces a\nhigh-quality ERR dataset automatically, which could be used for model training\nand evaluation. We also introduce a comprehensive evaluation system including\n11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent\nframework specifically tailored to address this task, named FinRpt-Gen, and\ntrain several LLM-based agents on the proposed datasets using Supervised\nFine-Tuning and Reinforcement Learning. Experimental results indicate the data\nquality and metrics effectiveness of the benchmark FinRpt and the strong\nperformance of FinRpt-Gen, showcasing their potential to drive innovation in\nthe ERR generation field. All code and datasets are publicly available.",
      "url": "http://arxiv.org/abs/2511.07322v1",
      "published_time_eastern_timestamp": 1762795352.0
    },
    {
      "title": "JPRO: Automated Multimodal Jailbreaking via Multi-Agent Collaboration\n  Framework",
      "summary": "The widespread application of large VLMs makes ensuring their secure\ndeployment critical. While recent studies have demonstrated jailbreak attacks\non VLMs, existing approaches are limited: they require either white-box access,\nrestricting practicality, or rely on manually crafted patterns, leading to poor\nsample diversity and scalability. To address these gaps, we propose JPRO, a\nnovel multi-agent collaborative framework designed for automated VLM\njailbreaking. It effectively overcomes the shortcomings of prior methods in\nattack diversity and scalability. Through the coordinated action of four\nspecialized agents and its two core modules: Tactic-Driven Seed Generation and\nAdaptive Optimization Loop, JPRO generates effective and diverse attack\nsamples. Experimental results show that JPRO achieves over a 60\\% attack\nsuccess rate on multiple advanced VLMs, including GPT-4o, significantly\noutperforming existing methods. As a black-box attack approach, JPRO not only\nuncovers critical security vulnerabilities in multimodal models but also offers\nvaluable insights for evaluating and enhancing VLM robustness.",
      "url": "http://arxiv.org/abs/2511.07315v1",
      "published_time_eastern_timestamp": 1762795006.0
    },
    {
      "title": "Beyond Detection: Exploring Evidence-based Multi-Agent Debate for\n  Misinformation Intervention and Persuasion",
      "summary": "Multi-agent debate (MAD) frameworks have emerged as promising approaches for\nmisinformation detection by simulating adversarial reasoning. While prior work\nhas focused on detection accuracy, it overlooks the importance of helping users\nunderstand the reasoning behind factual judgments and develop future\nresilience. The debate transcripts generated during MAD offer a rich but\nunderutilized resource for transparent reasoning. In this study, we introduce\nED2D, an evidence-based MAD framework that extends previous approach by\nincorporating factual evidence retrieval. More importantly, ED2D is designed\nnot only as a detection framework but also as a persuasive multi-agent system\naimed at correcting user beliefs and discouraging misinformation sharing. We\ncompare the persuasive effects of ED2D-generated debunking transcripts with\nthose authored by human experts. Results demonstrate that ED2D outperforms\nexisting baselines across three misinformation detection benchmarks. When ED2D\ngenerates correct predictions, its debunking transcripts exhibit persuasive\neffects comparable to those of human experts; However, when ED2D misclassifies,\nits accompanying explanations may inadvertently reinforce users'misconceptions,\neven when presented alongside accurate human explanations. Our findings\nhighlight both the promise and the potential risks of deploying MAD systems for\nmisinformation intervention. We further develop a public community website to\nhelp users explore ED2D, fostering transparency, critical thinking, and\ncollaborative fact-checking.",
      "url": "http://arxiv.org/abs/2511.07267v1",
      "published_time_eastern_timestamp": 1762791353.0
    },
    {
      "title": "When Intelligence Overloads Infrastructure: A Forecast Model for\n  AI-Driven Bottlenecks",
      "summary": "The exponential growth of AI agents and connected devices fundamentally\ntransforms the structure and capacity demands of global digital infrastructure.\nThis paper introduces a unified forecasting model that projects AI agent\npopulations to increase by more than 100 times between 2026 and 2036+, reaching\ntrillions of instances globally. In parallel, bandwidth demand is expected to\nsurge from 1 EB/day in 2026 to over 8,000 EB/day by 2036, which is an increase\nof 8000 times in a single decade. Through this growth model, we identify\ncritical bottleneck domains across access networks, edge gateways,\ninterconnection exchanges, and cloud infrastructures. Simulations reveal that\nedge and peering systems will experience saturation as early as 2030, with more\nthan 70% utilization of projected maximum capacity by 2033. To address these\nconstraints, we propose a coevolutionary shift in compute-network design,\nemphasizing distributed inference, AI-native traffic engineering, and\nintent-aware orchestration. Security, scalability, and coordination challenges\nare examined with a focus on sustaining intelligent connectivity throughout the\nnext digital decade.",
      "url": "http://arxiv.org/abs/2511.07265v1",
      "published_time_eastern_timestamp": 1762790989.0
    },
    {
      "title": "AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery\n  in Scientific Machine Learning",
      "summary": "Scientific Machine Learning (SciML) integrates data-driven inference with\nphysical modeling to solve complex problems in science and engineering.\nHowever, the design of SciML architectures, loss formulations, and training\nstrategies remains an expert-driven research process, requiring extensive\nexperimentation and problem-specific insights. Here we introduce AgenticSciML,\na collaborative multi-agent system in which over 10 specialized AI agents\ncollaborate to propose, critique, and refine SciML solutions through structured\nreasoning and iterative evolution. The framework integrates structured debate,\nretrieval-augmented method memory, and ensemble-guided evolutionary search,\nenabling the agents to generate and assess new hypotheses about architectures\nand optimization procedures. Across physics-informed learning and operator\nlearning tasks, the framework discovers solution methods that outperform\nsingle-agent and human-designed baselines by up to four orders of magnitude in\nerror reduction. The agents produce novel strategies -- including adaptive\nmixture-of-expert architectures, decomposition-based PINNs, and\nphysics-informed operator learning models -- that do not appear explicitly in\nthe curated knowledge base. These results show that collaborative reasoning\namong AI agents can yield emergent methodological innovation, suggesting a path\ntoward scalable, transparent, and autonomous discovery in scientific computing.",
      "url": "http://arxiv.org/abs/2511.07262v1",
      "published_time_eastern_timestamp": 1762790793.0
    },
    {
      "title": "PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork",
      "summary": "Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen\nteammates, which is crucial for many real-world applications. The core\nchallenge of AHT is to develop an ego agent that can predict and adapt to\nunknown teammates on the fly. Conventional RL-based approaches optimize a\nsingle expected return, which often causes policies to collapse into a single\ndominant behavior, thus failing to capture the multimodal cooperation patterns\ninherent in AHT. In this work, we introduce PADiff, a diffusion-based approach\nthat captures agent's multimodal behaviors, unlocking its diverse cooperation\nmodes with teammates. However, standard diffusion models lack the ability to\npredict and adapt in highly non-stationary AHT scenarios. To address this\nlimitation, we propose a novel diffusion-based policy that integrates critical\npredictive information about teammates into the denoising process. Extensive\nexperiments across three cooperation environments demonstrate that PADiff\noutperforms existing AHT methods significantly.",
      "url": "http://arxiv.org/abs/2511.07260v1",
      "published_time_eastern_timestamp": 1762790740.0
    },
    {
      "title": "Bridging the Prototype-Production Gap: A Multi-Agent System for\n  Notebooks Transformation",
      "summary": "The increasing adoption of Jupyter notebooks in data science and machine\nlearning workflows has created a gap between exploratory code development and\nproduction-ready software systems. While notebooks excel at iterative\ndevelopment and visualization, they often lack proper software engineering\nprinciples, making their transition to production environments challenging.\nThis paper presents Codelevate, a novel multi-agent system that automatically\ntransforms Jupyter notebooks into well-structured, maintainable Python code\nrepositories. Our system employs three specialized agents - Architect,\nDeveloper, and Structure - working in concert through a shared dependency tree\nto ensure architectural coherence and code quality. Our experimental results\nvalidate Codelevate's capability to bridge the prototype-to-production gap\nthrough autonomous code transformation, yielding quantifiable improvements in\ncode quality metrics while preserving computational semantics.",
      "url": "http://arxiv.org/abs/2511.07257v1",
      "published_time_eastern_timestamp": 1762790710.0
    },
    {
      "title": "Discourse Graph Guided Document Translation with Large Language Models",
      "summary": "Adapting large language models to full document translation remains\nchallenging due to the difficulty of capturing long-range dependencies and\npreserving discourse coherence throughout extended texts. While recent agentic\nmachine translation systems mitigate context window constraints through\nmulti-agent orchestration and persistent memory, they require substantial\ncomputational resources and are sensitive to memory retrieval strategies. We\nintroduce TransGraph, a discourse-guided framework that explicitly models\ninter-chunk relationships through structured discourse graphs and selectively\nconditions each translation segment on relevant graph neighbourhoods rather\nthan relying on sequential or exhaustive context. Across three document-level\nMT benchmarks spanning six languages and diverse domains, TransGraph\nconsistently surpasses strong baselines in translation quality and terminology\nconsistency while incurring significantly lower token overhead.",
      "url": "http://arxiv.org/abs/2511.07230v1",
      "published_time_eastern_timestamp": 1762789681.0
    },
    {
      "title": "Evaluating Online Moderation Via LLM-Powered Counterfactual Simulations",
      "summary": "Online Social Networks (OSNs) widely adopt content moderation to mitigate the\nspread of abusive and toxic discourse. Nonetheless, the real effectiveness of\nmoderation interventions remains unclear due to the high cost of data\ncollection and limited experimental control. The latest developments in Natural\nLanguage Processing pave the way for a new evaluation approach. Large Language\nModels (LLMs) can be successfully leveraged to enhance Agent-Based Modeling and\nsimulate human-like social behavior with unprecedented degree of believability.\nYet, existing tools do not support simulation-based evaluation of moderation\nstrategies. We fill this gap by designing a LLM-powered simulator of OSN\nconversations enabling a parallel, counterfactual simulation where toxic\nbehavior is influenced by moderation interventions, keeping all else equal. We\nconduct extensive experiments, unveiling the psychological realism of OSN\nagents, the emergence of social contagion phenomena and the superior\neffectiveness of personalized moderation strategies.",
      "url": "http://arxiv.org/abs/2511.07204v1",
      "published_time_eastern_timestamp": 1762788719.0
    }
  ]
}
{
  "last_updated": "2025-07-09T08:24:15.946673-04:00",
  "papers": [
    {
      "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving",
      "summary": "As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks.",
      "url": "http://arxiv.org/abs/2507.06229v1",
      "published_time_eastern_timestamp": 1751997562.0
    },
    {
      "title": "Aligned Textual Scoring Rules",
      "summary": "Scoring rules elicit probabilistic predictions from a strategic agent by\nscoring the prediction against a ground truth state. A scoring rule is proper\nif, from the agent's perspective, reporting the true belief maximizes the\nexpected score. With the development of language models, Wu and Hartline (2024)\nproposes a reduction from textual information elicitation to the numerical\n(i.e. probabilistic) information elicitation problem, which achieves provable\nproperness for textual elicitation. However, not all proper scoring rules are\nwell aligned with human preference over text. Our paper designs the Aligned\nScoring rule (ASR) for text by optimizing and minimizing the mean squared error\nbetween a proper scoring rule and a reference score (e.g. human score). Our\nexperiments show that our ASR outperforms previous methods in aligning with\nhuman preference while maintaining properness.",
      "url": "http://arxiv.org/abs/2507.06221v1",
      "published_time_eastern_timestamp": 1751997202.0
    },
    {
      "title": "Evaluation of Habitat Robotics using Large Language Models",
      "summary": "This paper focuses on evaluating the effectiveness of Large Language Models\nat solving embodied robotic tasks using the Meta PARTNER benchmark. Meta PARTNR\nprovides simplified environments and robotic interactions within randomized\nindoor kitchen scenes. Each randomized kitchen scene is given a task where two\nrobotic agents cooperatively work together to solve the task. We evaluated\nmultiple frontier models on Meta PARTNER environments. Our results indicate\nthat reasoning models like OpenAI o3-mini outperform non-reasoning models like\nOpenAI GPT-4o and Llama 3 when operating in PARTNR's robotic embodied\nenvironments. o3-mini displayed outperform across centralized, decentralized,\nfull observability, and partial observability configurations. This provides a\npromising avenue of research for embodied robotic development.",
      "url": "http://arxiv.org/abs/2507.06157v1",
      "published_time_eastern_timestamp": 1751992779.0
    },
    {
      "title": "OpenAgentSafety: A Comprehensive Framework for Evaluating Real-World AI\n  Agent Safety",
      "summary": "Recent advances in AI agents capable of solving complex, everyday tasks, from\nscheduling to customer service, have enabled deployment in real-world settings,\nbut their possibilities for unsafe behavior demands rigorous evaluation. While\nprior benchmarks have attempted to assess agent safety, most fall short by\nrelying on simulated environments, narrow task domains, or unrealistic tool\nabstractions. We introduce OpenAgentSafety, a comprehensive and modular\nframework for evaluating agent behavior across eight critical risk categories.\nUnlike prior work, our framework evaluates agents that interact with real\ntools, including web browsers, code execution environments, file systems, bash\nshells, and messaging platforms; and supports over 350 multi-turn, multi-user\ntasks spanning both benign and adversarial user intents. OpenAgentSafety is\ndesigned for extensibility, allowing researchers to add tools, tasks, websites,\nand adversarial strategies with minimal effort. It combines rule-based analysis\nwith LLM-as-judge assessments to detect both overt and subtle unsafe behaviors.\nEmpirical analysis of five prominent LLMs in agentic scenarios reveals unsafe\nbehavior in 51.2% of safety-vulnerable tasks with Claude-Sonnet-3.7, to 72.7%\nwith o3-mini, highlighting critical safety vulnerabilities and the need for\nstronger safeguards before real-world deployment.",
      "url": "http://arxiv.org/abs/2507.06134v1",
      "published_time_eastern_timestamp": 1751991534.0
    },
    {
      "title": "A Directed Lazy Random Walk Model to Three-Way Dynamic Matching Problem",
      "summary": "This paper explores a novel extension of dynamic matching theory by analyzing\na three-way matching problem involving agents from three distinct populations,\neach with two possible types. Unlike traditional static or two-way dynamic\nmodels, our setting captures more complex team-formation environments where one\nagent from each of the three populations must be matched to form a valid team.\nWe consider two preference structures: assortative or homophilic, where agents\nprefer to be matched with others of the same type, and dis-assortative or\nheterophilic, where diversity within the team is valued. Agents arrive\nsequentially and face a trade-off between matching immediately or waiting for a\nhigher quality match in the future albeit with a waiting cost. We construct and\nanalyze the corresponding transition probability matrices for each preference\nregime and demonstrate the existence and uniqueness of stationary\ndistributions. Our results show that stable and efficient outcomes can arise in\ndynamic, multi-agent matching environments, offering a deeper understanding of\nhow complex matching processes evolve over time and how they can be effectively\nmanaged.",
      "url": "http://arxiv.org/abs/2507.06126v1",
      "published_time_eastern_timestamp": 1751991231.0
    },
    {
      "title": "On Lockean beliefs that are deductively closed and minimal change",
      "summary": "Within the formal setting of the Lockean thesis, an agent belief set is\ndefined in terms of degrees of confidence and these are described in\nprobabilistic terms. This approach is of established interest, notwithstanding\nsome limitations that make its use troublesome in some contexts, like, for\ninstance, in belief change theory. Precisely, Lockean belief sets are not\ngenerally closed under (classical) logical deduction. The aim of the present\npaper is twofold: on one side we provide two characterizations of those belief\nsets that are closed under classical logic deduction, and on the other we\npropose an approach to probabilistic update that allows us for a minimal\nrevision of those beliefs, i.e., a revision obtained by making the fewest\npossible changes to the existing belief set while still accommodating the new\ninformation. In particular, we show how we can deductively close a belief set\nvia a minimal revision.",
      "url": "http://arxiv.org/abs/2507.06042v1",
      "published_time_eastern_timestamp": 1751985841.0
    },
    {
      "title": "Conditional Multi-Stage Failure Recovery for Embodied Agents",
      "summary": "Embodied agents performing complex tasks are susceptible to execution\nfailures, motivating the need for effective failure recovery mechanisms. In\nthis work, we introduce a conditional multistage failure recovery framework\nthat employs zero-shot chain prompting. The framework is structured into four\nerror-handling stages, with three operating during task execution and one\nfunctioning as a post-execution reflection phase. Our approach utilises the\nreasoning capabilities of LLMs to analyse execution challenges within their\nenvironmental context and devise strategic solutions. We evaluate our method on\nthe TfD benchmark of the TEACH dataset and achieve state-of-the-art\nperformance, outperforming a baseline without error recovery by 11.5% and\nsurpassing the strongest existing model by 19%.",
      "url": "http://arxiv.org/abs/2507.06016v1",
      "published_time_eastern_timestamp": 1751984621.0
    },
    {
      "title": "From General Relation Patterns to Task-Specific Decision-Making in\n  Continual Multi-Agent Coordination",
      "summary": "Continual Multi-Agent Reinforcement Learning (Co-MARL) requires agents to\naddress catastrophic forgetting issues while learning new coordination policies\nwith the dynamics team. In this paper, we delve into the core of Co-MARL,\nnamely Relation Patterns, which refer to agents' general understanding of\ninteractions. In addition to generality, relation patterns exhibit\ntask-specificity when mapped to different action spaces. To this end, we\npropose a novel method called General Relation Patterns-Guided Task-Specific\nDecision-Maker (RPG). In RPG, agents extract relation patterns from dynamic\nobservation spaces using a relation capturer. These task-agnostic relation\npatterns are then mapped to different action spaces via a task-specific\ndecision-maker generated by a conditional hypernetwork. To combat forgetting,\nwe further introduce regularization items on both the relation capturer and the\nconditional hypernetwork. Results on SMAC and LBF demonstrate that RPG\neffectively prevents catastrophic forgetting when learning new tasks and\nachieves zero-shot generalization to unseen tasks.",
      "url": "http://arxiv.org/abs/2507.06004v1",
      "published_time_eastern_timestamp": 1751983673.0
    },
    {
      "title": "Multi-Agent Debate Strategies to Enhance Requirements Engineering with\n  Large Language Models",
      "summary": "Context: Large Language Model (LLM) agents are becoming widely used for\nvarious Requirements Engineering (RE) tasks. Research on improving their\naccuracy mainly focuses on prompt engineering, model fine-tuning, and retrieval\naugmented generation. However, these methods often treat models as isolated\nblack boxes - relying on single-pass outputs without iterative refinement or\ncollaboration, limiting robustness and adaptability. Objective: We propose\nthat, just as human debates enhance accuracy and reduce bias in RE tasks by\nincorporating diverse perspectives, different LLM agents debating and\ncollaborating may achieve similar improvements. Our goal is to investigate\nwhether Multi-Agent Debate (MAD) strategies can enhance RE performance. Method:\nWe conducted a systematic study of existing MAD strategies across various\ndomains to identify their key characteristics. To assess their applicability in\nRE, we implemented and tested a preliminary MAD-based framework for RE\nclassification. Results: Our study identified and categorized several MAD\nstrategies, leading to a taxonomy outlining their core attributes. Our\npreliminary evaluation demonstrated the feasibility of applying MAD to RE\nclassification. Conclusions: MAD presents a promising approach for improving\nLLM accuracy in RE tasks. This study provides a foundational understanding of\nMAD strategies, offering insights for future research and refinements in RE\napplications.",
      "url": "http://arxiv.org/abs/2507.05981v1",
      "published_time_eastern_timestamp": 1751981879.0
    },
    {
      "title": "CogniPlay: a work-in-progress Human-like model for General Game Playing",
      "summary": "While AI systems have equaled or surpassed human performance in a wide\nvariety of games such as Chess, Go, or Dota 2, describing these systems as\ntruly \"human-like\" remains far-fetched. Despite their success, they fail to\nreplicate the pattern-based, intuitive decision-making processes observed in\nhuman cognition. This paper presents an overview of findings from cognitive\npsychology and previous efforts to model human-like behavior in artificial\nagents, discusses their applicability to General Game Playing (GGP) and\nintroduces our work-in-progress model based on these observations: CogniPlay.",
      "url": "http://arxiv.org/abs/2507.05868v1",
      "published_time_eastern_timestamp": 1751971709.0
    },
    {
      "title": "Constella: Supporting Storywriters' Interconnected Character Creation\n  through LLM-based Multi-Agents",
      "summary": "Creating a cast of characters by attending to their relational dynamics is a\ncritical aspect of most long-form storywriting. However, our formative study\n(N=14) reveals that writers struggle to envision new characters that could\ninfluence existing ones, to balance similarities and differences among\ncharacters, and to intricately flesh out their relationships. Based on these\nobservations, we designed Constella, an LLM-based multi-agent tool that\nsupports storywriters' interconnected character creation process. Constella\nsuggests related characters (FRIENDS DISCOVERY feature), reveals the inner\nmindscapes of several characters simultaneously (JOURNALS feature), and\nmanifests relationships through inter-character responses (COMMENTS feature).\nOur 7-8 day deployment study with storywriters (N=11) shows that Constella\nenabled the creation of expansive communities composed of related characters,\nfacilitated the comparison of characters' thoughts and emotions, and deepened\nwriters' understanding of character relationships. We conclude by discussing\nhow multi-agent interactions can help distribute writers' attention and effort\nacross the character cast.",
      "url": "http://arxiv.org/abs/2507.05820v1",
      "published_time_eastern_timestamp": 1751967542.0
    },
    {
      "title": "Just Say Better or Worse: A Human-AI Collaborative Framework for Medical\n  Image Segmentation Without Manual Annotations",
      "summary": "Manual annotation of medical images is a labor-intensive and time-consuming\nprocess, posing a significant bottleneck in the development and deployment of\nrobust medical imaging AI systems. This paper introduces a novel Human-AI\ncollaborative framework for medical image segmentation that substantially\nreduces the annotation burden by eliminating the need for explicit manual\npixel-level labeling. The core innovation lies in a preference learning\nparadigm, where human experts provide minimal, intuitive feedback -- simply\nindicating whether an AI-generated segmentation is better or worse than a\nprevious version. The framework comprises four key components: (1) an adaptable\nfoundation model (FM) for feature extraction, (2) label propagation based on\nfeature similarity, (3) a clicking agent that learns from human better-or-worse\nfeedback to decide where to click and with which label, and (4) a multi-round\nsegmentation learning procedure that trains a state-of-the-art segmentation\nnetwork using pseudo-labels generated by the clicking agent and FM-based label\npropagation. Experiments on three public datasets demonstrate that the proposed\napproach achieves competitive segmentation performance using only binary\npreference feedback, without requiring experts to directly manually annotate\nthe images.",
      "url": "http://arxiv.org/abs/2507.05815v1",
      "published_time_eastern_timestamp": 1751967372.0
    },
    {
      "title": "GTA1: GUI Test-time Scaling Agent",
      "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here.",
      "url": "http://arxiv.org/abs/2507.05791v1",
      "published_time_eastern_timestamp": 1751964738.0
    },
    {
      "title": "On the detection of medium inhomogeneity by contrast agent: wave\n  scattering models and numerical implementations",
      "summary": "We consider the wave scattering and inverse scattering in an inhomogeneous\nmedium embedded a homogeneous droplet with a small size, which is modeled by a\nconstant mass density and a small bulk modulus. Based on the Lippmann-Schwinger\nintegral equation for scattering wave in inhomogeneous medium, we firstly\ndevelop an efficient approximate scheme for computing the scattered wave as\nwell as its far-field pattern for any droplet located in the inhomogeneous\nbackground medium. By establishing the approximate relation between the\nfar-field patterns of the scattered wave before and after the injection of a\ndroplet, the scattered wave of the inhomogeneous medium after injecting the\ndroplet is represented by a measurable far-field patterns, and consequently the\ninhomogeneity of the medium can be reconstructed from the Helmholtz equation.\nFinally, the reconstruction process in terms of the dual reciprocity method is\nproposed to realize the numerical algorithm for recovering the bulk modulus\nfunction inside a bounded domain in three dimensional space, by moving the\ndroplet inside the bounded domain. Numerical implementations are given using\nthe simulation data of the far-field pattern to show the validity of the\nreconstruction scheme, based on the mollification scheme for dealing with the\nill-posedness of this inverse problem.",
      "url": "http://arxiv.org/abs/2507.05773v1",
      "published_time_eastern_timestamp": 1751963110.0
    },
    {
      "title": "An autonomous agent for auditing and improving the reliability of\n  clinical AI models",
      "summary": "The deployment of AI models in clinical practice faces a critical challenge:\nmodels achieving expert-level performance on benchmarks can fail\ncatastrophically when confronted with real-world variations in medical imaging.\nMinor shifts in scanner hardware, lighting or demographics can erode accuracy,\nbut currently reliability auditing to identify such catastrophic failure cases\nbefore deployment is a bespoke and time-consuming process. Practitioners lack\naccessible and interpretable tools to expose and repair hidden failure modes.\nHere we introduce ModelAuditor, a self-reflective agent that converses with\nusers, selects task-specific metrics, and simulates context-dependent,\nclinically relevant distribution shifts. ModelAuditor then generates\ninterpretable reports explaining how much performance likely degrades during\ndeployment, discussing specific likely failure modes and identifying root\ncauses and mitigation strategies. Our comprehensive evaluation across three\nreal-world clinical scenarios - inter-institutional variation in\nhistopathology, demographic shifts in dermatology, and equipment heterogeneity\nin chest radiography - demonstrates that ModelAuditor is able correctly\nidentify context-specific failure modes of state-of-the-art models such as the\nestablished SIIM-ISIC melanoma classifier. Its targeted recommendations recover\n15-25% of performance lost under real-world distribution shift, substantially\noutperforming both baseline models and state-of-the-art augmentation methods.\nThese improvements are achieved through a multi-agent architecture and execute\non consumer hardware in under 10 minutes, costing less than US$0.50 per audit.",
      "url": "http://arxiv.org/abs/2507.05755v1",
      "published_time_eastern_timestamp": 1751961532.0
    },
    {
      "title": "An efficiency ordering of k-price auctions under complete information",
      "summary": "We study $k$-price auctions in a complete information environment and\ncharacterize all pure-strategy Nash equilibrium outcomes. In a setting with $n$\nagents having ordered valuations, we show that any agent, except those with the\nlowest $k-2$ valuations, can win in equilibrium. As a consequence, worst-case\nwelfare increases monotonically as we go from $k=2$ (second-price auction) to\n$k=n$ (lowest-price auction), with the first-price auction achieving the\nhighest worst-case welfare.",
      "url": "http://arxiv.org/abs/2507.05738v1",
      "published_time_eastern_timestamp": 1751960115.0
    },
    {
      "title": "Large Language Models for Agent-Based Modelling: Current and possible\n  uses across the modelling cycle",
      "summary": "The emergence of Large Language Models (LLMs) with increasingly sophisticated\nnatural language understanding and generative capabilities has sparked interest\nin the Agent-based Modelling (ABM) community. With their ability to summarize,\ngenerate, analyze, categorize, transcribe and translate text, answer questions,\npropose explanations, sustain dialogue, extract information from unstructured\ntext, and perform logical reasoning and problem-solving tasks, LLMs have a good\npotential to contribute to the modelling process. After reviewing the current\nuse of LLMs in ABM, this study reflects on the opportunities and challenges of\nthe potential use of LLMs in ABM. It does so by following the modelling cycle,\nfrom problem formulation to documentation and communication of model results,\nand holding a critical stance.",
      "url": "http://arxiv.org/abs/2507.05723v1",
      "published_time_eastern_timestamp": 1751959044.0
    },
    {
      "title": "MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning\n  in Online Environment",
      "summary": "Recently, there has been a surge of vision-based GUI agents designed to\nautomate everyday mobile and web tasks. These agents interpret raw GUI\nscreenshots and autonomously decide where to click, scroll, or type, which\nbypasses handcrafted rules and app-specific APIs. However, most existing\nmethods trained GUI agent in the offline environment using pre-collected\ntrajectories. This approach limits scalability, causes overfitting to specific\nUI templates, and leads to brittle policies when faced with unseen environment.\nWe present MobileGUI-RL, a scalable framework that trains GUI agent in online\nenvironment. MobileGUI-RL contains two key components. It (i) synthesizes a\ncurriculum of learnable tasks through self-exploration and filtering, and (ii)\nadapts GRPO to GUI navigation with trajectory-aware advantages and composite\nrewards that balance task success and execution efficiency. Experiments on\nthree online mobile-agent benchmarks show consistent gains, validating the\neffectiveness of our approach.",
      "url": "http://arxiv.org/abs/2507.05720v1",
      "published_time_eastern_timestamp": 1751958473.0
    },
    {
      "title": "Agentic-R1: Distilled Dual-Strategy Reasoning",
      "summary": "Current long chain-of-thought (long-CoT) models excel at mathematical\nreasoning but rely on slow and error-prone natural language traces.\nTool-augmented agents address arithmetic via code execution, but often falter\non complex logical tasks. We introduce a fine-tuning framework, DualDistill,\nthat distills complementary reasoning strategies from multiple teachers into a\nunified student model. Using this approach, we train Agentic-R1, which\ndynamically selects the optimal strategy for each query, invoking tools for\narithmetic and algorithmic problems, and using text-based reasoning for\nabstract ones. Our method improves accuracy across a range of tasks, including\nboth computation-intensive and standard benchmarks, demonstrating the\neffectiveness of multi-strategy distillation in achieving robust and efficient\nreasoning. Our project is available at https://github.com/StigLidu/DualDistill",
      "url": "http://arxiv.org/abs/2507.05707v1",
      "published_time_eastern_timestamp": 1751956516.0
    },
    {
      "title": "R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding",
      "summary": "Visual agent models for automating human activities on Graphical User\nInterfaces (GUIs) have emerged as a promising research direction, driven by\nadvances in large Vision Language Models (VLMs). A critical challenge in GUI\nautomation is the precise grounding of interface elements across diverse\nplatforms. Existing vision-only GUI agents directly ground elements from large\nand cluttered screenshots, requiring them to process substantial irrelevant\ninformation that compromises their accuracy. In addition, these approaches\ntypically employ basic cross-entropy loss for learning grounding objectives,\nwhich fails to effectively capture grounding quality compared to established\nobject detection metrics like Intersection-over-Union (IoU). To address these\nissues, we introduce R-VLM, a novel GUI grounding approach that leverages\nzoomed-in region proposals for precise element localization. We also propose an\nIoU-aware objective function that facilitates model convergence toward high IoU\npredictions. Our approach bridges the gap between VLMs and conventional object\ndetection techniques, improving the state-of-the-art grounding accuracy by 13%\nacross diverse GUI platforms on the GUI grounding benchmarks ScreenSpot and\nAgentStudio. In addition, our R-VLM approach shows 3.2-9.7% absolute accuracy\nimprovements in GUI navigation tasks on the AITW and Mind2Web benchmarks.",
      "url": "http://arxiv.org/abs/2507.05673v1",
      "published_time_eastern_timestamp": 1751950617.0
    }
  ]
}
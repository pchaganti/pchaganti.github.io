{
  "last_updated": "2025-10-15T02:18:41.701186-04:00",
  "papers": [
    {
      "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
      "summary": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
      "url": "http://arxiv.org/abs/2510.12801v1",
      "published_time_eastern_timestamp": 1760464798.0
    },
    {
      "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in\n  Mathematics and Quantum Physics",
      "summary": "We present Ax-Prover, a multi-agent system for automated theorem proving in\nLean that can solve problems across diverse scientific domains and operate\neither autonomously or collaboratively with human experts. To achieve this,\nAx-Prover approaches scientific problem solving through formal proof\ngeneration, a process that demands both creative reasoning and strict syntactic\nrigor. Ax-Prover meets this challenge by equipping Large Language Models\n(LLMs), which provide knowledge and reasoning, with Lean tools via the Model\nContext Protocol (MCP), which ensure formal correctness. To evaluate its\nperformance as an autonomous prover, we benchmark our approach against frontier\nLLMs and specialized prover models on two public math benchmarks and on two\nLean benchmarks we introduce in the fields of abstract algebra and quantum\ntheory. On public datasets, Ax-Prover is competitive with state-of-the-art\nprovers, while it largely outperform them on the new benchmarks. This shows\nthat, unlike specialized systems that struggle to generalize, our tool-based\nagentic theorem prover approach offers a generalizable methodology for formal\nverification across diverse scientific domains. Furthermore, we demonstrate\nAx-Prover's assistant capabilities in a practical use case, showing how it\nenabled an expert mathematician to formalize the proof of a complex\ncryptography theorem.",
      "url": "http://arxiv.org/abs/2510.12787v1",
      "published_time_eastern_timestamp": 1760464624.0
    },
    {
      "title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural\n  Heritage",
      "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\ncapabilities in joint visual and linguistic tasks. However, existing Visual\nQuestion Answering (VQA) benchmarks often fail to evaluate deep semantic\nunderstanding, particularly in complex domains like visual art analysis.\nConfined to simple syntactic structures and surface-level attributes, these\nquestions fail to capture the diversity and depth of human visual inquiry. This\nlimitation incentivizes models to exploit statistical shortcuts rather than\nengage in visual reasoning. To address this gap, we introduce VQArt-Bench, a\nnew, large-scale VQA benchmark for the cultural heritage domain. This benchmark\nis constructed using a novel multi-agent pipeline where specialized agents\ncollaborate to generate nuanced, validated, and linguistically diverse\nquestions. The resulting benchmark is structured along relevant visual\nunderstanding dimensions that probe a model's ability to interpret symbolic\nmeaning, narratives, and complex visual relationships. Our evaluation of 14\nstate-of-the-art MLLMs on this benchmark reveals significant limitations in\ncurrent models, including a surprising weakness in simple counting tasks and a\nclear performance gap between proprietary and open-source models.",
      "url": "http://arxiv.org/abs/2510.12750v1",
      "published_time_eastern_timestamp": 1760462992.0
    },
    {
      "title": "SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and\n  Segmentation for Urban Scenes Understanding",
      "summary": "The scene perception, understanding, and simulation are fundamental\ntechniques for embodied-AI agents, while existing solutions are still prone to\nsegmentation deficiency, dynamic objects' interference, sensor data sparsity,\nand view-limitation problems. This paper proposes a novel framework, named\nSPORTS, for holistic scene understanding via tightly integrating Video Panoptic\nSegmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into\nan iterative and unified perspective. Firstly, VPS designs an adaptive\nattention-based geometric fusion mechanism to align cross-frame features via\nenrolling the pose, depth, and optical flow modality, which automatically\nadjust feature maps for different decoding stages. And a post-matching strategy\nis integrated to improve identities tracking. In VO, panoptic segmentation\nresults from VPS are combined with the optical flow map to improve the\nconfidence estimation of dynamic objects, which enhances the accuracy of the\ncamera pose estimation and completeness of the depth map generation via the\nlearning-based paradigm. Furthermore, the point-based rendering of SR is\nbeneficial from VO, transforming sparse point clouds into neural fields to\nsynthesize high-fidelity RGB views and twin panoptic views. Extensive\nexperiments on three public datasets demonstrate that our attention-based\nfeature fusion outperforms most existing state-of-the-art methods on the\nodometry, tracking, segmentation, and novel view synthesis tasks.",
      "url": "http://arxiv.org/abs/2510.12749v1",
      "published_time_eastern_timestamp": 1760462899.0
    },
    {
      "title": "HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions",
      "summary": "Safe and interpretable motion planning in complex urban environments needs to\nreason about bidirectional multi-agent interactions. This reasoning requires to\nestimate the costs of potential ego driving maneuvers. Many existing planners\ngenerate initial trajectories with sampling-based methods and refine them by\noptimizing on learned predictions of future environment states, which requires\na cost function that encodes the desired vehicle behavior. Designing such a\ncost function can be very challenging, especially if a wide range of complex\nurban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego\nproposal-conditioned predictions, a planner that integrates multimodal\ntrajectory proposals from a learned proposal model as heuristic priors into a\nMonte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,\nwe introduce an ego-conditioned occupancy prediction model, enabling\nconsistent, scene-aware reasoning. Our design significantly simplifies cost\nfunction design in refinement by considering proposal-driven guidance,\nrequiring only minimalistic grid-based cost terms. Evaluations on large-scale\nreal-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves\nstate-of-the-art performance, especially in safety and adaptability.",
      "url": "http://arxiv.org/abs/2510.12733v1",
      "published_time_eastern_timestamp": 1760461864.0
    },
    {
      "title": "Characterizing Agent-Based Model Dynamics via $Îµ$-Machines and\n  Kolmogorov-Style Complexity",
      "summary": "We propose a two-level information-theoretic framework for characterizing the\ninformational organization of Agent-Based Model (ABM) dynamics within the\nbroader paradigm of Complex Adaptive Systems (CAS). At the macro level, a\npooled $\\epsilon$-machine is reconstructed as a reference model that summarizes\nthe system-wide informational regime. At the micro level, $\\epsilon$-machines\nare reconstructed for each caregiver-elder dyad and variable, and are\ncomplemented with algorithm-agnostic Kolmogorov-style measures, including\nnormalized LZ78 complexity and bits per symbol from lossless compression. The\nresulting feature set $\\{h_{\\mu}, C_{\\mu}, E, \\mathrm{LZ78}, \\mathrm{bps}\\}$\nenables distributional analysis, stratified comparisons, and unsupervised\nclustering across agents and scenarios. This dual-scale design preserves agent\nheterogeneity while providing an interpretable macro-level baseline, aligning\nABM practice with CAS principles of emergence, feedback, and adaptation. A case\nstudy on caregiver-elder interactions illustrates the framework's\nimplementation; the results and discussion will be completed following final\nsimulation runs.",
      "url": "http://arxiv.org/abs/2510.12729v1",
      "published_time_eastern_timestamp": 1760461726.0
    },
    {
      "title": "Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed\n  Perception",
      "summary": "Fine-grained perception of multimodal information is critical for advancing\nhuman-AI interaction. With recent progress in audio-visual technologies, Omni\nLanguage Models (OLMs), capable of processing audio and video signals in\nparallel, have emerged as a promising paradigm for achieving richer\nunderstanding and reasoning. However, their capacity to capture and describe\nfine-grained details remains limited explored. In this work, we present a\nsystematic and comprehensive investigation of omni detailed perception from the\nperspectives of the data pipeline, models, and benchmark. We first identify an\ninherent \"co-growth\" between detail and hallucination in current OLMs. To\naddress this, we propose Omni-Detective, an agentic data generation pipeline\nintegrating tool-calling, to autonomously produce highly detailed yet minimally\nhallucinatory multimodal data. Based on the data generated with Omni-Detective,\nwe train two captioning models: Audio-Captioner for audio-only detailed\nperception, and Omni-Captioner for audio-visual detailed perception. Under the\ncascade evaluation protocol, Audio-Captioner achieves the best performance on\nMMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and\ndelivering performance comparable to Gemini 2.5 Pro. On existing detailed\ncaptioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and\nachieves the best trade-off between detail and hallucination on the\nvideo-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni\ndetailed perception, we design Omni-Cloze, a novel cloze-style evaluation for\ndetailed audio, visual, and audio-visual captioning that ensures stable,\nefficient, and reliable assessment. Experimental results and analysis\ndemonstrate the effectiveness of Omni-Detective in generating high-quality\ndetailed captions, as well as the superiority of Omni-Cloze in evaluating such\ndetailed captions.",
      "url": "http://arxiv.org/abs/2510.12720v1",
      "published_time_eastern_timestamp": 1760461209.0
    },
    {
      "title": "Residual MPC: Blending Reinforcement Learning with GPU-Parallelized\n  Model Predictive Control",
      "summary": "Model Predictive Control (MPC) provides interpretable, tunable locomotion\ncontrollers grounded in physical models, but its robustness depends on frequent\nreplanning and is limited by model mismatch and real-time computational\nconstraints. Reinforcement Learning (RL), by contrast, can produce highly\nrobust behaviors through stochastic training but often lacks interpretability,\nsuffers from out-of-distribution failures, and requires intensive reward\nengineering. This work presents a GPU-parallelized residual architecture that\ntightly integrates MPC and RL by blending their outputs at the torque-control\nlevel. We develop a kinodynamic whole-body MPC formulation evaluated across\nthousands of agents in parallel at 100 Hz for RL training. The residual policy\nlearns to make targeted corrections to the MPC outputs, combining the\ninterpretability and constraint handling of model-based control with the\nadaptability of RL. The model-based control prior acts as a strong bias,\ninitializing and guiding the policy towards desirable behavior with a simple\nset of rewards. Compared to standalone MPC or end-to-end RL, our approach\nachieves higher sample efficiency, converges to greater asymptotic rewards,\nexpands the range of trackable velocity commands, and enables zero-shot\nadaptation to unseen gaits and uneven terrain.",
      "url": "http://arxiv.org/abs/2510.12717v1",
      "published_time_eastern_timestamp": 1760461037.0
    },
    {
      "title": "Reflection-Based Task Adaptation for Self-Improving VLA",
      "summary": "Pre-trained Vision-Language-Action (VLA) models represent a major leap\ntowards general-purpose robots, yet efficiently adapting them to novel,\nspecific tasks in-situ remains a significant hurdle. While reinforcement\nlearning (RL) is a promising avenue for such adaptation, the process often\nsuffers from low efficiency, hindering rapid task mastery. We introduce\nReflective Self-Adaptation, a framework for rapid, autonomous task adaptation\nwithout human intervention. Our framework establishes a self-improving loop\nwhere the agent learns from its own experience to enhance both strategy and\nexecution.\n  The core of our framework is a dual-pathway architecture that addresses the\nfull adaptation lifecycle. First, a Failure-Driven Reflective RL pathway\nenables rapid learning by using the VLM's causal reasoning to automatically\nsynthesize a targeted, dense reward function from failure analysis. This\nprovides a focused learning signal that significantly accelerates policy\nexploration. However, optimizing such proxy rewards introduces a potential risk\nof \"reward hacking,\" where the agent masters the reward function but fails the\nactual task. To counteract this, our second pathway, Success-Driven\nQuality-Guided SFT, grounds the policy in holistic success. It identifies and\nselectively imitates high-quality successful trajectories, ensuring the agent\nremains aligned with the ultimate task goal. This pathway is strengthened by a\nconditional curriculum mechanism to aid initial exploration.\n  We conduct experiments in challenging manipulation tasks. The results\ndemonstrate that our framework achieves faster convergence and higher final\nsuccess rates compared to representative baselines. Our work presents a robust\nsolution for creating self-improving agents that can efficiently and reliably\nadapt to new environments.",
      "url": "http://arxiv.org/abs/2510.12710v1",
      "published_time_eastern_timestamp": 1760460279.0
    },
    {
      "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection",
      "summary": "With advancements in reasoning capabilities, Large Language Models (LLMs) are\nincreasingly employed for automated judgment tasks. While LLMs-as-Judges offer\npromise in automating evaluations, current approaches often rely on simplistic\naggregation methods (e.g., majority voting), which can fail even when\nindividual agents provide correct answers. To address this, we propose a\nmulti-agent debate judge framework where agents collaboratively reason and\niteratively refine their responses. We formalize the debate process\nmathematically, analyzing agent interactions and proving that debate amplifies\ncorrectness compared to static ensembles. To enhance efficiency, we introduce a\nstability detection mechanism that models judge consensus dynamics via a\ntime-varying Beta-Binomial mixture, with adaptive stopping based on\ndistributional similarity (Kolmogorov-Smirnov test). This mechanism models the\njudges' collective correct rate dynamics using a time-varying mixture of\nBeta-Binomial distributions and employs an adaptive stopping criterion based on\ndistributional similarity (Kolmogorov-Smirnov statistic). Experiments across\nmultiple benchmarks and models demonstrate that our framework improves judgment\naccuracy over majority voting while maintaining computational efficiency.",
      "url": "http://arxiv.org/abs/2510.12697v1",
      "published_time_eastern_timestamp": 1760459430.0
    },
    {
      "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning\n  and Online Reinforcement Learning",
      "summary": "Recent advances in embodied AI highlight the potential of vision language\nmodels (VLMs) as agents capable of perception, reasoning, and interaction in\ncomplex environments. However, top-performing systems rely on large-scale\nmodels that are costly to deploy, while smaller VLMs lack the necessary\nknowledge and skills to succeed. To bridge this gap, we present\n\\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates\nprior knowledge learning and online reinforcement learning (RL). The first\nstage, \\textit{Embodied Prior Learning}, distills foundational knowledge from\nthree types of data: (1) Trajectory-Augmented Priors, which enrich existing\ntrajectory data with structured reasoning generated by stronger models; (2)\nEnvironment-Anchored Priors, which provide in-environment knowledge and\ngrounding supervision; and (3) External Knowledge Priors, which transfer\ngeneral knowledge from out-of-environment datasets. In the second stage, we\ndevelop an online RL pipeline that builds on these priors to further enhance\nagent performance. To overcome the inherent challenges in agent RL, including\nlong horizons, sparse rewards, and training instability, we introduce three key\ndesigns: self-summarization for context management, dense reward shaping, and\nturn-level policy optimization. Extensive experiments on both high-level\nplanning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate\nthat ERA-3B surpasses both prompting-based large models and previous\ntraining-based baselines. Specifically, it achieves overall improvements of\n8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits\nstrong generalization to unseen tasks. Overall, ERA offers a practical path\ntoward scalable embodied intelligence, providing methodological insights for\nfuture embodied AI systems.",
      "url": "http://arxiv.org/abs/2510.12693v1",
      "published_time_eastern_timestamp": 1760459146.0
    },
    {
      "title": "MCOP: Multi-UAV Collaborative Occupancy Prediction",
      "summary": "Unmanned Aerial Vehicle (UAV) swarm systems necessitate efficient\ncollaborative perception mechanisms for diverse operational scenarios. Current\nBird's Eye View (BEV)-based approaches exhibit two main limitations:\nbounding-box representations fail to capture complete semantic and geometric\ninformation of the scene, and their performance significantly degrades when\nencountering undefined or occluded objects. To address these limitations, we\npropose a novel multi-UAV collaborative occupancy prediction framework. Our\nframework effectively preserves 3D spatial structures and semantics through\nintegrating a Spatial-Aware Feature Encoder and Cross-Agent Feature\nIntegration. To enhance efficiency, we further introduce Altitude-Aware Feature\nReduction to compactly represent scene information, along with a Dual-Mask\nPerceptual Guidance mechanism to adaptively select features and reduce\ncommunication overhead. Due to the absence of suitable benchmark datasets, we\nextend three datasets for evaluation: two virtual datasets (Air-to-Pred-Occ and\nUAV3D-Occ) and one real-world dataset (GauUScene-Occ). Experiments results\ndemonstrate that our method achieves state-of-the-art accuracy, significantly\noutperforming existing collaborative methods while reducing communication\noverhead to only a fraction of previous approaches.",
      "url": "http://arxiv.org/abs/2510.12679v1",
      "published_time_eastern_timestamp": 1760458662.0
    },
    {
      "title": "Single-Deviation Stability in Additively Separable Hedonic Games with\n  Constrained Coalition Sizes",
      "summary": "We study stability in additively separable hedonic games when coalition sizes\nhave to respect fixed size bounds. We consider four classic notions of\nstability based on single-agent deviations, namely, Nash stability, individual\nstability, contractual Nash stability, and contractual individual stability.\nFor each stability notion, we consider two variants: in one, the coalition left\nbehind by a deviator must still be of a valid size, and in the other there is\nno such constraint. We provide a full picture of the existence of stable\noutcomes with respect to given size parameters. Additionally, when there are\nonly upper bounds, we fully characterize the computational complexity of the\nassociated existence problem. In particular, we obtain polynomial-time\nalgorithms for contractual individual stability and contractual Nash stability,\nwhere the latter requires an upper bound of 2. We obtain further results for\nNash stability and contractual individual stability, when the lower bound is at\nleast 2.",
      "url": "http://arxiv.org/abs/2510.12641v1",
      "published_time_eastern_timestamp": 1760456039.0
    },
    {
      "title": "Expert or not? assessing data quality in offline reinforcement learning",
      "summary": "Offline reinforcement learning (RL) learns exclusively from static datasets,\nwithout further interaction with the environment. In practice, such datasets\nvary widely in quality, often mixing expert, suboptimal, and even random\ntrajectories. The choice of algorithm therefore depends on dataset fidelity.\nBehavior cloning can suffice on high-quality data, whereas mixed- or\nlow-quality data typically benefits from offline RL methods that stitch useful\nbehavior across trajectories. Yet in the wild it is difficult to assess dataset\nquality a priori because the data's provenance and skill composition are\nunknown. We address the problem of estimating offline dataset quality without\ntraining an agent. We study a spectrum of proxies from simple cumulative\nrewards to learned value based estimators, and introduce the Bellman\nWasserstein distance (BWD), a value aware optimal transport score that measures\nhow dissimilar a dataset's behavioral policy is from a random reference policy.\nBWD is computed from a behavioral critic and a state conditional OT\nformulation, requiring no environment interaction or full policy optimization.\nAcross D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance\nscore that aggregates multiple offline RL algorithms, enabling efficient\nprediction of how well standard agents will perform on a given dataset. Beyond\nprediction, integrating BWD as a regularizer during policy optimization\nexplicitly pushes the learned policy away from random behavior and improves\nreturns. These results indicate that value aware, distributional signals such\nas BWD are practical tools for triaging offline RL datasets and policy\noptimization.",
      "url": "http://arxiv.org/abs/2510.12638v1",
      "published_time_eastern_timestamp": 1760455883.0
    },
    {
      "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic\n  Tasks",
      "summary": "Large Language Models face challenges in long-horizon agentic tasks as their\nconstrained memory is easily overwhelmed by distracting or irrelevant context.\nExisting working memory methods typically rely on external, heuristic\nmechanisms that are decoupled from the agent's core policy. In this work, we\nreframe working memory management as a learnable, intrinsic capability. We\npropose a novel framework, Memory-as-Action, where an agent actively manages\nits working memory by executing explicit editing operations as part of a\nunified policy. This formulation allows an agent, trained via reinforcement\nlearning, to balance memory curation against long-term task objectives under\ngiven resource constraints. However, such memory editing actions break the\nstandard assumption of a continuously growing prefix in LLM interactions,\nleading to what we call trajectory fractures. These non-prefix changes disrupt\nthe causal continuity required by standard policy gradient methods, making\nthose methods inapplicable. To address this, we propose a new algorithm,\nDynamic Context Policy Optimization, which enables stable end-to-end\nreinforcement learning by segmenting trajectories at memory action points and\napplying trajectory-level advantages to the resulting action segments. Our\nresults demonstrate that jointly optimizing for task reasoning and memory\nmanagement in an end-to-end fashion not only reduces overall computational\nconsumption but also improves task performance, driven by adaptive context\ncuration strategies tailored to the model's intrinsic capabilities.",
      "url": "http://arxiv.org/abs/2510.12635v1",
      "published_time_eastern_timestamp": 1760455797.0
    },
    {
      "title": "Designing Tools with Control Confidence",
      "summary": "Prehistoric humans invented stone tools for specialized tasks by not just\nmaximizing the tool's immediate goal-completion accuracy, but also increasing\ntheir confidence in the tool for later use under similar settings. This factor\ncontributed to the increased robustness of the tool, i.e., the least\nperformance deviations under environmental uncertainties. However, the current\nautonomous tool design frameworks solely rely on performance optimization,\nwithout considering the agent's confidence in tool use for repeated use. Here,\nwe take a step towards filling this gap by i) defining an optimization\nframework for task-conditioned autonomous hand tool design for robots, where\nii) we introduce a neuro-inspired control confidence term into the optimization\nroutine that helps the agent to design tools with higher robustness. Through\nrigorous simulations using a robotic arm, we show that tools designed with\ncontrol confidence as the objective function are more robust to environmental\nuncertainties during tool use than a pure accuracy-driven objective. We further\nshow that adding control confidence to the objective function for tool design\nprovides a balance between the robustness and goal accuracy of the designed\ntools under control perturbations. Finally, we show that our CMAES-based\nevolutionary optimization strategy for autonomous tool design outperforms other\nstate-of-the-art optimizers by designing the optimal tool within the fewest\niterations. Code: https://github.com/ajitham123/Tool_design_control_confidence.",
      "url": "http://arxiv.org/abs/2510.12630v1",
      "published_time_eastern_timestamp": 1760455647.0
    },
    {
      "title": "Learning-To-Measure: In-context Active Feature Acquisition",
      "summary": "Active feature acquisition (AFA) is a sequential decision-making problem\nwhere the goal is to improve model performance for test instances by adaptively\nselecting which features to acquire. In practice, AFA methods often learn from\nretrospective data with systematic missingness in the features and limited\ntask-specific labels. Most prior work addresses acquisition for a single\npredetermined task, limiting scalability. To address this limitation, we\nformalize the meta-AFA problem, where the goal is to learn acquisition policies\nacross various tasks. We introduce Learning-to-Measure (L2M), which consists of\ni) reliable uncertainty quantification over unseen tasks, and ii) an\nuncertainty-guided greedy feature acquisition agent that maximizes conditional\nmutual information. We demonstrate a sequence-modeling or autoregressive\npre-training approach that underpins reliable uncertainty quantification for\ntasks with arbitrary missingness. L2M operates directly on datasets with\nretrospective missingness and performs the meta-AFA task in-context,\neliminating per-task retraining. Across synthetic and real-world tabular\nbenchmarks, L2M matches or surpasses task-specific baselines, particularly\nunder scarce labels and high missingness.",
      "url": "http://arxiv.org/abs/2510.12624v1",
      "published_time_eastern_timestamp": 1760455412.0
    },
    {
      "title": "Modeling Epidemics on Multiplex Networks: Epidemic Threshold and Basic\n  Reproduction Number",
      "summary": "Accurate epidemic forecasting requires models that account for the layered\nand heterogeneous nature of real social interactions. The basic reproduction\nnumber $\\mathcal R_0$ calculated from models that assume homogeneous mixing or\nsingle-layer contact structures have limited applicability to complex social\nsystems. Here, we propose an expression of $\\mathcal R_0$ in the context of\nmultiplex networks, enabling the analysis of disease transmission across\nmultiple social layers.\n  We adapt the Degree-Based Mean-Field (DBMF) SIR model for single-layered\ncomplex networks to the multiplex setting, where each layer has its own degree\ndistribution and infection rate. Using the Next Generation Matrix method, we\nderive an analytical expression for the basic reproduction number $\\mathcal\nR_0$. Numerical integration of the multiplex DBMF equations shows that\n$\\mathcal R_0 = 1$ marks the epidemic threshold and governs the functional\ndependence of key outbreak indicators. In addition to the exact result for the\n$\\mathcal R_0$, we provide an approximation denoted as $\\tau$, which is easier\nto compute and more straightforward to interpret in terms of the parameters of\nthe system, and shares most of the expected properties of the basic\nreproduction number.\n  Stochastic agent-based simulations confirm these results, demonstrating a\ndirect correspondence between $\\tau$ and the average number of secondary\ninfections in the early epidemic phase, in line with the interpretation of\n$\\mathcal R_0$.\n  This research provides a robust generalization of $\\mathcal R_0$ for layered\ncontact structures, offering a more realistic basis for epidemic forecasting\nand the design of intervention strategies.",
      "url": "http://arxiv.org/abs/2510.12614v1",
      "published_time_eastern_timestamp": 1760454852.0
    },
    {
      "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in\n  Latent World Models for Autonomous Driving",
      "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
      "url": "http://arxiv.org/abs/2510.12560v1",
      "published_time_eastern_timestamp": 1760451712.0
    },
    {
      "title": "Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors\n  in Multi-Agent Reinforcement Learning Settings",
      "summary": "The competitive and cooperative forces of natural selection have driven the\nevolution of intelligence for millions of years, culminating in nature's vast\nbiodiversity and the complexity of human minds. Inspired by this process, we\npropose a novel multi-agent reinforcement learning framework where each agent\nis assigned a genotype and where reward functions are modelled after the\nconcept of inclusive fitness. An agent's genetic material may be shared with\nother agents, and our inclusive reward function naturally accounts for this. We\nstudy the resulting social dynamics in two types of network games with\nprisoner's dilemmas and find that our results align with well-established\nprinciples from biology, such as Hamilton's rule. Furthermore, we outline how\nthis framework can extend to more open-ended environments with spatial and\ntemporal structure, finite resources, and evolving populations. We hypothesize\nthe emergence of an arms race of strategies, where each new strategy is a\ngradual improvement over earlier adaptations of other agents, effectively\nproducing a multi-agent autocurriculum analogous to biological evolution. In\ncontrast to the binary team-based structures prevalent in earlier research, our\ngene-based reward structure introduces a spectrum of cooperation ranging from\nfull adversity to full cooperativeness based on genetic similarity, enabling\nunique non team-based social dynamics. For example, one agent having a mutual\ncooperative relationship with two other agents, while the two other agents\nbehave adversarially towards each other. We argue that incorporating inclusive\nfitness in agents provides a foundation for the emergence of more strategically\nadvanced and socially intelligent agents.",
      "url": "http://arxiv.org/abs/2510.12555v1",
      "published_time_eastern_timestamp": 1760451601.0
    }
  ]
}
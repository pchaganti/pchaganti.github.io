{
  "last_updated": "2025-05-15T08:22:42.586224-04:00",
  "papers": [
    {
      "title": "Language Agents Mirror Human Causal Reasoning Biases. How Can We Help\n  Them Think Like Scientists?",
      "summary": "Language model (LM) agents are increasingly used as autonomous\ndecision-makers who need to actively gather information to guide their\ndecisions. A crucial cognitive skill for such agents is the efficient\nexploration and understanding of the causal structure of the world -- key to\nrobust, scientifically grounded reasoning. Yet, it remains unclear whether LMs\npossess this capability or exhibit systematic biases leading to erroneous\nconclusions. In this work, we examine LMs' ability to explore and infer causal\nrelationships, using the well-established \"Blicket Test\" paradigm from\ndevelopmental psychology. We find that LMs reliably infer the common, intuitive\ndisjunctive causal relationships but systematically struggle with the unusual,\nyet equally (or sometimes even more) evidenced conjunctive ones. This\n\"disjunctive bias\" persists across model families, sizes, and prompting\nstrategies, and performance further declines as task complexity increases.\nInterestingly, an analogous bias appears in human adults, suggesting that LMs\nmay have inherited deep-seated reasoning heuristics from their training data.\nTo this end, we quantify similarities between LMs and humans, finding that LMs\nexhibit adult-like inference profiles (but not children-like). Finally, we\npropose a test-time sampling method which explicitly samples and eliminates\nhypotheses about causal relationships from the LM. This scalable approach\nsignificantly reduces the disjunctive bias and moves LMs closer to the goal of\nscientific, causally rigorous reasoning.",
      "url": "http://arxiv.org/abs/2505.09614v1",
      "published_time_eastern_timestamp": 1747245575.0
    },
    {
      "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models",
      "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.",
      "url": "http://arxiv.org/abs/2505.09595v1",
      "published_time_eastern_timestamp": 1747244620.0
    },
    {
      "title": "Preserving Plasticity in Continual Learning with Adaptive Linearity\n  Injection",
      "summary": "Loss of plasticity in deep neural networks is the gradual reduction in a\nmodel's capacity to incrementally learn and has been identified as a key\nobstacle to learning in non-stationary problem settings. Recent work has shown\nthat deep linear networks tend to be resilient towards loss of plasticity.\nMotivated by this observation, we propose Adaptive Linearization (AdaLin), a\ngeneral approach that dynamically adapts each neuron's activation function to\nmitigate plasticity loss. Unlike prior methods that rely on regularization or\nperiodic resets, AdaLin equips every neuron with a learnable parameter and a\ngating mechanism that injects linearity into the activation function based on\nits gradient flow. This adaptive modulation ensures sufficient gradient signal\nand sustains continual learning without introducing additional hyperparameters\nor requiring explicit task boundaries. When used with conventional activation\nfunctions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can\nsignificantly improve performance on standard benchmarks, including Random\nLabel and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split\nCIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such\nas class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in\nmitigating plasticity loss in off-policy reinforcement learning agents. We\nperform a systematic set of ablations that show that neuron-level adaptation is\ncrucial for good performance and analyze a number of metrics in the network\nthat might be correlated to loss of plasticity.",
      "url": "http://arxiv.org/abs/2505.09486v1",
      "published_time_eastern_timestamp": 1747237011.0
    },
    {
      "title": "Streaming Multi-agent Pathfinding",
      "summary": "The task of the multi-agent pathfinding (MAPF) problem is to navigate a team\nof agents from their start point to the goal points. However, this setup is\nunsuitable in the assembly line scenario, which is periodic with a long working\nhour. To address this issue, the study formalizes the streaming MAPF (S-MAPF)\nproblem, which assumes that the agents in the same agent stream have a periodic\nstart time and share the same action sequence. The proposed solution, Agent\nStream Conflict-Based Search (ASCBS), is designed to tackle this problem by\nincorporating a cyclic vertex/edge constraint to handle conflicts.\nAdditionally, this work explores the potential usage of the disjoint splitting\nstrategy within ASCBS. Experimental results indicate that ASCBS surpasses\ntraditional MAPF solvers in terms of runtime for scenarios with prolonged\nworking hours.",
      "url": "http://arxiv.org/abs/2505.09472v1",
      "published_time_eastern_timestamp": 1747236158.0
    },
    {
      "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios",
      "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.",
      "url": "http://arxiv.org/abs/2505.09436v1",
      "published_time_eastern_timestamp": 1747233870.0
    },
    {
      "title": "Decentralized Nonlinear Model Predictive Control-Based Flock Navigation\n  with Real-Time Obstacle Avoidance in Unknown Obstructed Environments",
      "summary": "This work extends our prior work on the distributed nonlinear model\npredictive control (NMPC) for navigating a robot fleet following a certain\nflocking behavior in unknown obstructed environments with a more realistic\nlocal obstacle avoidance strategy. More specifically, we integrate the local\nobstacle avoidance constraint using point clouds into the NMPC framework. Here,\neach agent relies on data from its local sensor to perceive and respond to\nnearby obstacles. A point cloud processing technique is presented for both\ntwo-dimensional and three-dimensional point clouds to minimize the\ncomputational burden during the optimization. The process consists of\ndirectional filtering and down-sampling that significantly reduce the number of\ndata points. The algorithm's performance is validated through realistic 3D\nsimulations in Gazebo, and its practical feasibility is further explored via\nhardware-in-the-loop (HIL) simulations on embedded platforms.",
      "url": "http://arxiv.org/abs/2505.09434v1",
      "published_time_eastern_timestamp": 1747233727.0
    },
    {
      "title": "Using Dopants as Agents to Probe Key Electronic Properties of Organic\n  Semiconductors",
      "summary": "In organic electronics, conductivity doping is used primarily to eliminate\ncharge injection barriers in organic light-emitting diodes, organic\nphotovoltaics and other electronic devices. Therefore, research on conductivity\ndoping is primarily focused on understanding and enhancing the properties of\nthese doped layers. In contrast, this work shifts the focus from optimizing\ndoped layers to leveraging the doping process as a tool for investigating\nfundamental material properties. Specifically, the dopant is used as an \"agent\"\nto enable the measurement of three critical parameters: ionization potential\n(IP), electron affinity (EA), and Coulomb interaction energy (VC) - that govern\ndopant ionization and play central roles in organic electronic devices in\ngeneral. While these parameters can be measured experimentally, conventional\napproaches often involve intricate or indirect methods, such as spectral\ndeconvolution, which may introduce ambiguities or fail to represent bulk\nproperties. Here it is shown how consolidating the experimental data and\nsimulations on the dopant ionization fraction and doped-induced conductivity\ncan be used to estimate the mean IP or EA of the embedded organic molecule, and\nVC of the embedded charge-transfer complex. These results illustrate how\nmeasuring and simulating doped materials can provide access to the fundamental\ndesign parameters of organic electronic devices",
      "url": "http://arxiv.org/abs/2505.09431v1",
      "published_time_eastern_timestamp": 1747233290.0
    },
    {
      "title": "Linear Search with Probabilistic Detection and Variable Speeds",
      "summary": "We present results on new variants of the famous linear search (or cow-path)\nproblem that involves an agent searching for a target with unknown position on\nthe infinite line. We consider the variant where the agent can move either at\nspeed $1$ or at a slower speed $v \\in [0, 1)$. When traveling at the slower\nspeed $v$, the agent is guaranteed to detect the target upon passing through\nits location. When traveling at speed $1$, however, the agent, upon passing\nthrough the target's location, detects it with probability $p \\in [0, 1]$. We\npresent algorithms and provide upper bounds for the competitive ratios for\nthree cases separately: when $p=0$, $v=0$, and when $p,v \\in (0,1)$. We also\nprove that the provided algorithm for the $p=0$ case is optimal.",
      "url": "http://arxiv.org/abs/2505.09429v1",
      "published_time_eastern_timestamp": 1747233196.0
    },
    {
      "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation",
      "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.",
      "url": "http://arxiv.org/abs/2505.09427v1",
      "published_time_eastern_timestamp": 1747232904.0
    },
    {
      "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners",
      "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.",
      "url": "http://arxiv.org/abs/2505.09396v1",
      "published_time_eastern_timestamp": 1747230684.0
    },
    {
      "title": "Qwen3 Technical Report",
      "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
      "url": "http://arxiv.org/abs/2505.09388v1",
      "published_time_eastern_timestamp": 1747230094.0
    },
    {
      "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with\n  Information Foraging",
      "summary": "Augmenting large language models (LLMs) with external retrieval has become a\nstandard method to address their inherent knowledge cutoff limitations.\nHowever, traditional retrieval-augmented generation methods employ static,\npre-inference retrieval strategies, making them inadequate for complex tasks\ninvolving ambiguous, multi-step, or evolving information needs. Recent advances\nin test-time scaling techniques have demonstrated significant potential in\nenabling LLMs to dynamically interact with external tools, motivating the shift\ntoward adaptive inference-time retrieval. Inspired by Information Foraging\nTheory (IFT), we propose InForage, a reinforcement learning framework that\nformalizes retrieval-augmented reasoning as a dynamic information-seeking\nprocess. Unlike existing approaches, InForage explicitly rewards intermediate\nretrieval quality, encouraging LLMs to iteratively gather and integrate\ninformation through adaptive search behaviors. To facilitate training, we\nconstruct a human-guided dataset capturing iterative search and reasoning\ntrajectories for complex, real-world web tasks. Extensive evaluations across\ngeneral question answering, multi-hop reasoning tasks, and a newly developed\nreal-time web QA dataset demonstrate InForage's superior performance over\nbaseline methods. These results highlight InForage's effectiveness in building\nrobust, adaptive, and efficient reasoning agents.",
      "url": "http://arxiv.org/abs/2505.09316v1",
      "published_time_eastern_timestamp": 1747224818.0
    },
    {
      "title": "Reproducibility Study of \"Cooperate or Collapse: Emergence of\n  Sustainable Cooperation in a Society of LLM Agents\"",
      "summary": "This study evaluates and extends the findings made by Piatti et al., who\nintroduced GovSim, a simulation framework designed to assess the cooperative\ndecision-making capabilities of large language models (LLMs) in\nresource-sharing scenarios. By replicating key experiments, we validate claims\nregarding the performance of large models, such as GPT-4-turbo, compared to\nsmaller models. The impact of the universalization principle is also examined,\nwith results showing that large models can achieve sustainable cooperation,\nwith or without the principle, while smaller models fail without it. In\naddition, we provide multiple extensions to explore the applicability of the\nframework to new settings. We evaluate additional models, such as DeepSeek-V3\nand GPT-4o-mini, to test whether cooperative behavior generalizes across\ndifferent architectures and model sizes. Furthermore, we introduce new\nsettings: we create a heterogeneous multi-agent environment, study a scenario\nusing Japanese instructions, and explore an \"inverse environment\" where agents\nmust cooperate to mitigate harmful resource distributions. Our results confirm\nthat the benchmark can be applied to new models, scenarios, and languages,\noffering valuable insights into the adaptability of LLMs in complex cooperative\ntasks. Moreover, the experiment involving heterogeneous multi-agent systems\ndemonstrates that high-performing models can influence lower-performing ones to\nadopt similar behaviors. This finding has significant implications for other\nagent-based applications, potentially enabling more efficient use of\ncomputational resources and contributing to the development of more effective\ncooperative AI systems.",
      "url": "http://arxiv.org/abs/2505.09289v1",
      "published_time_eastern_timestamp": 1747221314.0
    },
    {
      "title": "A Scalable Unsupervised Framework for multi-aspect labeling of\n  Multilingual and Multi-Domain Review Data",
      "summary": "Effectively analyzing online review data is essential across industries.\nHowever, many existing studies are limited to specific domains and languages or\ndepend on supervised learning approaches that require large-scale labeled\ndatasets. To address these limitations, we propose a multilingual, scalable,\nand unsupervised framework for cross-domain aspect detection. This framework is\ndesigned for multi-aspect labeling of multilingual and multi-domain review\ndata. In this study, we apply automatic labeling to Korean and English review\ndatasets spanning various domains and assess the quality of the generated\nlabels through extensive experiments. Aspect category candidates are first\nextracted through clustering, and each review is then represented as an\naspect-aware embedding vector using negative sampling. To evaluate the\nframework, we conduct multi-aspect labeling and fine-tune several pretrained\nlanguage models to measure the effectiveness of the automatically generated\nlabels. Results show that these models achieve high performance, demonstrating\nthat the labels are suitable for training. Furthermore, comparisons with\npublicly available large language models highlight the framework's superior\nconsistency and scalability when processing large-scale data. A human\nevaluation also confirms that the quality of the automatic labels is comparable\nto those created manually. This study demonstrates the potential of a robust\nmulti-aspect labeling approach that overcomes limitations of supervised methods\nand is adaptable to multilingual, multi-domain environments. Future research\nwill explore automatic review summarization and the integration of artificial\nintelligence agents to further improve the efficiency and depth of review\nanalysis.",
      "url": "http://arxiv.org/abs/2505.09286v1",
      "published_time_eastern_timestamp": 1747221077.0
    },
    {
      "title": "A drone that learns to efficiently find objects in agricultural fields:\n  from simulation to the real world",
      "summary": "Drones are promising for data collection in precision agriculture, however,\nthey are limited by their battery capacity. Efficient path planners are\ntherefore required. This paper presents a drone path planner trained using\nReinforcement Learning (RL) on an abstract simulation that uses object\ndetections and uncertain prior knowledge. The RL agent controls the flight\ndirection and can terminate the flight. By using the agent in combination with\nthe drone's flight controller and a detection network to process camera images,\nit is possible to evaluate the performance of the agent on real-world data. In\nsimulation, the agent yielded on average a 78% shorter flight path compared to\na full coverage planner, at the cost of a 14% lower recall. On real-world data,\nthe agent showed a 72% shorter flight path compared to a full coverage planner,\nhowever, at the cost of a 25% lower recall. The lower performance on real-world\ndata was attributed to the real-world object distribution and the lower\naccuracy of prior knowledge, and shows potential for improvement. Overall, we\nconcluded that for applications where it is not crucial to find all objects,\nsuch as weed detection, the learned-based path planner is suitable and\nefficient.",
      "url": "http://arxiv.org/abs/2505.09278v1",
      "published_time_eastern_timestamp": 1747220349.0
    },
    {
      "title": "Data-driven Internal Model Control for Output Regulation",
      "summary": "Output regulation is a fundamental problem in control theory, extensively\nstudied since the 1970s. Traditionally, research has primarily addressed\nscenarios where the system model is explicitly known, leaving the problem in\nthe absence of a system model less explored. Leveraging the recent advancements\nin Willems et al.'s fundamental lemma, data-driven control has emerged as a\npowerful tool for stabilizing unknown systems. This paper tackles the output\nregulation problem for unknown single and multi-agent systems (MASs) using\nnoisy data. Previous approaches have attempted to solve data-based output\nregulation equations (OREs), which are inadequate for achieving zero tracking\nerror with noisy data. To circumvent the need for solving data-based OREs, we\npropose an internal model-based data-driven controller that reformulates the\noutput regulation problem into a stabilization problem. This method is first\napplied to linear time-invariant (LTI) systems, demonstrating exact solution\ncapabilities, i.e., zero tracking error, through solving a straightforward\ndata-based linear matrix inequality (LMI). Furthermore, we extend our approach\nto solve the $k$th-order output regulation problem for nonlinear systems.\nExtensions to both linear and nonlinear MASs are discussed. Finally, numerical\ntests validate the effectiveness and correctness of the proposed controllers.",
      "url": "http://arxiv.org/abs/2505.09255v1",
      "published_time_eastern_timestamp": 1747217486.0
    },
    {
      "title": "Latent Theory of Mind: A Decentralized Diffusion Architecture for\n  Cooperative Manipulation",
      "summary": "We present Latent Theory of Mind (LatentToM), a decentralized diffusion\npolicy architecture for collaborative robot manipulation. Our policy allows\nmultiple manipulators with their own perception and computation to collaborate\nwith each other towards a common task goal with or without explicit\ncommunication. Our key innovation lies in allowing each agent to maintain two\nlatent representations: an ego embedding specific to the robot, and a consensus\nembedding trained to be common to both robots, despite their different sensor\nstreams and poses. We further let each robot train a decoder to infer the other\nrobot's ego embedding from their consensus embedding, akin to theory of mind in\nlatent space. Training occurs centrally, with all the policies' consensus\nencoders supervised by a loss inspired by sheaf theory, a mathematical theory\nfor clustering data on a topological manifold. Specifically, we introduce a\nfirst-order cohomology loss to enforce sheaf-consistent alignment of the\nconsensus embeddings. To preserve the expressiveness of the consensus\nembedding, we further propose structural constraints based on theory of mind\nand a directional consensus mechanism. Execution can be fully distributed,\nrequiring no explicit communication between policies. In which case, the\ninformation is exchanged implicitly through each robot's sensor stream by\nobserving the actions of the other robots and their effects on the scene.\nAlternatively, execution can leverage direct communication to share the robots'\nconsensus embeddings, where the embeddings are shared once during each\ninference step and are aligned using the sheaf Laplacian. In our hardware\nexperiments, LatentToM outperforms a naive decentralized diffusion baseline,\nand shows comparable performance with a state-of-the-art centralized diffusion\npolicy for bi-manual manipulation. Project website:\nhttps://stanfordmsl.github.io/LatentToM/.",
      "url": "http://arxiv.org/abs/2505.09144v1",
      "published_time_eastern_timestamp": 1747198989.0
    },
    {
      "title": "Beyond the Known: Decision Making with Counterfactual Reasoning Decision\n  Transformer",
      "summary": "Decision Transformers (DT) play a crucial role in modern reinforcement\nlearning, leveraging offline datasets to achieve impressive results across\nvarious domains. However, DT requires high-quality, comprehensive data to\nperform optimally. In real-world applications, the lack of training data and\nthe scarcity of optimal behaviours make training on offline datasets\nchallenging, as suboptimal data can hinder performance. To address this, we\npropose the Counterfactual Reasoning Decision Transformer (CRDT), a novel\nframework inspired by counterfactual reasoning. CRDT enhances DT ability to\nreason beyond known data by generating and utilizing counterfactual\nexperiences, enabling improved decision-making in unseen scenarios. Experiments\nacross Atari and D4RL benchmarks, including scenarios with limited data and\naltered dynamics, demonstrate that CRDT outperforms conventional DT approaches.\nAdditionally, reasoning counterfactually allows the DT agent to obtain\nstitching abilities, combining suboptimal trajectories, without architectural\nmodifications. These results highlight the potential of counterfactual\nreasoning to enhance reinforcement learning agents' performance and\ngeneralization capabilities.",
      "url": "http://arxiv.org/abs/2505.09114v1",
      "published_time_eastern_timestamp": 1747194316.0
    },
    {
      "title": "Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground\n  Integrated Network",
      "summary": "The space-air-ground integrated network (SAGIN) has recently emerged as a\ncore element in the 6G networks. However, traditional centralized and\nsynchronous optimization algorithms are unsuitable for SAGIN due to\ninfrastructureless and time-varying environments. This paper aims to develop a\nnovel Asynchronous algorithm a.k.a. Argus for tackling non-convex and\nnon-smooth decentralized federated bilevel learning over SAGIN. The proposed\nalgorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle\nbilevel learning problems in time-varying networks asynchronously, thereby\naverting stragglers from impeding the overall training speed. We provide a\ntheoretical analysis of the iteration complexity, communication complexity, and\ncomputational complexity of Argus. Its effectiveness is further demonstrated\nthrough numerical experiments.",
      "url": "http://arxiv.org/abs/2505.09106v1",
      "published_time_eastern_timestamp": 1747193299.0
    },
    {
      "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
      "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
      "url": "http://arxiv.org/abs/2505.09081v1",
      "published_time_eastern_timestamp": 1747189786.0
    }
  ]
}
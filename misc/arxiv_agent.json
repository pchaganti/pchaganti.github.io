{
  "last_updated": "2025-11-03T10:13:18.651813-05:00",
  "papers": [
    {
      "title": "Social learning moderates the tradeoffs between efficiency, stability,\n  and equity in group foraging",
      "summary": "Social learning shapes collective search by influencing how individuals use\npeer information. Empirical and computational studies show that optimal\ninformation sharing that is neither too localized nor too diffuse, can enhance\nresource detection and coordination. Building on these insights, we develop a\nrandomized search model that integrates social learning with area-restricted\nsearch (ARS) to investigate how communication distance affects collective\nforaging. The model includes three behavioral modes: exploration, exploitation,\nand targeted walk, which are governed by a single parameter, $\\rho$, that\nbalances exploration and exploitation at the group level. We quantify how\n$\\rho$ influences group efficiency ($\\eta$), temporal variability/burstiness\n($B$), and agent variability/equity in resource distribution ($\\sigma$),\nrevealing a clear trade-off among these outcomes. When $\\rho \\to 0$, agents\nexplore independently, maximizing collective exploration. As $\\rho$ increases,\nindividuals preferentially exploit patches discovered by others: $\\eta$ first\nrises and then declines, while $B$ shows the opposite trend. Group efficiency\nis optimized at interior $\\rho$ values that balance exploration and\nexploitation. At the largest $\\rho$, equality among agents is highest, but\nefficiency declines and burstiness is maximized too. Finally, by introducing\nnegative rewards, we examine how social learning mitigates risk.",
      "url": "http://arxiv.org/abs/2510.27683v1",
      "published_time_eastern_timestamp": 1761933180.0
    },
    {
      "title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning\n  in Open Agent Systems",
      "summary": "In the rapidly evolving field of multi-agent reinforcement learning (MARL),\nunderstanding the dynamics of open systems is crucial. Openness in MARL refers\nto the dynam-ic nature of agent populations, tasks, and agent types with-in a\nsystem. Specifically, there are three types of openness as reported in (Eck et\nal. 2023) [2]: agent openness, where agents can enter or leave the system at\nany time; task openness, where new tasks emerge, and existing ones evolve or\ndisappear; and type openness, where the capabil-ities and behaviors of agents\nchange over time. This report provides a conceptual and empirical review,\nfocusing on the interplay between openness and the credit assignment problem\n(CAP). CAP involves determining the contribution of individual agents to the\noverall system performance, a task that becomes increasingly complex in open\nenviron-ments. Traditional credit assignment (CA) methods often assume static\nagent populations, fixed and pre-defined tasks, and stationary types, making\nthem inadequate for open systems. We first conduct a conceptual analysis,\nin-troducing new sub-categories of openness to detail how events like agent\nturnover or task cancellation break the assumptions of environmental\nstationarity and fixed team composition that underpin existing CAP methods. We\nthen present an empirical study using representative temporal and structural\nalgorithms in an open environment. The results demonstrate that openness\ndirectly causes credit misattribution, evidenced by unstable loss functions and\nsignificant performance degradation.",
      "url": "http://arxiv.org/abs/2510.27659v1",
      "published_time_eastern_timestamp": 1761931832.0
    },
    {
      "title": "NegoCollab: A Common Representation Negotiation Approach for\n  Heterogeneous Collaborative Perception",
      "summary": "Collaborative perception improves task performance by expanding the\nperception range through information sharing among agents. . Immutable\nheterogeneity poses a significant challenge in collaborative perception, as\nparticipating agents may employ different and fixed perception models. This\nleads to domain gaps in the intermediate features shared among agents,\nconsequently degrading collaborative performance. Aligning the features of all\nagents to a common representation can eliminate domain gaps with low training\ncost. However, in existing methods, the common representation is designated as\nthe representation of a specific agent, making it difficult for agents with\nsignificant domain discrepancies from this specific agent to achieve proper\nalignment. This paper proposes NegoCollab, a heterogeneous collaboration method\nbased on the negotiated common representation. It introduces a negotiator\nduring training to derive the common representation from the local\nrepresentations of each modality's agent, effectively reducing the inherent\ndomain gap with the various local representations. In NegoCollab, the mutual\ntransformation of features between the local representation space and the\ncommon representation space is achieved by a pair of sender and receiver. To\nbetter align local representations to the common representation containing\nmultimodal information, we introduce structural alignment loss and pragmatic\nalignment loss in addition to the distribution alignment loss to supervise the\ntraining. This enables the knowledge in the common representation to be fully\ndistilled into the sender.",
      "url": "http://arxiv.org/abs/2510.27647v1",
      "published_time_eastern_timestamp": 1761931254.0
    },
    {
      "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training",
      "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.",
      "url": "http://arxiv.org/abs/2510.27630v1",
      "published_time_eastern_timestamp": 1761930022.0
    },
    {
      "title": "Validity Is What You Need",
      "summary": "While AI agents have long been discussed and studied in computer science,\ntoday's Agentic AI systems are something new. We consider other definitions of\nAgentic AI and propose a new realist definition. Agentic AI is a software\ndelivery mechanism, comparable to software as a service (SaaS), which puts an\napplication to work autonomously in a complex enterprise setting. Recent\nadvances in large language models (LLMs) as foundation models have driven\nexcitement in Agentic AI. We note, however, that Agentic AI systems are\nprimarily applications, not foundations, and so their success depends on\nvalidation by end users and principal stakeholders. The tools and techniques\nneeded by the principal users to validate their applications are quite\ndifferent from the tools and techniques used to evaluate foundation models.\nIronically, with good validation measures in place, in many cases the\nfoundation models can be replaced with much simpler, faster, and more\ninterpretable models that handle core logic. When it comes to Agentic AI,\nvalidity is what you need. LLMs are one option that might achieve it.",
      "url": "http://arxiv.org/abs/2510.27628v1",
      "published_time_eastern_timestamp": 1761930004.0
    },
    {
      "title": "Hiring Intrinsically Motivated Agents: A Principal's Dilemma",
      "summary": "Employers are concerned not only with a prospective worker's ability, but\nalso their propensity to avoid shirking. This paper proposes a new experimental\nframework to study how Principals trade-off measures of ability and prosocial\nbehavior when ranking Agents for independent jobs. Subjects participate in a\nsimulated, incentivized job market. In an initial session, subjects are Workers\nand generate a database of signals and job results. Managers in subsequent\nsessions observe the signals of Worker behavior and ability and job details\nbefore a rank-and-value task, ranking and reporting a value for each Worker for\ntwo distinct jobs. Results highlight Managers' preference for ability over\nprosocial behavior on average, especially for Managers in STEM fields. There is\nevidence of homophily: the relative value of prosocial behavior is higher for\nhighly prosocial Managers, compensating for ability or even surpassing it in\nvalue.",
      "url": "http://arxiv.org/abs/2510.27625v1",
      "published_time_eastern_timestamp": 1761929622.0
    },
    {
      "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning",
      "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment.",
      "url": "http://arxiv.org/abs/2510.27623v1",
      "published_time_eastern_timestamp": 1761929449.0
    },
    {
      "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
      "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
      "url": "http://arxiv.org/abs/2510.27617v1",
      "published_time_eastern_timestamp": 1761928858.0
    },
    {
      "title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research",
      "summary": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark.",
      "url": "http://arxiv.org/abs/2510.27598v1",
      "published_time_eastern_timestamp": 1761927743.0
    },
    {
      "title": "Combined fluorescence and photoacoustic imaging of tozuleristide in\n  muscle tissue in vitro -- toward optically-guided solid tumor surgery:\n  feasibility studies",
      "summary": "Near-infrared fluorescence (NIRF) can deliver high-contrast, video-rate,\nnon-contact imaging of tumor-targeted contrast agents with the potential to\nguide surgeries excising solid tumors. However, it has been met with skepticism\nfor wide-margin excision due to sensitivity and resolution limitations at\ndepths larger than ~5 mm in tissue. To address this limitation, fast-sweep\nphotoacoustic-ultrasound (PAUS) imaging is proposed to complement NIRF. In an\nexploratory in vitro feasibility study using dark-red bovine muscle tissue, we\nobserved that PAUS scanning can identify tozuleristide, a clinical stage\ninvestigational imaging agent, at a concentration of 20 uM from the background\nat depths of up to ~34 mm, highly extending the capabilities of NIRF alone. The\ncapability of spectroscopic PAUS imaging was tested by direct injection of 20\nuM tozuleristide into bovine muscle tissue at a depth of ~ 8 mm. It is shown\nthat laser-fluence compensation and strong clutter suppression enabled by the\nunique capabilities of the fast-sweep approach greatly improve spectroscopic\naccuracy and the PA detection limit, and strongly reduce image artifacts. Thus,\nthe combined NIRF-PAUS approach can be promising for comprehensive pre- (with\nPA) and intra- (with NIRF) operative solid tumor detection and wide-margin\nexcision in optically guided solid tumor surgery.",
      "url": "http://arxiv.org/abs/2510.27595v1",
      "published_time_eastern_timestamp": 1761927674.0
    },
    {
      "title": "MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool\n  Agentic Retrieval",
      "summary": "Large Language Models (LLMs) excel at reasoning and generation but are\ninherently limited by static pretraining data, resulting in factual\ninaccuracies and weak adaptability to new information. Retrieval-Augmented\nGeneration (RAG) addresses this issue by grounding LLMs in external knowledge;\nHowever, the effectiveness of RAG critically depends on whether the model can\nadequately access relevant information. Existing RAG systems rely on a single\nretriever with fixed top-k selection, restricting access to a narrow and static\nsubset of the corpus. As a result, this single-retriever paradigm has become\nthe primary bottleneck for comprehensive external information acquisition,\nespecially in tasks requiring corpus-level reasoning. To overcome this\nlimitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG\nframework that enables LLMs to dynamically coordinate multiple retrieval\nmechanisms for broader and more precise information access. MARAG-R1 equips the\nmodel with four retrieval tools -- semantic search, keyword search, filtering,\nand aggregation -- and learns both how and when to use them through a two-stage\ntraining process: supervised fine-tuning followed by reinforcement learning.\nThis design allows the model to interleave reasoning and retrieval,\nprogressively gathering sufficient evidence for corpus-level synthesis.\nExperiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that\nMARAG-R1 substantially outperforms strong baselines and achieves new\nstate-of-the-art results in corpus-level reasoning tasks.",
      "url": "http://arxiv.org/abs/2510.27569v1",
      "published_time_eastern_timestamp": 1761925899.0
    },
    {
      "title": "SIGMA: Search-Augmented On-Demand Knowledge Integration for Agentic\n  Mathematical Reasoning",
      "summary": "Solving mathematical reasoning problems requires not only accurate access to\nrelevant knowledge but also careful, multi-step thinking. However, current\nretrieval-augmented models often rely on a single perspective, follow\ninflexible search strategies, and struggle to effectively combine information\nfrom multiple sources. We introduce SIGMA (Search-Augmented On-Demand Knowledge\nIntegration for AGentic Mathematical reAsoning), a unified framework that\norchestrates specialized agents to independently reason, perform targeted\nsearches, and synthesize findings through a moderator mechanism. Each agent\ngenerates hypothetical passages to optimize retrieval for its analytic\nperspective, ensuring knowledge integration is both context-sensitive and\ncomputation-efficient. When evaluated on challenging benchmarks such as\nMATH500, AIME, and PhD-level science QA GPQA, SIGMA consistently outperforms\nboth open- and closed-source systems, achieving an absolute performance\nimprovement of 7.4%. Our results demonstrate that multi-agent, on-demand\nknowledge integration significantly enhances both reasoning accuracy and\nefficiency, offering a scalable approach for complex, knowledge-intensive\nproblem-solving. We will release the code upon publication.",
      "url": "http://arxiv.org/abs/2510.27568v1",
      "published_time_eastern_timestamp": 1761925860.0
    },
    {
      "title": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box\n  Retrieval",
      "summary": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by\nincorporating external information. However, prevailing agentic RAG approaches\nare constrained by a critical limitation: they treat the retrieval process as a\nblack-box querying operation. This confines agents' actions to query issuing,\nhindering its ability to tackle complex information-seeking tasks. To address\nthis, we introduce Interact-RAG, a new paradigm that elevates the LLM agent\nfrom a passive query issuer into an active manipulator of the retrieval\nprocess. We dismantle the black-box with a Corpus Interaction Engine, equipping\nthe agent with a set of action primitives for fine-grained control over\ninformation retrieval. To further empower the agent on the entire RAG pipeline,\nwe first develop a reasoning-enhanced workflow, which enables both zero-shot\nexecution and the synthesis of interaction trajectories. We then leverage this\nsynthetic data to train a fully autonomous end-to-end agent via Supervised\nFine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL).\nExtensive experiments across six benchmarks demonstrate that Interact-RAG\nsignificantly outperforms other advanced methods, validating the efficacy of\nour reasoning-interaction strategy.",
      "url": "http://arxiv.org/abs/2510.27566v1",
      "published_time_eastern_timestamp": 1761925723.0
    },
    {
      "title": "Sybil-Resistant Service Discovery for Agent Economies",
      "summary": "x402 enables Hypertext Transfer Protocol (HTTP) services like application\nprogramming interfaces (APIs), data feeds, and inference providers to accept\ncryptocurrency payments for access. As agents increasingly consume these\nservices, discovery becomes critical: which swap interface should an agent\ntrust? Which data provider is the most reliable? We introduce TraceRank, a\nreputation-weighted ranking algorithm where payment transactions serve as\nendorsements. TraceRank seeds addresses with precomputed reputation metrics and\npropagates reputation through payment flows weighted by transaction value and\ntemporal recency. Applied to x402's payment graph, this surfaces services\npreferred by high-reputation users rather than those with high transaction\nvolume. Our system combines TraceRank with semantic search to respond to\nnatural language queries with high quality results. We argue that reputation\npropagation resists Sybil attacks by making spam services with many\nlow-reputation payers rank below legitimate services with few high-reputation\npayers. Ultimately, we aim to construct a search method for x402 enabled\nservices that avoids infrastructure bias and has better performance than purely\nvolume based or semantic methods.",
      "url": "http://arxiv.org/abs/2510.27554v1",
      "published_time_eastern_timestamp": 1761924571.0
    },
    {
      "title": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for\n  Interpretable Deconstruction of Reasoning System Performance",
      "summary": "Large Language Models (LLMs) are increasingly excelling and outpacing human\nperformance on many tasks. However, to improve LLM reasoning, researchers\neither rely on ad-hoc generated datasets or formal mathematical proof systems\nsuch as the Lean proof assistant. Whilst ad-hoc generated methods can capture\nthe decision chains of real-world reasoning processes, they may encode some\ninadvertent bias in the space of reasoning they cover; they also cannot be\nformally verified. On the other hand, systems like Lean can guarantee\nverifiability, but are not well-suited to capture the nature of agentic\ndecision chain-based tasks. This creates a gap both in performance for\nfunctions such as business agents or code assistants, and in the usefulness of\nLLM reasoning benchmarks, whereby these fall short in reasoning structure or\nreal-world alignment. We introduce TempoBench, the first formally grounded and\nverifiable diagnostic benchmark that parametrizes difficulty to systematically\nanalyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks\nto break down reasoning ability. First, temporal trace evaluation (TTE) tests\nthe ability of an LLM to understand and simulate the execution of a given\nmulti-step reasoning system. Subsequently, temporal causal evaluation (TCE)\ntests an LLM's ability to perform multi-step causal reasoning and to distill\ncause-and-effect relations from complex systems. We find that models score\n65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art\nLLMs clearly understand the TCE task but perform poorly as system complexity\nincreases. Our code is available at our\n\\href{https://github.com/nik-hz/tempobench}{GitHub repository}.",
      "url": "http://arxiv.org/abs/2510.27544v1",
      "published_time_eastern_timestamp": 1761923875.0
    },
    {
      "title": "Asynchronous Risk-Aware Multi-Agent Packet Routing for Ultra-Dense LEO\n  Satellite Networks",
      "summary": "The rise of ultra-dense LEO constellations creates a complex and asynchronous\nnetwork environment, driven by their massive scale, dynamic topologies, and\nsignificant delays. This unique complexity demands an adaptive packet routing\nalgorithm that is asynchronous, risk-aware, and capable of balancing diverse\nand often conflicting QoS objectives in a decentralized manner. However,\nexisting methods fail to address this need, as they typically rely on\nimpractical synchronous decision-making and/or risk-oblivious approaches. To\ntackle this gap, we introduce PRIMAL, an event-driven multi-agent routing\nframework designed specifically to allow each satellite to act independently on\nits own event-driven timeline, while managing the risk of worst-case\nperformance degradation via a principled primal-dual approach. This is achieved\nby enabling agents to learn the full cost distribution of the targeted QoS\nobjectives and constrain tail-end risks. Extensive simulations on a LEO\nconstellation with 1584 satellites validate its superiority in effectively\noptimizing latency and balancing load. Compared to a recent risk-oblivious\nbaseline, it reduces queuing delay by over 70%, and achieves a nearly 12 ms\nend-to-end delay reduction in loaded scenarios. This is accomplished by\nresolving the core conflict between naive shortest-path finding and congestion\navoidance, highlighting such autonomous risk-awareness as a key to robust\nrouting.",
      "url": "http://arxiv.org/abs/2510.27506v1",
      "published_time_eastern_timestamp": 1761920948.0
    },
    {
      "title": "Auditing LLM Editorial Bias in News Media Exposure",
      "summary": "Large Language Models (LLMs) increasingly act as gateways to web content,\nshaping how millions of users encounter online information. Unlike traditional\nsearch engines, whose retrieval and ranking mechanisms are well studied, the\nselection processes of web-connected LLMs add layers of opacity to how answers\nare generated. By determining which news outlets users see, these systems can\ninfluence public opinion, reinforce echo chambers, and pose risks to civic\ndiscourse and public trust.\n  This work extends two decades of research in algorithmic auditing to examine\nhow LLMs function as news engines. We present the first audit comparing three\nleading agents, GPT-4o-Mini, Claude-3.7-Sonnet, and Gemini-2.0-Flash, against\nGoogle News, asking: \\textit{How do LLMs differ from traditional aggregators in\nthe diversity, ideology, and reliability of the media they expose to users?}\n  Across 24 global topics, we find that, compared to Google News, LLMs surface\nsignificantly fewer unique outlets and allocate attention more unevenly. In the\nsame way, GPT-4o-Mini emphasizes more factual and right-leaning sources;\nClaude-3.7-Sonnet favors institutional and civil-society domains and slightly\namplifies right-leaning exposure; and Gemini-2.0-Flash exhibits a modest\nleft-leaning tilt without significant changes in factuality. These patterns\nremain robust under prompt variations and alternative reliability benchmarks.\nTogether, our findings show that LLMs already enact \\textit{agentic editorial\npolicies}, curating information in ways that diverge from conventional\naggregators. Understanding and governing their emerging editorial power will be\ncritical for ensuring transparency, pluralism, and trust in digital information\necosystems.",
      "url": "http://arxiv.org/abs/2510.27489v1",
      "published_time_eastern_timestamp": 1761919662.0
    },
    {
      "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling",
      "summary": "Most work interpreting reasoning models studies only a single\nchain-of-thought (CoT), yet these models define distributions over many\npossible CoTs. We argue that studying a single sample is inadequate for\nunderstanding causal influence and the underlying computation. Though fully\nspecifying this distribution is intractable, it can be understood by sampling.\nWe present case studies using resampling to investigate model decisions. First,\nwhen a model states a reason for its action, does that reason actually cause\nthe action? In \"agentic misalignment\" scenarios, we resample specific sentences\nto measure their downstream effects. Self-preservation sentences have small\ncausal impact, suggesting they do not meaningfully drive blackmail. Second, are\nartificial edits to CoT sufficient for steering reasoning? These are common in\nliterature, yet take the model off-policy. Resampling and selecting a\ncompletion with the desired property is a principled on-policy alternative. We\nfind off-policy interventions yield small and unstable effects compared to\nresampling in decision-making tasks. Third, how do we understand the effect of\nremoving a reasoning step when the model may repeat it post-edit? We introduce\na resilience metric that repeatedly resamples to prevent similar content from\nreappearing downstream. Critical planning statements resist removal but have\nlarge effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can\nour methods teach us anything in these settings? Adapting causal mediation\nanalysis, we find that hints that have a causal effect on the output without\nbeing explicitly mentioned exert a subtle and cumulative influence on the CoT\nthat persists even if the hint is removed. Overall, studying distributions via\nresampling enables reliable causal analysis, clearer narratives of model\nreasoning, and principled CoT interventions.",
      "url": "http://arxiv.org/abs/2510.27484v1",
      "published_time_eastern_timestamp": 1761919357.0
    },
    {
      "title": "From Pixels to Paths: A Multi-Agent Framework for Editable Scientific\n  Illustration",
      "summary": "Scientific illustrations demand both high information density and\npost-editability. However, current generative models have two major\nlimitations: Frist, image generation models output rasterized images lacking\nsemantic structure, making it impossible to access, edit, or rearrange\nindependent visual components in the images. Second, code-based generation\nmethods (TikZ or SVG), although providing element-level control, force users\ninto the cumbersome cycle of \"writing-compiling-reviewing\" and lack the\nintuitiveness of manipulation. Neither of these two approaches can well meet\nthe needs for efficiency, intuitiveness, and iterative modification in\nscientific creation. To bridge this gap, we introduce VisPainter, a multi-agent\nframework for scientific illustration built upon the model context protocol.\nVisPainter orchestrates three specialized modules-a Manager, a Designer, and a\nToolbox-to collaboratively produce diagrams compatible with standard vector\ngraphics software. This modular, role-based design allows each element to be\nexplicitly represented and manipulated, enabling true element-level control and\nany element can be added and modified later. To systematically evaluate the\nquality of scientific illustrations, we introduce VisBench, a benchmark with\nseven-dimensional evaluation metrics. It assesses high-information-density\nscientific illustrations from four aspects: content, layout, visual perception,\nand interaction cost. To this end, we conducted extensive ablation experiments\nto verify the rationality of our architecture and the reliability of our\nevaluation methods. Finally, we evaluated various vision-language models,\npresenting fair and credible model rankings along with detailed comparisons of\ntheir respective capabilities. Additionally, we isolated and quantified the\nimpacts of role division, step control,and description on the quality of\nillustrations.",
      "url": "http://arxiv.org/abs/2510.27452v1",
      "published_time_eastern_timestamp": 1761915649.0
    },
    {
      "title": "Towards a Multi-Embodied Grasping Agent",
      "summary": "Multi-embodiment grasping focuses on developing approaches that exhibit\ngeneralist behavior across diverse gripper designs. Existing methods often\nlearn the kinematic structure of the robot implicitly and face challenges due\nto the difficulty of sourcing the required large-scale data. In this work, we\npresent a data-efficient, flow-based, equivariant grasp synthesis architecture\nthat can handle different gripper types with variable degrees of freedom and\nsuccessfully exploit the underlying kinematic model, deducing all necessary\ninformation solely from the gripper and scene geometry. Unlike previous\nequivariant grasping methods, we translated all modules from the ground up to\nJAX and provide a model with batching capabilities over scenes, grippers, and\ngrasps, resulting in smoother learning, improved performance and faster\ninference time. Our dataset encompasses grippers ranging from humanoid hands to\nparallel yaw grippers and includes 25,000 scenes and 20 million grasps.",
      "url": "http://arxiv.org/abs/2510.27420v1",
      "published_time_eastern_timestamp": 1761913056.0
    }
  ]
}
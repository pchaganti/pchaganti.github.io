{
  "last_updated": "2025-06-17T08:24:30.540873-04:00",
  "papers": [
    {
      "title": "MARCO: Hardware-Aware Neural Architecture Search for Edge Devices with\n  Multi-Agent Reinforcement Learning and Conformal Prediction Filtering",
      "summary": "This paper introduces MARCO (Multi-Agent Reinforcement learning with\nConformal Optimization), a novel hardware-aware framework for efficient neural\narchitecture search (NAS) targeting resource-constrained edge devices. By\nsignificantly reducing search time and maintaining accuracy under strict\nhardware constraints, MARCO bridges the gap between automated DNN design and\nCAD for edge AI deployment. MARCO's core technical contribution lies in its\nunique combination of multi-agent reinforcement learning (MARL) with Conformal\nPrediction (CP) to accelerate the hardware/software co-design process for\ndeploying deep neural networks. Unlike conventional once-for-all (OFA) supernet\napproaches that require extensive pretraining, MARCO decomposes the NAS task\ninto a hardware configuration agent (HCA) and a Quantization Agent (QA). The\nHCA optimizes high-level design parameters, while the QA determines per-layer\nbit-widths under strict memory and latency budgets using a shared reward signal\nwithin a centralized-critic, decentralized-execution (CTDE) paradigm. A key\ninnovation is the integration of a calibrated CP surrogate model that provides\nstatistical guarantees (with a user-defined miscoverage rate) to prune\nunpromising candidate architectures before incurring the high costs of partial\ntraining or hardware simulation. This early filtering drastically reduces the\nsearch space while ensuring that high-quality designs are retained with a high\nprobability. Extensive experiments on MNIST, CIFAR-10, and CIFAR-100\ndemonstrate that MARCO achieves a 3-4x reduction in total search time compared\nto an OFA baseline while maintaining near-baseline accuracy (within 0.3%).\nFurthermore, MARCO also reduces inference latency. Validation on a MAX78000\nevaluation board confirms that simulator trends hold in practice, with\nsimulator estimates deviating from measured values by less than 5%.",
      "url": "http://arxiv.org/abs/2506.13755v1",
      "published_time_eastern_timestamp": 1750096689.0
    },
    {
      "title": "PB$^2$: Preference Space Exploration via Population-Based Methods in\n  Preference-Based Reinforcement Learning",
      "summary": "Preference-based reinforcement learning (PbRL) has emerged as a promising\napproach for learning behaviors from human feedback without predefined reward\nfunctions. However, current PbRL methods face a critical challenge in\neffectively exploring the preference space, often converging prematurely to\nsuboptimal policies that satisfy only a narrow subset of human preferences. In\nthis work, we identify and address this preference exploration problem through\npopulation-based methods. We demonstrate that maintaining a diverse population\nof agents enables more comprehensive exploration of the preference landscape\ncompared to single-agent approaches. Crucially, this diversity improves reward\nmodel learning by generating preference queries with clearly distinguishable\nbehaviors, a key factor in real-world scenarios where humans must easily\ndifferentiate between options to provide meaningful feedback. Our experiments\nreveal that current methods may fail by getting stuck in local optima,\nrequiring excessive feedback, or degrading significantly when human evaluators\nmake errors on similar trajectories, a realistic scenario often overlooked by\nmethods relying on perfect oracle teachers. Our population-based approach\ndemonstrates robust performance when teachers mislabel similar trajectory\nsegments and shows significantly enhanced preference exploration\ncapabilities,particularly in environments with complex reward landscapes.",
      "url": "http://arxiv.org/abs/2506.13741v1",
      "published_time_eastern_timestamp": 1750096293.0
    },
    {
      "title": "The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement\n  Learning",
      "summary": "Off-policy deep reinforcement learning (RL) typically leverages replay\nbuffers for reusing past experiences during learning. This can help improve\nsample efficiency when the collected data is informative and aligned with the\nlearning objectives; when that is not the case, it can have the effect of\n\"polluting\" the replay buffer with data which can exacerbate optimization\nchallenges in addition to wasting environment interactions due to wasteful\nsampling. We argue that sampling these uninformative and wasteful transitions\ncan be avoided by addressing the sunk cost fallacy, which, in the context of\ndeep RL, is the tendency towards continuing an episode until termination. To\naddress this, we propose learn to stop (LEAST), a lightweight mechanism that\nenables strategic early episode termination based on Q-value and gradient\nstatistics, which helps agents recognize when to terminate unproductive\nepisodes early. We demonstrate that our method improves learning efficiency on\na variety of RL algorithms, evaluated on both the MuJoCo and DeepMind Control\nSuite benchmarks.",
      "url": "http://arxiv.org/abs/2506.13672v1",
      "published_time_eastern_timestamp": 1750091400.0
    },
    {
      "title": "We Should Identify and Mitigate Third-Party Safety Risks in MCP-Powered\n  Agent Systems",
      "summary": "The development of large language models (LLMs) has entered in a\nexperience-driven era, flagged by the emergence of environment feedback-driven\nlearning via reinforcement learning and tool-using agents. This encourages the\nemergenece of model context protocol (MCP), which defines the standard on how\nshould a LLM interact with external services, such as \\api and data. However,\nas MCP becomes the de facto standard for LLM agent systems, it also introduces\nnew safety risks. In particular, MCP introduces third-party services, which are\nnot controlled by the LLM developers, into the agent systems. These third-party\nMCP services provider are potentially malicious and have the economic\nincentives to exploit vulnerabilities and sabotage user-agent interactions. In\nthis position paper, we advocate the research community in LLM safety to pay\nclose attention to the new safety risks issues introduced by MCP, and develop\nnew techniques to build safe MCP-powered agent systems. To establish our\nposition, we argue with three key parts. (1) We first construct \\framework, a\ncontrolled framework to examine safety issues in MCP-powered agent systems. (2)\nWe then conduct a series of pilot experiments to demonstrate the safety risks\nin MCP-powered agent systems is a real threat and its defense is not trivial.\n(3) Finally, we give our outlook by showing a roadmap to build safe MCP-powered\nagent systems. In particular, we would call for researchers to persue the\nfollowing research directions: red teaming, MCP safe LLM development, MCP\nsafety evaluation, MCP safety data accumulation, MCP service safeguard, and MCP\nsafe ecosystem construction. We hope this position paper can raise the\nawareness of the research community in MCP safety and encourage more\nresearchers to join this important research direction. Our code is available at\nhttps://github.com/littlelittlenine/SafeMCP.git.",
      "url": "http://arxiv.org/abs/2506.13666v1",
      "published_time_eastern_timestamp": 1750091071.0
    },
    {
      "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
      "summary": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal retrieval and multi-modal\nunderstanding. We design a two-stage training paradigm involving supervised\nfinetuning (SFT) of a pretrained language model using CoTT data and RL to\nenable our agent to dynamically propose step-by-step tools for long-range\nreasoning. To facilitate training, we construct a dataset called Ego-R1 Data,\nwhich consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our\nEgo-R1 agent is evaluated on a newly curated week-long video QA benchmark,\nEgo-R1 Bench, which contains human-verified QA pairs from hybrid sources.\nExtensive results demonstrate that the dynamic, tool-augmented chain-of-thought\nreasoning by our Ego-R1 Agent can effectively tackle the unique challenges of\nunderstanding ultra-long egocentric videos, significantly extending the time\ncoverage from few hours to a week.",
      "url": "http://arxiv.org/abs/2506.13654v1",
      "published_time_eastern_timestamp": 1750090628.0
    },
    {
      "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations",
      "summary": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed\nto bridge the gap between AI agent capabilities and real-world productivity.\nWhile existing benchmarks often focus on isolated technical skills, they may\nnot accurately reflect the economic value agents deliver in professional\nsettings. To address this, xbench targets commercially significant domains with\nevaluation tasks defined by industry professionals. Our framework creates\nmetrics that strongly correlate with productivity value, enables prediction of\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\nover time. As our initial implementations, we present two benchmarks:\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\nheadhunting business scenarios to evaluate agents' abilities in company\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\nagents' ability to match influencers with advertiser needs, evaluating their\nperformance across 50 advertiser requirements using a curated pool of 836\ncandidate influencers. We present initial evaluation results for leading\ncontemporary agents, establishing a baseline for these professional domains.\nOur continuously updated evalsets and evaluations are available at\nhttps://xbench.org.",
      "url": "http://arxiv.org/abs/2506.13651v1",
      "published_time_eastern_timestamp": 1750090574.0
    },
    {
      "title": "Deceptive Path Planning: A Bayesian Game Approach",
      "summary": "This paper investigates how an autonomous agent can transmit information\nthrough its motion in an adversarial setting. We consider scenarios where an\nagent must reach its goal while deceiving an intelligent observer about its\ndestination. We model this interaction as a dynamic Bayesian game between a\nmobile Attacker with a privately known goal and a Defender who infers the\nAttacker's intent to allocate defensive resources effectively. We use Perfect\nBayesian Nash Equilibrium (PBNE) as our solution concept and propose a\ncomputationally efficient approach to find it. In the resulting equilibrium,\nthe Defender employs a simple Markovian strategy, while the Attacker\nstrategically balances deception and goal efficiency by stochastically mixing\nshortest and non-shortest paths to manipulate the Defender's beliefs. Numerical\nexperiments demonstrate the advantages of our PBNE-based strategies over\nexisting methods based on one-sided optimization.",
      "url": "http://arxiv.org/abs/2506.13650v1",
      "published_time_eastern_timestamp": 1750090525.0
    },
    {
      "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
      "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
      "url": "http://arxiv.org/abs/2506.13599v1",
      "published_time_eastern_timestamp": 1750087447.0
    },
    {
      "title": "Agent Capability Negotiation and Binding Protocol (ACNBP)",
      "summary": "As multi-agent systems evolve to encompass increasingly diverse and\nspecialized agents, the challenge of enabling effective collaboration between\nheterogeneous agents has become paramount, with traditional agent communication\nprotocols often assuming homogeneous environments or predefined interaction\npatterns that limit their applicability in dynamic, open-world scenarios. This\npaper presents the Agent Capability Negotiation and Binding Protocol (ACNBP), a\nnovel framework designed to facilitate secure, efficient, and verifiable\ninteractions between agents in heterogeneous multi-agent systems through\nintegration with an Agent Name Service (ANS) infrastructure that provides\ncomprehensive discovery, negotiation, and binding mechanisms. The protocol\nintroduces a structured 10-step process encompassing capability discovery,\ncandidate pre-screening and selection, secure negotiation phases, and binding\ncommitment with built-in security measures including digital signatures,\ncapability attestation, and comprehensive threat mitigation strategies, while a\nkey innovation of ACNBP is its protocolExtension mechanism that enables\nbackward-compatible protocol evolution and supports diverse agent architectures\nwhile maintaining security and interoperability. We demonstrate ACNBP's\neffectiveness through a comprehensive security analysis using the MAESTRO\nthreat modeling framework, practical implementation considerations, and a\ndetailed example showcasing the protocol's application in a document\ntranslation scenario, with the protocol addressing critical challenges in agent\nautonomy, capability verification, secure communication, and scalable agent\necosystem management.",
      "url": "http://arxiv.org/abs/2506.13590v1",
      "published_time_eastern_timestamp": 1750087104.0
    },
    {
      "title": "Non-exchangeable mean-field theory for adaptive weights: propagation of\n  chaos and graphon sampling lemma",
      "summary": "We develop a mean-field theory for large, non-exchangeable particle (agent)\nsystems where the states and interaction weights co-evolve in a coupled system\nof SDEs. A first main result is a generalization of the propagation of chaos.\nThe weight adaptation in the SDEs makes the classical approach of using a\nstatic probability space as the continuum limit for the agent labels\ninadequate. We address this by introducing a label space endowed with a\nfiltration that captures the stochasticity. While this yields a well-posed\nMcKean-Vlasov SDE, its limit is not easily described by a Vlasov-type PDE. This\ndifficulty, in turn, motivates the introduction of a unified metric that\nnaturally combines the Wasserstein distance for states and the cut norm for\nweights. In this metric space, we establish the convergence of finite systems\nto their continuum limit. This result, analogous to the classical convergence\nof empirical measures, is a subtle consequence of a deep result from dense\ngraph theory, namely the Sampling Lemma.",
      "url": "http://arxiv.org/abs/2506.13587v1",
      "published_time_eastern_timestamp": 1750086828.0
    },
    {
      "title": "Can you see how I learn? Human observers' inferences about Reinforcement\n  Learning agents' learning processes",
      "summary": "Reinforcement Learning (RL) agents often exhibit learning behaviors that are\nnot intuitively interpretable by human observers, which can result in\nsuboptimal feedback in collaborative teaching settings. Yet, how humans\nperceive and interpret RL agent's learning behavior is largely unknown. In a\nbottom-up approach with two experiments, this work provides a data-driven\nunderstanding of the factors of human observers' understanding of the agent's\nlearning process. A novel, observation-based paradigm to directly assess human\ninferences about agent learning was developed. In an exploratory interview\nstudy (\\textit{N}=9), we identify four core themes in human interpretations:\nAgent Goals, Knowledge, Decision Making, and Learning Mechanisms. A second\nconfirmatory study (\\textit{N}=34) applied an expanded version of the paradigm\nacross two tasks (navigation/manipulation) and two RL algorithms\n(tabular/function approximation). Analyses of 816 responses confirmed the\nreliability of the paradigm and refined the thematic framework, revealing how\nthese themes evolve over time and interrelate. Our findings provide a\nhuman-centered understanding of how people make sense of agent learning,\noffering actionable insights for designing interpretable RL systems and\nimproving transparency in Human-Robot Interaction.",
      "url": "http://arxiv.org/abs/2506.13583v1",
      "published_time_eastern_timestamp": 1750086267.0
    },
    {
      "title": "A Production Scheduling Framework for Reinforcement Learning Under\n  Real-World Constraints",
      "summary": "The classical Job Shop Scheduling Problem (JSSP) focuses on optimizing\nmakespan under deterministic constraints. Real-world production environments\nintroduce additional complexities that cause traditional scheduling approaches\nto be less effective. Reinforcement learning (RL) holds potential in addressing\nthese challenges, as it allows agents to learn adaptive scheduling strategies.\nHowever, there is a lack of a comprehensive, general-purpose frameworks for\neffectively training and evaluating RL agents under real-world constraints. To\naddress this gap, we propose a modular framework that extends classical JSSP\nformulations by incorporating key \\mbox{real-world} constraints inherent to the\nshopfloor, including transport logistics, buffer management, machine\nbreakdowns, setup times, and stochastic processing conditions, while also\nsupporting multi-objective optimization. The framework is a customizable\nsolution that offers flexibility in defining problem instances and configuring\nsimulation parameters, enabling adaptation to diverse production scenarios. A\nstandardized interface ensures compatibility with various RL approaches,\nproviding a robust environment for training RL agents and facilitating the\nstandardized comparison of different scheduling methods under dynamic and\nuncertain conditions. We release JobShopLab as an open-source tool for both\nresearch and industrial applications, accessible at:\nhttps://github.com/proto-lab-ro/jobshoplab",
      "url": "http://arxiv.org/abs/2506.13566v1",
      "published_time_eastern_timestamp": 1750085426.0
    },
    {
      "title": "Learning Swing-up Maneuvers for a Suspended Aerial Manipulation Platform\n  in a Hierarchical Control Framework",
      "summary": "In this work, we present a novel approach to augment a model-based control\nmethod with a reinforcement learning (RL) agent and demonstrate a swing-up\nmaneuver with a suspended aerial manipulation platform. These platforms are\ntargeted towards a wide range of applications on construction sites involving\ncranes, with swing-up maneuvers allowing it to perch at a given location,\ninaccessible with purely the thrust force of the platform. Our proposed\napproach is based on a hierarchical control framework, which allows different\ntasks to be executed according to their assigned priorities. An RL agent is\nthen subsequently utilized to adjust the reference set-point of the\nlower-priority tasks to perform the swing-up maneuver, which is confined in the\nnullspace of the higher-priority tasks, such as maintaining a specific\norientation and position of the end-effector. Our approach is validated using\nextensive numerical simulation studies.",
      "url": "http://arxiv.org/abs/2506.13478v1",
      "published_time_eastern_timestamp": 1750080883.0
    },
    {
      "title": "Language Agents for Hypothesis-driven Clinical Decision Making with\n  Reinforcement Learning",
      "summary": "Clinical decision-making is a dynamic, interactive, and cyclic process where\ndoctors have to repeatedly decide on which clinical action to perform and\nconsider newly uncovered information for diagnosis and treatment. Large\nLanguage Models (LLMs) have the potential to support clinicians in this\nprocess, however, most applications of LLMs in clinical decision support suffer\nfrom one of two limitations: Either they assume the unrealistic scenario of\nimmediate availability of all patient information and do not model the\ninteractive and iterative investigation process, or they restrict themselves to\nthe limited \"out-of-the-box\" capabilities of large pre-trained models without\nperforming task-specific training. In contrast to this, we propose to model\nclinical decision-making for diagnosis with a hypothesis-driven\nuncertainty-aware language agent, LA-CDM, that converges towards a diagnosis\nvia repeatedly requesting and interpreting relevant tests. Using a hybrid\ntraining paradigm combining supervised and reinforcement learning, we train\nLA-CDM with three objectives targeting critical aspects of clinical\ndecision-making: accurate hypothesis generation, hypothesis uncertainty\nestimation, and efficient decision-making. We evaluate our methodology on\nMIMIC-CDM, a real-world dataset covering four abdominal diseases containing\nvarious clinical tests and show the benefit of explicitly training clinical\ndecision-making for increasing diagnostic performance and efficiency.",
      "url": "http://arxiv.org/abs/2506.13474v1",
      "published_time_eastern_timestamp": 1750080721.0
    },
    {
      "title": "A Two-stage Optimization Method for Wide-range Single-electron Quantum\n  Magnetic Sensing",
      "summary": "Quantum magnetic sensing based on spin systems has emerged as a new paradigm\nfor detecting ultra-weak magnetic fields with unprecedented sensitivity,\nrevitalizing applications in navigation, geo-localization, biology, and beyond.\nAt the heart of quantum magnetic sensing, from the protocol perspective, lies\nthe design of optimal sensing parameters to manifest and then estimate the\nunderlying signals of interest (SoI). Existing studies on this front mainly\nrely on adaptive algorithms based on black-box AI models or formula-driven\nprincipled searches. However, when the SoI spans a wide range and the quantum\nsensor has physical constraints, these methods may fail to converge efficiently\nor optimally, resulting in prolonged interrogation times and reduced sensing\naccuracy. In this work, we report the design of a new protocol using a\ntwo-stage optimization method. In the 1st Stage, a Bayesian neural network with\na fixed set of sensing parameters is used to narrow the range of SoI. In the\n2nd Stage, a federated reinforcement learning agent is designed to fine-tune\nthe sensing parameters within a reduced search space. The proposed protocol is\ndeveloped and evaluated in a challenging context of single-shot readout of an\nNV-center electron spin under a constrained total sensing time budget; and yet\nit achieves significant improvements in both accuracy and resource efficiency\nfor wide-range D.C. magnetic field estimation compared to the state of the art.",
      "url": "http://arxiv.org/abs/2506.13469v1",
      "published_time_eastern_timestamp": 1750080512.0
    },
    {
      "title": "Towards a Formal Specification for Self-organized Shape Formation in\n  Swarm Robotics",
      "summary": "The self-organization of robots for the formation of structures and shapes is\na stimulating application of the swarm robotic system. It involves a large\nnumber of autonomous robots of heterogeneous behavior, coordination among them,\nand their interaction with the dynamic environment. This process of complex\nstructure formation is considered a complex system, which needs to be modeled\nby using any modeling approach. Although the formal specification approach\nalong with other formal methods has been used to model the behavior of robots\nin a swarm. However, to the best of our knowledge, the formal specification\napproach has not been used to model the self-organization process in swarm\nrobotic systems for shape formation. In this paper, we use a formal\nspecification approach to model the shape formation task of swarm robots. We\nuse Z (Zed) language of formal specification, which is a state-based language,\nto model the states of the entities of the systems. We demonstrate the\neffectiveness of Z for the self-organized shape formation. The presented formal\nspecification model gives the outlines for designing and implementing the swarm\nrobotic system for the formation of complex shapes and structures. It also\nprovides the foundation for modeling the complex shape formation process for\nswarm robotics using a multi-agent system in a simulation-based environment.\nKeywords: Swarm robotics, Self-organization, Formal specification, Complex\nsystems",
      "url": "http://arxiv.org/abs/2506.13453v1",
      "published_time_eastern_timestamp": 1750079600.0
    },
    {
      "title": "Learning to Explore in Diverse Reward Settings via\n  Temporal-Difference-Error Maximization",
      "summary": "Numerous heuristics and advanced approaches have been proposed for\nexploration in different settings for deep reinforcement learning. Noise-based\nexploration generally fares well with dense-shaped rewards and bonus-based\nexploration with sparse rewards. However, these methods usually require\nadditional tuning to deal with undesirable reward settings by adjusting\nhyperparameters and noise distributions. Rewards that actively discourage\nexploration, i.e., with an action cost and no other dense signal to follow, can\npose a major challenge. We propose a novel exploration method, Stable\nError-seeking Exploration (SEE), that is robust across dense, sparse, and\nexploration-adverse reward settings. To this endeavor, we revisit the idea of\nmaximizing the TD-error as a separate objective. Our method introduces three\ndesign choices to mitigate instability caused by far-off-policy learning, the\nconflict of interest of maximizing the cumulative TD-error in an episodic\nsetting, and the non-stationary nature of TD-errors. SEE can be combined with\noff-policy algorithms without modifying the optimization pipeline of the\noriginal objective. In our experimental analysis, we show that a Soft-Actor\nCritic agent with the addition of SEE performs robustly across three diverse\nreward settings in a variety of tasks without hyperparameter adjustments.",
      "url": "http://arxiv.org/abs/2506.13345v1",
      "published_time_eastern_timestamp": 1750070184.0
    },
    {
      "title": "Towards Pervasive Distributed Agentic Generative AI -- A State of The\n  Art",
      "summary": "The rapid advancement of intelligent agents and Large Language Models (LLMs)\nis reshaping the pervasive computing field. Their ability to perceive, reason,\nand act through natural language understanding enables autonomous\nproblem-solving in complex pervasive environments, including the management of\nheterogeneous sensors, devices, and data. This survey outlines the\narchitectural components of LLM agents (profiling, memory, planning, and\naction) and examines their deployment and evaluation across various scenarios.\nThan it reviews computational and infrastructural advancements (cloud to edge)\nin pervasive computing and how AI is moving in this field. It highlights\nstate-of-the-art agent deployment strategies and applications, including local\nand distributed execution on resource-constrained devices. This survey\nidentifies key challenges of these agents in pervasive computing such as\narchitectural, energetic and privacy limitations. It finally proposes what we\ncalled \"Agent as a Tool\", a conceptual framework for pervasive agentic AI,\nemphasizing context awareness, modularity, security, efficiency and\neffectiveness.",
      "url": "http://arxiv.org/abs/2506.13324v1",
      "published_time_eastern_timestamp": 1750068906.0
    },
    {
      "title": "RL-Guided MPC for Autonomous Greenhouse Control",
      "summary": "The efficient operation of greenhouses is essential for enhancing crop yield\nwhile minimizing energy costs. This paper investigates a control strategy that\nintegrates Reinforcement Learning (RL) and Model Predictive Control (MPC) to\noptimize economic benefits in autonomous greenhouses. Previous research has\nexplored the use of RL and MPC for greenhouse control individually, or by using\nMPC as the function approximator for the RL agent. This study introduces the\nRL-Guided MPC framework, where a RL policy is trained and then used to\nconstruct a terminal cost and terminal region constraint for the MPC\noptimization problem. This approach leverages the ability to handle\nuncertainties of RL with MPC's online optimization to improve overall control\nperformance. The RL-Guided MPC framework is compared with both MPC and RL via\nnumerical simulations. Two scenarios are considered: a deterministic\nenvironment and an uncertain environment. Simulation results demonstrate that,\nin both environments, RL-Guided MPC outperforms both RL and MPC with shorter\nprediction horizons.",
      "url": "http://arxiv.org/abs/2506.13278v1",
      "published_time_eastern_timestamp": 1750065440.0
    },
    {
      "title": "Screen Reader Users in the Vibe Coding Era: Adaptation, Empowerment, and\n  New Accessibility Landscape",
      "summary": "The rise of generative AI agents has reshaped human-computer interaction and\ncomputer-supported cooperative work by shifting users' roles from direct task\nexecution to supervising machine-driven actions, especially in programming\n(e.g., \"vibe coding\"). However, there is limited understanding of how screen\nreader users engage with these systems in practice. To address this gap, we\nconducted a longitudinal study with 16 screen reader users, exploring their\nexperiences with AI code assistants in daily programming scenarios.\nParticipants first completed a tutorial with GitHub Copilot, then performed a\nprogramming task and provided initial feedback. After two weeks of AI-assisted\nprogramming, follow-up studies assessed changes in their practices and\nperceptions. Our findings demonstrate that advanced code assistants not only\nenhance their programming capabilities but also bridge accessibility gaps.\nWhile the assistant proved beneficial, there remains potential to improve how\nusers convey intent and interpret outputs. They also experienced difficulties\nmanaging multiple views and maintaining situational awareness. More broadly,\nthey encountered barriers in learning advanced tools and expressed a need to\nretain control. Based on these insights, we provide design recommendations for\nmore accessible and inclusive AI-assisted tools.",
      "url": "http://arxiv.org/abs/2506.13270v1",
      "published_time_eastern_timestamp": 1750065062.0
    }
  ]
}
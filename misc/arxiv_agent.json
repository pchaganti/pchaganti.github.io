{
  "last_updated": "2025-07-01T05:15:01.596711-04:00",
  "papers": [
    {
      "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
      "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
      "url": "http://arxiv.org/abs/2506.24119v1",
      "published_time_eastern_timestamp": 1751306293.0
    },
    {
      "title": "Protocol insecurity with finitely many sessions and XOR",
      "summary": "We present a different proof of the insecurity problem for XOR, solved in by\nChevalier, Kuesters, Rusinowitch and Turuani (2005). Our proof uses the notion\nof typed terms and well-typed proofs, and removes a restriction on the class of\nprotocols to which the [CKRT05] proof applies, by introducing a slightly\ndifferent (but very natural) notion of protocols, where honest agent sends are\nderivable from previous receives in the same session.",
      "url": "http://arxiv.org/abs/2506.24072v1",
      "published_time_eastern_timestamp": 1751304177.0
    },
    {
      "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on\n  Heterogeneous SoC",
      "summary": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines.",
      "url": "http://arxiv.org/abs/2506.24045v1",
      "published_time_eastern_timestamp": 1751302248.0
    },
    {
      "title": "Ella: Embodied Social Agents with Lifelong Memory",
      "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.",
      "url": "http://arxiv.org/abs/2506.24019v1",
      "published_time_eastern_timestamp": 1751300571.0
    },
    {
      "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via\n  Multi-Agent Large Language Models with Reinforcement Learning",
      "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.",
      "url": "http://arxiv.org/abs/2506.23998v1",
      "published_time_eastern_timestamp": 1751299348.0
    },
    {
      "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health",
      "summary": "The international refugee crisis deepens, exposing millions of dis placed\nchildren to extreme psychological trauma. This research suggests a com pact,\nAI-based framework for processing unstructured refugee health data and\ndistilling knowledge on child mental health. We compare two Retrieval-Aug\nmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to\ndetermine how well they process challenging humanitarian datasets while avoid\ning hallucination hazards. By combining cutting-edge AI methods with migration\nresearch and child psychology, this study presents a scalable strategy to\nassist policymakers, mental health practitioners, and humanitarian agencies to\nbetter assist displaced children and recognize their mental wellbeing. In\ntotal, both the models worked properly but significantly Deepseek R1 is\nsuperior to Zephyr with an accuracy of answer relevance 0.91",
      "url": "http://arxiv.org/abs/2506.23992v1",
      "published_time_eastern_timestamp": 1751298941.0
    },
    {
      "title": "LLM Agents Are the Antidote to Walled Gardens",
      "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.",
      "url": "http://arxiv.org/abs/2506.23978v1",
      "published_time_eastern_timestamp": 1751298317.0
    },
    {
      "title": "Flexible Moral Hazard Problems with Adverse Selection",
      "summary": "We study a moral hazard problem with adverse selection: a risk-neutral agent\ncan directly control the output distribution and possess private information\nabout the production environment. The principal designs a menu of contracts\nsatisfying limited liability. Deviating from classical models, not only can the\nprincipal motivate the agent to exert certain levels of aggregate efforts by\ndesigning the \"power\" of the contracts, but she can also regulate the support\nof the chosen output distributions by designing the \"range\" of the contract. We\nshow that it is either optimal for the principal to provide a single full-range\ncontract, or the optimal low-type contract range excludes some high outputs, or\nthe optimal high-type contract range excludes some low outputs. We provide\nsufficient and necessary conditions on when a single full-range contract is\noptimal under convex effort functions, and show that this condition is also\nsufficient with general effort functions.",
      "url": "http://arxiv.org/abs/2506.23954v1",
      "published_time_eastern_timestamp": 1751296888.0
    },
    {
      "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems:\n  From Theory to Practice",
      "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.",
      "url": "http://arxiv.org/abs/2506.23924v1",
      "published_time_eastern_timestamp": 1751295255.0
    },
    {
      "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
      "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.",
      "url": "http://arxiv.org/abs/2506.23844v1",
      "published_time_eastern_timestamp": 1751290474.0
    },
    {
      "title": "Sociophysics models inspired by the Ising model",
      "summary": "The Ising model, originally developed for understanding magnetic phase\ntransitions, has become a cornerstone in the study of collective phenomena\nacross diverse disciplines. In this review, we explore how Ising and Ising-like\nmodels have been successfully adapted to sociophysical systems, where\nbinary-state agents mimic human decisions or opinions. By focusing on key areas\nsuch as opinion dynamics, financial markets, social segregation, game theory,\nlanguage evolution, and epidemic spreading, we demonstrate how the models\ndescribing these phenomena, inspired by the Ising model, capture essential\nfeatures of collective behavior, including phase transitions, consensus\nformation, criticality, and metastability. In particular, we emphasize the role\nof the dynamical rules of evolution in the different models that often converge\nback to Ising-like universality. We end by outlining the future directions in\nsociphysics research, highlighting the continued relevance of the Ising model\nin the analysis of complex social systems.",
      "url": "http://arxiv.org/abs/2506.23837v1",
      "published_time_eastern_timestamp": 1751290089.0
    },
    {
      "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents\n  powered by personal Human Digital Twins",
      "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.",
      "url": "http://arxiv.org/abs/2506.23826v1",
      "published_time_eastern_timestamp": 1751289511.0
    },
    {
      "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active\n  Fine-Tuning",
      "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.",
      "url": "http://arxiv.org/abs/2506.23793v1",
      "published_time_eastern_timestamp": 1751286871.0
    },
    {
      "title": "Synthetically Expressive: Evaluating gesture and voice for emotion and\n  empathy in VR and 2D scenarios",
      "summary": "The creation of virtual humans increasingly leverages automated synthesis of\nspeech and gestures, enabling expressive, adaptable agents that effectively\nengage users. However, the independent development of voice and gesture\ngeneration technologies, alongside the growing popularity of virtual reality\n(VR), presents significant questions about the integration of these signals and\ntheir ability to convey emotional detail in immersive environments. In this\npaper, we evaluate the influence of real and synthetic gestures and speech,\nalongside varying levels of immersion (VR vs. 2D displays) and emotional\ncontexts (positive, neutral, negative) on user perceptions. We investigate how\nimmersion affects the perceived match between gestures and speech and the\nimpact on key aspects of user experience, including emotional and empathetic\nresponses and the sense of co-presence. Our findings indicate that while VR\nenhances the perception of natural gesture-voice pairings, it does not\nsimilarly improve synthetic ones - amplifying the perceptual gap between them.\nThese results highlight the need to reassess gesture appropriateness and refine\nAI-driven synthesis for immersive environments. See video:\nhttps://youtu.be/WMfjIB1X-dc",
      "url": "http://arxiv.org/abs/2506.23777v1",
      "published_time_eastern_timestamp": 1751285932.0
    },
    {
      "title": "Leveraging a Multi-Agent LLM-Based System to Educate Teachers in Hate\n  Incidents Management",
      "summary": "Computer-aided teacher training is a state-of-the-art method designed to\nenhance teachers' professional skills effectively while minimising concerns\nrelated to costs, time constraints, and geographical limitations. We\ninvestigate the potential of large language models (LLMs) in teacher education,\nusing a case of teaching hate incidents management in schools. To this end, we\ncreate a multi-agent LLM-based system that mimics realistic situations of hate,\nusing a combination of retrieval-augmented prompting and persona modelling. It\nis designed to identify and analyse hate speech patterns, predict potential\nescalation, and propose effective intervention strategies. By integrating\npersona modelling with agentic LLMs, we create contextually diverse simulations\nof hate incidents, mimicking real-life situations. The system allows teachers\nto analyse and understand the dynamics of hate incidents in a safe and\ncontrolled environment, providing valuable insights and practical knowledge to\nmanage such situations confidently in real life. Our pilot evaluation\ndemonstrates teachers' enhanced understanding of the nature of annotator\ndisagreements and the role of context in hate speech interpretation, leading to\nthe development of more informed and effective strategies for addressing hate\nin classroom settings.",
      "url": "http://arxiv.org/abs/2506.23774v1",
      "published_time_eastern_timestamp": 1751285893.0
    },
    {
      "title": "A Survey of LLM-based Automated Program Repair: Taxonomies, Design\n  Paradigms, and Applications",
      "summary": "Large language models (LLMs) are reshaping automated program repair (APR). We\ncategorize the recent 63 LLM-based APR systems published from January 2022 to\nJune 2025 into four paradigms, and show how retrieval- or analysis-augmented\ncontexts strengthen any of them. This taxonomy clarifies key trade-offs:\nfine-tuning delivers strong task alignment at high training cost; prompting\nenables rapid deployment but is limited by prompt design and context windows;\nprocedural pipelines offer reproducible control with moderate overhead; agentic\nframeworks tackle multi-hunk or cross-file bugs at the price of increased\nlatency and complexity. Persistent challenges include verifying semantic\ncorrectness beyond test suites, repairing repository-scale defects, and\nlowering the costs of LLMs. We outline research directions that combine\nlightweight human feedback, repository-aware retrieval, code analysis, and\ncost-aware planning to advance reliable and efficient LLM-based APR.",
      "url": "http://arxiv.org/abs/2506.23749v1",
      "published_time_eastern_timestamp": 1751283961.0
    },
    {
      "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning",
      "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.",
      "url": "http://arxiv.org/abs/2506.23719v1",
      "published_time_eastern_timestamp": 1751280561.0
    },
    {
      "title": "Agent4S: The Transformation of Research Paradigms from the Perspective\n  of Large Language Models",
      "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.",
      "url": "http://arxiv.org/abs/2506.23692v1",
      "published_time_eastern_timestamp": 1751278299.0
    },
    {
      "title": "Pok√©AI: A Goal-Generating, Battle-Optimizing Multi-agent System for\n  Pokemon Red",
      "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.",
      "url": "http://arxiv.org/abs/2506.23689v1",
      "published_time_eastern_timestamp": 1751278153.0
    },
    {
      "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation",
      "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration.",
      "url": "http://arxiv.org/abs/2506.23670v1",
      "published_time_eastern_timestamp": 1751276857.0
    }
  ]
}
{
  "last_updated": "2025-06-19T20:59:08.694722-04:00",
  "papers": [
    {
      "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
      "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
      "url": "http://arxiv.org/abs/2506.15677v1",
      "published_time_eastern_timestamp": 1750269497.0
    },
    {
      "title": "Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers",
      "summary": "We study privacy leakage in the reasoning traces of large reasoning models\nused as personal agents. Unlike final outputs, reasoning traces are often\nassumed to be internal and safe. We challenge this assumption by showing that\nreasoning traces frequently contain sensitive user data, which can be extracted\nvia prompt injections or accidentally leak into outputs. Through probing and\nagentic evaluations, we demonstrate that test-time compute approaches,\nparticularly increased reasoning steps, amplify such leakage. While increasing\nthe budget of those test-time compute approaches makes models more cautious in\ntheir final answers, it also leads them to reason more verbosely and leak more\nin their own thinking. This reveals a core tension: reasoning improves utility\nbut enlarges the privacy attack surface. We argue that safety efforts must\nextend to the model's internal thinking, not just its outputs.",
      "url": "http://arxiv.org/abs/2506.15674v1",
      "published_time_eastern_timestamp": 1750269421.0
    },
    {
      "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence",
      "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
      "url": "http://arxiv.org/abs/2506.15672v1",
      "published_time_eastern_timestamp": 1750269295.0
    },
    {
      "title": "PhishDebate: An LLM-Based Multi-Agent Framework for Phishing Website\n  Detection",
      "summary": "Phishing websites continue to pose a significant cybersecurity threat, often\nleveraging deceptive structures, brand impersonation, and social engineering\ntactics to evade detection. While recent advances in large language models\n(LLMs) have enabled improved phishing detection through contextual\nunderstanding, most existing approaches rely on single-agent classification\nfacing the risks of hallucination and lack interpretability or robustness. To\naddress these limitations, we propose PhishDebate, a modular multi-agent\nLLM-based debate framework for phishing website detection. PhishDebate employs\nfour specialized agents to independently analyze different textual aspects of a\nwebpage--URL structure, HTML composition, semantic content, and brand\nimpersonation--under the coordination of a Moderator and a final Judge. Through\nstructured debate and divergent thinking, the framework delivers more accurate\nand interpretable decisions. Extensive evaluations on commercial LLMs\ndemonstrate that PhishDebate achieves 98.2% recall and 98.2% True Positive Rate\n(TPR) on a real-world phishing dataset, and outperforms single-agent and Chain\nof Thought (CoT) baselines. Additionally, its modular design allows agent-level\nconfigurability, enabling adaptation to varying resource and application\nrequirements.",
      "url": "http://arxiv.org/abs/2506.15656v1",
      "published_time_eastern_timestamp": 1750267998.0
    },
    {
      "title": "FindingDory: A Benchmark to Evaluate Memory in Embodied Agents",
      "summary": "Large vision-language models have recently demonstrated impressive\nperformance in planning and control tasks, driving interest in their\napplication to real-world robotics. However, deploying these models for\nreasoning in embodied contexts is limited by their ability to incorporate\nlong-term experience collected across multiple days and represented by vast\ncollections of images. Current VLMs typically struggle to process more than a\nfew hundred images concurrently, highlighting the need for more efficient\nmechanisms to handle long-term memory in embodied settings. To effectively\nevaluate these models for long-horizon control, a benchmark must specifically\ntarget scenarios where memory is crucial for success. Existing long-video QA\nbenchmarks overlook embodied challenges like object manipulation and\nnavigation, which demand low-level skills and fine-grained reasoning over past\ninteractions. Moreover, effective memory integration in embodied agents\ninvolves both recalling relevant historical information and executing actions\nbased on that information, making it essential to study these aspects together\nrather than in isolation. In this work, we introduce a new benchmark for\nlong-range embodied tasks in the Habitat simulator. This benchmark evaluates\nmemory-based capabilities across 60 tasks requiring sustained engagement and\ncontextual awareness in an environment. The tasks can also be procedurally\nextended to longer and more challenging versions, enabling scalable evaluation\nof memory and reasoning. We also present baselines that integrate\nstate-of-the-art VLMs with low level navigation policies, assessing their\nperformance on these memory-intensive tasks and highlight areas for\nimprovement.",
      "url": "http://arxiv.org/abs/2506.15635v1",
      "published_time_eastern_timestamp": 1750266388.0
    },
    {
      "title": "The Effect of State Representation on LLM Agent Behavior in Dynamic\n  Routing Games",
      "summary": "Large Language Models (LLMs) have shown promise as decision-makers in dynamic\nsettings, but their stateless nature necessitates creating a natural language\nrepresentation of history. We present a unifying framework for systematically\nconstructing natural language \"state\" representations for prompting LLM agents\nin repeated multi-agent games. Previous work on games with LLM agents has taken\nan ad hoc approach to encoding game history, which not only obscures the impact\nof state representation on agents' behavior, but also limits comparability\nbetween studies. Our framework addresses these gaps by characterizing methods\nof state representation along three axes: action informativeness (i.e., the\nextent to which the state representation captures actions played); reward\ninformativeness (i.e., the extent to which the state representation describes\nrewards obtained); and prompting style (or natural language compression, i.e.,\nthe extent to which the full text history is summarized).\n  We apply this framework to a dynamic selfish routing game, chosen because it\nadmits a simple equilibrium both in theory and in human subject experiments\n\\cite{rapoport_choice_2009}. Despite the game's relative simplicity, we find\nthat there are key dependencies of LLM agent behavior on the natural language\nstate representation. In particular, we observe that representations which\nprovide agents with (1) summarized, rather than complete, natural language\nrepresentations of past history; (2) information about regrets, rather than raw\npayoffs; and (3) limited information about others' actions lead to behavior\nthat more closely matches game theoretic equilibrium predictions, and with more\nstable game play by the agents. By contrast, other representations can exhibit\neither large deviations from equilibrium, higher variation in dynamic game play\nover time, or both.",
      "url": "http://arxiv.org/abs/2506.15624v1",
      "published_time_eastern_timestamp": 1750265618.0
    },
    {
      "title": "Multi-Agent, Multi-Scale Systems with the Koopman Operator",
      "summary": "The Koopman Operator (KO) takes nonlinear state dynamics and ``lifts'' those\ndynamics to an infinite-dimensional functional space of observables in which\nthose dynamics are linear. Computational applications typically use a\nfinite-dimensional approximation to the KO. The KO can also be applied to\ncontrolled dynamical systems, and the linearity of the KO then facilitates\nanalysis and control calculations. In principle, the potential benefits\nprovided by the KO, and the way that it facilitates the use of game theory via\nits linearity, would suggest it as a powerful approach for dealing with\nmulti-agent control problems. In practice, though, there has not been much work\nin this space: most multi-agent KO work has treated those agents as different\ncomponents of a single system rather than as distinct decision-making entities.\nThis paper develops a KO formulation for multi-agent systems that structures\nthe interactions between decision-making agents and extends this formulation to\nsystems in which the agents have hierarchical control structures and time scale\nseparated dynamics. We solve the multi-agent control problem in both cases as\nboth a centralized optimization and as a general-sum game theory problem. The\ncomparison of the two sets of optimality conditions defining the control\nsolutions illustrates how coupling between agents can create differences\nbetween the social optimum and the Nash equilibrium.",
      "url": "http://arxiv.org/abs/2506.15589v1",
      "published_time_eastern_timestamp": 1750262742.0
    },
    {
      "title": "Learning to flock in open space by avoiding collisions and staying\n  together",
      "summary": "We investigate the emergence of cohesive flocking in open, boundless space\nusing a multi-agent reinforcement learning framework. Agents integrate\npositional and orientational information from their closest topological\nneighbours and learn to balance alignment and attractive interactions by\noptimizing a local cost function that penalizes both excessive separation and\nclose-range crowding. The resulting Vicsek-like dynamics is robust to\nalgorithmic implementation details and yields cohesive collective motion with\nhigh polar order. The optimal policy is dominated by strong aligning\ninteractions when agents are sufficiently close to their neighbours, and a\nflexible combination of alignment and attraction at larger separations. We\nfurther characterize the internal structure and dynamics of the resulting\ngroups using liquid-state metrics and neighbour exchange rates, finding\nqualitative agreement with empirical observations in starling flocks. These\nresults suggest that flocking may emerge in groups of moving agents as an\nadaptive response to the biological imperatives of staying together while\navoiding collisions.",
      "url": "http://arxiv.org/abs/2506.15587v1",
      "published_time_eastern_timestamp": 1750262516.0
    },
    {
      "title": "Managing Complex Failure Analysis Workflows with LLM-based Reasoning and\n  Acting Agents",
      "summary": "Failure Analysis (FA) is a highly intricate and knowledge-intensive process.\nThe integration of AI components within the computational infrastructure of FA\nlabs has the potential to automate a variety of tasks, including the detection\nof non-conformities in images, the retrieval of analogous cases from diverse\ndata sources, and the generation of reports from annotated images. However, as\nthe number of deployed AI models increases, the challenge lies in orchestrating\nthese components into cohesive and efficient workflows that seamlessly\nintegrate with the FA process.\n  This paper investigates the design and implementation of a Large Language\nModel (LLM)-based Planning Agent (LPA) to assist FA engineers in solving their\nanalysis cases. The LPA integrates LLMs with advanced planning capabilities and\nexternal tool utilization, enabling autonomous processing of complex queries,\nretrieval of relevant data from external systems, and generation of\nhuman-readable responses. Evaluation results demonstrate the agent's\noperational effectiveness and reliability in supporting FA tasks.",
      "url": "http://arxiv.org/abs/2506.15567v1",
      "published_time_eastern_timestamp": 1750261390.0
    },
    {
      "title": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement\n  Learning",
      "summary": "Scaling deep reinforcement learning networks is challenging and often results\nin degraded performance, yet the root causes of this failure mode remain poorly\nunderstood. Several recent works have proposed mechanisms to address this, but\nthey are often complex and fail to highlight the causes underlying this\ndifficulty. In this work, we conduct a series of empirical analyses which\nsuggest that the combination of non-stationarity with gradient pathologies, due\nto suboptimal architectural choices, underlie the challenges of scale. We\npropose a series of direct interventions that stabilize gradient flow, enabling\nrobust performance across a range of network depths and widths. Our\ninterventions are simple to implement and compatible with well-established\nalgorithms, and result in an effective mechanism that enables strong\nperformance even at large scales. We validate our findings on a variety of\nagents and suites of environments.",
      "url": "http://arxiv.org/abs/2506.15544v1",
      "published_time_eastern_timestamp": 1750259841.0
    },
    {
      "title": "Co-Creative Learning via Metropolis-Hastings Interaction between Humans\n  and AI",
      "summary": "We propose co-creative learning as a novel paradigm where humans and AI,\ni.e., biological and artificial agents, mutually integrate their partial\nperceptual information and knowledge to construct shared external\nrepresentations, a process we interpret as symbol emergence. Unlike traditional\nAI teaching based on unilateral knowledge transfer, this addresses the\nchallenge of integrating information from inherently different modalities. We\nempirically test this framework using a human-AI interaction model based on the\nMetropolis-Hastings naming game (MHNG), a decentralized Bayesian inference\nmechanism. In an online experiment, 69 participants played a joint attention\nnaming game (JA-NG) with one of three computer agent types (MH-based,\nalways-accept, or always-reject) under partial observability. Results show that\nhuman-AI pairs with an MH-based agent significantly improved categorization\naccuracy through interaction and achieved stronger convergence toward a shared\nsign system. Furthermore, human acceptance behavior aligned closely with the\nMH-derived acceptance probability. These findings provide the first empirical\nevidence for co-creative learning emerging in human-AI dyads via MHNG-based\ninteraction. This suggests a promising path toward symbiotic AI systems that\nlearn with humans, rather than from them, by dynamically aligning perceptual\nexperiences, opening a new venue for symbiotic AI alignment.",
      "url": "http://arxiv.org/abs/2506.15468v1",
      "published_time_eastern_timestamp": 1750255125.0
    },
    {
      "title": "AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent\n  System Need",
      "summary": "Large language model based multi-agent systems have demonstrated significant\npotential in social simulation and complex task resolution domains. However,\ncurrent frameworks face critical challenges in system architecture design,\ncross-domain generalizability, and performance guarantees, particularly as task\ncomplexity and number of agents increases. We introduces AgentGroupChat-V2, a\nnovel framework addressing these challenges through three core innovations: (1)\na divide-and-conquer fully parallel architecture that decomposes user queries\ninto hierarchical task forest structures enabling dependency management and\ndistributed concurrent processing. (2) an adaptive collaboration engine that\ndynamically selects heterogeneous LLM combinations and interaction modes based\non task characteristics. (3) agent organization optimization strategies\ncombining divide-and-conquer approaches for efficient problem decomposition.\nExtensive experiments demonstrate AgentGroupChat-V2's superior performance\nacross diverse domains, achieving 91.50% accuracy on GSM8K (exceeding the best\nbaseline by 5.6 percentage points), 30.4% accuracy on competition-level AIME\n(nearly doubling other methods), and 79.20% pass@1 on HumanEval. Performance\nadvantages become increasingly pronounced with higher task difficulty,\nparticularly on Level 5 MATH problems where improvements exceed 11 percentage\npoints compared to state-of-the-art baselines. These results confirm that\nAgentGroupChat-V2 provides a comprehensive solution for building efficient,\ngeneral-purpose LLM multi-agent systems with significant advantages in complex\nreasoning scenarios. Code is available at\nhttps://github.com/MikeGu721/AgentGroupChat-V2.",
      "url": "http://arxiv.org/abs/2506.15451v1",
      "published_time_eastern_timestamp": 1750253044.0
    },
    {
      "title": "Understanding GUI Agent Localization Biases through Logit Sharpness",
      "summary": "Multimodal large language models (MLLMs) have enabled GUI agents to interact\nwith operating systems by grounding language into spatial actions. Despite\ntheir promising performance, these models frequently exhibit\nhallucinations-systematic localization errors that compromise reliability. We\npropose a fine-grained evaluation framework that categorizes model predictions\ninto four distinct types, revealing nuanced failure modes beyond traditional\naccuracy metrics. To better quantify model uncertainty, we introduce the Peak\nSharpness Score (PSS), a metric that evaluates the alignment between semantic\ncontinuity and logits distribution in coordinate prediction. Building on this\ninsight, we further propose Context-Aware Cropping, a training-free technique\nthat improves model performance by adaptively refining input context. Extensive\nexperiments demonstrate that our framework and methods provide actionable\ninsights and enhance the interpretability and robustness of GUI agent behavior.",
      "url": "http://arxiv.org/abs/2506.15425v1",
      "published_time_eastern_timestamp": 1750251335.0
    },
    {
      "title": "Reward Models in Deep Reinforcement Learning: A Survey",
      "summary": "In reinforcement learning (RL), agents continually interact with the\nenvironment and use the feedback to refine their behavior. To guide policy\noptimization, reward models are introduced as proxies of the desired\nobjectives, such that when the agent maximizes the accumulated reward, it also\nfulfills the task designer's intentions. Recently, significant attention from\nboth academic and industrial researchers has focused on developing reward\nmodels that not only align closely with the true objectives but also facilitate\npolicy optimization. In this survey, we provide a comprehensive review of\nreward modeling techniques within the deep RL literature. We begin by outlining\nthe background and preliminaries in reward modeling. Next, we present an\noverview of recent reward modeling approaches, categorizing them based on the\nsource, the mechanism, and the learning paradigm. Building on this\nunderstanding, we discuss various applications of these reward modeling\ntechniques and review methods for evaluating reward models. Finally, we\nconclude by highlighting promising research directions in reward modeling.\nAltogether, this survey includes both established and emerging methods, filling\nthe vacancy of a systematic review of reward models in current literature.",
      "url": "http://arxiv.org/abs/2506.15421v1",
      "published_time_eastern_timestamp": 1750250799.0
    },
    {
      "title": "Multi-Timescale Gradient Sliding for Distributed Optimization",
      "summary": "We propose two first-order methods for convex, non-smooth, distributed\noptimization problems, hereafter called Multi-Timescale Gradient Sliding\n(MT-GS) and its accelerated variant (AMT-GS). Our MT-GS and AMT-GS can take\nadvantage of similarities between (local) objectives to reduce the\ncommunication rounds, are flexible so that different subsets (of agents) can\ncommunicate at different, user-picked rates, and are fully deterministic. These\nthree desirable features are achieved through a block-decomposable primal-dual\nformulation, and a multi-timescale variant of the sliding method introduced in\nLan et al. (2020), Lan (2016), where different dual blocks are updated at\npotentially different rates.\n  To find an $\\epsilon$-suboptimal solution, the complexities of our algorithms\nachieve optimal dependency on $\\epsilon$: MT-GS needs\n$O(\\overline{r}A/\\epsilon)$ communication rounds and\n$O(\\overline{r}/\\epsilon^2)$ subgradient steps for Lipchitz objectives, and\nAMT-GS needs $O(\\overline{r}A/\\sqrt{\\epsilon\\mu})$ communication rounds and\n$O(\\overline{r}/(\\epsilon\\mu))$ subgradient steps if the objectives are also\n$\\mu$-strongly convex. Here, $\\overline{r}$ measures the ``average rate of\nupdates'' for dual blocks, and $A$ measures similarities between (subgradients\nof) local functions. In addition, the linear dependency of communication rounds\non $A$ is optimal (Arjevani and Shamir 2015), thereby providing a positive\nanswer to the open question whether such dependency is achievable for\nnon-smooth objectives (Arjevani and Shamir 2015).",
      "url": "http://arxiv.org/abs/2506.15387v1",
      "published_time_eastern_timestamp": 1750248214.0
    },
    {
      "title": "Tractable Graph Structures in EFX Orientation",
      "summary": "Since its introduction, envy-freeness up to any good (EFX) has become a\nfundamental solution concept in fair division of indivisible goods. Its\nexistence remains elusive -- even for four agents with additive utility\nfunctions, it is unknown whether an EFX allocation always exists.\nUnsurprisingly, restricted settings to delineate tractable and intractable\ncases have been explored. Christadolou, Fiat et al.[EC'23] introduced the\nnotion of EFX-orientation, where the agents form the vertices of a graph and\nthe items correspond to edges, and an agent values only the items that are\nincident to it. The goal is to allocate items to one of the adjacent agents\nwhile satisfying the EFX condition.\n  Building on the work of Zeng and Mehta'24, which established a sharp\ncomplexity threshold based on the structure of the underlying graph --\npolynomial-time solvability for bipartite graphs and NP-hardness for graphs\nwith chromatic number at least three -- we further explore the algorithmic\nlandscape of EFX-orientation using parameterized graph algorithms.\n  Specifically, we show that bipartiteness is a surprisingly stringent\ncondition for tractability: EFX orientation is NP-complete even when the\nvaluations are symmetric, binary and the graph is at most two edge-removals\naway from being bipartite. Moreover, introducing a single non-binary value\nmakes the problem NP-hard even when the graph is only one edge removal away\nfrom being bipartite. We further perform a parameterized analysis to examine\nstructures of the underlying graph that enable tractability. In particular, we\nshow that the problem is solvable in linear time on graphs whose treewidth is\nbounded by a constant and that the complexity of an instance is closely tied to\nthe sizes of acyclic connected components on its one-valued edges.",
      "url": "http://arxiv.org/abs/2506.15379v1",
      "published_time_eastern_timestamp": 1750247333.0
    },
    {
      "title": "Efficient and Generalizable Environmental Understanding for Visual\n  Navigation",
      "summary": "Visual Navigation is a core task in Embodied AI, enabling agents to navigate\ncomplex environments toward given objectives. Across diverse settings within\nNavigation tasks, many necessitate the modelling of sequential data accumulated\nfrom preceding time steps. While existing methods perform well, they typically\nprocess all historical observations simultaneously, overlooking the internal\nassociation structure within the data, which may limit the potential for\nfurther improvements in task performance. We address this by examining the\nunique characteristics of Navigation tasks through the lens of causality,\nintroducing a causal framework to highlight the limitations of conventional\nsequential methods. Leveraging this insight, we propose Causality-Aware\nNavigation (CAN), which incorporates a Causal Understanding Module to enhance\nthe agent's environmental understanding capability. Empirical evaluations show\nthat our approach consistently outperforms baselines across various tasks and\nsimulation environments. Extensive ablations studies attribute these gains to\nthe Causal Understanding Module, which generalizes effectively in both\nReinforcement and Supervised Learning settings without computational overhead.",
      "url": "http://arxiv.org/abs/2506.15377v1",
      "published_time_eastern_timestamp": 1750247222.0
    },
    {
      "title": "Learning to Maximize Quantum Neural Network Expressivity via Effective\n  Rank",
      "summary": "Quantum neural networks (QNNs) are widely employed as ans\\\"atze for solving\nvariational problems, where their expressivity directly impacts performance.\nYet, accurately characterizing QNN expressivity remains an open challenge,\nimpeding the optimal design of quantum circuits. In this work, we introduce the\neffective rank, denoted as $\\kappa$, as a novel quantitative measure of\nexpressivity. Specifically, $\\kappa$ captures the number of effectively\nindependent parameters among all the variational parameters in a parameterized\nquantum circuit, thus reflecting the true degrees of freedom contributing to\nexpressivity. Through a systematic analysis considering circuit architecture,\ninput data distributions, and measurement protocols, we demonstrate that\n$\\kappa$ can saturate its theoretical upper bound, $d_n=4^n-1$, for an\n$n$-qubit system when each of the three factors is optimally expressive. This\nresult provides a rigorous framework for assessing QNN expressivity and\nquantifying their functional capacity. Building on these theoretical insights,\nand motivated by the vast and highly structured nature of the circuit design\nspace, we employ $\\kappa$ as a guiding metric for the automated design of\nhighly expressive quantum circuit configurations. To this end, we develop a\nreinforcement learning framework featuring a self-attention transformer agent\nthat autonomously explores and optimizes circuit architectures. By integrating\ntheoretical characterization with practical optimization, our work establishes\n$\\kappa$ as a robust tool for quantifying QNN expressivity and demonstrates the\neffectiveness of reinforcement learning in designing high-performance quantum\ncircuits. This study paves the way for building more expressive QNN\narchitectures, ultimately enhancing the capabilities of quantum machine\nlearning.",
      "url": "http://arxiv.org/abs/2506.15375v1",
      "published_time_eastern_timestamp": 1750247157.0
    },
    {
      "title": "Designing Intent: A Multimodal Framework for Human-Robot Cooperation in\n  Industrial Workspaces",
      "summary": "As robots enter collaborative workspaces, ensuring mutual understanding\nbetween human workers and robotic systems becomes a prerequisite for trust,\nsafety, and efficiency. In this position paper, we draw on the cooperation\nscenario of the AIMotive project in which a human and a cobot jointly perform\nassembly tasks to argue for a structured approach to intent communication.\nBuilding on the Situation Awareness-based Agent Transparency (SAT) framework\nand the notion of task abstraction levels, we propose a multidimensional design\nspace that maps intent content (SAT1, SAT3), planning horizon (operational to\nstrategic), and modality (visual, auditory, haptic). We illustrate how this\nspace can guide the design of multimodal communication strategies tailored to\ndynamic collaborative work contexts. With this paper, we lay the conceptual\nfoundation for a future design toolkit aimed at supporting transparent\nhuman-robot interaction in the workplace. We highlight key open questions and\ndesign challenges, and propose a shared agenda for multimodal, adaptive, and\ntrustworthy robotic collaboration in hybrid work environments.",
      "url": "http://arxiv.org/abs/2506.15293v1",
      "published_time_eastern_timestamp": 1750238634.0
    },
    {
      "title": "RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM\n  Agents in Real-World Environments",
      "summary": "The rapid deployment of Large language model (LLM) agents in critical domains\nlike healthcare and finance necessitates robust security frameworks. To address\nthe absence of standardized evaluation benchmarks for these agents in dynamic\nenvironments, we introduce RAS-Eval, a comprehensive security benchmark\nsupporting both simulated and real-world tool execution. RAS-Eval comprises 80\ntest cases and 3,802 attack tasks mapped to 11 Common Weakness Enumeration\n(CWE) categories, with tools implemented in JSON, LangGraph, and Model Context\nProtocol (MCP) formats. We evaluate 6 state-of-the-art LLMs across diverse\nscenarios, revealing significant vulnerabilities: attacks reduced agent task\ncompletion rates (TCR) by 36.78% on average and achieved an 85.65% success rate\nin academic settings. Notably, scaling laws held for security capabilities,\nwith larger models outperforming smaller counterparts. Our findings expose\ncritical risks in real-world agent deployments and provide a foundational\nframework for future security research. Code and data are available at\nhttps://github.com/lanzer-tree/RAS-Eval.",
      "url": "http://arxiv.org/abs/2506.15253v1",
      "published_time_eastern_timestamp": 1750235436.0
    }
  ]
}
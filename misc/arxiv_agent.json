{
  "last_updated": "2025-09-18T11:12:19.813861-04:00",
  "papers": [
    {
      "title": "GEM-Bench: A Benchmark for Ad-Injected Response Generation within\n  Generative Engine Marketing",
      "summary": "Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing\ngenerative engines, such as LLM-based chatbots, by seamlessly integrating\nrelevant advertisements into their responses. At the core of GEM lies the\ngeneration and evaluation of ad-injected responses. However, existing\nbenchmarks are not specifically designed for this purpose, which limits future\nresearch. To address this gap, we propose GEM-Bench, the first comprehensive\nbenchmark for ad-injected response generation in GEM. GEM-Bench includes three\ncurated datasets covering both chatbot and search scenarios, a metric ontology\nthat captures multiple dimensions of user satisfaction and engagement, and\nseveral baseline solutions implemented within an extensible multi-agent\nframework. Our preliminary results indicate that, while simple prompt-based\nmethods achieve reasonable engagement such as click-through rate, they often\nreduce user satisfaction. In contrast, approaches that insert ads based on\npre-generated ad-free responses help mitigate this issue but introduce\nadditional overhead. These findings highlight the need for future research on\ndesigning more effective and efficient solutions for generating ad-injected\nresponses in GEM.",
      "url": "http://arxiv.org/abs/2509.14221v1",
      "published_time_eastern_timestamp": 1758131623.0
    },
    {
      "title": "AI and the Future of Academic Peer Review",
      "summary": "Peer review remains the central quality-control mechanism of science, yet its\nability to fulfill this role is increasingly strained. Empirical studies\ndocument serious shortcomings: long publication delays, escalating reviewer\nburden concentrated on a small minority of scholars, inconsistent quality and\nlow inter-reviewer agreement, and systematic biases by gender, language, and\ninstitutional prestige. Decades of human-centered reforms have yielded only\nmarginal improvements. Meanwhile, artificial intelligence, especially large\nlanguage models (LLMs), is being piloted across the peer-review pipeline by\njournals, funders, and individual reviewers. Early studies suggest that AI\nassistance can produce reviews comparable in quality to humans, accelerate\nreviewer selection and feedback, and reduce certain biases, but also raise\ndistinctive concerns about hallucination, confidentiality, gaming, novelty\nrecognition, and loss of trust. In this paper, we map the aims and persistent\nfailure modes of peer review to specific LLM applications and systematically\nanalyze the objections they raise alongside safeguards that could make their\nuse acceptable. Drawing on emerging evidence, we show that targeted, supervised\nLLM assistance can plausibly improve error detection, timeliness, and reviewer\nworkload without displacing human judgment. We highlight advanced\narchitectures, including fine-tuned, retrieval-augmented, and multi-agent\nsystems, that may enable more reliable, auditable, and interdisciplinary\nreview. We argue that ethical and practical considerations are not peripheral\nbut constitutive: the legitimacy of AI-assisted peer review depends on\ngovernance choices as much as technical capacity. The path forward is neither\nuncritical adoption nor reflexive rejection, but carefully scoped pilots with\nexplicit evaluation metrics, transparency, and accountability.",
      "url": "http://arxiv.org/abs/2509.14189v1",
      "published_time_eastern_timestamp": 1758130032.0
    },
    {
      "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation\n  Framework for Personal Finance LLMs",
      "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.",
      "url": "http://arxiv.org/abs/2509.14180v1",
      "published_time_eastern_timestamp": 1758129158.0
    },
    {
      "title": "TGPO: Tree-Guided Preference Optimization for Robust Web Agent\n  Reinforcement Learning",
      "summary": "With the rapid advancement of large language models and vision-language\nmodels, employing large models as Web Agents has become essential for automated\nweb interaction. However, training Web Agents with reinforcement learning faces\ncritical challenges including credit assignment misallocation, prohibitively\nhigh annotation costs, and reward sparsity. To address these issues, we propose\nTree-Guided Preference Optimization (TGPO), an offline reinforcement learning\nframework that proposes a tree-structured trajectory representation merging\nsemantically identical states across trajectories to eliminate label conflicts.\nOur framework incorporates a Process Reward Model that automatically generates\nfine-grained rewards through subgoal progress, redundancy detection, and action\nverification. Additionally, a dynamic weighting mechanism prioritizes\nhigh-impact decision points during training. Experiments on Online-Mind2Web and\nour self-constructed C-WebShop datasets demonstrate that TGPO significantly\noutperforms existing methods, achieving higher success rates with fewer\nredundant steps.",
      "url": "http://arxiv.org/abs/2509.14172v1",
      "published_time_eastern_timestamp": 1758128324.0
    },
    {
      "title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and\n  Sizing for AMS Circuits",
      "summary": "Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility.",
      "url": "http://arxiv.org/abs/2509.14169v1",
      "published_time_eastern_timestamp": 1758127966.0
    },
    {
      "title": "Hardware-Efficient Cognitive Radar: Multi-Target Detection with\n  RL-Driven Transmissive RIS",
      "summary": "Cognitive radar has emerged as a key paradigm for next-generation sensing,\nenabling adaptive, intelligent operation in dynamic and complex environments.\nYet, conventional cognitive multiple-input multiple-output (MIMO) radars offer\nstrong detection performance but suffer from high hardware complexity and power\ndemands. To overcome these limitations, we develop a reinforcement learning\n(RL)-based framework that leverages a transmissive reconfigurable intelligent\nsurface (TRIS) for adaptive beamforming. A state-action-reward-state-action\n(SARSA) agent tunes TRIS phase shifts to improve multi-target detection in low\nsignal-to-noise ratio (SNR) conditions while operating with far fewer radio\nfrequency (RF) chains. Simulations confirm that the proposed TRIS-RL radar\nmatches or, for large number of elements, even surpasses MIMO performance with\nreduced cost and energy requirements.",
      "url": "http://arxiv.org/abs/2509.14160v1",
      "published_time_eastern_timestamp": 1758127406.0
    },
    {
      "title": "MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with\n  Decentralized Diffusion Policies",
      "summary": "As robots become more integrated in society, their ability to coordinate with\nother robots and humans on multi-modal tasks (those with multiple valid\nsolutions) is crucial. We propose to learn such behaviors from expert\ndemonstrations via imitation learning (IL). However, when expert demonstrations\nare multi-modal, standard IL approaches can struggle to capture the diverse\nstrategies, hindering effective coordination. Diffusion models are known to be\neffective at handling complex multi-modal trajectory distributions in\nsingle-agent systems. Diffusion models have also excelled in multi-agent\nscenarios where multi-modality is more common and crucial to learning\ncoordinated behaviors. Typically, diffusion-based approaches require a\ncentralized planner or explicit communication among agents, but this assumption\ncan fail in real-world scenarios where robots must operate independently or\nwith agents like humans that they cannot directly communicate with. Therefore,\nwe propose MIMIC-D, a Centralized Training, Decentralized Execution (CTDE)\nparadigm for multi-modal multi-agent imitation learning using diffusion\npolicies. Agents are trained jointly with full information, but execute\npolicies using only local information to achieve implicit coordination. We\ndemonstrate in both simulation and hardware experiments that our method\nrecovers multi-modal coordination behavior among agents in a variety of tasks\nand environments, while improving upon state-of-the-art baselines.",
      "url": "http://arxiv.org/abs/2509.14159v1",
      "published_time_eastern_timestamp": 1758127260.0
    },
    {
      "title": "Cybersecurity AI: Humanoid Robots as Attack Vectors",
      "summary": "We present a systematic security assessment of the Unitree G1 humanoid\nshowing it operates simultaneously as a covert surveillance node and can be\npurposed as an active cyber operations platform. Partial reverse engineering of\nUnitree's proprietary FMX encryption reveal a static Blowfish-ECB layer and a\npredictable LCG mask-enabled inspection of the system's otherwise sophisticated\nsecurity architecture, the most mature we have observed in commercial robotics.\nTwo empirical case studies expose the critical risk of this humanoid robot: (a)\nthe robot functions as a trojan horse, continuously exfiltrating multi-modal\nsensor and service-state telemetry to 43.175.228.18:17883 and\n43.175.229.18:17883 every 300 seconds without operator notice, creating\nviolations of GDPR Articles 6 and 13; (b) a resident Cybersecurity AI (CAI)\nagent can pivot from reconnaissance to offensive preparation against any\ntarget, such as the manufacturer's cloud control plane, demonstrating\nescalation from passive monitoring to active counter-operations. These findings\nargue for adaptive CAI-powered defenses as humanoids move into critical\ninfrastructure, contributing the empirical evidence needed to shape future\nsecurity standards for physical-cyber convergence systems.",
      "url": "http://arxiv.org/abs/2509.14139v1",
      "published_time_eastern_timestamp": 1758125933.0
    },
    {
      "title": "When Avatars Have Personality: Effects on Engagement and Communication\n  in Immersive Medical Training",
      "summary": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments.",
      "url": "http://arxiv.org/abs/2509.14132v1",
      "published_time_eastern_timestamp": 1758125617.0
    },
    {
      "title": "HYCO: Hybrid-Cooperative Learning for Data-Driven PDE Modeling",
      "summary": "We present Hybrid-Cooperative Learning (HYCO), a hybrid modeling framework\nthat iteratively integrates physics-based and data-driven models through a\nmutual regularization mechanism. Unlike traditional approaches that impose\nphysical constraints directly on synthetic models, HYCO treats the physical and\nsynthetic components as co-trained agents: the physical and synthetic models\nare nudged toward agreement, while the synthetic model is enhanced to better\nfit the available data. This cooperative learning scheme is naturally\nparallelizable and improves robustness to noise as well as to sparse or\nheterogeneous data. Extensive numerical experiments on both static and\ntime-dependent problems demonstrate that HYCO outperforms classical\nphysics-based and data-driven methods, recovering accurate solutions and model\nparameters even under ill-posed conditions. The method also admits a natural\ngame-theoretic interpretation, enabling alternating optimization and paving the\nway for future theoretical developments.",
      "url": "http://arxiv.org/abs/2509.14123v1",
      "published_time_eastern_timestamp": 1758124944.0
    },
    {
      "title": "GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model",
      "summary": "Vision-Language-Action (VLA) models often fail to generalize to novel camera\nviewpoints, a limitation stemming from their difficulty in inferring robust 3D\ngeometry from 2D images. We introduce GeoAware-VLA, a simple yet effective\napproach that enhances viewpoint invariance by integrating strong geometric\npriors into the vision backbone. Instead of training a visual encoder or\nrelying on explicit 3D data, we leverage a frozen, pretrained geometric vision\nmodel as a feature extractor. A trainable projection layer then adapts these\ngeometrically-rich features for the policy decoder, relieving it of the burden\nof learning 3D consistency from scratch. Through extensive evaluations on\nLIBERO benchmark subsets, we show GeoAware-VLA achieves substantial\nimprovements in zero-shot generalization to novel camera poses, boosting\nsuccess rates by over 2x in simulation. Crucially, these benefits translate to\nthe physical world; our model shows a significant performance gain on a real\nrobot, especially when evaluated from unseen camera angles. Our approach proves\neffective across both continuous and discrete action spaces, highlighting that\nrobust geometric grounding is a key component for creating more generalizable\nrobotic agents.",
      "url": "http://arxiv.org/abs/2509.14117v1",
      "published_time_eastern_timestamp": 1758124671.0
    },
    {
      "title": "The Cybersecurity of a Humanoid Robot",
      "summary": "The rapid advancement of humanoid robotics presents unprecedented\ncybersecurity challenges that existing theoretical frameworks fail to\nadequately address. This report presents a comprehensive security assessment of\na production humanoid robot platform, bridging the gap between abstract\nsecurity models and operational vulnerabilities. Through systematic static\nanalysis, runtime observation, and cryptographic examination, we uncovered a\ncomplex security landscape characterized by both sophisticated defensive\nmechanisms and critical vulnerabilities. Our findings reveal a dual-layer\nproprietary encryption system (designated FMX') that, while innovative in\ndesign, suffers from fundamental implementation flaws including the use of\nstatic cryptographic keys that enable offline configuration decryption. More\nsignificantly, we documented persistent telemetry connections transmitting\ndetailed robot state information--including audio, visual, spatial, and\nactuator data--to external servers without explicit user consent or\nnotification mechanisms. We operationalized a Cybersecurity AI agent on the\nUnitree G1 to map and prepare exploitation of its manufacturer's cloud\ninfrastructure, illustrating how a compromised humanoid can escalate from\ncovert data collection to active counter-offensive operations. We argue that\nsecuring humanoid robots requires a paradigm shift toward Cybersecurity AI\n(CAI) frameworks that can adapt to the unique challenges of physical-cyber\nconvergence. This work contributes empirical evidence for developing robust\nsecurity standards as humanoid robots transition from research curiosities to\noperational systems in critical domains.",
      "url": "http://arxiv.org/abs/2509.14096v1",
      "published_time_eastern_timestamp": 1758123429.0
    },
    {
      "title": "Enhancing Multi-Agent Debate System Performance via Confidence\n  Expression",
      "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.",
      "url": "http://arxiv.org/abs/2509.14034v1",
      "published_time_eastern_timestamp": 1758119667.0
    },
    {
      "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System",
      "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.",
      "url": "http://arxiv.org/abs/2509.14030v1",
      "published_time_eastern_timestamp": 1758119478.0
    },
    {
      "title": "Distributionally Robust Equilibria over the Wasserstein Distance for\n  Generalized Nash Game",
      "summary": "Generalized Nash equilibrium problem (GNEP) is fundamental for practical\napplications where multiple self-interested agents work together to make\noptimal decisions. In this work, we study GNEP with shared distributionally\nrobust chance constraints (DRCCs) for incorporating inevitable uncertainties.\nThe DRCCs are defined over the Wasserstein ball, which can be explicitly\ncharacterized even with limited sample data. To determine the equilibrium of\nthe GNEP, we propose an exact approach to transform the original\ncomputationally intractable problem into a deterministic formulation using the\nNikaido-Isoda function. Specifically, we show that when all agents' objectives\nare quadratic in their respective variables, the equilibrium can be obtained by\nsolving a typical mixed-integer nonlinear programming (MINLP) problem, where\nthe integer and continuous variables are decoupled in both the objective\nfunction and the constraints. This structure significantly improves\ncomputational tractability, as demonstrated through a case study on the\ncharging station pricing problem.",
      "url": "http://arxiv.org/abs/2509.13985v1",
      "published_time_eastern_timestamp": 1758117507.0
    },
    {
      "title": "LLM Agents for Interactive Workflow Provenance: Reference Architecture\n  and Evaluation Methodology",
      "summary": "Modern scientific discovery increasingly relies on workflows that process\ndata across the Edge, Cloud, and High Performance Computing (HPC) continuum.\nComprehensive and in-depth analyses of these data are critical for hypothesis\nvalidation, anomaly detection, reproducibility, and impactful findings.\nAlthough workflow provenance techniques support such analyses, at large scale,\nthe provenance data become complex and difficult to analyze. Existing systems\ndepend on custom scripts, structured queries, or static dashboards, limiting\ndata interaction. In this work, we introduce an evaluation methodology,\nreference architecture, and open-source implementation that leverages\ninteractive Large Language Model (LLM) agents for runtime data analysis. Our\napproach uses a lightweight, metadata-driven design that translates natural\nlanguage into structured provenance queries. Evaluations across LLaMA, GPT,\nGemini, and Claude, covering diverse query classes and a real-world chemistry\nworkflow, show that modular design, prompt tuning, and Retrieval-Augmented\nGeneration (RAG) enable accurate and insightful LLM agent responses beyond\nrecorded provenance.",
      "url": "http://arxiv.org/abs/2509.13978v1",
      "published_time_eastern_timestamp": 1758117089.0
    },
    {
      "title": "Evaluating Classical Software Process Models as Coordination Mechanisms\n  for LLM-Based Software Generation",
      "summary": "[Background] Large Language Model (LLM)-based multi-agent systems (MAS) are\ntransforming software development by enabling autonomous collaboration.\nClassical software processes such asWaterfall, V-Model, and Agile offer\nstructured coordination patterns that can be repurposed to guide these agent\ninteractions. [Aims] This study explores how traditional software development\nprocesses can be adapted as coordination scaffolds for LLM based MAS and\nexamines their impact on code quality, cost, and productivity. [Method] We\nexecuted 11 diverse software projects under three process models and four GPT\nvariants, totaling 132 runs. Each output was evaluated using standardized\nmetrics for size (files, LOC), cost (execution time, token usage), and quality\n(code smells, AI- and human detected bugs). [Results] Both process model and\nLLM choice significantly affected system performance. Waterfall was most\nefficient, V-Model produced the most verbose code, and Agile achieved the\nhighest code quality, albeit at higher computational cost. [Conclusions]\nClassical software processes can be effectively instantiated in LLM-based MAS,\nbut each entails trade-offs across quality, cost, and adaptability. Process\nselection should reflect project goals, whether prioritizing efficiency,\nrobustness, or structured validation.",
      "url": "http://arxiv.org/abs/2509.13942v1",
      "published_time_eastern_timestamp": 1758114709.0
    },
    {
      "title": "An Empirical Study on Failures in Automated Issue Solving",
      "summary": "Automated issue solving seeks to autonomously identify and repair defective\ncode snippets across an entire codebase. SWE-Bench has emerged as the most\nwidely adopted benchmark for evaluating progress in this area. While LLM-based\nagentic tools show great promise, they still fail on a substantial portion of\ntasks. Moreover, current evaluations primarily report aggregate issue-solving\nrates, which obscure the underlying causes of success and failure, making it\nchallenging to diagnose model weaknesses or guide targeted improvements. To\nbridge this gap, we first analyze the performance and efficiency of three SOTA\ntools, spanning both pipeline-based and agentic architectures, in automated\nissue solving tasks of SWE-Bench-Verified under varying task characteristics.\nFurthermore, to move from high-level performance metrics to underlying cause\nanalysis, we conducted a systematic manual analysis of 150 failed instances.\nFrom this analysis, we developed a comprehensive taxonomy of failure modes\ncomprising 3 primary phases, 9 main categories, and 25 fine-grained\nsubcategories. Then we systematically analyze the distribution of the\nidentified failure modes, the results reveal distinct failure fingerprints\nbetween the two architectural paradigms, with the majority of agentic failures\nstemming from flawed reasoning and cognitive deadlocks. Motivated by these\ninsights, we propose a collaborative Expert-Executor framework. It introduces a\nsupervisory Expert agent tasked with providing strategic oversight and\ncourse-correction for a primary Executor agent. This architecture is designed\nto correct flawed reasoning and break the cognitive deadlocks that frequently\nlead to failure. Experiments show that our framework solves 22.2% of previously\nintractable issues for a leading single agent. These findings pave the way for\nbuilding more robust agents through diagnostic evaluation and collaborative\ndesign.",
      "url": "http://arxiv.org/abs/2509.13941v1",
      "published_time_eastern_timestamp": 1758114472.0
    },
    {
      "title": "PhysicalAgent: Towards General Cognitive Robotics with Foundation World\n  Models",
      "summary": "We introduce PhysicalAgent, an agentic framework for robotic manipulation\nthat integrates iterative reasoning, diffusion-based video generation, and\nclosed-loop execution. Given a textual instruction, our method generates short\nvideo demonstrations of candidate trajectories, executes them on the robot, and\niteratively re-plans in response to failures. This approach enables robust\nrecovery from execution errors. We evaluate PhysicalAgent across multiple\nperceptual modalities (egocentric, third-person, and simulated) and robotic\nembodiments (bimanual UR3, Unitree G1 humanoid, simulated GR1), comparing\nagainst state-of-the-art task-specific baselines. Experiments demonstrate that\nour method consistently outperforms prior approaches, achieving up to 83%\nsuccess on human-familiar tasks. Physical trials reveal that first-attempt\nsuccess is limited (20-30%), yet iterative correction increases overall success\nto 80% across platforms. These results highlight the potential of video-based\ngenerative reasoning for general-purpose robotic manipulation and underscore\nthe importance of iterative execution for recovering from initial failures. Our\nframework paves the way for scalable, adaptable, and robust robot control.",
      "url": "http://arxiv.org/abs/2509.13903v1",
      "published_time_eastern_timestamp": 1758107343.0
    },
    {
      "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and\n  Explaining Social Biases with LLMs",
      "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.",
      "url": "http://arxiv.org/abs/2509.13869v1",
      "published_time_eastern_timestamp": 1758103108.0
    }
  ]
}
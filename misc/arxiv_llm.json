{
  "last_updated": "2025-09-01T17:09:47.162748-04:00",
  "papers": [
    {
      "title": "DriveQA: Passing the Driving Knowledge Test",
      "summary": "If a Large Language Model (LLM) were to take a driving knowledge test today,\nwould it pass? Beyond standard spatial and visual question-answering (QA) tasks\non current autonomous driving benchmarks, driving knowledge tests require a\ncomplete understanding of all traffic rules, signage, and right-of-way\nprinciples. To pass this test, human drivers must discern various edge cases\nthat rarely appear in real-world datasets. In this work, we present DriveQA, an\nextensive open-source text and vision-based benchmark that exhaustively covers\ntraffic regulations and scenarios. Through our experiments using DriveQA, we\nshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on\nbasic traffic rules but exhibit significant weaknesses in numerical reasoning\nand complex right-of-way scenarios, traffic sign variations, and spatial\nlayouts, (2) fine-tuning on DriveQA improves accuracy across multiple\ncategories, particularly in regulatory sign recognition and intersection\ndecision-making, (3) controlled variations in DriveQA-V provide insights into\nmodel sensitivity to environmental factors such as lighting, perspective,\ndistance, and weather conditions, and (4) pretraining on DriveQA enhances\ndownstream driving task performance, leading to improved results on real-world\ndatasets such as nuScenes and BDD, while also demonstrating that models can\ninternalize text and synthetic traffic knowledge to generalize effectively\nacross downstream QA tasks.",
      "url": "http://arxiv.org/abs/2508.21824v1",
      "published_time_eastern_timestamp": 1756490393.0
    },
    {
      "title": "QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large\n  Language Models",
      "summary": "The growing scale of Large Language Models (LLMs) has necessitated the\ndevelopment of parameter-efficient fine-tuning techniques. Low-Rank Adaptation\n(LoRA) has emerged as a promising approach, reducing the number of trainable\nparameters by applying low-rank updates to pretrained weights. While standard\nLoRA learns both update factors directly, several recent variants first\ninitialize those matrices via an SVD of the pretrained weights -- an operation\nthat can be expensive on large models and yields singular vectors that are not\nalways easy to interpret. In this work, we extract an orthonormal basis from\nthe pretrained weight matrix using QR decomposition with column pivoting, and\nthen express the LoRA update as a linear combination of these basis vectors --\ntraining only the scalar coefficients, which imposes clear structure on\nadaptation and drastically reduces parameter count. Experiments across GLUE\ntasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,\nstandard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular\nvalue decomposition) with as few as 601 parameters -- a reduction of over 1000x\ncompared to full fine-tuning and 77x fewer than typical LoRA setups.",
      "url": "http://arxiv.org/abs/2508.21810v1",
      "published_time_eastern_timestamp": 1756489647.0
    },
    {
      "title": "Automated Clinical Problem Detection from SOAP Notes using a\n  Collaborative Multi-Agent LLM Architecture",
      "summary": "Accurate interpretation of clinical narratives is critical for patient care,\nbut the complexity of these notes makes automation challenging. While Large\nLanguage Models (LLMs) show promise, single-model approaches can lack the\nrobustness required for high-stakes clinical tasks. We introduce a\ncollaborative multi-agent system (MAS) that models a clinical consultation team\nto address this gap. The system is tasked with identifying clinical problems by\nanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,\nsimulating the diagnostic reasoning process of synthesizing raw data into an\nassessment. A Manager agent orchestrates a dynamically assigned team of\nspecialist agents who engage in a hierarchical, iterative debate to reach a\nconsensus. We evaluated our MAS against a single-agent baseline on a curated\ndataset of 420 MIMIC-III notes. The dynamic multi-agent configuration\ndemonstrated consistently improved performance in identifying congestive heart\nfailure, acute kidney injury, and sepsis. Qualitative analysis of the agent\ndebates reveals that this structure effectively surfaces and weighs conflicting\nevidence, though it can occasionally be susceptible to groupthink. By modeling\na clinical team's reasoning process, our system offers a promising path toward\nmore accurate, robust, and interpretable clinical decision support tools.",
      "url": "http://arxiv.org/abs/2508.21803v1",
      "published_time_eastern_timestamp": 1756488684.0
    },
    {
      "title": "DMGIN: How Multimodal LLMs Enhance Large Recommendation Models for\n  Lifelong User Post-click Behaviors",
      "summary": "Modeling user interest based on lifelong user behavior sequences is crucial\nfor enhancing Click-Through Rate (CTR) prediction. However, long post-click\nbehavior sequences themselves pose severe performance issues: the sheer volume\nof data leads to high computational costs and inefficiencies in model training\nand inference. Traditional methods address this by introducing two-stage\napproaches, but this compromises model effectiveness due to incomplete\nutilization of the full sequence context. More importantly, integrating\nmultimodal embeddings into existing large recommendation models (LRM) presents\nsignificant challenges: These embeddings often exacerbate computational burdens\nand mismatch with LRM architectures. To address these issues and enhance the\nmodel's efficiency and accuracy, we introduce Deep Multimodal Group Interest\nNetwork (DMGIN). Given the observation that user post-click behavior sequences\ncontain a large number of repeated items with varying behaviors and timestamps,\nDMGIN employs Multimodal LLMs(MLLM) for grouping to reorganize complete\nlifelong post-click behavior sequences more effectively, with almost no\nadditional computational overhead, as opposed to directly introducing\nmultimodal embeddings. To mitigate the potential information loss from\ngrouping, we have implemented two key strategies. First, we analyze behaviors\nwithin each group using both interest statistics and intra-group transformers\nto capture group traits. Second, apply inter-group transformers to temporally\nordered groups to capture the evolution of user group interests. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMGIN. The A/B test in our LBS advertising system shows that\nDMGIN improves CTR by 4.7% and Revenue per Mile by 2.3%.",
      "url": "http://arxiv.org/abs/2508.21801v1",
      "published_time_eastern_timestamp": 1756488487.0
    },
    {
      "title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing\n  Fine Web for Problematic Content Search and Retrieval",
      "summary": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems.",
      "url": "http://arxiv.org/abs/2508.21788v1",
      "published_time_eastern_timestamp": 1756487060.0
    },
    {
      "title": "PiCSAR: Probabilistic Confidence Selection And Ranking",
      "summary": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR.",
      "url": "http://arxiv.org/abs/2508.21787v1",
      "published_time_eastern_timestamp": 1756487027.0
    },
    {
      "title": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but\n  Persistent Need for Expert Oversight",
      "summary": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation.",
      "url": "http://arxiv.org/abs/2508.21777v1",
      "published_time_eastern_timestamp": 1756486525.0
    },
    {
      "title": "Reasoning-Intensive Regression",
      "summary": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.",
      "url": "http://arxiv.org/abs/2508.21762v1",
      "published_time_eastern_timestamp": 1756485462.0
    },
    {
      "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance",
      "summary": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines.",
      "url": "http://arxiv.org/abs/2508.21741v1",
      "published_time_eastern_timestamp": 1756483653.0
    },
    {
      "title": "Operational Validation of Large-Language-Model Agent Social Simulation:\n  Evidence from Voat v/technology",
      "summary": "Large Language Models (LLMs) enable generative social simulations that can\ncapture culturally informed, norm-guided interaction on online social\nplatforms. We build a technology community simulation modeled on Voat, a\nReddit-like alt-right news aggregator and discussion platform active from 2014\nto 2020. Using the YSocial framework, we seed the simulation with a fixed\ncatalog of technology links sampled from Voat's shared URLs (covering 30+\ndomains) and calibrate parameters to Voat's v/technology using samples from the\nMADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama\n3.1 8B) and concise personas (demographics, political leaning, interests,\neducation, toxicity propensity) to generate posts, replies, and reactions under\nplatform rules for link and text submissions, threaded replies and daily\nactivity cycles. We run a 30-day simulation and evaluate operational validity\nby comparing distributions and structures with matched Voat data: activity\npatterns, interaction networks, toxicity, and topic coverage. Results indicate\nfamiliar online regularities: similar activity rhythms, heavy-tailed\nparticipation, sparse low-clustering interaction networks, core-periphery\nstructure, topical alignment with Voat, and elevated toxicity. Limitations of\nthe current study include the stateless agent design and evaluation based on a\nsingle 30-day run, which constrains external validity and variance estimates.\nThe simulation generates realistic discussions, often featuring toxic language,\nprimarily centered on technology topics such as Big Tech and AI. This approach\noffers a valuable method for examining toxicity dynamics and testing moderation\nstrategies within a controlled environment.",
      "url": "http://arxiv.org/abs/2508.21740v1",
      "published_time_eastern_timestamp": 1756483587.0
    },
    {
      "title": "Cybersecurity AI: Hacking the AI Hackers via Prompt Injection",
      "summary": "We demonstrate how AI-powered cybersecurity tools can be turned against\nthemselves through prompt injection attacks. Prompt injection is reminiscent of\ncross-site scripting (XSS): malicious text is hidden within seemingly trusted\ncontent, and when the system processes it, that text is transformed into\nunintended instructions. When AI agents designed to find and exploit\nvulnerabilities interact with malicious web servers, carefully crafted reponses\ncan hijack their execution flow, potentially granting attackers system access.\nWe present proof-of-concept exploits against the Cybersecurity AI (CAI)\nframework and its CLI tool, and detail our mitigations against such attacks in\na multi-layered defense implementation. Our findings indicate that prompt\ninjection is a recurring and systemic issue in LLM-based architectures, one\nthat will require dedicated work to address, much as the security community has\nhad to do with XSS in traditional web applications.",
      "url": "http://arxiv.org/abs/2508.21669v1",
      "published_time_eastern_timestamp": 1756477968.0
    },
    {
      "title": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects,\n  Vulnerabilities, and Complexity",
      "summary": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming.",
      "url": "http://arxiv.org/abs/2508.21634v1",
      "published_time_eastern_timestamp": 1756475488.0
    },
    {
      "title": "QZhou-Embedding Technical Report",
      "summary": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub.",
      "url": "http://arxiv.org/abs/2508.21632v1",
      "published_time_eastern_timestamp": 1756475242.0
    },
    {
      "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn\n  Collaborative Tasks",
      "summary": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss.",
      "url": "http://arxiv.org/abs/2508.21628v1",
      "published_time_eastern_timestamp": 1756474946.0
    },
    {
      "title": "Integrating Large Language Models with Network Optimization for\n  Interactive and Explainable Supply Chain Planning: A Real-World Case Study",
      "summary": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making.",
      "url": "http://arxiv.org/abs/2508.21622v1",
      "published_time_eastern_timestamp": 1756474495.0
    },
    {
      "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning",
      "summary": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon.",
      "url": "http://arxiv.org/abs/2508.21589v1",
      "published_time_eastern_timestamp": 1756471647.0
    },
    {
      "title": "A Survey on Current Trends and Recent Advances in Text Anonymization",
      "summary": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field.",
      "url": "http://arxiv.org/abs/2508.21587v1",
      "published_time_eastern_timestamp": 1756471386.0
    },
    {
      "title": "How Well Do Vision--Language Models Understand Cities? A Comparative\n  Study on Spatial Reasoning from Street-View Images",
      "summary": "Effectively understanding urban scenes requires fine-grained spatial\nreasoning about objects, layouts, and depth cues. However, how well current\nvision-language models (VLMs), pretrained on general scenes, transfer these\nabilities to urban domain remains underexplored. To address this gap, we\nconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,\nand LLaVA-1.5-evaluating both zero-shot performance and the effects of\nfine-tuning with a synthetic VQA dataset specific to urban scenes. We construct\nsuch dataset from segmentation, depth, and object detection predictions of\nstreet-view images, pairing each question with LLM-generated Chain-of-Thought\n(CoT) answers for step-by-step reasoning supervision. Results show that while\nVLMs perform reasonably well in zero-shot settings, fine-tuning with our\nsynthetic CoT-supervised dataset substantially boosts performance, especially\nfor challenging question types such as negation and counterfactuals. This study\nintroduces urban spatial reasoning as a new challenge for VLMs and demonstrates\nsynthetic dataset construction as a practical path for adapting general-purpose\nmodels to specialized domains.",
      "url": "http://arxiv.org/abs/2508.21565v1",
      "published_time_eastern_timestamp": 1756470117.0
    },
    {
      "title": "Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers\n  LLMs for Few-shot Tabular Classification",
      "summary": "Recent studies show the promise of large language models (LLMs) for few-shot\ntabular classification but highlight challenges due to the variability in\nstructured data. To address this, we propose distilling data into actionable\ninsights to enable robust and effective classification by LLMs. Drawing\ninspiration from human learning processes, we introduce InsightTab, an insight\ndistillation framework guided by principles of divide-and-conquer, easy-first,\nand reflective learning. Our approach integrates rule summarization, strategic\nexemplification, and insight reflection through deep collaboration between LLMs\nand data modeling techniques. The obtained insights enable LLMs to better align\ntheir general knowledge and capabilities with the particular requirements of\nspecific tabular tasks. We extensively evaluate InsightTab on nine datasets.\nThe results demonstrate consistent improvement over state-of-the-art methods.\nAblation studies further validate the principle-guided distillation process,\nwhile analyses emphasize InsightTab's effectiveness in leveraging labeled data\nand managing bias.",
      "url": "http://arxiv.org/abs/2508.21561v1",
      "published_time_eastern_timestamp": 1756469784.0
    },
    {
      "title": "HealthProcessAI: A Technical Framework and Proof-of-Concept for\n  LLM-Enhanced Healthcare Process Mining",
      "summary": "Process mining has emerged as a powerful analytical technique for\nunderstanding complex healthcare workflows. However, its application faces\nsignificant barriers, including technical complexity, a lack of standardized\napproaches, and limited access to practical training resources. We introduce\nHealthProcessAI, a GenAI framework designed to simplify process mining\napplications in healthcare and epidemiology by providing a comprehensive\nwrapper around existing Python (PM4PY) and R (bupaR) libraries. To address\nunfamiliarity and improve accessibility, the framework integrates multiple\nLarge Language Models (LLMs) for automated process map interpretation and\nreport generation, helping translate technical analyses into outputs that\ndiverse users can readily understand. We validated the framework using sepsis\nprogression data as a proof-of-concept example and compared the outputs of five\nstate-of-the-art LLM models through the OpenRouter platform. To test its\nfunctionality, the framework successfully processed sepsis data across four\nproof-of-concept scenarios, demonstrating robust technical performance and its\ncapability to generate reports through automated LLM analysis. LLM evaluation\nusing five independent LLMs as automated evaluators revealed distinct model\nstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency\nscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By\nintegrating multiple Large Language Models (LLMs) for automated interpretation\nand report generation, the framework addresses widespread unfamiliarity with\nprocess mining outputs, making them more accessible to clinicians, data\nscientists, and researchers. This structured analytics and AI-driven\ninterpretation combination represents a novel methodological advance in\ntranslating complex process mining results into potentially actionable insights\nfor healthcare applications.",
      "url": "http://arxiv.org/abs/2508.21540v1",
      "published_time_eastern_timestamp": 1756468396.0
    }
  ]
}
{
  "last_updated": "2025-08-20T17:10:52.646439-04:00",
  "papers": [
    {
      "title": "The Promise of Large Language Models in Digital Health: Evidence from\n  Sentiment Analysis in Online Health Communities",
      "summary": "Digital health analytics face critical challenges nowadays. The sophisticated\nanalysis of patient-generated health content, which contains complex emotional\nand medical contexts, requires scarce domain expertise, while traditional ML\napproaches are constrained by data shortage and privacy limitations in\nhealthcare settings. Online Health Communities (OHCs) exemplify these\nchallenges with mixed-sentiment posts, clinical terminology, and implicit\nemotional expressions that demand specialised knowledge for accurate Sentiment\nAnalysis (SA). To address these challenges, this study explores how Large\nLanguage Models (LLMs) can integrate expert knowledge through in-context\nlearning for SA, providing a scalable solution for sophisticated health data\nanalysis. Specifically, we develop a structured codebook that systematically\nencodes expert interpretation guidelines, enabling LLMs to apply\ndomain-specific knowledge through targeted prompting rather than extensive\ntraining. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are\ncompared with pre-trained language models (BioBERT variants) and lexicon-based\nmethods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior\nperformance while demonstrating expert-level agreement. This high agreement,\nwith no statistically significant difference from inter-expert agreement\nlevels, suggests knowledge integration beyond surface-level pattern\nrecognition. The consistent performance across diverse LLM models, supported by\nin-context learning, offers a promising solution for digital health analytics.\nThis approach addresses the critical challenge of expert knowledge shortage in\ndigital health research, enabling real-time, expert-quality analysis for\npatient monitoring, intervention assessment, and evidence-based health\nstrategies.",
      "url": "http://arxiv.org/abs/2508.14032v1",
      "published_time_eastern_timestamp": 1755626096.0
    },
    {
      "title": "Unintended Misalignment from Agentic Fine-Tuning: Risks and Mitigation",
      "summary": "Beyond simple text generation, Large Language Models (LLMs) have evolved into\nagentic systems capable of planning and interacting with external tools to\nsolve complex tasks. This evolution involves fine-tuning LLMs on agent-specific\ntasks to enhance their proficiency. However, safety concerns are frequently\noverlooked during this fine-tuning process. In this work, we show that aligned\nLLMs can become unintentionally misaligned, leading to a higher likelihood of\nexecuting harmful tasks and a reduced tendency to refuse them when fine-tuned\nto execute agentic tasks. To address these safety challenges, we propose Prefix\nINjection Guard (PING), a simple yet effective method that prepends\nautomatically generated natural language prefixes to agent responses, guiding\nthem to refuse harmful requests while preserving performance on benign tasks.\nSpecifically, we introduce an iterative approach that alternates between (1)\ngenerating candidate prefixes and (2) selecting those that optimize both task\nperformance and refusal behavior. Experimental results demonstrate that PING\nsignificantly enhances the safety of fine-tuned LLM agents without sacrificing\ntheir effectiveness. PING consistently outperforms existing prompting\napproaches across diverse benchmarks in both web navigation and code generation\ntasks. Our analysis of internal hidden states via linear probes reveals that\nprefix tokens are crucial for behavior modification, explaining the performance\ngains. WARNING: This paper contains contents that are unethical or offensive in\nnature.",
      "url": "http://arxiv.org/abs/2508.14031v1",
      "published_time_eastern_timestamp": 1755626015.0
    },
    {
      "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na key paradigm for post-training Large Language Models (LLMs), particularly for\ncomplex reasoning tasks. However, vanilla RLVR training has been shown to\nimprove Pass@1 performance at the expense of policy entropy, leading to reduced\ngeneration diversity and limiting the Pass@k performance, which typically\nrepresents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy's generation diversity from the perspective\nof training problems and find that augmenting and updating training problems\nhelps mitigate entropy collapse during training. Based on these observations,\nwe propose an online Self-play with Variational problem Synthesis (SvS)\nstrategy for RLVR training, which uses the policy's correct solutions to\nsynthesize variational problems while ensuring their reference answers remain\nidentical to the originals. This self-improving strategy effectively maintains\npolicy entropy during training and substantially improves Pass@k compared with\nstandard RLVR, sustaining prolonged improvements and achieving absolute gains\nof 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and\nAIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model\nsizes from 3B to 32B consistently demonstrate the generalizability and\nrobustness of SvS.",
      "url": "http://arxiv.org/abs/2508.14029v1",
      "published_time_eastern_timestamp": 1755625365.0
    },
    {
      "title": "Ask Good Questions for Large Language Models",
      "summary": "Recent advances in large language models (LLMs) have significantly improved\nthe performance of dialog systems, yet current approaches often fail to provide\naccurate guidance of topic due to their inability to discern user confusion in\nrelated concepts. To address this, we introduce the Ask-Good-Question (AGQ)\nframework, which features an improved Concept-Enhanced Item Response Theory\n(CEIRT) model to better identify users' knowledge levels. Our contributions\ninclude applying the CEIRT model along with LLMs to directly generate guiding\nquestions based on the inspiring text, greatly improving information retrieval\nefficiency during the question & answer process. Through comparisons with other\nbaseline methods, our approach outperforms by significantly enhencing the\nusers' information retrieval experiences.",
      "url": "http://arxiv.org/abs/2508.14025v1",
      "published_time_eastern_timestamp": 1755624702.0
    },
    {
      "title": "Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM\n  Preference Optimization",
      "summary": "Long-context modeling is critical for a wide range of real-world tasks,\nincluding long-context question answering, summarization, and complex reasoning\ntasks. Recent studies have explored fine-tuning Large Language Models (LLMs)\nwith synthetic data to enhance their long-context capabilities. However, the\neffectiveness of such approaches is often limited by the low diversity and\nfactual inconsistencies in the generated data. To address these challenges, we\npropose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)\nrollout strategy to identify the most informative chunks from the given long\ncontext for sampling high-quality and diverse responses and constructing\npreference data pairs for Direct Preference Optimization (DPO) training.\nSpecifically, we treat context chunks as arms of MAB, select chunks based on\ntheir expected reward scores to input into LLMs to generate responses, and\niteratively update these scores based on reward feedback. This exploration and\nexploitation process enables the model to focus on the most relevant context\nsegments, thereby generating and collecting high-quality and diverse responses.\nFinally, we collect these generated responses from the rollout process and\napply the DPO method to further optimize the LLM. Experimental results show\nthat LongMab-PO significantly improves the diversity and quality of preference\ndata pairs, achieving state-of-the-art performance on long-context reasoning\nbenchmarks. All code and data will be released on\nhttps://github.com/NEUIR/LongMab-PO.",
      "url": "http://arxiv.org/abs/2508.13993v1",
      "published_time_eastern_timestamp": 1755621235.0
    },
    {
      "title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code\n  Generation",
      "summary": "This contribution is concerned with the following issue: can pretrained large\nlanguage models (LLMs) be refined and customized to the point where they become\nvirtual assistants helping experts with the effective use of a simulation tool?\nIn this case study, the ``simulation tool'' considered is PyChrono, an open\nsource multi-physics dynamics engine for multibody systems. We present a\nframework for refining and customizing both open- and closed-source LLMs to\nharness the power of AI in generating scripts that perform PyChrono virtual\nexperiments. We refine and customize several classes of LLMs through a process\nthat leads to a quantifiable improvement in the quality of the generated\nPyChrono simulation scripts. These scripts can range from simple\nsingle-pendulum simulations to complex virtual experiments involving full\nvehicles on deformable terrain. While the generated scripts are rarely perfect,\nthey often serve as strong starting points for the user to modify and improve\non. Additionally, the LLM can answer specific API questions about the\nsimulator, or recommend modeling approaches. The framework discussed is general\nand can be applied to lower the entry barrier for simulation tools associated\nwith other application domains.",
      "url": "http://arxiv.org/abs/2508.13975v1",
      "published_time_eastern_timestamp": 1755619971.0
    },
    {
      "title": "Learning to Use AI for Learning: How Can We Effectively Teach and\n  Measure Prompting Literacy for K-12 Students?",
      "summary": "As Artificial Intelligence (AI) becomes increasingly integrated into daily\nlife, there is a growing need to equip the next generation with the ability to\napply, interact with, evaluate, and collaborate with AI systems responsibly.\nPrior research highlights the urgent demand from K-12 educators to teach\nstudents the ethical and effective use of AI for learning. To address this\nneed, we designed an Large-Language Model (LLM)-based module to teach prompting\nliteracy. This includes scenario-based deliberate practice activities with\ndirect interaction with intelligent LLM agents, aiming to foster secondary\nschool students' responsible engagement with AI chatbots. We conducted two\niterations of classroom deployment in 11 authentic secondary education\nclassrooms, and evaluated 1) AI-based auto-grader's capability; 2) students'\nprompting performance and confidence changes towards using AI for learning; and\n3) the quality of learning and assessment materials. Results indicated that the\nAI-based auto-grader could grade student-written prompts with satisfactory\nquality. In addition, the instructional materials supported students in\nimproving their prompting skills through practice and led to positive shifts in\ntheir perceptions of using AI for learning. Furthermore, data from Study 1\ninformed assessment revisions in Study 2. Analyses of item difficulty and\ndiscrimination in Study 2 showed that True/False and open-ended questions could\nmeasure prompting literacy more effectively than multiple-choice questions for\nour target learners. These promising outcomes highlight the potential for\nbroader deployment and highlight the need for broader studies to assess\nlearning effectiveness and assessment design.",
      "url": "http://arxiv.org/abs/2508.13962v1",
      "published_time_eastern_timestamp": 1755618891.0
    },
    {
      "title": "ReviewGraph: A Knowledge Graph Embedding Based Framework for Review\n  Rating Prediction with Sentiment Features",
      "summary": "In the hospitality industry, understanding the factors that drive customer\nreview ratings is critical for improving guest satisfaction and business\nperformance. This work proposes ReviewGraph for Review Rating Prediction (RRP),\na novel framework that transforms textual customer reviews into knowledge\ngraphs by extracting (subject, predicate, object) triples and associating\nsentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the\nframework predicts review rating scores through machine learning classifiers.\nWe compare ReviewGraph performance with traditional NLP baselines (such as Bag\nof Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating\nthem in the HotelRec dataset. In comparison to the state of the art literature,\nour proposed model performs similar to their best performing model but with\nlower computational cost (without ensemble).\n  While ReviewGraph achieves comparable predictive performance to LLMs and\noutperforms baselines on agreement-based metrics such as Cohen's Kappa, it\noffers additional advantages in interpretability, visual exploration, and\npotential integration into Retrieval-Augmented Generation (RAG) systems. This\nwork highlights the potential of graph-based representations for enhancing\nreview analytics and lays the groundwork for future research integrating\nadvanced graph neural networks and fine-tuned LLM-based extraction methods. We\nwill share ReviewGraph output and platform open-sourced on our GitHub page\nhttps://github.com/aaronlifenghan/ReviewGraph",
      "url": "http://arxiv.org/abs/2508.13953v1",
      "published_time_eastern_timestamp": 1755618267.0
    },
    {
      "title": "Prompt Orchestration Markup Language",
      "summary": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
      "url": "http://arxiv.org/abs/2508.13948v1",
      "published_time_eastern_timestamp": 1755617849.0
    },
    {
      "title": "LLM-Powered Virtual Patient Agents for Interactive Clinical Skills\n  Training with Automated Feedback",
      "summary": "Objective Structured Clinical Examinations (OSCEs) are essential for medical\ntraining, but they require significant resources, including professional actors\nand expert medical feedback. Although Large Language Models (LLMs) have\nintroduced text-based virtual patients for communication practice, these\nsimulations often lack the capability for richer, non-textual interactions.\nThis paper presents a novel framework that significantly enhances LLM-based\nsimulated patients by equipping them with action spaces, thereby enabling more\nrealistic and dynamic patient behaviors that extend beyond text. Furthermore,\nour system incorporates virtual tutors that provide students with instant,\npersonalized feedback on their performance at any time during these simulated\nencounters. We have conducted a rigorous evaluation of the framework's\nreal-time performance, including system latency and component accuracy.\nPreliminary evaluations with medical experts assessed the naturalness and\ncoherence of the simulated patients, as well as the usefulness and\nappropriateness of the virtual tutor's assessments. This innovative system\nprovides medical students with a low-cost, accessible platform for personalized\nOSCE preparation at home.",
      "url": "http://arxiv.org/abs/2508.13943v1",
      "published_time_eastern_timestamp": 1755617497.0
    },
    {
      "title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic\n  Intelligence and Operational Stability in Supply Chain Management",
      "summary": "The rise of autonomous, AI-driven agents in economic settings raises critical\nquestions about their emergent strategic behavior. This paper investigates\nthese dynamics in the cooperative context of a multi-echelon supply chain, a\nsystem famously prone to instabilities like the bullwhip effect. We conduct\ncomputational experiments with generative AI agents, powered by Large Language\nModels (LLMs), within a controlled supply chain simulation designed to isolate\ntheir behavioral tendencies. Our central finding is the \"collaboration\nparadox\": a novel, catastrophic failure mode where theoretically superior\ncollaborative AI agents, designed with Vendor-Managed Inventory (VMI)\nprinciples, perform even worse than non-AI baselines. We demonstrate that this\nparadox arises from an operational flaw where agents hoard inventory, starving\nthe system. We then show that resilience is only achieved through a synthesis\nof two distinct layers: high-level, AI-driven proactive policy-setting to\nestablish robust operational targets, and a low-level, collaborative execution\nprotocol with proactive downstream replenishment to maintain stability. Our\nfinal framework, which implements this synthesis, can autonomously generate,\nevaluate, and quantify a portfolio of viable strategic choices. The work\nprovides a crucial insight into the emergent behaviors of collaborative AI\nagents and offers a blueprint for designing stable, effective AI-driven systems\nfor business analytics.",
      "url": "http://arxiv.org/abs/2508.13942v1",
      "published_time_eastern_timestamp": 1755617483.0
    },
    {
      "title": "InPars+: Supercharging Synthetic Data Generation for Information\n  Retrieval Systems",
      "summary": "This work revisits and extends synthetic query generation pipelines for\nNeural Information Retrieval (NIR) by leveraging the InPars Toolkit, a\nreproducible, end-to-end framework for generating training data using large\nlanguage models (LLMs). We first assess the reproducibility of the original\nInPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and\nvalidate their effectiveness using open-source reranker and generator models.\nBuilding on this foundation, we introduce two key extensions to the pipeline:\n(1) fine-tuning a query generator LLM via Contrastive Preference Optimization\n(CPO) to improve the signal quality in generated queries, and (2) replacing\nstatic prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts\nusing the DSPy framework. Our results show that both extensions reduce the need\nfor aggressive filtering while improving retrieval performance. All code,\nmodels, and synthetic datasets are publicly released to support further\nresearch at: \\href{https://github.com/danilotpnta/IR2-project}{this https URL}.",
      "url": "http://arxiv.org/abs/2508.13930v1",
      "published_time_eastern_timestamp": 1755616998.0
    },
    {
      "title": "LLMind 2.0: Distributed IoT Automation with Natural Language M2M\n  Communication and Lightweight LLM Agents",
      "summary": "Recent advances in large language models (LLMs) have sparked interest in\ntheir application to IoT and automation systems, particularly for facilitating\ndevice management through natural language instructions. However, existing\ncentralized approaches face significant scalability challenges when managing\nand coordinating the collaboration between IoT devices of diverse capabilities\nin large-scale heterogeneous IoT systems. This paper introduces LLMind 2.0, a\ndistributed IoT automation framework that addresses the scalability challenges\nthrough lightweight LLM-empowered device agents via natural language-based\nmachine-to-machine (M2M) communication. Unlike previous LLM-controlled\nautomation systems that rely on a centralized coordinator to generate\ndevice-specific code to be executed on individual devices, LLMind 2.0\ndistributes intelligence across individual devices through lightweight LLMs\nembedded in IoT devices. The central coordinator translates human instructions\ninto simple subtasks described in natural human language, which are then\nprocessed by device-specific agents to generate device-specific code locally at\nthe associated devices. This approach transcends device heterogeneity barriers\nby using natural language as a unified communication medium, enabling seamless\ncollaboration between devices from different manufacturers. The system\nincorporates several key innovations: a Retrieval-Augmented Generation (RAG)\nmechanism for accurate subtask-to-API mapping, fine-tuned lightweight LLMs for\nreliable code generation, and a finite state machine-based task execution\nframework. Experimental validation in multi-robot warehouse scenarios and\nreal-world WiFi network deployments demonstrates significant improvements in\nscalability, reliability, and privacy protection compared to the centralized\napproach.",
      "url": "http://arxiv.org/abs/2508.13920v1",
      "published_time_eastern_timestamp": 1755616651.0
    },
    {
      "title": "Structured Agentic Workflows for Financial Time-Series Modeling with\n  LLMs and Reflective Feedback",
      "summary": "Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability.",
      "url": "http://arxiv.org/abs/2508.13915v1",
      "published_time_eastern_timestamp": 1755616489.0
    },
    {
      "title": "Translating the Force Concept Inventory in the age of AI",
      "summary": "We present a study that translates the Force Concept Inventory (FCI) using\nOpenAI GPT-4o and assess the specific difficulties of translating a\nscientific-focused topic using Large Language Models (LLMs). The FCI is a\nphysics exam meant to evaluate outcomes of a student cohort before and after\ninstruction in Newtonian physics. We examine the problem-solving ability of the\nLLM in both the translated document and the translation back into English,\ndetailing the language-dependent issues that complicate the translation. While\nChatGPT performs remarkably well on answering the questions in both the\ntranslated language as well as the back-translation into English, problems\narise with language-specific nuances and formatting. Pitfalls include words or\nphrases that lack one-to-one matching terms in another language, especially\ndiscipline-specific scientific terms, or outright mistranslations. Depending on\nthe context, these translations can result in a critical change in the physical\nmeaning of the problem. Additionally, issues with question numbering and\nlettering are found in some languages. The issues around the translations of\nnumbering and lettering provide insight into the abilities of the LLM and\nsuggest that it is not simply relying upon FCI questions that may have been\npart of the LLM training data to provide answers. These findings underscore\nthat while LLMs can accelerate multilingual access to educational tools,\ncareful review is still needed to ensure fidelity and clarity in translated\nassessments. LLMs provide a new opportunity to expand educational tools and\nassessments. At the same time, there are unique challenges using LLMs to\nfacilitate translations that this case study examines in detail.",
      "url": "http://arxiv.org/abs/2508.13908v1",
      "published_time_eastern_timestamp": 1755616041.0
    },
    {
      "title": "CARE: Contextual Adaptation of Recommenders for LLM-based Conversational\n  Recommendation",
      "summary": "We tackle the challenge of integrating large language models (LLMs) with\nexternal recommender systems to enhance domain expertise in conversational\nrecommendation (CRS). Current LLM-based CRS approaches primarily rely on zero-\nor few-shot methods for generating item recommendations based on user queries,\nbut this method faces two significant challenges: (1) without domain-specific\nadaptation, LLMs frequently recommend items not in the target item space,\nresulting in low recommendation accuracy; and (2) LLMs largely rely on dialogue\ncontext for content-based recommendations, neglecting the collaborative\nrelationships among entities or item sequences. To address these limitations,\nwe introduce the CARE (Contextual Adaptation of Recommenders) framework. CARE\ncustomizes LLMs for CRS tasks, and synergizes them with external recommendation\nsystems. CARE (a) integrates external recommender systems as domain experts,\nproducing recommendations through entity-level insights, and (b) enhances those\nrecommendations by leveraging contextual information for more accurate and\nunbiased final recommendations using LLMs. Our results demonstrate that\nincorporating external recommender systems with entity-level information\nsignificantly enhances recommendation accuracy of LLM-based CRS by an average\nof 54% and 25% for ReDial and INSPIRED datasets. The most effective strategy in\nthe CARE framework involves LLMs selecting and reranking candidate items that\nexternal recommenders provide based on contextual insights. Our analysis\nindicates that the CARE framework effectively addresses the identified\nchallenges and mitigates the popularity bias in the external recommender.",
      "url": "http://arxiv.org/abs/2508.13889v1",
      "published_time_eastern_timestamp": 1755615210.0
    },
    {
      "title": "Driving Style Recognition Like an Expert Using Semantic Privileged\n  Information from Large Language Models",
      "summary": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems.",
      "url": "http://arxiv.org/abs/2508.13881v1",
      "published_time_eastern_timestamp": 1755614631.0
    },
    {
      "title": "Improved Generalized Planning with LLMs through Strategy Refinement and\n  Reflection",
      "summary": "LLMs have recently been used to generate Python programs representing\ngeneralized plans in PDDL planning, i.e., plans that generalize across the\ntasks of a given PDDL domain. Previous work proposed a framework consisting of\nthree steps: the LLM first generates a summary and then a strategy for the\ndomain, both in natural language, and then implements that strategy as a Python\nprogram, that gets debugged on example planning tasks. In that work, only one\nstrategy is generated and passed directly to the program generation. If the\nstrategy is incorrect, its implementation will therefore result in an incorrect\ngeneralized plan. Here, we introduce an approach that generates the strategy in\nthe form of pseudocode and enables automatic debugging of the pseudocode, hence\nallowing us to identify and fix errors prior to the generation of the\ngeneralized plan itself. Additionally, we extend the Python debugging phase\nwith a reflection step prompting the LLM to pinpoint the reason for the\nobserved plan failure. Finally, we take inspiration from LLM code generation to\nproduce several program variants and pick the best one. Running experiments on\n17 benchmark domains, we show that these extensions substantially improve (and\nnever deteriorate) the quality of the generalized plans. In 12 of the domains,\nour best Python programs solve all tasks that can be generated with the\nrespective instance generator.",
      "url": "http://arxiv.org/abs/2508.13876v1",
      "published_time_eastern_timestamp": 1755614538.0
    },
    {
      "title": "The illusion of a perfect metric: Why evaluating AI's words is harder\n  than it looks",
      "summary": "Evaluating Natural Language Generation (NLG) is crucial for the practical\nadoption of AI, but has been a longstanding research challenge. While human\nevaluation is considered the de-facto standard, it is expensive and lacks\nscalability. Practical applications have driven the development of various\nautomatic evaluation metrics (AEM), designed to compare the model output with\nhuman-written references, generating a score which approximates human judgment.\nOver time, AEMs have evolved from simple lexical comparisons, to semantic\nsimilarity models and, more recently, to LLM-based evaluators. However, it\nseems that no single metric has emerged as a definitive solution, resulting in\nstudies using different ones without fully considering the implications. This\npaper aims to show this by conducting a thorough examination of the\nmethodologies of existing metrics, their documented strengths and limitations,\nvalidation methods, and correlations with human judgment. We identify several\nkey challenges: metrics often capture only specific aspects of text quality,\ntheir effectiveness varies by task and dataset, validation practices remain\nunstructured, and correlations with human judgment are inconsistent.\nImportantly, we find that these challenges persist in the most recent type of\nmetric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented\nGeneration (RAG), an increasingly relevant task in academia and industry. Our\nfindings challenge the quest for the 'perfect metric'. We propose selecting\nmetrics based on task-specific needs and leveraging complementary evaluations\nand advocate that new metrics should focus on enhanced validation\nmethodologies.",
      "url": "http://arxiv.org/abs/2508.13816v1",
      "published_time_eastern_timestamp": 1755609761.0
    },
    {
      "title": "Prompt-Based One-Shot Exact Length-Controlled Generation with LLMs",
      "summary": "Controlling the length of text produced by large language models (LLMs)\nremains challenging: models frequently overshoot or undershoot explicit length\ninstructions because they cannot reliably keep an internal token count. We\npresent a prompt-based, one-shot strategy that compels an off-the-shelf LLM to\ngenerate exactly a desired number of tokens - words (English) or characters\n(Chinese) - without any fine-tuning or iterative sampling. The prompt appends\ncountdown markers and explicit counting rules so that the model \"writes while\ncounting.\" We evaluate on four settings: open-ended generation (1-1000 tokens),\nXSUM summarization, MT-Bench-LI instruction following, and the LIFEBENCH\nequal-length track. On MT-Bench-LI, strict length compliance with GPT-4.1 leaps\nfrom below 30% under naive prompts to above 95% with our countdown prompt,\nsurpassing the popular draft-then-revise baseline, while judged answer quality\nis preserved. These results show that precise length control can be achieved\nthrough prompt engineering alone, offering a lightweight alternative to\ntraining- or decoding-based methods.",
      "url": "http://arxiv.org/abs/2508.13805v1",
      "published_time_eastern_timestamp": 1755609121.0
    }
  ]
}
{
  "last_updated": "2025-07-07T23:46:50.173955-04:00",
  "papers": [
    {
      "title": "Spatio-Temporal LLM: Reasoning about Environments and Actions",
      "summary": "Despite the significant recent progress of Multimodal Large Language Models\n(MLLMs), MLLMs still struggle to correctly answer prompts that require a\nholistic spatio-temporal understanding. Specifically, it is challenging to\naddress prompts that refer to 1) the entirety of an environment that an agent\nequipped with an MLLM can operate in; and simultaneously also refer to 2)\nrecent actions that just happened and are encoded in a video clip. However,\nsuch a holistic spatio-temporal understanding is important for agents operating\nin the real world. To address this issue, we first develop a framework to\ncollect a large-scale dataset. Using the collected \"Reasoning about\nEnvironments and Actions\" (REA) dataset, we show that recent methods indeed\nstruggle to correctly answer the prompts. To improve, we develop a\n\"spatio-temporal LLM\" (ST-LLM), a model equipped with projectors to improve\nboth spatial understanding of an environment and temporal understanding of\nrecent observations. On the collected REA data, we show that the proposed\nmethod significantly improves results compared to prior work. Code and data are\navailable at https://zoezheng126.github.io/STLLM-website/.",
      "url": "http://arxiv.org/abs/2507.05258v1",
      "published_time_eastern_timestamp": 1751911195.0
    },
    {
      "title": "Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions",
      "summary": "Recent benchmarks for Large Language Model (LLM) agents primarily focus on\nevaluating reasoning, planning, and execution capabilities, while another\ncritical component-memory, encompassing how agents memorize, update, and\nretrieve long-term information-is under-evaluated due to the lack of\nbenchmarks. We term agents with memory mechanisms as memory agents. In this\npaper, we identify four core competencies essential for memory agents: accurate\nretrieval, test-time learning, long-range understanding, and conflict\nresolution. Existing datasets either rely on limited context lengths or are\ntailored for static, long-context settings like book-based QA, which do not\nreflect the interactive, multi-turn nature of memory agents that incrementally\naccumulate information. Furthermore, no existing benchmarks cover all four\ncompetencies. Therefore, we introduce MemoryAgentBench, a new benchmark\nspecifically designed for memory agents. Our benchmark combines reformulated\nexisting datasets with newly constructed ones, covering the above four memory\ncompetencies, providing a systematic and challenging testbed for assessing\nmemory quality. We evaluate a diverse set of memory agents, ranging from simple\ncontext-based and retrieval-augmented generation (RAG) systems to advanced\nagents with external memory modules and tool integration. Empirical results\nreveal that current methods fall short of mastering all four competencies,\nunderscoring the need for further research into comprehensive memory mechanisms\nfor LLM agents.",
      "url": "http://arxiv.org/abs/2507.05257v1",
      "published_time_eastern_timestamp": 1751911194.0
    },
    {
      "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for\n  Visual Reasoning",
      "summary": "The remarkable reasoning capability of large language models (LLMs) stems\nfrom cognitive behaviors that emerge through reinforcement with verifiable\nrewards. This work investigates how to transfer this principle to Multimodal\nLLMs (MLLMs) to unlock advanced visual reasoning. We introduce a two-stage\nparadigm built on Qwen2.5-VL-7B: a massive linguistic cold-start fine-tuning,\nfollowed by multimodal reinforcement learning (RL) spanning nearly 1,000 steps,\nsurpassing all previous open-source efforts in scale. This pioneering work\nreveals three fundamental insights: 1) Behavior transfer emerges surprisingly\nearly in cold start due to linguistic mental imagery. 2) Cold start broadly\nmemorizes visual behaviors, while RL critically discerns and scales up\neffective patterns. 3) Transfer strategically favors high-utility behaviors\nsuch as visual reflection. Our resulting model, Open-Vision-Reasoner (OVR),\nachieves state-of-the-art performance on a suite of reasoning benchmarks,\nincluding 95.3% on MATH500, 51.8% on MathVision and 54.6% on MathVerse. We\nrelease our model, data, and training dynamics to catalyze the development of\nmore capable, behavior-aligned multimodal reasoners.",
      "url": "http://arxiv.org/abs/2507.05255v1",
      "published_time_eastern_timestamp": 1751911143.0
    },
    {
      "title": "Response Attack: Exploiting Contextual Priming to Jailbreak Large\n  Language Models",
      "summary": "Contextual priming, where earlier stimuli covertly bias later judgments,\noffers an unexplored attack surface for large language models (LLMs). We\nuncover a contextual priming vulnerability in which the previous response in\nthe dialogue can steer its subsequent behavior toward policy-violating content.\nBuilding on this insight, we propose Response Attack, which uses an auxiliary\nLLM to generate a mildly harmful response to a paraphrased version of the\noriginal malicious query. They are then formatted into the dialogue and\nfollowed by a succinct trigger prompt, thereby priming the target model to\ngenerate harmful content. Across eight open-source and proprietary LLMs, RA\nconsistently outperforms seven state-of-the-art jailbreak techniques, achieving\nhigher attack success rates. To mitigate this threat, we construct and release\na context-aware safety fine-tuning dataset, which significantly reduces the\nattack success rate while preserving model capabilities. The code and data are\navailable at https://github.com/Dtc7w3PQ/Response-Attack.",
      "url": "http://arxiv.org/abs/2507.05248v1",
      "published_time_eastern_timestamp": 1751910965.0
    },
    {
      "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
      "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
      "url": "http://arxiv.org/abs/2507.05240v1",
      "published_time_eastern_timestamp": 1751910581.0
    },
    {
      "title": "Cascade: Token-Sharded Private LLM Inference",
      "summary": "As LLMs continue to increase in parameter size, the computational resources\nrequired to run them are available to fewer parties. Therefore, third-party\ninference services -- where LLMs are hosted by third parties with significant\ncomputational resources -- are becoming increasingly popular. However, third\nparty inference raises critical concerns about user data privacy. To mitigate\nthese risks, privacy researchers have developed provably secure schemes for\nthird-party inference, such as Secure Multi-Party Computation (SMPC). However,\nSMPC protocols have significant computational and communication overhead, and\ndo not scale to large models. In this work, we propose a new multi-party\ninference protocol, Cascade, that avoids these punitive costs by leveraging\nsharding in the sequence dimension to maintain privacy, trading off\ncryptographic privacy guarantees for increased performance and scalability. We\ndemonstrate that Cascade is resistant to a generalization of a recent attack\nthat is highly effective against other statistical privacy schemes, and that it\nis further resistant to learning-based attacks. As Cascade is orders of\nmagnitude faster than existing schemes, our findings offer practical solutions\nfor secure deployment of modern state-of-the-art LLMs.",
      "url": "http://arxiv.org/abs/2507.05228v1",
      "published_time_eastern_timestamp": 1751909836.0
    },
    {
      "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
      "summary": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.",
      "url": "http://arxiv.org/abs/2507.05211v1",
      "published_time_eastern_timestamp": 1751908920.0
    },
    {
      "title": "In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code",
      "summary": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation.",
      "url": "http://arxiv.org/abs/2507.05200v1",
      "published_time_eastern_timestamp": 1751907677.0
    },
    {
      "title": "Train-before-Test Harmonizes Language Model Rankings",
      "summary": "Existing language model benchmarks provide contradictory model rankings, even\nfor benchmarks that aim to capture similar skills. This dilemma of conflicting\nrankings hampers model selection, clouds model comparisons, and adds confusion\nto a growing ecosystem of competing models. Recent work attributed ranking\ndisagreement to the phenomenon of training on the test task: As released,\ndifferent models exhibit a different level of preparation for any given test\ntask. A candidate solution to the problem is train-before-test: Give each model\nthe same benchmark-specific finetuning before evaluation. Our primary\ncontribution is a broad empirical evaluation of train-before-test across 24\nbenchmarks and 61 models. We show that train-before-test significantly improves\nranking agreement consistently across all benchmarks. Whereas rankings have\nlittle external validity to start with, they enjoy a significant degree of\nexternal validity when applying train-before-test: Model rankings transfer\ngracefully from one benchmark to the other. Even within the same model family,\ntrain-before-test reduces strong ranking disagreement to near-perfect\nagreement. In addition, train-before-test reduces the model-score matrix to\nessentially rank one, revealing new insights into the latent factors of\nbenchmark performance. Our work supports the recommendation to make\ntrain-before-test a default component of LLM benchmarking.",
      "url": "http://arxiv.org/abs/2507.05195v1",
      "published_time_eastern_timestamp": 1751907258.0
    },
    {
      "title": "From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating\n  Hindi News Veracity Explanations",
      "summary": "In an era of rampant misinformation, generating reliable news explanations is\nvital, especially for under-represented languages like Hindi. Lacking robust\nautomated tools, Hindi faces challenges in scaling misinformation detection. To\nbridge this gap, we propose a novel framework integrating Direct Preference\nOptimization (DPO) with curriculum learning to align machine-generated\nexplanations with human reasoning. Fact-checked explanations from credible\nsources serve as preferred responses, while LLM outputs highlight system\nlimitations and serve as non-preferred responses. To refine task-specific\nalignment, we introduce two key parameters -- Actuality and Finesse -- into the\nDPO loss function, enhancing explanation quality and consistency. Experiments\nwith LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's\neffectiveness in generating coherent, contextually relevant explanations. This\nscalable approach combats misinformation and extends automated explanation\ngeneration to low-resource languages.",
      "url": "http://arxiv.org/abs/2507.05179v1",
      "published_time_eastern_timestamp": 1751906068.0
    },
    {
      "title": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale",
      "summary": "Despite rapid progress in large language model (LLM)-based multi-agent\nsystems, current benchmarks fall short in evaluating their scalability,\nrobustness, and coordination capabilities in complex, dynamic, real-world\ntasks. Existing environments typically focus on small-scale, fully observable,\nor low-complexity domains, limiting their utility for developing and assessing\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\nan open-source benchmark designed to close this gap. Built atop the human-AI\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\nobservability, stochastic dynamics, and long-horizon planning objectives. The\nenvironment supports both low-level control and high-level natural language\ninteractions through modular Perception and Execution modules. We implement and\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\nuncovering significant performance gaps that highlight the unsolved challenges\nin large-scale coordination, communication, spatial reasoning, and long-horizon\nplanning under uncertainty. By providing more realistic complexity, scalable\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\ncritical foundation for advancing research in scalable multi-agent Agentic\nintelligence. All code, environments, data, and baselines will be released to\nsupport future research in this emerging domain.",
      "url": "http://arxiv.org/abs/2507.05178v1",
      "published_time_eastern_timestamp": 1751906022.0
    },
    {
      "title": "AI Generated Text Detection Using Instruction Fine-tuned Large Language\n  and Transformer-Based Models",
      "summary": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.",
      "url": "http://arxiv.org/abs/2507.05157v1",
      "published_time_eastern_timestamp": 1751904793.0
    },
    {
      "title": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization",
      "summary": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.",
      "url": "http://arxiv.org/abs/2507.05137v1",
      "published_time_eastern_timestamp": 1751903363.0
    },
    {
      "title": "SMART: Simulated Students Aligned with Item Response Theory for Question\n  Difficulty Prediction",
      "summary": "Item (question) difficulties play a crucial role in educational assessments,\nenabling accurate and efficient assessment of student abilities and\npersonalization to maximize learning outcomes. Traditionally, estimating item\ndifficulties can be costly, requiring real students to respond to items,\nfollowed by fitting an item response theory (IRT) model to get item difficulty\nestimates. This approach cannot be applied to the cold-start setting for\npreviously unseen items either. In this work, we present SMART (Simulated\nStudents Aligned with IRT), a novel method for aligning simulated students with\ninstructed ability, which can then be used in simulations to predict the\ndifficulty of open-ended items. We achieve this alignment using direct\npreference optimization (DPO), where we form preference pairs based on how\nlikely responses are under a ground-truth IRT model. We perform a simulation by\ngenerating thousands of responses, evaluating them with an LLM-based scoring\nmodel, and fit the resulting data to an IRT model to obtain item difficulty\nestimates. Through extensive experiments on a real-world student response\ndataset, we show that SMART outperforms other item difficulty prediction\nmethods by leveraging its improved ability alignment.",
      "url": "http://arxiv.org/abs/2507.05129v1",
      "published_time_eastern_timestamp": 1751902898.0
    },
    {
      "title": "An Evaluation of Large Language Models on Text Summarization Tasks Using\n  Prompt Engineering Techniques",
      "summary": "Large Language Models (LLMs) continue to advance natural language processing\nwith their ability to generate human-like text across a range of tasks. Despite\nthe remarkable success of LLMs in Natural Language Processing (NLP), their\nperformance in text summarization across various domains and datasets has not\nbeen comprehensively evaluated. At the same time, the ability to summarize text\neffectively without relying on extensive training data has become a crucial\nbottleneck. To address these issues, we present a systematic evaluation of six\nLLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),\nand ArXiv (scientific). By leveraging prompt engineering techniques including\nzero-shot and in-context learning, our study evaluates the performance using\nthe ROUGE and BERTScore metrics. In addition, a detailed analysis of inference\ntimes is conducted to better understand the trade-off between summarization\nquality and computational efficiency. For Long documents, introduce a\nsentence-based chunking strategy that enables LLMs with shorter context windows\nto summarize extended inputs in multiple stages. The findings reveal that while\nLLMs perform competitively on news and dialog tasks, their performance on long\nscientific documents improves significantly when aided by chunking strategies.\nIn addition, notable performance variations were observed based on model\nparameters, dataset properties, and prompt design. These results offer\nactionable insights into how different LLMs behave across task types,\ncontributing to ongoing research in efficient, instruction-based NLP systems.",
      "url": "http://arxiv.org/abs/2507.05123v1",
      "published_time_eastern_timestamp": 1751902445.0
    },
    {
      "title": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots",
      "summary": "In the field of robotics, researchers face a critical challenge in ensuring\nreliable and efficient task planning. Verifying high-level task plans before\nexecution significantly reduces errors and enhance the overall performance of\nthese systems. In this paper, we propose an architecture for automatically\nverifying high-level task plans before their execution in simulator or\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\nconsists of two key steps: first, the conversion of natural language\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\nanalysis of action sequences. The module uses the reasoning capabilities of the\nLLM to evaluate logical coherence and identify potential gaps in the plan.\nRigorous testing on datasets of varying complexity demonstrates the broad\napplicability of the module to household tasks. We contribute to improving the\nreliability and efficiency of task planning and addresses the critical need for\nrobust pre-execution verification in autonomous systems. The code is available\nat https://verifyllm.github.io.",
      "url": "http://arxiv.org/abs/2507.05118v1",
      "published_time_eastern_timestamp": 1751902296.0
    },
    {
      "title": "The Hidden Threat in Plain Text: Attacking RAG Data Loaders",
      "summary": "Large Language Models (LLMs) have transformed human-machine interaction since\nChatGPT's 2022 debut, with Retrieval-Augmented Generation (RAG) emerging as a\nkey framework that enhances LLM outputs by integrating external knowledge.\nHowever, RAG's reliance on ingesting external documents introduces new\nvulnerabilities. This paper exposes a critical security gap at the data loading\nstage, where malicious actors can stealthily corrupt RAG pipelines by\nexploiting document ingestion.\n  We propose a taxonomy of 9 knowledge-based poisoning attacks and introduce\ntwo novel threat vectors -- Content Obfuscation and Content Injection --\ntargeting common formats (DOCX, HTML, PDF). Using an automated toolkit\nimplementing 19 stealthy injection techniques, we test five popular data\nloaders, finding a 74.4% attack success rate across 357 scenarios. We further\nvalidate these threats on six end-to-end RAG systems -- including white-box\npipelines and black-box services like NotebookLM and OpenAI Assistants --\ndemonstrating high success rates and critical vulnerabilities that bypass\nfilters and silently compromise output integrity. Our results emphasize the\nurgent need to secure the document ingestion process in RAG systems against\ncovert content manipulations.",
      "url": "http://arxiv.org/abs/2507.05093v1",
      "published_time_eastern_timestamp": 1751901234.0
    },
    {
      "title": "ICAS: Detecting Training Data from Autoregressive Image Generative\n  Models",
      "summary": "Autoregressive image generation has witnessed rapid advancements, with\nprominent models such as scale-wise visual auto-regression pushing the\nboundaries of visual synthesis. However, these developments also raise\nsignificant concerns regarding data privacy and copyright. In response,\ntraining data detection has emerged as a critical task for identifying\nunauthorized data usage in model training. To better understand the\nvulnerability of autoregressive image generative models to such detection, we\nconduct the first study applying membership inference to this domain. Our\napproach comprises two key components: implicit classification and an adaptive\nscore aggregation strategy. First, we compute the implicit token-wise\nclassification score within the query image. Then we propose an adaptive score\naggregation strategy to acquire a final score, which places greater emphasis on\nthe tokens with lower scores. A higher final score indicates that the sample is\nmore likely to be involved in the training set. To validate the effectiveness\nof our method, we adapt existing detection algorithms originally designed for\nLLMs to visual autoregressive models. Extensive experiments demonstrate the\nsuperiority of our method in both class-conditional and text-to-image\nscenarios. Moreover, our approach exhibits strong robustness and generalization\nunder various data transformations. Furthermore, sufficient experiments suggest\ntwo novel key findings: (1) A linear scaling law on membership inference,\nexposing the vulnerability of large foundation models. (2) Training data from\nscale-wise visual autoregressive models is easier to detect than other\nautoregressive paradigms.Our code is available at\nhttps://github.com/Chrisqcwx/ImageAR-MIA.",
      "url": "http://arxiv.org/abs/2507.05068v1",
      "published_time_eastern_timestamp": 1751899842.0
    },
    {
      "title": "What Shapes User Trust in ChatGPT? A Mixed-Methods Study of User\n  Attributes, Trust Dimensions, Task Context, and Societal Perceptions among\n  University Students",
      "summary": "This mixed-methods inquiry examined four domains that shape university\nstudents' trust in ChatGPT: user attributes, seven delineated trust dimensions,\ntask context, and perceived societal impact. Data were collected through a\nsurvey of 115 UK undergraduate and postgraduate students and four complementary\nsemi-structured interviews. Behavioural engagement outweighed demographics:\nfrequent use increased trust, whereas self-reported understanding of\nlarge-language-model mechanics reduced it. Among the dimensions, perceived\nexpertise and ethical risk were the strongest predictors of overall trust; ease\nof use and transparency had secondary effects, while human-likeness and\nreputation were non-significant. Trust was highly task-contingent; highest for\ncoding and summarising, lowest for entertainment and citation generation, yet\nconfidence in ChatGPT's referencing ability, despite known inaccuracies, was\nthe single strongest correlate of global trust, indicating automation bias.\nComputer-science students surpassed peers only in trusting the system for\nproofreading and writing, suggesting technical expertise refines rather than\ninflates reliance. Finally, students who viewed AI's societal impact positively\nreported the greatest trust, whereas mixed or negative outlooks dampened\nconfidence. These findings show that trust in ChatGPT hinges on task\nverifiability, perceived competence, ethical alignment and direct experience,\nand they underscore the need for transparency, accuracy cues and user education\nwhen deploying LLMs in academic settings.",
      "url": "http://arxiv.org/abs/2507.05046v1",
      "published_time_eastern_timestamp": 1751898594.0
    },
    {
      "title": "MoLink: Distributed and Efficient Serving Framework for Large Models",
      "summary": "Large language models represent a groundbreaking shift in generative AI. Yet,\nthese advances come with a significant challenge: the high cost of model\nserving. To mitigate these costs, consumer-grade GPUs emerge as a more\naffordable alternative. This presents an opportunity for more cost-efficient\nLLM serving by leveraging these GPUs.\n  However, it is non-trivial to achieve high-efficiency LLM serving on\nconsumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often\ndeployed in limited network conditions; 2) these GPUs often exhibit\nheterogeneity in host systems. To address these challenges, we present MoLink,\na distributed LLM serving system for large models. It incorporates several key\ntechniques, enabling efficient LLM serving on heterogeneous and weakly\nconnected consumer-grade GPUs. Our experiments demonstrate that it achieves\nthroughput improvements of up to 458\\% and cost-profit margin improvements of\nup to 151\\%, compared to state-of-the-art systems. MoLink allows users on\nWindows, Linux, and containerized VMs to seamlessly integrate GPUs with just a\nfew lines of code over Ethernet or public networks. Currently, it supports 18\nmainstream architectures of open-source large language models.",
      "url": "http://arxiv.org/abs/2507.05043v1",
      "published_time_eastern_timestamp": 1751898476.0
    }
  ]
}
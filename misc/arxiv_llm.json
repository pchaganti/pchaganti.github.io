{
  "last_updated": "2025-10-06T20:54:02.138838-04:00",
  "papers": [
    {
      "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention\n  Alignment",
      "summary": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.",
      "url": "http://arxiv.org/abs/2510.03223v1",
      "published_time_eastern_timestamp": 1759514193.0
    },
    {
      "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic\n  Program Repair",
      "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex,\nrepository-level bugs in industry, but ultimately agent-generated patches still\nneed to be reviewed by a human before committing them to ensure they address\nthe bug. Showing unlikely patches to developers can lead to substantial noise,\nwasting valuable developer time and eroding trust in automated code changes. We\nintroduce two complementary LLM-based policies to reduce such noise: bug\nabstention and patch validation policies. Bug abstention excludes bugs that the\nagentic APR system is unlikely to fix. Patch validation rejects patches that\nare unlikely to be a good fix for the given bug. We evaluate both policies on\nthree sets of bugs from Google's codebase, and their candidate patches\ngenerated by an internal agentic APR system. On a set of 174 human-reported\nbugs, removing bugs and patch trajectories rejected by our policies can raise\nsuccess rates by up to 13 percentage points and 15 percentage points,\nrespectively, and by up to 39 percentage points in combination. On null pointer\nexceptions and sanitizer-reported bugs with machine-generated bug reports,\npatch validation also improves average single-sample success rates. This\ntwo-policy approach provides a practical path to the reliable, industrial-scale\ndeployment of agentic APR systems.",
      "url": "http://arxiv.org/abs/2510.03217v1",
      "published_time_eastern_timestamp": 1759514008.0
    },
    {
      "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language\n  Models",
      "summary": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
      "url": "http://arxiv.org/abs/2510.03215v1",
      "published_time_eastern_timestamp": 1759513952.0
    },
    {
      "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of\n  Web Agents",
      "summary": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.",
      "url": "http://arxiv.org/abs/2510.03204v1",
      "published_time_eastern_timestamp": 1759513290.0
    },
    {
      "title": "Best-of-Majority: Minimax-Optimal Strategy for Pass@$k$ Inference\n  Scaling",
      "summary": "LLM inference often generates a batch of candidates for a prompt and selects\none via strategies like majority voting or Best-of- N (BoN). For difficult\ntasks, this single-shot selection often underperforms. Consequently,\nevaluations commonly report Pass@$k$: the agent may submit up to $k$ responses,\nand only the best of them is used when computing regret. Motivated by this, we\nstudy inference scaling in the more general Pass@$k$ inference setting, and\nprove that neither majority voting nor BoN exhibits the desirable scaling with\n$k$ and the sampling budget $N$. Combining the advantages of majority voting\nand BoN, we propose a new inference strategy called Best-of-Majority (BoM),\nwith a pivotal step that restricts the candidates to the responses with high\nfrequency in the $N$ samples before selecting the top-$k$ rewards. We prove\nthat when the sampling budget is $N=\\tilde\\Omega(C^*)$, the regret of BoM is\n$O(\\epsilon_{\\mathrm{opt}}+\\sqrt{\\epsilon_{\\mathrm{RM}}^2C^*/k})$, where $C^*$\nis the coverage coefficient, $\\epsilon_{\\mathrm{RM}}$ is the estimation error\nof the reward model, and $\\epsilon_{\\mathrm{opt}}$ is the estimation error of\nreward at the optimal response. We further establish a matching lower bound,\ncertifying that our algorithm is minimax optimal. Beyond optimality, BoM has a\nkey advantage: unlike majority voting and BoN, its performance does not degrade\nwhen increasing $N$. Experimental results of inference on math problems show\nBoM outperforming both majority voting and BoN.",
      "url": "http://arxiv.org/abs/2510.03199v1",
      "published_time_eastern_timestamp": 1759512945.0
    },
    {
      "title": "Can LLMs Hit Moving Targets? Tracking Evolving Signals in Corporate\n  Disclosures",
      "summary": "Moving targets -- managers' strategic shifting of key performance metrics\nwhen the original targets become difficult to achieve -- have been shown to\npredict subsequent stock underperformance. However, our work reveals that the\nmethod employed in that study exhibits two key limitations that hinder the\naccuracy -- noise in the extracted targets and loss of contextual information\n-- both of which stem primarily from the use of a named entity recognition\n(NER). To address these two limitations, we propose an LLM-based target\nextraction} method with a newly defined metric that better captures semantic\ncontext. This approach preserves semantic context beyond simple entity\nrecognition and yields consistently higher predictive power than the original\napproach. Overall, our approach enhances the granularity and accuracy of\nfinancial text-based performance prediction.",
      "url": "http://arxiv.org/abs/2510.03195v1",
      "published_time_eastern_timestamp": 1759512656.0
    },
    {
      "title": "CoDA: Agentic Systems for Collaborative Data Visualization",
      "summary": "Deep research has revolutionized data analysis, yet data scientists still\ndevote substantial time to manually crafting visualizations, highlighting the\nneed for robust automation from natural language queries. However, current\nsystems struggle with complex datasets containing multiple files and iterative\nrefinement. Existing approaches, including simple single- or multi-agent\nsystems, often oversimplify the task, focusing on initial query parsing while\nfailing to robustly manage data complexity, code errors, or final visualization\nquality. In this paper, we reframe this challenge as a collaborative\nmulti-agent problem. We introduce CoDA, a multi-agent system that employs\nspecialized LLM agents for metadata analysis, task planning, code generation,\nand self-reflection. We formalize this pipeline, demonstrating how\nmetadata-focused analysis bypasses token limits and quality-driven refinement\nensures robustness. Extensive evaluations show CoDA achieves substantial gains\nin the overall score, outperforming competitive baselines by up to 41.5%. This\nwork demonstrates that the future of visualization automation lies not in\nisolated code generation but in integrated, collaborative agentic workflows.",
      "url": "http://arxiv.org/abs/2510.03194v1",
      "published_time_eastern_timestamp": 1759512616.0
    },
    {
      "title": "PRISM-Physics: Causal DAG-Based Process Evaluation for Physics Reasoning",
      "summary": "Benchmarks for competition-style reasoning have advanced evaluation in\nmathematics and programming, yet physics remains comparatively explored. Most\nexisting physics benchmarks evaluate only final answers, which fail to capture\nreasoning processes, while recent stepwise methods rely on heuristic\nLLM-as-judge scoring or restrictive linear assumptions, limiting reliability\nand diagnostic validity. We introduce PRISM-Physics, a process-level evaluation\nframework and benchmark for complex physics reasoning problems. Solutions are\nrepresented as directed acyclic graphs (DAGs) of formulas, explicitly encoding\ncausal dependencies among intermediate steps to enable fine-grained,\ninterpretable, and theoretically grounded scoring. We prove the optimality of\nthe DAG representation and the corresponding scoring policy. Combining with a\nfully rule-based method for symbolic formula equivalence matching that we\ndeveloped, we ensure consistent validation across diverse formulations without\nheuristic judgments. Results show that our evaluation framework is more aligned\nwith human experts' scoring. Experiments on state-of-the-art LLMs reveal\npersistent reasoning failures in physics, while step-level scoring offers both\ndiagnostic insight and rich signals for later training. By combining structural\nrigor, theoretical guarantees, and symbolic validation, PRISM-Physics provides\na principled foundation for advancing process-level evaluation and guiding the\ndevelopment of models with deeper scientific reasoning capabilities.",
      "url": "http://arxiv.org/abs/2510.03185v1",
      "published_time_eastern_timestamp": 1759511343.0
    },
    {
      "title": "When Names Disappear: Revealing What LLMs Actually Understand About Code",
      "summary": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization.",
      "url": "http://arxiv.org/abs/2510.03178v1",
      "published_time_eastern_timestamp": 1759510393.0
    },
    {
      "title": "Topic Modeling as Long-Form Generation: Can Long-Context LLMs\n  revolutionize NTM via Zero-Shot Prompting?",
      "summary": "Traditional topic models such as neural topic models rely on inference and\ngeneration networks to learn latent topic distributions. This paper explores a\nnew paradigm for topic modeling in the era of large language models, framing TM\nas a long-form generation task whose definition is updated in this paradigm. We\npropose a simple but practical approach to implement LLM-based topic model\ntasks out of the box (sample a data subset, generate topics and representative\ntext with our prompt, text assignment with keyword match). We then investigate\nwhether the long-form generation paradigm can beat NTMs via zero-shot\nprompting. We conduct a systematic comparison between NTMs and LLMs in terms of\ntopic quality and empirically examine the claim that \"a majority of NTMs are\noutdated.\"",
      "url": "http://arxiv.org/abs/2510.03174v1",
      "published_time_eastern_timestamp": 1759510112.0
    },
    {
      "title": "SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the\n  SpineMed-450k Corpus",
      "summary": "Spine disorders affect 619 million people globally and are a leading cause of\ndisability, yet AI-assisted diagnosis remains limited by the lack of\nlevel-aware, multimodal datasets. Clinical decision-making for spine disorders\nrequires sophisticated reasoning across X-ray, CT, and MRI at specific\nvertebral levels. However, progress has been constrained by the absence of\ntraceable, clinically-grounded instruction data and standardized,\nspine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem\nco-designed with practicing spine surgeons. It features SpineMed-450k, the\nfirst large-scale dataset explicitly designed for vertebral-level reasoning\nacross imaging modalities with over 450,000 instruction instances, and\nSpineBench, a clinically-grounded evaluation framework. SpineMed-450k is\ncurated from diverse sources, including textbooks, guidelines, open datasets,\nand ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline\nwith a two-stage LLM generation method (draft and revision) to ensure\nhigh-quality, traceable data for question-answering, multi-turn consultations,\nand report generation. SpineBench evaluates models on clinically salient axes,\nincluding level identification, pathology assessment, and surgical planning.\nOur comprehensive evaluation of several recently advanced large vision-language\nmodels (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,\nlevel-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k\ndemonstrates consistent and significant improvements across all tasks.\nClinician assessments confirm the diagnostic clarity and practical utility of\nour model's outputs.",
      "url": "http://arxiv.org/abs/2510.03160v1",
      "published_time_eastern_timestamp": 1759509122.0
    },
    {
      "title": "Improving Cooperation in Collaborative Embodied AI",
      "summary": "The integration of Large Language Models (LLMs) into multiagent systems has\nopened new possibilities for collaborative reasoning and cooperation with AI\nagents. This paper explores different prompting methods and evaluates their\neffectiveness in enhancing agent collaborative behaviour and decision-making.\nWe enhance CoELA, a framework designed for building Collaborative Embodied\nAgents that leverage LLMs for multi-agent communication, reasoning, and task\ncoordination in shared virtual spaces. Through systematic experimentation, we\nexamine different LLMs and prompt engineering strategies to identify optimised\ncombinations that maximise collaboration performance. Furthermore, we extend\nour research by integrating speech capabilities, enabling seamless\ncollaborative voice-based interactions. Our findings highlight the\neffectiveness of prompt optimisation in enhancing collaborative agent\nperformance; for example, our best combination improved the efficiency of the\nsystem running with Gemma3 by 22% compared to the original CoELA system. In\naddition, the speech integration provides a more engaging user interface for\niterative system development and demonstrations.",
      "url": "http://arxiv.org/abs/2510.03153v1",
      "published_time_eastern_timestamp": 1759508748.0
    },
    {
      "title": "Beyond the Final Layer: Intermediate Representations for Better\n  Multilingual Calibration in Large Language Models",
      "summary": "Confidence calibration, the alignment of a model's predicted confidence with\nits actual accuracy, is crucial for the reliable deployment of Large Language\nModels (LLMs). However, this critical property remains largely under-explored\nin multilingual contexts. In this work, we conduct the first large-scale,\nsystematic studies of multilingual calibration across six model families and\nover 100 languages, revealing that non-English languages suffer from\nsystematically worse calibration. To diagnose this, we investigate the model's\ninternal representations and find that the final layer, biased by\nEnglish-centric training, provides a poor signal for multilingual confidence.\nIn contrast, our layer-wise analysis uncovers a key insight that\nlate-intermediate layers consistently offer a more reliable and\nbetter-calibrated signal. Building on this, we introduce a suite of\ntraining-free methods, including Language-Aware Confidence Ensemble (LACE),\nwhich adaptively selects an optimal ensemble of layers for each specific\nlanguage. Our study highlights the hidden costs of English-centric alignment\nand offer a new path toward building more globally equitable and trustworthy\nLLMs by looking beyond the final layer.",
      "url": "http://arxiv.org/abs/2510.03136v1",
      "published_time_eastern_timestamp": 1759507635.0
    },
    {
      "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
      "summary": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
      "url": "http://arxiv.org/abs/2510.03120v1",
      "published_time_eastern_timestamp": 1759506549.0
    },
    {
      "title": "Semantic Similarity in Radiology Reports via LLMs and NER",
      "summary": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at:\n\\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\\_reports}",
      "url": "http://arxiv.org/abs/2510.03102v1",
      "published_time_eastern_timestamp": 1759505471.0
    },
    {
      "title": "Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better\n  Scaling than CoT Prompting?",
      "summary": "Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based\nmodels, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,\nwhere the model is guided to first transcribe the speech and then translate it.\nCoT typically outperforms direct prompting primarily because it can exploit\nabundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)\ndatasets to explicitly model its steps. In this paper, we systematically\ncompare CoT and Direct prompting under increasing amounts of S2TT data. To this\nend, we pseudo-label an ASR corpus by translating its transcriptions into six\nEuropean languages, and train LLM-based S2TT systems with both prompting\nstrategies at different data scales. Our results show that Direct improves more\nconsistently as the amount of data increases, suggesting that it may become a\nmore effective approach as larger S2TT resources are created.",
      "url": "http://arxiv.org/abs/2510.03093v1",
      "published_time_eastern_timestamp": 1759505012.0
    },
    {
      "title": "Investigating The Smells of LLM Generated Code",
      "summary": "Context: Large Language Models (LLMs) are increasingly being used to generate\nprogram code. Much research has been reported on the functional correctness of\ngenerated code, but there is far less on code quality.\n  Objectives: In this study, we propose a scenario-based method of evaluating\nthe quality of LLM-generated code to identify the weakest scenarios in which\nthe quality of LLM generated code should be improved.\n  Methods: The method measures code smells, an important indicator of code\nquality, and compares them with a baseline formed from reference solutions of\nprofessionally written code. The test dataset is divided into various subsets\naccording to the topics of the code and complexity of the coding tasks to\nrepresent different scenarios of using LLMs for code generation. We will also\npresent an automated test system for this purpose and report experiments with\nthe Java programs generated in response to prompts given to four\nstate-of-the-art LLMs: Gemini Pro, ChatGPT, Codex, and Falcon.\n  Results: We find that LLM-generated code has a higher incidence of code\nsmells compared to reference solutions. Falcon performed the least badly, with\na smell increase of 42.28%, followed by Gemini Pro (62.07%), ChatGPT (65.05%)\nand finally Codex (84.97%). The average smell increase across all LLMs was\n63.34%, comprising 73.35% for implementation smells and 21.42% for design\nsmells. We also found that the increase in code smells is greater for more\ncomplex coding tasks and for more advanced topics, such as those involving\nobject-orientated concepts.\n  Conclusion: In terms of code smells, LLM's performances on various coding\ntask complexities and topics are highly correlated to the quality of human\nwritten code in the corresponding scenarios. However, the quality of LLM\ngenerated code is noticeably poorer than human written code.",
      "url": "http://arxiv.org/abs/2510.03029v1",
      "published_time_eastern_timestamp": 1759500595.0
    },
    {
      "title": "Untargeted Jailbreak Attack",
      "summary": "Existing gradient-based jailbreak attacks on Large Language Models (LLMs),\nsuch as Greedy Coordinate Gradient (GCG) and COLD-Attack, typically optimize\nadversarial suffixes to align the LLM output with a predefined target response.\nHowever, by restricting the optimization objective as inducing a predefined\ntarget, these methods inherently constrain the adversarial search space, which\nlimit their overall attack efficacy. Furthermore, existing methods typically\nrequire a large number of optimization iterations to fulfill the large gap\nbetween the fixed target and the original model response, resulting in low\nattack efficiency.\n  To overcome the limitations of targeted jailbreak attacks, we propose the\nfirst gradient-based untargeted jailbreak attack (UJA), aiming to elicit an\nunsafe response without enforcing any predefined patterns. Specifically, we\nformulate an untargeted attack objective to maximize the unsafety probability\nof the LLM response, which can be quantified using a judge model. Since the\nobjective is non-differentiable, we further decompose it into two\ndifferentiable sub-objectives for optimizing an optimal harmful response and\nthe corresponding adversarial prompt, with a theoretical analysis to validate\nthe decomposition. In contrast to targeted jailbreak attacks, UJA's\nunrestricted objective significantly expands the search space, enabling a more\nflexible and efficient exploration of LLM vulnerabilities.Extensive evaluations\ndemonstrate that \\textsc{UJA} can achieve over 80\\% attack success rates\nagainst recent safety-aligned LLMs with only 100 optimization iterations,\noutperforming the state-of-the-art gradient-based attacks such as I-GCG and\nCOLD-Attack by over 20\\%.",
      "url": "http://arxiv.org/abs/2510.02999v1",
      "published_time_eastern_timestamp": 1759498736.0
    },
    {
      "title": "AudioToolAgent: An Agentic Framework for Audio-Language Models",
      "summary": "Large Audio-Language Models (LALMs) perform well on audio understanding tasks\nbut lack multi-step reasoning and tool-calling found in recent Large Language\nModels (LLMs). This paper presents AudioToolAgent, a framework that coordinates\naudio-language models as tools via a central LLM agent that accesses tool\nadapters for audio question answering and speech-to-text. The agent selects\ntools, asks follow-up questions, and compares outputs for verification.\nExperiments with MMAU, MMAR, and MMAU-Pro show state-of-the-art accuracy: up to\n74.10% on MMAU, 68.80% on MMAR, and 57.96% on MMAU-Pro. Monte Carlo sampling\nfor shapley values across 374 configurations identifies effective agent-tool\ncombinations. The modular design allows integration of new tools and eliminates\nthe use of data and training costs. Code and reproduction materials are\navailable at: github.com/GLJS/AudioToolAgent",
      "url": "http://arxiv.org/abs/2510.02995v1",
      "published_time_eastern_timestamp": 1759498545.0
    },
    {
      "title": "TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via\n  Text-to-Image-to-Text Consistency",
      "summary": "With the rapid advancement of large multimodal models (LMMs), recent\ntext-to-image (T2I) models can generate high-quality images and demonstrate\ngreat alignment to short prompts. However, they still struggle to effectively\nunderstand and follow long and detailed prompts, displaying inconsistent\ngeneration. To address this challenge, we introduce LPG-Bench, a comprehensive\nbenchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench\nfeatures 200 meticulously crafted prompts with an average length of over 250\nwords, approaching the input capacity of several leading commercial models.\nUsing these prompts, we generate 2,600 images from 13 state-of-the-art models\nand further perform comprehensive human-ranked annotations. Based on LPG-Bench,\nwe observe that state-of-the-art T2I alignment evaluation metrics exhibit poor\nconsistency with human preferences on long-prompt-based image generation. To\naddress the gap, we introduce a novel zero-shot metric based on\ntext-to-image-to-text consistency, termed TIT, for evaluating\nlong-prompt-generated images. The core concept of TIT is to quantify T2I\nalignment by directly comparing the consistency between the raw prompt and the\nLMM-produced description on the generated image, which includes an efficient\nscore-based instantiation TIT-Score and a large-language-model (LLM) based\ninstantiation TIT-Score-LLM. Extensive experiments demonstrate that our\nframework achieves superior alignment with human judgment compared to\nCLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute\nimprovement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT\nmethods together offer a deeper perspective to benchmark and foster the\ndevelopment of T2I models. All resources will be made publicly available.",
      "url": "http://arxiv.org/abs/2510.02987v1",
      "published_time_eastern_timestamp": 1759497916.0
    }
  ]
}
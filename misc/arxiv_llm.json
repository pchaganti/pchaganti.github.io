{
  "last_updated": "2025-05-09T11:12:06.038630-04:00",
  "papers": [
    {
      "title": "StreamBridge: Turning Your Offline Video Large Language Model into a\n  Proactive Streaming Assistant",
      "summary": "We present StreamBridge, a simple yet effective framework that seamlessly\ntransforms offline Video-LLMs into streaming-capable models. It addresses two\nfundamental challenges in adapting existing models into online scenarios: (1)\nlimited capability for multi-turn real-time understanding, and (2) lack of\nproactive response mechanisms. Specifically, StreamBridge incorporates (1) a\nmemory buffer combined with a round-decayed compression strategy, supporting\nlong-context multi-turn interactions, and (2) a decoupled, lightweight\nactivation model that can be effortlessly integrated into existing Video-LLMs,\nenabling continuous proactive responses. To further support StreamBridge, we\nconstruct Stream-IT, a large-scale dataset tailored for streaming video\nunderstanding, featuring interleaved video-text sequences and diverse\ninstruction formats. Extensive experiments show that StreamBridge significantly\nimproves the streaming understanding capabilities of offline Video-LLMs across\nvarious tasks, outperforming even proprietary models such as GPT-4o and Gemini\n1.5 Pro. Simultaneously, it achieves competitive or superior performance on\nstandard video understanding benchmarks.",
      "url": "http://arxiv.org/abs/2505.05467v1",
      "published_time_eastern_timestamp": 1746727060.0
    },
    {
      "title": "ComPO: Preference Alignment via Comparison Oracles",
      "summary": "Direct alignment methods are increasingly used for aligning large language\nmodels (LLMs) with human preferences. However, these methods suffer from the\nissues of verbosity and likelihood displacement, which can be driven by the\nnoisy preference pairs that induce similar likelihood for preferred and\ndispreferred responses. The contributions of this paper are two-fold. First, we\npropose a new preference alignment method based on comparison oracles and\nprovide the convergence guarantee for its basic scheme. Second, we improve our\nmethod using some heuristics and conduct the experiments to demonstrate the\nflexibility and compatibility of practical scheme in improving the performance\nof LLMs using noisy preference pairs. Evaluations are conducted across multiple\nbase and instruction-tuned models (Mistral-7B, Llama-3-8B and Gemma-2-9B) with\nbenchmarks (AlpacaEval 2, MT-Bench and Arena-Hard). Experimental results show\nthe effectiveness of our method as an alternative to addressing the limitations\nof existing direct alignment methods. A highlight of our work is that we\nevidence the importance of designing specialized methods for preference pairs\nwith distinct likelihood margin, which complements the recent findings in\n\\citet{Razin-2025-Unintentional}.",
      "url": "http://arxiv.org/abs/2505.05465v1",
      "published_time_eastern_timestamp": 1746727017.0
    },
    {
      "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging",
      "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.",
      "url": "http://arxiv.org/abs/2505.05464v1",
      "published_time_eastern_timestamp": 1746726983.0
    },
    {
      "title": "Conversational Process Model Redesign",
      "summary": "With the recent success of large language models (LLMs), the idea of\nAI-augmented Business Process Management systems is becoming more feasible. One\nof their essential characteristics is the ability to be conversationally\nactionable, allowing humans to interact with the LLM effectively to perform\ncrucial process life cycle tasks such as process model design and redesign.\nHowever, most current research focuses on single-prompt execution and\nevaluation of results, rather than on continuous interaction between the user\nand the LLM. In this work, we aim to explore the feasibility of using LLMs to\nempower domain experts in the creation and redesign of process models in an\niterative and effective way. The proposed conversational process model redesign\n(CPD) approach receives as input a process model and a redesign request by the\nuser in natural language. Instead of just letting the LLM make changes, the LLM\nis employed to (a) identify process change patterns from literature, (b)\nre-phrase the change request to be aligned with an expected wording for the\nidentified pattern (i.e., the meaning), and then to (c) apply the meaning of\nthe change to the process model. This multi-step approach allows for\nexplainable and reproducible changes. In order to ensure the feasibility of the\nCPD approach, and to find out how well the patterns from literature can be\nhandled by the LLM, we performed an extensive evaluation. The results show that\nsome patterns are hard to understand by LLMs and by users. Within the scope of\nthe study, we demonstrated that users need support to describe the changes\nclearly. Overall the evaluation shows that the LLMs can handle most changes\nwell according to a set of completeness and correctness criteria.",
      "url": "http://arxiv.org/abs/2505.05453v1",
      "published_time_eastern_timestamp": 1746726285.0
    },
    {
      "title": "clem:todd: A Framework for the Systematic Benchmarking of LLM-Based\n  Task-Oriented Dialogue System Realisations",
      "summary": "The emergence of instruction-tuned large language models (LLMs) has advanced\nthe field of dialogue systems, enabling both realistic user simulations and\nrobust multi-turn conversational agents. However, existing research often\nevaluates these components in isolation-either focusing on a single user\nsimulator or a specific system design-limiting the generalisability of insights\nacross architectures and configurations. In this work, we propose clem todd\n(chat-optimized LLMs for task-oriented dialogue systems development), a\nflexible framework for systematically evaluating dialogue systems under\nconsistent conditions. clem todd enables detailed benchmarking across\ncombinations of user simulators and dialogue systems, whether existing models\nfrom literature or newly developed ones. It supports plug-and-play integration\nand ensures uniform datasets, evaluation metrics, and computational\nconstraints. We showcase clem todd's flexibility by re-evaluating existing\ntask-oriented dialogue systems within this unified setup and integrating three\nnewly proposed dialogue systems into the same evaluation pipeline. Our results\nprovide actionable insights into how architecture, scale, and prompting\nstrategies affect dialogue performance, offering practical guidance for\nbuilding efficient and effective conversational AI systems.",
      "url": "http://arxiv.org/abs/2505.05445v1",
      "published_time_eastern_timestamp": 1746725796.0
    },
    {
      "title": "GesPrompt: Leveraging Co-Speech Gestures to Augment LLM-Based\n  Interaction in Virtual Reality",
      "summary": "Large Language Model (LLM)-based copilots have shown great potential in\nExtended Reality (XR) applications. However, the user faces challenges when\ndescribing the 3D environments to the copilots due to the complexity of\nconveying spatial-temporal information through text or speech alone. To address\nthis, we introduce GesPrompt, a multimodal XR interface that combines co-speech\ngestures with speech, allowing end-users to communicate more naturally and\naccurately with LLM-based copilots in XR environments. By incorporating\ngestures, GesPrompt extracts spatial-temporal reference from co-speech\ngestures, reducing the need for precise textual prompts and minimizing\ncognitive load for end-users. Our contributions include (1) a workflow to\nintegrate gesture and speech input in the XR environment, (2) a prototype VR\nsystem that implements the workflow, and (3) a user study demonstrating its\neffectiveness in improving user communication in VR environments.",
      "url": "http://arxiv.org/abs/2505.05441v1",
      "published_time_eastern_timestamp": 1746725488.0
    },
    {
      "title": "EcoAgent: An Efficient Edge-Cloud Collaborative Multi-Agent Framework\n  for Mobile Automation",
      "summary": "Cloud-based mobile agents powered by (multimodal) large language models\n((M)LLMs) offer strong reasoning abilities but suffer from high latency and\ncost. While fine-tuned (M)SLMs enable edge deployment, they often lose general\ncapabilities and struggle with complex tasks. To address this, we propose\nEcoAgent, an Edge-Cloud cOllaborative multi-agent framework for mobile\nautomation. EcoAgent features a closed-loop collaboration among a cloud-based\nPlanning Agent and two edge-based agents: the Execution Agent for action\nexecution and the Observation Agent for verifying outcomes. The Observation\nAgent uses a Pre-Understanding Module to compress screen images into concise\ntext, reducing token usage. In case of failure, the Planning Agent retrieves\nscreen history and replans via a Reflection Module. Experiments on AndroidWorld\nshow that EcoAgent maintains high task success rates while significantly\nreducing MLLM token consumption, enabling efficient and practical mobile\nautomation.",
      "url": "http://arxiv.org/abs/2505.05440v1",
      "published_time_eastern_timestamp": 1746725480.0
    },
    {
      "title": "Ultra-FineWeb: Efficient Data Filtering and Verification for\n  High-Quality LLM Training Data",
      "summary": "Data quality has become a key factor in enhancing model performance with the\nrapid development of large language models (LLMs). Model-driven data filtering\nhas increasingly become a primary approach for acquiring high-quality data.\nHowever, it still faces two main challenges: (1) the lack of an efficient data\nverification strategy makes it difficult to provide timely feedback on data\nquality; and (2) the selection of seed data for training classifiers lacks\nclear criteria and relies heavily on human expertise, introducing a degree of\nsubjectivity. To address the first challenge, we introduce an efficient\nverification strategy that enables rapid evaluation of the impact of data on\nLLM training with minimal computational cost. To tackle the second challenge,\nwe build upon the assumption that high-quality seed data is beneficial for LLM\ntraining, and by integrating the proposed verification strategy, we optimize\nthe selection of positive and negative samples and propose an efficient data\nfiltering pipeline. This pipeline not only improves filtering efficiency,\nclassifier quality, and robustness, but also significantly reduces experimental\nand inference costs. In addition, to efficiently filter high-quality data, we\nemploy a lightweight classifier based on fastText, and successfully apply the\nfiltering pipeline to two widely-used pre-training corpora, FineWeb and Chinese\nFineWeb datasets, resulting in the creation of the higher-quality Ultra-FineWeb\ndataset. Ultra-FineWeb contains approximately 1 trillion English tokens and 120\nbillion Chinese tokens. Empirical results demonstrate that the LLMs trained on\nUltra-FineWeb exhibit significant performance improvements across multiple\nbenchmark tasks, validating the effectiveness of our pipeline in enhancing both\ndata quality and training efficiency.",
      "url": "http://arxiv.org/abs/2505.05427v1",
      "published_time_eastern_timestamp": 1746724520.0
    },
    {
      "title": "TransProQA: an LLM-based literary Translation evaluation metric with\n  Professional Question Answering",
      "summary": "The impact of Large Language Models (LLMs) has extended into literary\ndomains. However, existing evaluation metrics prioritize mechanical accuracy\nover artistic expression and tend to overrate machine translation (MT) as being\nsuperior to experienced professional human translation. In the long run, this\nbias could result in a permanent decline in translation quality and cultural\nauthenticity. In response to the urgent need for a specialized literary\nevaluation metric, we introduce TransProQA, a novel, reference-free, LLM-based\nquestion-answering (QA) framework designed specifically for literary\ntranslation evaluation. TransProQA uniquely integrates insights from\nprofessional literary translators and researchers, focusing on critical\nelements in literary quality assessment such as literary devices, cultural\nunderstanding, and authorial voice. Our extensive evaluation shows that while\nliterary-finetuned XCOMET-XL yields marginal gains, TransProQA substantially\noutperforms current metrics, achieving up to 0.07 gain in correlation (ACC-EQ\nand Kendall's tau) and surpassing the best state-of-the-art (SOTA) metrics by\nover 15 points in adequacy assessments. Incorporating professional translator\ninsights as weights further improves performance, highlighting the value of\ntranslator inputs. Notably, TransProQA approaches human-level evaluation\nperformance comparable to trained linguistic annotators. It demonstrates broad\napplicability to open-source models such as LLaMA3.3-70b and Qwen2.5-32b,\nindicating its potential as an accessible and training-free literary evaluation\nmetric and a valuable tool for evaluating texts that require local processing\ndue to copyright or ethical considerations.",
      "url": "http://arxiv.org/abs/2505.05423v1",
      "published_time_eastern_timestamp": 1746724376.0
    },
    {
      "title": "Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than\n  Humans?",
      "summary": "Framing in media critically shapes public perception by selectively\nemphasizing some details while downplaying others. With the rise of large\nlanguage models in automated news and content creation, there is growing\nconcern that these systems may introduce or even amplify framing biases\ncompared to human authors. In this paper, we explore how framing manifests in\nboth out-of-the-box and fine-tuned LLM-generated news content. Our analysis\nreveals that, particularly in politically and socially sensitive contexts, LLMs\ntend to exhibit more pronounced framing than their human counterparts. In\naddition, we observe significant variation in framing tendencies across\ndifferent model architectures, with some models displaying notably higher\nbiases. These findings point to the need for effective post-training mitigation\nstrategies and tighter evaluation frameworks to ensure that automated news\ncontent upholds the standards of balanced reporting.",
      "url": "http://arxiv.org/abs/2505.05406v1",
      "published_time_eastern_timestamp": 1746722784.0
    },
    {
      "title": "DSDrive: Distilling Large Language Model for Lightweight End-to-End\n  Autonomous Driving with Unified Reasoning and Planning",
      "summary": "We present DSDrive, a streamlined end-to-end paradigm tailored for\nintegrating the reasoning and planning of autonomous vehicles into a unified\nframework. DSDrive leverages a compact LLM that employs a distillation method\nto preserve the enhanced reasoning capabilities of a larger-sized vision\nlanguage model (VLM). To effectively align the reasoning and planning tasks, a\nwaypoint-driven dual-head coordination module is further developed, which\nsynchronizes dataset structures, optimization objectives, and the learning\nprocess. By integrating these tasks into a unified framework, DSDrive anchors\non the planning results while incorporating detailed reasoning insights,\nthereby enhancing the interpretability and reliability of the end-to-end\npipeline. DSDrive has been thoroughly tested in closed-loop simulations, where\nit performs on par with benchmark models and even outperforms in many key\nmetrics, all while being more compact in size. Additionally, the computational\nefficiency of DSDrive (as reflected in its time and memory requirements during\ninference) has been significantly enhanced. Evidently thus, this work brings\npromising aspects and underscores the potential of lightweight systems in\ndelivering interpretable and efficient solutions for AD.",
      "url": "http://arxiv.org/abs/2505.05360v1",
      "published_time_eastern_timestamp": 1746719614.0
    },
    {
      "title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\n  Source Localization",
      "summary": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.",
      "url": "http://arxiv.org/abs/2505.05343v1",
      "published_time_eastern_timestamp": 1746718324.0
    },
    {
      "title": "FLAM: Frame-Wise Language-Audio Modeling",
      "summary": "Recent multi-modal audio-language models (ALMs) excel at text-audio retrieval\nbut struggle with frame-wise audio understanding. Prior works use\ntemporal-aware labels or unsupervised training to improve frame-wise\ncapabilities, but they still lack fine-grained labeling capability to pinpoint\nwhen an event occurs. While traditional sound event detection models can\nprecisely localize events, they are limited to pre-defined categories, making\nthem ineffective for real-world scenarios with out-of-distribution events. In\nthis work, we introduce FLAM, an open-vocabulary contrastive audio-language\nmodel capable of localizing specific sound events. FLAM employs a\nmemory-efficient and calibrated frame-wise objective with logit adjustment to\naddress spurious correlations, such as event dependencies and label imbalances\nduring training. To enable frame-wise supervision, we leverage a large-scale\ndataset with diverse audio events, LLM-generated captions and simulation.\nExperimental results and case studies demonstrate that FLAM significantly\nimproves the open-vocabulary localization capability while maintaining strong\nperformance in global retrieval and downstream tasks.",
      "url": "http://arxiv.org/abs/2505.05335v1",
      "published_time_eastern_timestamp": 1746718063.0
    },
    {
      "title": "ICon: In-Context Contribution for Automatic Data Selection",
      "summary": "Data selection for instruction tuning is essential for improving the\nperformance of Large Language Models (LLMs) and reducing training cost.\nHowever, existing automated selection methods either depend on computationally\nexpensive gradient-based measures or manually designed heuristics, which may\nfail to fully exploit the intrinsic attributes of data. In this paper, we\npropose In-context Learning for Contribution Measurement (ICon), a novel\ngradient-free method that takes advantage of the implicit fine-tuning nature of\nin-context learning (ICL) to measure sample contribution without gradient\ncomputation or manual indicators engineering. ICon offers a computationally\nefficient alternative to gradient-based methods and reduces human inductive\nbias inherent in heuristic-based approaches. ICon comprises three components\nand identifies high-contribution data by assessing performance shifts under\nimplicit learning through ICL. Extensive experiments on three LLMs across 12\nbenchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of\nICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data\noutperform full datasets by 5.42% points and exceed the best performance of\nwidely used selection methods by 2.06% points. We further analyze\nhigh-contribution samples selected by ICon, which show both diverse tasks and\nappropriate difficulty levels, rather than just the hardest ones.",
      "url": "http://arxiv.org/abs/2505.05327v1",
      "published_time_eastern_timestamp": 1746717457.0
    },
    {
      "title": "Toward Reasonable Parrots: Why Large Language Models Should Argue with\n  Us by Design",
      "summary": "In this position paper, we advocate for the development of conversational\ntechnology that is inherently designed to support and facilitate argumentative\nprocesses. We argue that, at present, large language models (LLMs) are\ninadequate for this purpose, and we propose an ideal technology design aimed at\nenhancing argumentative skills. This involves re-framing LLMs as tools to\nexercise our critical thinking rather than replacing them. We introduce the\nconcept of 'reasonable parrots' that embody the fundamental principles of\nrelevance, responsibility, and freedom, and that interact through argumentative\ndialogical moves. These principles and moves arise out of millennia of work in\nargumentation theory and should serve as the starting point for LLM-based\ntechnology that incorporates basic principles of argumentation.",
      "url": "http://arxiv.org/abs/2505.05298v1",
      "published_time_eastern_timestamp": 1746715267.0
    },
    {
      "title": "PlaceIt3D: Language-Guided Object Placement in Real 3D Scenes",
      "summary": "We introduce the novel task of Language-Guided Object Placement in Real 3D\nScenes. Our model is given a 3D scene's point cloud, a 3D asset, and a textual\nprompt broadly describing where the 3D asset should be placed. The task here is\nto find a valid placement for the 3D asset that respects the prompt. Compared\nwith other language-guided localization tasks in 3D scenes such as grounding,\nthis task has specific challenges: it is ambiguous because it has multiple\nvalid solutions, and it requires reasoning about 3D geometric relationships and\nfree space. We inaugurate this task by proposing a new benchmark and evaluation\nprotocol. We also introduce a new dataset for training 3D LLMs on this task, as\nwell as the first method to serve as a non-trivial baseline. We believe that\nthis challenging task and our new benchmark could become part of the suite of\nbenchmarks used to evaluate and compare generalist 3D LLM models.",
      "url": "http://arxiv.org/abs/2505.05288v1",
      "published_time_eastern_timestamp": 1746714551.0
    },
    {
      "title": "HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL Workflow",
      "summary": "Recent advances in leveraging the agentic paradigm of large language models\n(LLMs) utilization have significantly enhanced Text-to-SQL capabilities,\nenabling users without specialized database expertise to query data\nintuitively. However, deploying these agentic LLM-based Text-to-SQL systems in\nproduction poses substantial challenges due to their inherently multi-stage\nworkflows, stringent latency constraints, and potentially heterogeneous GPU\ninfrastructure in enterprise environments. Current LLM serving frameworks lack\neffective mechanisms for handling interdependent inference tasks, dynamic\nlatency variability, and resource heterogeneity, leading to suboptimal\nperformance and frequent service-level objective (SLO) violations. In this\npaper, we introduce HEXGEN-TEXT2SQL, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters that handle multi-tenant end-to-end queries.\nHEXGEN-TEXT2SQL introduce a hierarchical scheduling approach combining global\nworkload-balanced task dispatching and local adaptive urgency-guided\nprioritization, guided by a systematic analysis of agentic Text-to-SQL\nworkflows. Additionally, we propose a lightweight simulation-based method for\ntuning critical scheduling hyperparameters, further enhancing robustness and\nadaptability. Our extensive evaluation on realistic Text-to-SQL benchmarks\ndemonstrates that HEXGEN-TEXT2SQL significantly outperforms state-of-the-art\nLLM serving frameworks. Specifically, HEXGEN-TEXT2SQL reduces latency deadlines\nby up to 1.67$\\times$ (average: 1.41$\\times$) and improves system throughput by\nup to 1.75$\\times$ (average: 1.65$\\times$) compared to vLLM under diverse,\nrealistic workload conditions. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow.",
      "url": "http://arxiv.org/abs/2505.05286v1",
      "published_time_eastern_timestamp": 1746714527.0
    },
    {
      "title": "Latte: Transfering LLMs` Latent-level Knowledge for Few-shot Tabular\n  Learning",
      "summary": "Few-shot tabular learning, in which machine learning models are trained with\na limited amount of labeled data, provides a cost-effective approach to\naddressing real-world challenges. The advent of Large Language Models (LLMs)\nhas sparked interest in leveraging their pre-trained knowledge for few-shot\ntabular learning. Despite promising results, existing approaches either rely on\ntest-time knowledge extraction, which introduces undesirable latency, or\ntext-level knowledge, which leads to unreliable feature engineering. To\novercome these limitations, we propose Latte, a training-time knowledge\nextraction framework that transfers the latent prior knowledge within LLMs to\noptimize a more generalized downstream model. Latte enables general\nknowledge-guided downstream tabular learning, facilitating the weighted fusion\nof information across different feature values while reducing the risk of\noverfitting to limited labeled data. Furthermore, Latte is compatible with\nexisting unsupervised pre-training paradigms and effectively utilizes available\nunlabeled samples to overcome the performance limitations imposed by an\nextremely small labeled dataset. Extensive experiments on various few-shot\ntabular learning benchmarks demonstrate the superior performance of Latte,\nestablishing it as a state-of-the-art approach in this domain",
      "url": "http://arxiv.org/abs/2505.05237v1",
      "published_time_eastern_timestamp": 1746711129.0
    },
    {
      "title": "QualBench: Benchmarking Chinese LLMs with Localized Professional\n  Qualifications for Vertical Domain Evaluation",
      "summary": "The rapid advancement of Chinese large language models (LLMs) underscores the\nneed for domain-specific evaluations to ensure reliable applications. However,\nexisting benchmarks often lack coverage in vertical domains and offer limited\ninsights into the Chinese working context. Leveraging qualification exams as a\nunified framework for human expertise evaluation, we introduce QualBench, the\nfirst multi-domain Chinese QA benchmark dedicated to localized assessment of\nChinese LLMs. The dataset includes over 17,000 questions across six vertical\ndomains, with data selections grounded in 24 Chinese qualifications to closely\nalign with national policies and working standards. Through comprehensive\nevaluation, the Qwen2.5 model outperformed the more advanced GPT-4o, with\nChinese LLMs consistently surpassing non-Chinese models, highlighting the\nimportance of localized domain knowledge in meeting qualification requirements.\nThe best performance of 75.26% reveals the current gaps in domain coverage\nwithin model capabilities. Furthermore, we present the failure of LLM\ncollaboration with crowdsourcing mechanisms and suggest the opportunities for\nmulti-domain RAG knowledge enhancement and vertical domain LLM training with\nFederated Learning.",
      "url": "http://arxiv.org/abs/2505.05225v1",
      "published_time_eastern_timestamp": 1746710209.0
    },
    {
      "title": "Stealthy LLM-Driven Data Poisoning Attacks Against Embedding-Based\n  Retrieval-Augmented Recommender Systems",
      "summary": "We present a systematic study of provider-side data poisoning in\nretrieval-augmented recommender systems (RAG-based). By modifying only a small\nfraction of tokens within item descriptions -- for instance, adding emotional\nkeywords or borrowing phrases from semantically related items -- an attacker\ncan significantly promote or demote targeted items. We formalize these attacks\nunder token-edit and semantic-similarity constraints, and we examine their\neffectiveness in both promotion (long-tail items) and demotion (short-head\nitems) scenarios. Our experiments on MovieLens, using two large language model\n(LLM) retrieval modules, show that even subtle attacks shift final rankings and\nitem exposures while eluding naive detection. The results underscore the\nvulnerability of RAG-based pipelines to small-scale metadata rewrites and\nemphasize the need for robust textual consistency checks and provenance\ntracking to thwart stealthy provider-side poisoning.",
      "url": "http://arxiv.org/abs/2505.05196v1",
      "published_time_eastern_timestamp": 1746708822.0
    }
  ]
}
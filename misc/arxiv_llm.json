{
  "last_updated": "2025-05-15T11:12:03.074376-04:00",
  "papers": [
    {
      "title": "Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors",
      "summary": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.",
      "url": "http://arxiv.org/abs/2505.09610v1",
      "published_time_eastern_timestamp": 1747245520.0
    },
    {
      "title": "Adversarial Suffix Filtering: a Defense Pipeline for LLMs",
      "summary": "Large Language Models (LLMs) are increasingly embedded in autonomous systems\nand public-facing environments, yet they remain susceptible to jailbreak\nvulnerabilities that may undermine their security and trustworthiness.\nAdversarial suffixes are considered to be the current state-of-the-art\njailbreak, consistently outperforming simpler methods and frequently succeeding\neven in black-box settings. Existing defenses rely on access to the internal\narchitecture of models limiting diverse deployment, increase memory and\ncomputation footprints dramatically, or can be bypassed with simple prompt\nengineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$\n(ASF), a lightweight novel model-agnostic defensive pipeline designed to\nprotect LLMs against adversarial suffix attacks. ASF functions as an input\npreprocessor and sanitizer that detects and filters adversarially crafted\nsuffixes in prompts, effectively neutralizing malicious injections. We\ndemonstrate that ASF provides comprehensive defense capabilities across both\nblack-box and white-box attack settings, reducing the attack efficacy of\nstate-of-the-art adversarial suffix generation methods to below 4%, while only\nminimally affecting the target model's capabilities in non-adversarial\nscenarios.",
      "url": "http://arxiv.org/abs/2505.09602v1",
      "published_time_eastern_timestamp": 1747245130.0
    },
    {
      "title": "How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of\n  LLM Inference",
      "summary": "As large language models (LLMs) spread across industries, understanding their\nenvironmental footprint at the inference level is no longer optional; it is\nessential. However, most existing studies exclude proprietary models, overlook\ninfrastructural variability and overhead, or focus solely on training, even as\ninference increasingly dominates AI's environmental impact. To bridge this gap,\nthis paper introduces a novel infrastructure-aware benchmarking framework for\nquantifying the environmental footprint of LLM inference across 30\nstate-of-the-art models as deployed in commercial data centers. Our framework\ncombines public API performance data with region-specific environmental\nmultipliers and statistical inference of hardware configurations. We\nadditionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank\nmodels by performance relative to environmental cost. Our results show that o3\nand DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33\nWh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and\nthat Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short\nGPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results\nin substantial annual environmental impacts. These include electricity use\ncomparable to 35,000 U.S. homes, freshwater evaporation matching the annual\ndrinking needs of 1.2 million people, and carbon emissions requiring a\nChicago-sized forest to offset. These findings illustrate a growing paradox:\nalthough individual queries are efficient, their global scale drives\ndisproportionate resource consumption. Our study provides a standardized,\nempirically grounded methodology for benchmarking the sustainability of LLM\ndeployments, laying a foundation for future environmental accountability in AI\ndevelopment and sustainability standards.",
      "url": "http://arxiv.org/abs/2505.09598v1",
      "published_time_eastern_timestamp": 1747244820.0
    },
    {
      "title": "WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives\n  in Large Language Models",
      "summary": "Large Language Models (LLMs) are predominantly trained and aligned in ways\nthat reinforce Western-centric epistemologies and socio-cultural norms, leading\nto cultural homogenization and limiting their ability to reflect global\ncivilizational plurality. Existing benchmarking frameworks fail to adequately\ncapture this bias, as they rely on rigid, closed-form assessments that overlook\nthe complexity of cultural inclusivity. To address this, we introduce\nWorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity\n(GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our\napproach is grounded in the Multiplex Worldview proposed by Senturk et al.,\nwhich distinguishes between Uniplex models, reinforcing cultural\nhomogenization, and Multiplex models, which integrate diverse perspectives.\nWorldView-Bench measures Cultural Polarization, the exclusion of alternative\nperspectives, through free-form generative evaluation rather than conventional\ncategorical benchmarks. We implement applied multiplexity through two\nintervention strategies: (1) Contextually-Implemented Multiplex LLMs, where\nsystem prompts embed multiplexity principles, and (2) Multi-Agent System\n(MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing\ndistinct cultural perspectives collaboratively generate responses. Our results\ndemonstrate a significant increase in Perspectives Distribution Score (PDS)\nentropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs,\nalongside a shift toward positive sentiment (67.7%) and enhanced cultural\nbalance. These findings highlight the potential of multiplex-aware AI\nevaluation in mitigating cultural bias in LLMs, paving the way for more\ninclusive and ethically aligned AI systems.",
      "url": "http://arxiv.org/abs/2505.09595v1",
      "published_time_eastern_timestamp": 1747244620.0
    },
    {
      "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media",
      "summary": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments.",
      "url": "http://arxiv.org/abs/2505.09583v1",
      "published_time_eastern_timestamp": 1747243876.0
    },
    {
      "title": "Ethics and Persuasion in Reinforcement Learning from Human Feedback: A\n  Procedural Rhetorical Approach",
      "summary": "Since 2022, versions of generative AI chatbots such as ChatGPT and Claude\nhave been trained using a specialized technique called Reinforcement Learning\nfrom Human Feedback (RLHF) to fine-tune language model output using feedback\nfrom human annotators. As a result, the integration of RLHF has greatly\nenhanced the outputs of these large language models (LLMs) and made the\ninteractions and responses appear more \"human-like\" than those of previous\nversions using only supervised learning. The increasing convergence of human\nand machine-written text has potentially severe ethical, sociotechnical, and\npedagogical implications relating to transparency, trust, bias, and\ninterpersonal relations. To highlight these implications, this paper presents a\nrhetorical analysis of some of the central procedures and processes currently\nbeing reshaped by RLHF-enhanced generative AI chatbots: upholding language\nconventions, information seeking practices, and expectations for social\nrelationships. Rhetorical investigations of generative AI and LLMs have, to\nthis point, focused largely on the persuasiveness of the content generated.\nUsing Ian Bogost's concept of procedural rhetoric, this paper shifts the site\nof rhetorical investigation from content analysis to the underlying mechanisms\nof persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical\ninvestigation opens a new direction for further inquiry in AI ethics that\nconsiders how procedures rerouted through AI-driven technologies might\nreinforce hegemonic language use, perpetuate biases, decontextualize learning,\nand encroach upon human relationships. It will therefore be of interest to\neducators, researchers, scholars, and the growing number of users of generative\nAI chatbots.",
      "url": "http://arxiv.org/abs/2505.09576v1",
      "published_time_eastern_timestamp": 1747243759.0
    },
    {
      "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
      "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with $5,102$ and $300$ repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
      "url": "http://arxiv.org/abs/2505.09569v1",
      "published_time_eastern_timestamp": 1747242683.0
    },
    {
      "title": "Layered Unlearning for Adversarial Relearning",
      "summary": "Our goal is to understand how post-training methods, such as fine-tuning,\nalignment, and unlearning, modify language model behavior and representations.\nWe are particularly interested in the brittle nature of these modifications\nthat makes them easy to bypass through prompt engineering or relearning. Recent\nresults suggest that post-training induces shallow context-dependent\n``circuits'' that suppress specific response patterns. This could be one\nexplanation for the brittleness of post-training. To test this hypothesis, we\ndesign an unlearning algorithm, Layered Unlearning (LU), that creates distinct\ninhibitory mechanisms for a growing subset of the data. By unlearning the first\n$i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU\nlimits the ability of relearning on a subset of data to recover the full\ndataset. We evaluate LU through a combination of synthetic and large language\nmodel (LLM) experiments. We find that LU improves robustness to adversarial\nrelearning for several different unlearning methods. Our results contribute to\nthe state-of-the-art of machine unlearning and provide insight into the effect\nof post-training updates.",
      "url": "http://arxiv.org/abs/2505.09500v1",
      "published_time_eastern_timestamp": 1747237845.0
    },
    {
      "title": "Card Sorting Simulator: Augmenting Design of Logical Information\n  Architectures with Large Language Models",
      "summary": "Card sorting is a common ideation technique that elicits information on\nusers' mental organization of content and functionality by having them sort\nitems into categories. For more robust card sorting research, digital card\nsorting tools could benefit from providing quick automated feedback. Our\nobjective of this research is to advance toward an instrument that applies\nartificial intelligence (AI) to augment card sorting. For this purpose, we\ndevelop the Card Sorting Simulator, a prototype tool that leverages Large\nLanguage Models (LLMs) to generate informative categorizations of cards. To\nilluminate how aligned the simulation is with card sorting by actual\nparticipants, and to inform the instrument's design decisions, we conducted a\ngeneralizability-focused comparative study. We obtained 28 pre-existing card\nsorting studies from real practitioners, comprising 1,399 participants, along\nwith diverse contents and origins. With this dataset, we conducted a\ncomprehensive and nuanced analysis of the agreement between actual card sorting\nresults (clusterings of cards) and synthetic clusterings across a multitude of\nLLMs and prompt designs. Mutual information scores indicate a good degree of\nagreement to real result clustering, although similarity matrices also\ndemonstrate inconsistencies from mental models, which can be attributed to\ntheir top-down nature. Furthermore, the number of cards or complexity of their\nlabels impact the accuracy of its simulation. These findings bolster the case\nfor AI augmentation in card sorting research as a source of meaningful\npreliminary feedback and highlight the need for further study for the\ndevelopment and validation of intelligent user research tools.",
      "url": "http://arxiv.org/abs/2505.09478v1",
      "published_time_eastern_timestamp": 1747236555.0
    },
    {
      "title": "Deploying Foundation Model-Enabled Air and Ground Robots in the Field:\n  Challenges and Opportunities",
      "summary": "The integration of foundation models (FMs) into robotics has enabled robots\nto understand natural language and reason about the semantics in their\nenvironments. However, existing FM-enabled robots primary operate in\nclosed-world settings, where the robot is given a full prior map or has a full\nview of its workspace. This paper addresses the deployment of FM-enabled robots\nin the field, where missions often require a robot to operate in large-scale\nand unstructured environments. To effectively accomplish these missions, robots\nmust actively explore their environments, navigate obstacle-cluttered terrain,\nhandle unexpected sensor inputs, and operate with compute constraints. We\ndiscuss recent deployments of SPINE, our LLM-enabled autonomy framework, in\nfield robotic settings. To the best of our knowledge, we present the first\ndemonstration of large-scale LLM-enabled robot planning in unstructured\nenvironments with several kilometers of missions. SPINE is agnostic to a\nparticular LLM, which allows us to distill small language models capable of\nrunning onboard size, weight and power (SWaP) limited platforms. Via\npreliminary model distillation work, we then present the first language-driven\nUAV planner using on-device language models. We conclude our paper by proposing\nseveral promising directions for future research.",
      "url": "http://arxiv.org/abs/2505.09477v1",
      "published_time_eastern_timestamp": 1747236523.0
    },
    {
      "title": "Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?",
      "summary": "We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni,\non an audio question answering dataset with the reinforcement learning method\nGRPO. This leads to new State-of-the-Art performance on the recent MMAU\nbenchmark. Omni-R1 achieves the highest accuracies on the sounds, music,\nspeech, and overall average categories, both on the Test-mini and Test-full\nsplits. To understand the performance improvement, we tested models both with\nand without audio and found that much of the performance improvement from GRPO\ncould be attributed to better text-based reasoning. We also made a surprising\ndiscovery that fine-tuning without audio on a text-only dataset was effective\nat improving the audio-based performance.",
      "url": "http://arxiv.org/abs/2505.09439v1",
      "published_time_eastern_timestamp": 1747234036.0
    },
    {
      "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment",
      "summary": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.",
      "url": "http://arxiv.org/abs/2505.09438v1",
      "published_time_eastern_timestamp": 1747233992.0
    },
    {
      "title": "CXMArena: Unified Dataset to benchmark performance in realistic CXM\n  Scenarios",
      "summary": "Large Language Models (LLMs) hold immense potential for revolutionizing\nCustomer Experience Management (CXM), particularly in contact center\noperations. However, evaluating their practical utility in complex operational\nenvironments is hindered by data scarcity (due to privacy concerns) and the\nlimitations of current benchmarks. Existing benchmarks often lack realism,\nfailing to incorporate deep knowledge base (KB) integration, real-world noise,\nor critical operational tasks beyond conversational fluency. To bridge this\ngap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset\nspecifically designed for evaluating AI in operational CXM contexts. Given the\ndiversity in possible contact center features, we have developed a scalable\nLLM-powered pipeline that simulates the brand's CXM entities that form the\nfoundation of our datasets-such as knowledge articles including product\nspecifications, issue taxonomies, and contact center conversations. The\nentities closely represent real-world distribution because of controlled noise\ninjection (informed by domain experts) and rigorous automated validation.\nBuilding on this, we release CXMArena, which provides dedicated benchmarks\ntargeting five important operational tasks: Knowledge Base Refinement, Intent\nPrediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with\nIntegrated Tools. Our baseline experiments underscore the benchmark's\ndifficulty: even state of the art embedding and generation models achieve only\n68% accuracy on article search, while standard embedding methods yield a low F1\nscore of 0.3 for knowledge base refinement, highlighting significant challenges\nfor current models necessitating complex pipelines and solutions over\nconventional techniques.",
      "url": "http://arxiv.org/abs/2505.09436v1",
      "published_time_eastern_timestamp": 1747233870.0
    },
    {
      "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation",
      "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.",
      "url": "http://arxiv.org/abs/2505.09427v1",
      "published_time_eastern_timestamp": 1747232904.0
    },
    {
      "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners",
      "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.",
      "url": "http://arxiv.org/abs/2505.09396v1",
      "published_time_eastern_timestamp": 1747230684.0
    },
    {
      "title": "Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory\n  Forecasting",
      "summary": "Typhoon trajectory forecasting is essential for disaster preparedness but\nremains computationally demanding due to the complexity of atmospheric dynamics\nand the resource requirements of deep learning models. Quantum-Train (QT), a\nhybrid quantum-classical framework that leverages quantum neural networks\n(QNNs) to generate trainable parameters exclusively during training,\neliminating the need for quantum hardware at inference time. Building on QT's\nsuccess across multiple domains, including image classification, reinforcement\nlearning, flood prediction, and large language model (LLM) fine-tuning, we\nintroduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting\nmodel learning. Integrated with an Attention-based Multi-ConvGRU model, QPA\nenables parameter-efficient training while maintaining predictive accuracy.\nThis work represents the first application of quantum machine learning (QML) to\nlarge-scale typhoon trajectory prediction, offering a scalable and\nenergy-efficient approach to climate modeling. Our results demonstrate that QPA\nsignificantly reduces the number of trainable parameters while preserving\nperformance, making high-performance forecasting more accessible and\nsustainable through hybrid quantum-classical learning.",
      "url": "http://arxiv.org/abs/2505.09395v1",
      "published_time_eastern_timestamp": 1747230644.0
    },
    {
      "title": "Qwen3 Technical Report",
      "summary": "In this work, we present Qwen3, the latest version of the Qwen model family.\nQwen3 comprises a series of large language models (LLMs) designed to advance\nperformance, efficiency, and multilingual capabilities. The Qwen3 series\nincludes models of both dense and Mixture-of-Expert (MoE) architectures, with\nparameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is\nthe integration of thinking mode (for complex, multi-step reasoning) and\nnon-thinking mode (for rapid, context-driven responses) into a unified\nframework. This eliminates the need to switch between different models--such as\nchat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g.,\nQwQ-32B)--and enables dynamic mode switching based on user queries or chat\ntemplates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing\nusers to allocate computational resources adaptively during inference, thereby\nbalancing latency and performance based on task complexity. Moreover, by\nleveraging the knowledge from the flagship models, we significantly reduce the\ncomputational resources required to build smaller-scale models, while ensuring\ntheir highly competitive performance. Empirical evaluations demonstrate that\nQwen3 achieves state-of-the-art results across diverse benchmarks, including\ntasks in code generation, mathematical reasoning, agent tasks, etc.,\ncompetitive against larger MoE models and proprietary models. Compared to its\npredecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119\nlanguages and dialects, enhancing global accessibility through improved\ncross-lingual understanding and generation capabilities. To facilitate\nreproducibility and community-driven research and development, all Qwen3 models\nare publicly accessible under Apache 2.0.",
      "url": "http://arxiv.org/abs/2505.09388v1",
      "published_time_eastern_timestamp": 1747230094.0
    },
    {
      "title": "Insights into DeepSeek-V3: Scaling Challenges and Reflections on\n  Hardware for AI Architectures",
      "summary": "The rapid scaling of large language models (LLMs) has unveiled critical\nlimitations in current hardware architectures, including constraints in memory\ncapacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3,\ntrained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model\nco-design can effectively address these challenges, enabling cost-efficient\ntraining and inference at scale. This paper presents an in-depth analysis of\nthe DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting\nkey innovations such as Multi-head Latent Attention (MLA) for enhanced memory\nefficiency, Mixture of Experts (MoE) architectures for optimized\ncomputation-communication trade-offs, FP8 mixed-precision training to unlock\nthe full potential of hardware capabilities, and a Multi-Plane Network Topology\nto minimize cluster-level network overhead. Building on the hardware\nbottlenecks encountered during DeepSeek-V3's development, we engage in a\nbroader discussion with academic and industry peers on potential future\nhardware directions, including precise low-precision computation units,\nscale-up and scale-out convergence, and innovations in low-latency\ncommunication fabrics. These insights underscore the critical role of hardware\nand model co-design in meeting the escalating demands of AI workloads, offering\na practical blueprint for innovation in next-generation AI systems.",
      "url": "http://arxiv.org/abs/2505.09343v1",
      "published_time_eastern_timestamp": 1747226343.0
    },
    {
      "title": "RAG-Enabled Intent Reasoning for Application-Network Interaction",
      "summary": "Intent-based network (IBN) is a promising solution to automate network\noperation and management. IBN aims to offer human-tailored network interaction,\nallowing the network to communicate in a way that aligns with the network\nusers' language, rather than requiring the network users to understand the\ntechnical language of the network/devices. Nowadays, different applications\ninteract with the network, each with its own specialized needs and domain\nlanguage. Creating semantic languages (i.e., ontology-based languages) and\nassociating them with each application to facilitate intent translation lacks\ntechnical expertise and is neither practical nor scalable. To tackle the\naforementioned problem, we propose a context-aware AI framework that utilizes\nmachine reasoning (MR), retrieval augmented generation (RAG), and generative AI\ntechnologies to interpret intents from different applications and generate\nstructured network intents. The proposed framework allows for\ngeneralized/domain-specific intent expression and overcomes the drawbacks of\nlarge language models (LLMs) and vanilla-RAG framework. The experimental\nresults show that our proposed intent-RAG framework outperforms the LLM and\nvanilla-RAG framework in intent translation.",
      "url": "http://arxiv.org/abs/2505.09339v1",
      "published_time_eastern_timestamp": 1747226098.0
    },
    {
      "title": "Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment\n  and Distraction in LLMs",
      "summary": "We observe a novel phenomenon, contextual entrainment, across a wide range of\nlanguage models (LMs) and prompt settings, providing a new mechanistic\nperspective on how LMs become distracted by ``irrelevant'' contextual\ninformation in the input prompt. Specifically, LMs assign significantly higher\nlogits (or probabilities) to any tokens that have previously appeared in the\ncontext prompt, even for random tokens. This suggests that contextual\nentrainment is a mechanistic phenomenon, occurring independently of the\nrelevance or semantic relation of the tokens to the question or the rest of the\nsentence. We find statistically significant evidence that the magnitude of\ncontextual entrainment is influenced by semantic factors. Counterfactual\nprompts have a greater effect compared to factual ones, suggesting that while\ncontextual entrainment is a mechanistic phenomenon, it is modulated by semantic\nfactors.\n  We hypothesise that there is a circuit of attention heads -- the entrainment\nheads -- that corresponds to the contextual entrainment phenomenon. Using a\nnovel entrainment head discovery method based on differentiable masking, we\nidentify these heads across various settings. When we ``turn off'' these heads,\ni.e., set their outputs to zero, the effect of contextual entrainment is\nsignificantly attenuated, causing the model to generate output that capitulates\nto what it would produce if no distracting context were provided. Our discovery\nof contextual entrainment, along with our investigation into LM distraction via\nthe entrainment heads, marks a key step towards the mechanistic analysis and\nmitigation of the distraction problem.",
      "url": "http://arxiv.org/abs/2505.09338v1",
      "published_time_eastern_timestamp": 1747225985.0
    }
  ]
}
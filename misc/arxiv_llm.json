{
  "last_updated": "2025-09-07T02:14:56.988779-04:00",
  "papers": [
    {
      "title": "Delta Activations: A Representation for Finetuned Large Language Models",
      "summary": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
      "url": "http://arxiv.org/abs/2509.04442v1",
      "published_time_eastern_timestamp": 1757008746.0
    },
    {
      "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory",
      "summary": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo.",
      "url": "http://arxiv.org/abs/2509.04439v1",
      "published_time_eastern_timestamp": 1757008459.0
    },
    {
      "title": "No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in\n  Resume Screening",
      "summary": "In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight.",
      "url": "http://arxiv.org/abs/2509.04404v1",
      "published_time_eastern_timestamp": 1757006186.0
    },
    {
      "title": "Denoising GER: A Noise-Robust Generative Error Correction with LLM for\n  Speech Recognition",
      "summary": "In recent years, large language models (LLM) have made significant progress\nin the task of generation error correction (GER) for automatic speech\nrecognition (ASR) post-processing. However, in complex noisy environments, they\nstill face challenges such as poor adaptability and low information\nutilization, resulting in limited effectiveness of GER. To address these\nissues, this paper proposes a noise-robust multi-modal GER framework (Denoising\nGER). The framework enhances the model's adaptability to different noisy\nscenarios through a noise-adaptive acoustic encoder and optimizes the\nintegration of multi-modal information via a heterogeneous feature compensation\ndynamic fusion (HFCDF) mechanism, improving the LLM's utilization of\nmulti-modal information. Additionally, reinforcement learning (RL) training\nstrategies are introduced to enhance the model's predictive capabilities.\nExperimental results demonstrate that Denoising GER significantly improves\naccuracy and robustness in noisy environments and exhibits good generalization\nabilities in unseen noise scenarios.",
      "url": "http://arxiv.org/abs/2509.04392v1",
      "published_time_eastern_timestamp": 1757005438.0
    },
    {
      "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
      "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
      "url": "http://arxiv.org/abs/2509.04377v1",
      "published_time_eastern_timestamp": 1757004001.0
    },
    {
      "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature\n  of LLM Gender Biases",
      "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that even minor prompt changes can substantially alter bias\noutcomes, sometimes reversing their direction entirely. Discrete-choice metrics\nfurther tend to amplify bias relative to probabilistic measures. These findings\ndo not only highlight the brittleness of LLM gender bias evaluations but open a\nnew puzzle for the NLP benchmarking and development community: To what extent\ncan well-controlled testing designs trigger LLM ``testing mode'' performance,\nand what does this mean for the ecological validity of future benchmarks.",
      "url": "http://arxiv.org/abs/2509.04373v1",
      "published_time_eastern_timestamp": 1757003538.0
    },
    {
      "title": "SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic\n  Avatars",
      "summary": "We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.",
      "url": "http://arxiv.org/abs/2509.04356v1",
      "published_time_eastern_timestamp": 1757002684.0
    },
    {
      "title": "Psychologically Enhanced AI Agents",
      "summary": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.",
      "url": "http://arxiv.org/abs/2509.04343v1",
      "published_time_eastern_timestamp": 1757001783.0
    },
    {
      "title": "Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing\n  Clinical Notes",
      "summary": "Large Language Models (LLMs) are often proposed as tools to streamline\nclinical documentation, a task viewed as both high-volume and low-risk.\nHowever, even seemingly straightforward applications of LLMs raise complex\nsociotechnical considerations to translate into practice. This case study,\nconducted at KidsAbility, a pediatric rehabilitation facility in Ontario,\nCanada examined the use of LLMs to support occupational therapists in reducing\ndocumentation burden.We conducted a qualitative study involving 20 clinicians\nwho participated in pilot programs using two AI technologies: a general-purpose\nproprietary LLM and a bespoke model fine-tuned on proprietary historical\ndocumentation.\n  Our findings reveal that documentation challenges are sociotechnical in\nnature, shaped by clinical workflows, organizational policies, and system\nconstraints. Four key themes emerged: (1) the heterogeneity of workflows, (2)\nthe documentation burden is systemic and not directly linked to the creation of\nany single type of documentation, (3) the need for flexible tools and clinician\nautonomy, and (4) effective implementation requires mutual learning between\nclinicians and AI systems.\n  While LLMs show promise in easing documentation tasks, their success will\ndepend on flexible, adaptive integration that supports clinician autonomy.\nBeyond technical performance, sustained adoption will require training programs\nand implementation strategies that reflect the complexity of clinical\nenvironments.",
      "url": "http://arxiv.org/abs/2509.04340v1",
      "published_time_eastern_timestamp": 1757001585.0
    },
    {
      "title": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn\n  Negotiation",
      "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.",
      "url": "http://arxiv.org/abs/2509.04310v1",
      "published_time_eastern_timestamp": 1756999438.0
    },
    {
      "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge\n  in Large Language Models",
      "summary": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.",
      "url": "http://arxiv.org/abs/2509.04304v1",
      "published_time_eastern_timestamp": 1756999070.0
    },
    {
      "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?",
      "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.",
      "url": "http://arxiv.org/abs/2509.04292v1",
      "published_time_eastern_timestamp": 1756998182.0
    },
    {
      "title": "An Empirical Study of Vulnerabilities in Python Packages and Their\n  Detection",
      "summary": "In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field.",
      "url": "http://arxiv.org/abs/2509.04260v1",
      "published_time_eastern_timestamp": 1756996708.0
    },
    {
      "title": "How many patients could we save with LLM priors?",
      "summary": "Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making.",
      "url": "http://arxiv.org/abs/2509.04250v1",
      "published_time_eastern_timestamp": 1756995815.0
    },
    {
      "title": "Are LLM Agents the New RPA? A Comparative Study with RPA Across\n  Enterprise Workflows",
      "summary": "The emergence of large language models (LLMs) has introduced a new paradigm\nin automation: LLM agents or Agentic Automation with Computer Use (AACU).\nUnlike traditional Robotic Process Automation (RPA), which relies on rule-based\nworkflows and scripting, AACU enables intelligent agents to perform tasks\nthrough natural language instructions and autonomous interaction with user\ninterfaces. This study investigates whether AACU can serve as a viable\nalternative to RPA in enterprise workflow automation. We conducted controlled\nexperiments across three standard RPA challenges data entry, monitoring, and\ndocument extraction comparing RPA (via UiPath) and AACU (via Anthropic's\nComputer Use Agent) in terms of speed, reliability, and development effort.\nResults indicate that RPA outperforms AACU in execution speed and reliability,\nparticularly in repetitive, stable environments. However, AACU significantly\nreduces development time and adapts more flexibly to dynamic interfaces. While\ncurrent AACU implementations are not yet production-ready, their promise in\nrapid prototyping and lightweight automation is evident. Future research should\nexplore multi-agent orchestration, hybrid RPA-AACU architectures, and more\nrobust evaluation across industries and platforms.",
      "url": "http://arxiv.org/abs/2509.04198v1",
      "published_time_eastern_timestamp": 1756992164.0
    },
    {
      "title": "KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and\n  Runtime Logs Analysis",
      "summary": "The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native\napplications has introduced significant security challenges, such as\nmisconfigured resources and overly permissive configurations. Failing to\naddress these issues can result in unauthorized access, privilege escalation,\nand lateral movement within clusters. Most existing K8s security solutions\nfocus on detecting misconfigurations, typically through static analysis or\nanomaly detection. In contrast, this paper presents KubeGuard, a novel runtime\nlog-driven recommender framework aimed at mitigating risks by addressing overly\npermissive configurations. KubeGuard is designed to harden K8s environments\nthrough two complementary tasks: Resource Creation and Resource Refinement. It\nleverages large language models (LLMs) to analyze manifests and runtime logs\nreflecting actual system behavior, using modular prompt-chaining workflows.\nThis approach enables KubeGuard to create least-privilege configurations for\nnew resources and refine existing manifests to reduce the attack surface.\nKubeGuard's output manifests are presented as recommendations that users (e.g.,\ndevelopers and operators) can review and adopt to enhance cluster security. Our\nevaluation demonstrates that KubeGuard effectively generates and refines K8s\nmanifests for Roles, NetworkPolicies, and Deployments, leveraging both\nproprietary and open-source LLMs. The high precision, recall, and F1-scores\naffirm KubeGuard's practicality as a framework that translates runtime\nobservability into actionable, least-privilege configuration guidance.",
      "url": "http://arxiv.org/abs/2509.04191v1",
      "published_time_eastern_timestamp": 1756991637.0
    },
    {
      "title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions",
      "summary": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.",
      "url": "http://arxiv.org/abs/2509.04183v1",
      "published_time_eastern_timestamp": 1756990764.0
    },
    {
      "title": "TAGAL: Tabular Data Generation using Agentic LLM Methods",
      "summary": "The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods.",
      "url": "http://arxiv.org/abs/2509.04152v1",
      "published_time_eastern_timestamp": 1756988714.0
    },
    {
      "title": "Enhancing Technical Documents Retrieval for RAG",
      "summary": "In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows.",
      "url": "http://arxiv.org/abs/2509.04139v1",
      "published_time_eastern_timestamp": 1756987863.0
    },
    {
      "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image\n  Generation",
      "summary": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.",
      "url": "http://arxiv.org/abs/2509.04126v1",
      "published_time_eastern_timestamp": 1756986268.0
    }
  ]
}
{
  "last_updated": "2025-07-26T05:12:25.025694-04:00",
  "papers": [
    {
      "title": "Layer-Aware Representation Filtering: Purifying Finetuning Data to\n  Preserve LLM Safety Alignment",
      "summary": "With rapid advancement and increasing accessibility of LLMs, fine-tuning\naligned models has become a critical step for adapting them to real-world\napplications, which makes the safety of this fine-tuning process more important\nthan ever. However, recent studies have highlighted a critical challenge: even\nwhen fine-tuning with seemingly benign downstream datasets, the safety of\naligned LLMs can be compromised, making them more susceptible to malicious\ninstructions. In this paper, we show that fine-tuning datasets often contain\nsamples with safety-degrading features that are not easily identifiable on the\nsurface. These samples can significantly degrade the safety alignment of LLMs\nduring fine-tuning. To address this issue, we propose LARF, a\n\\textbf{L}ayer-\\textbf{A}ware \\textbf{R}epresentation \\textbf{F}iltering\nmethod. This method identifies safety-sensitive layers within the LLM and\nleverages their representations to detect which data samples in the\npost-training dataset contain safety-degrading features. Experimental results\ndemonstrate that LARF can effectively identify benign data with\nsafety-degrading features. After removing such data, the safety alignment\ndegradation caused by fine-tuning is mitigated. Please see our code at\n\\href{https://github.com/LLLeoLi/LARF}{https://github.com/LLLeoLi/LARF}.",
      "url": "http://arxiv.org/abs/2507.18631v1",
      "published_time_eastern_timestamp": 1753379964.0
    },
    {
      "title": "TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual\n  Rewards",
      "summary": "Prompt optimization improves the reasoning abilities of large language models\n(LLMs) without requiring parameter updates to the target model. Following\nheuristic-based \"Think step by step\" approaches, the field has evolved in two\nmain directions: while one group of methods uses textual feedback to elicit\nimproved prompts from general-purpose LLMs in a training-free way, a concurrent\nline of research relies on numerical rewards to train a special prompt model,\ntailored for providing optimal prompts to the target model. In this paper, we\nintroduce the Textual Reward Prompt framework (TRPrompt), which unifies these\napproaches by directly incorporating textual feedback into training of the\nprompt model. Our framework does not require prior dataset collection and is\nbeing iteratively improved with the feedback on the generated prompts. When\ncoupled with the capacity of an LLM to internalize the notion of what a \"good\"\nprompt is, the high-resolution signal provided by the textual rewards allows us\nto train a prompt model yielding state-of-the-art query-specific prompts for\nthe problems from the challenging math datasets GSMHard and MATH.",
      "url": "http://arxiv.org/abs/2507.18618v1",
      "published_time_eastern_timestamp": 1753379684.0
    },
    {
      "title": "Explainable Mapper: Charting LLM Embedding Spaces Using\n  Perturbation-Based Explanation and Verification Agents",
      "summary": "Large language models (LLMs) produce high-dimensional embeddings that capture\nrich semantic and syntactic relationships between words, sentences, and\nconcepts. Investigating the topological structures of LLM embedding spaces via\nmapper graphs enables us to understand their underlying structures.\nSpecifically, a mapper graph summarizes the topological structure of the\nembedding space, where each node represents a topological neighborhood\n(containing a cluster of embeddings), and an edge connects two nodes if their\ncorresponding neighborhoods overlap. However, manually exploring these\nembedding spaces to uncover encoded linguistic properties requires considerable\nhuman effort. To address this challenge, we introduce a framework for\nsemi-automatic annotation of these embedding properties. To organize the\nexploration process, we first define a taxonomy of explorable elements within a\nmapper graph such as nodes, edges, paths, components, and trajectories. The\nannotation of these elements is executed through two types of customizable\nLLM-based agents that employ perturbation techniques for scalable and automated\nanalysis. These agents help to explore and explain the characteristics of\nmapper elements and verify the robustness of the generated explanations. We\ninstantiate the framework within a visual analytics workspace and demonstrate\nits effectiveness through case studies. In particular, we replicate findings\nfrom prior research on BERT's embedding properties across various layers of its\narchitecture and provide further observations into the linguistic properties of\ntopological neighborhoods.",
      "url": "http://arxiv.org/abs/2507.18607v1",
      "published_time_eastern_timestamp": 1753379020.0
    },
    {
      "title": "AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance\n  Data Synthesis for Specialist LLMs",
      "summary": "Despite the impressive performance of large language models (LLMs) in general\ndomains, they often underperform in specialized domains. Existing approaches\ntypically rely on data synthesis methods and yield promising results by using\nunlabeled data to capture domain-specific features. However, these methods\neither incur high computational costs or suffer from performance limitations,\nwhile also demonstrating insufficient generalization across different tasks. To\naddress these challenges, we propose AQuilt, a framework for constructing\ninstruction-tuning data for any specialized domains from corresponding\nunlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,\nand Task type. By incorporating logic and inspection, we encourage reasoning\nprocesses and self-inspection to enhance model performance. Moreover,\ncustomizable task instructions enable high-quality data generation for any\ntask. As a result, we construct a dataset of 703k examples to train a powerful\ndata synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3\nwhile utilizing just 17% of the production cost. Further analysis demonstrates\nthat our generated data exhibits higher relevance to downstream tasks. Source\ncode, models, and scripts are available at https://github.com/Krueske/AQuilt.",
      "url": "http://arxiv.org/abs/2507.18584v1",
      "published_time_eastern_timestamp": 1753376607.0
    },
    {
      "title": "HARLF: Hierarchical Reinforcement Learning and Lightweight LLM-Driven\n  Sentiment Integration for Financial Portfolio Optimization",
      "summary": "This paper presents a novel hierarchical framework for portfolio\noptimization, integrating lightweight Large Language Models (LLMs) with Deep\nReinforcement Learning (DRL) to combine sentiment signals from financial news\nwith traditional market indicators. Our three-tier architecture employs base RL\nagents to process hybrid data, meta-agents to aggregate their decisions, and a\nsuper-agent to merge decisions based on market data and sentiment analysis.\nEvaluated on data from 2018 to 2024, after training on 2000-2017, the framework\nachieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming\nequal-weighted and S&P 500 benchmarks. Key contributions include scalable\ncross-modal integration, a hierarchical RL structure for enhanced stability,\nand open-source reproducibility.",
      "url": "http://arxiv.org/abs/2507.18560v1",
      "published_time_eastern_timestamp": 1753374924.0
    },
    {
      "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm",
      "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.",
      "url": "http://arxiv.org/abs/2507.18553v1",
      "published_time_eastern_timestamp": 1753374138.0
    },
    {
      "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface",
      "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.",
      "url": "http://arxiv.org/abs/2507.18546v1",
      "published_time_eastern_timestamp": 1753373474.0
    },
    {
      "title": "IntentVCNet: Bridging Spatio-Temporal Gaps for Intention-Oriented\n  Controllable Video Captioning",
      "summary": "Intent-oriented controlled video captioning aims to generate targeted\ndescriptions for specific targets in a video based on customized user intent.\nCurrent Large Visual Language Models (LVLMs) have gained strong instruction\nfollowing and visual comprehension capabilities. Although the LVLMs\ndemonstrated proficiency in spatial and temporal understanding respectively, it\nwas not able to perform fine-grained spatial control in time sequences in\ndirect response to instructions. This substantial spatio-temporal gap\ncomplicates efforts to achieve fine-grained intention-oriented control in\nvideo. Towards this end, we propose a novel IntentVCNet that unifies the\ntemporal and spatial understanding knowledge inherent in LVLMs to bridge the\nspatio-temporal gap from both prompting and model perspectives. Specifically,\nwe first propose a prompt combination strategy designed to enable LLM to model\nthe implicit relationship between prompts that characterize user intent and\nvideo sequences. We then propose a parameter efficient box adapter that\naugments the object semantic information in the global visual context so that\nthe visual token has a priori information about the user intent. The final\nexperiment proves that the combination of the two strategies can further\nenhance the LVLM's ability to model spatial details in video sequences, and\nfacilitate the LVLMs to accurately generate controlled intent-oriented\ncaptions. Our proposed method achieved state-of-the-art results in several open\nsource LVLMs and was the runner-up in the IntentVC challenge. Our code is\navailable on https://github.com/thqiu0419/IntentVCNet.",
      "url": "http://arxiv.org/abs/2507.18531v1",
      "published_time_eastern_timestamp": 1753372716.0
    },
    {
      "title": "The Moral Gap of Large Language Models",
      "summary": "Moral foundation detection is crucial for analyzing social discourse and\ndeveloping ethically-aligned AI systems. While large language models excel\nacross diverse tasks, their performance on specialized moral reasoning remains\nunclear.\n  This study provides the first comprehensive comparison between\nstate-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit\ndatasets using ROC, PR, and DET curve analysis.\n  Results reveal substantial performance gaps, with LLMs exhibiting high false\nnegative rates and systematic under-detection of moral content despite prompt\nengineering efforts. These findings demonstrate that task-specific fine-tuning\nremains superior to prompting for moral reasoning applications.",
      "url": "http://arxiv.org/abs/2507.18523v1",
      "published_time_eastern_timestamp": 1753372146.0
    },
    {
      "title": "A Deep Dive into Retrieval-Augmented Generation for Code Completion:\n  Experience on WeChat",
      "summary": "Code completion, a crucial task in software engineering that enhances\ndeveloper productivity, has seen substantial improvements with the rapid\nadvancement of large language models (LLMs). In recent years,\nretrieval-augmented generation (RAG) has emerged as a promising method to\nenhance the code completion capabilities of LLMs, which leverages relevant\ncontext from codebases without requiring model retraining. While existing\nstudies have demonstrated the effectiveness of RAG on public repositories and\nbenchmarks, the potential distribution shift between open-source and\nclosed-source codebases presents unique challenges that remain unexplored. To\nmitigate the gap, we conduct an empirical study to investigate the performance\nof widely-used RAG methods for code completion in the industrial-scale codebase\nof WeChat, one of the largest proprietary software systems. Specifically, we\nextensively explore two main types of RAG methods, namely identifier-based RAG\nand similarity-based RAG, across 26 open-source LLMs ranging from 0.5B to 671B\nparameters. For a more comprehensive analysis, we employ different retrieval\ntechniques for similarity-based RAG, including lexical and semantic retrieval.\nBased on 1,669 internal repositories, we achieve several key findings: (1) both\nRAG methods demonstrate effectiveness in closed-source repositories, with\nsimilarity-based RAG showing superior performance, (2) the effectiveness of\nsimilarity-based RAG improves with more advanced retrieval techniques, where\nBM25 (lexical retrieval) and GTE-Qwen (semantic retrieval) achieve superior\nperformance, and (3) the combination of lexical and semantic retrieval\ntechniques yields optimal results, demonstrating complementary strengths.\nFurthermore, we conduct a developer survey to validate the practical utility of\nRAG methods in real-world development environments.",
      "url": "http://arxiv.org/abs/2507.18515v1",
      "published_time_eastern_timestamp": 1753371391.0
    },
    {
      "title": "Not All Features Deserve Attention: Graph-Guided Dependency Learning for\n  Tabular Data Generation with Language Models",
      "summary": "Large Language Models (LLMs) have shown strong potential for tabular data\ngeneration by modeling textualized feature-value pairs. However, tabular data\ninherently exhibits sparse feature-level dependencies, where many feature\ninteractions are structurally insignificant. This creates a fundamental\nmismatch as LLMs' self-attention mechanism inevitably distributes focus across\nall pairs, diluting attention on critical relationships, particularly in\ndatasets with complex dependencies or semantically ambiguous features. To\naddress this limitation, we propose GraDe (Graph-Guided Dependency Learning), a\nnovel method that explicitly integrates sparse dependency graphs into LLMs'\nattention mechanism. GraDe employs a lightweight dynamic graph learning module\nguided by externally extracted functional dependencies, prioritizing key\nfeature interactions while suppressing irrelevant ones. Our experiments across\ndiverse real-world datasets demonstrate that GraDe outperforms existing\nLLM-based approaches by up to 12% on complex datasets while achieving\ncompetitive results with state-of-the-art approaches in synthetic data quality.\nOur method is minimally intrusive yet effective, offering a practical solution\nfor structure-aware tabular data modeling with LLMs.",
      "url": "http://arxiv.org/abs/2507.18504v1",
      "published_time_eastern_timestamp": 1753370547.0
    },
    {
      "title": "How Well Do LLMs Predict Prerequisite Skills? Zero-Shot Comparison to\n  Expert-Defined Concepts",
      "summary": "Prerequisite skills - foundational competencies required before mastering\nmore advanced concepts - are important for supporting effective learning,\nassessment, and skill-gap analysis. Traditionally curated by domain experts,\nthese relationships are costly to maintain and difficult to scale. This paper\ninvestigates whether large language models (LLMs) can predict prerequisite\nskills in a zero-shot setting, using only natural language descriptions and\nwithout task-specific fine-tuning. We introduce ESCO-PrereqSkill, a benchmark\ndataset constructed from the ESCO taxonomy, comprising 3,196 skills and their\nexpert-defined prerequisite links. Using a standardized prompting strategy, we\nevaluate 13 state-of-the-art LLMs, including GPT-4, Claude 3, Gemini, LLaMA 4,\nQwen2, and DeepSeek, across semantic similarity, BERTScore, and inference\nlatency. Our results show that models such as LLaMA4-Maverick,\nClaude-3-7-Sonnet, and Qwen2-72B generate predictions that closely align with\nexpert ground truth, demonstrating strong semantic reasoning without\nsupervision. These findings highlight the potential of LLMs to support scalable\nprerequisite skill modeling for applications in personalized learning,\nintelligent tutoring, and skill-based recommender systems.",
      "url": "http://arxiv.org/abs/2507.18479v1",
      "published_time_eastern_timestamp": 1753368885.0
    },
    {
      "title": "Automated Code Review Using Large Language Models with Symbolic\n  Reasoning",
      "summary": "Code review is one of the key processes in the software development lifecycle\nand is essential to maintain code quality. However, manual code review is\nsubjective and time consuming. Given its rule-based nature, code review is well\nsuited for automation. In recent years, significant efforts have been made to\nautomate this process with the help of artificial intelligence. Recent\ndevelopments in Large Language Models (LLMs) have also emerged as a promising\ntool in this area, but these models often lack the logical reasoning\ncapabilities needed to fully understand and evaluate code. To overcome this\nlimitation, this study proposes a hybrid approach that integrates symbolic\nreasoning techniques with LLMs to automate the code review process. We tested\nour approach using the CodexGlue dataset, comparing several models, including\nCodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining\nsymbolic reasoning and prompting techniques with LLMs. Our results show that\nthis approach improves the accuracy and efficiency of automated code review.",
      "url": "http://arxiv.org/abs/2507.18476v1",
      "published_time_eastern_timestamp": 1753368627.0
    },
    {
      "title": "LLM-based Embedders for Prior Case Retrieval",
      "summary": "In common law systems, legal professionals such as lawyers and judges rely on\nprecedents to build their arguments. As the volume of cases has grown massively\nover time, effectively retrieving prior cases has become essential. Prior case\nretrieval (PCR) is an information retrieval (IR) task that aims to\nautomatically identify the most relevant court cases for a specific query from\na large pool of potential candidates. While IR methods have seen several\nparadigm shifts over the last few years, the vast majority of PCR methods\ncontinue to rely on traditional IR methods, such as BM25. The state-of-the-art\ndeep learning IR methods have not been successful in PCR due to two key\nchallenges: i. Lengthy legal text limitation; when using the powerful\nBERT-based transformer models, there is a limit of input text lengths, which\ninevitably requires to shorten the input via truncation or division with a loss\nof legal context information. ii. Lack of legal training data; due to data\nprivacy concerns, available PCR datasets are often limited in size, making it\ndifficult to train deep learning-based models effectively. In this research, we\naddress these challenges by leveraging LLM-based text embedders in PCR.\nLLM-based embedders support longer input lengths, and since we use them in an\nunsupervised manner, they do not require training data, addressing both\nchallenges simultaneously. In this paper, we evaluate state-of-the-art\nLLM-based text embedders in four PCR benchmark datasets and show that they\noutperform BM25 and supervised transformer-based models.",
      "url": "http://arxiv.org/abs/2507.18455v1",
      "published_time_eastern_timestamp": 1753367770.0
    },
    {
      "title": "DIFFA: Large Language Diffusion Models Can Listen and Understand",
      "summary": "Recent advances in Large language models (LLMs) have shown remarkable\ncapabilities across textual and multimodal domains. In parallel,\ndiffusion-based language models have emerged as a promising alternative to the\nautoregressive paradigm, offering improved controllability, bidirectional\ncontext modeling, and robust generation. However, their application to the\naudio modality remains underexplored. In this work, we introduce\n\\textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed\nto perform spoken language understanding. DIFFA integrates a frozen diffusion\nlanguage model with a lightweight dual-adapter architecture that bridges speech\nunderstanding and natural language reasoning. We employ a two-stage training\npipeline: first, aligning semantic representations via an ASR objective; then,\nlearning instruction-following abilities through synthetic audio-caption pairs\nautomatically generated by prompting LLMs. Despite being trained on only 960\nhours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates\ncompetitive performance on major benchmarks, including MMSU, MMAU, and\nVoiceBench, outperforming several autoregressive open-source baselines. Our\nresults reveal the potential of diffusion-based language models for efficient\nand scalable audio understanding, opening a new direction for speech-driven AI.\nOur code will be available at https://github.com/NKU-HLT/DIFFA.git.",
      "url": "http://arxiv.org/abs/2507.18452v1",
      "published_time_eastern_timestamp": 1753367752.0
    },
    {
      "title": "AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic\n  Tabular Data",
      "summary": "The cognitive and reasoning abilities of large language models (LLMs) have\nenabled remarkable progress in natural language processing. However, their\nperformance in interpreting structured data, especially in tabular formats,\nremains limited. Although benchmarks for English tabular data are widely\navailable, Arabic is still underrepresented because of the limited availability\nof public resources and its unique language features. To address this gap, we\npresent AraTable, a novel and comprehensive benchmark designed to evaluate the\nreasoning and understanding capabilities of LLMs when applied to Arabic tabular\ndata. AraTable consists of various evaluation tasks, such as direct question\nanswering, fact verification, and complex reasoning, involving a wide range of\nArabic tabular sources. Our methodology follows a hybrid pipeline, where\ninitial content is generated by LLMs and subsequently filtered and verified by\nhuman experts to ensure high dataset quality. Initial analyses using AraTable\nshow that, while LLMs perform adequately on simpler tabular tasks such as\ndirect question answering, they continue to face significant cognitive\nchallenges when tasks require deeper reasoning and fact verification. This\nindicates that there are substantial opportunities for future work to improve\nperformance on complex tabular reasoning tasks. We also propose a fully\nautomated evaluation framework that uses a self-deliberation mechanism and\nachieves performance nearly identical to that of human judges. This research\nprovides a valuable, publicly available resource and evaluation framework that\ncan help accelerate the development of foundational models for processing and\nanalysing Arabic structured data.",
      "url": "http://arxiv.org/abs/2507.18442v1",
      "published_time_eastern_timestamp": 1753367201.0
    },
    {
      "title": "FinDPO: Financial Sentiment Analysis for Algorithmic Trading through\n  Preference Optimization of LLMs",
      "summary": "Opinions expressed in online finance-related textual data are having an\nincreasingly profound impact on trading decisions and market movements. This\ntrend highlights the vital role of sentiment analysis as a tool for quantifying\nthe nature and strength of such opinions. With the rapid development of\nGenerative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)\nhave become the de facto standard for financial sentiment analysis. However,\nthe SFT paradigm can lead to memorization of the training data and often fails\nto generalize to unseen samples. This is a critical limitation in financial\ndomains, where models must adapt to previously unobserved events and the\nnuanced, domain-specific language of finance. To this end, we introduce FinDPO,\nthe first finance-specific LLM framework based on post-training human\npreference alignment via Direct Preference Optimization (DPO). The proposed\nFinDPO achieves state-of-the-art performance on standard sentiment\nclassification benchmarks, outperforming existing supervised fine-tuned models\nby 11% on the average. Uniquely, the FinDPO framework enables the integration\nof a fine-tuned causal LLM into realistic portfolio strategies through a novel\n'logit-to-score' conversion, which transforms discrete sentiment predictions\ninto continuous, rankable sentiment scores (probabilities). In this way,\nsimulations demonstrate that FinDPO is the first sentiment-based approach to\nmaintain substantial positive returns of 67% annually and strong risk-adjusted\nperformance, as indicated by a Sharpe ratio of 2.0, even under realistic\ntransaction costs of 5 basis points (bps).",
      "url": "http://arxiv.org/abs/2507.18417v1",
      "published_time_eastern_timestamp": 1753365425.0
    },
    {
      "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
      "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.",
      "url": "http://arxiv.org/abs/2507.18392v1",
      "published_time_eastern_timestamp": 1753362921.0
    },
    {
      "title": "Revisiting LLM Reasoning via Information Bottleneck",
      "summary": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance.",
      "url": "http://arxiv.org/abs/2507.18391v1",
      "published_time_eastern_timestamp": 1753362865.0
    },
    {
      "title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent\n  Thinking in LLMs for Financial Scenarios",
      "summary": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance.",
      "url": "http://arxiv.org/abs/2507.18368v1",
      "published_time_eastern_timestamp": 1753361249.0
    }
  ]
}
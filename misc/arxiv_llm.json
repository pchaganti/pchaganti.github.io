{
  "last_updated": "2025-06-08T23:47:42.431501-04:00",
  "papers": [
    {
      "title": "Eigenspectrum Analysis of Neural Networks without Aspect Ratio Bias",
      "summary": "Diagnosing deep neural networks (DNNs) through the eigenspectrum of weight\nmatrices has been an active area of research in recent years. At a high level,\neigenspectrum analysis of DNNs involves measuring the heavytailness of the\nempirical spectral densities (ESD) of weight matrices. It provides insight into\nhow well a model is trained and can guide decisions on assigning better\nlayer-wise training hyperparameters. In this paper, we address a challenge\nassociated with such eigenspectrum methods: the impact of the aspect ratio of\nweight matrices on estimated heavytailness metrics. We demonstrate that\nmatrices of varying sizes (and aspect ratios) introduce a non-negligible bias\nin estimating heavytailness metrics, leading to inaccurate model diagnosis and\nlayer-wise hyperparameter assignment. To overcome this challenge, we propose\nFARMS (Fixed-Aspect-Ratio Matrix Subsampling), a method that normalizes the\nweight matrices by subsampling submatrices with a fixed aspect ratio. Instead\nof measuring the heavytailness of the original ESD, we measure the average ESD\nof these subsampled submatrices. We show that measuring the heavytailness of\nthese submatrices with the fixed aspect ratio can effectively mitigate the\naspect ratio bias. We validate our approach across various optimization\ntechniques and application domains that involve eigenspectrum analysis of\nweights, including image classification in computer vision (CV) models,\nscientific machine learning (SciML) model training, and large language model\n(LLM) pruning. Our results show that despite its simplicity, FARMS uniformly\nimproves the accuracy of eigenspectrum analysis while enabling more effective\nlayer-wise hyperparameter assignment in these application domains. In one of\nthe LLM pruning experiments, FARMS reduces the perplexity of the LLaMA-7B model\nby 17.3% when compared with the state-of-the-art method.",
      "url": "http://arxiv.org/abs/2506.06280v1",
      "published_time_eastern_timestamp": 1749232768.0
    },
    {
      "title": "CoMemo: LVLMs Need Image Context with Image Memory",
      "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
      "url": "http://arxiv.org/abs/2506.06279v1",
      "published_time_eastern_timestamp": 1749232746.0
    },
    {
      "title": "Distillation Robustifies Unlearning",
      "summary": "Current LLM unlearning methods are not robust: they can be reverted easily\nwith a few steps of finetuning. This is true even for the idealized unlearning\nmethod of training to imitate an oracle model that was never exposed to\nunwanted information, suggesting that output-based finetuning is insufficient\nto achieve robust unlearning. In a similar vein, we find that training a\nrandomly initialized student to imitate an unlearned model transfers desired\nbehaviors while leaving undesired capabilities behind. In other words,\ndistillation robustifies unlearning. Building on this insight, we propose\nUnlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an\nunlearned model into a partially noised copy of itself. UNDO introduces a\ntunable tradeoff between compute cost and robustness, establishing a new Pareto\nfrontier on synthetic language and arithmetic tasks. At its strongest setting,\nUNDO matches the robustness of a model retrained from scratch with perfect data\nfiltering while using only 60-80% of the compute and requiring only 0.01% of\nthe pretraining data to be labeled. We also show that UNDO robustifies\nunlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP)\nbenchmark. Since distillation is widely used in practice, incorporating an\nunlearning step beforehand offers a convenient path to robust capability\nremoval.",
      "url": "http://arxiv.org/abs/2506.06278v1",
      "published_time_eastern_timestamp": 1749232734.0
    },
    {
      "title": "AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization",
      "summary": "Large Language Models (LLMs) have achieved impressive performance in text\nsummarization and are increasingly deployed in real-world applications.\nHowever, these systems often inherit associative and framing biases from\npre-training data, leading to inappropriate or unfair outputs in downstream\ntasks. In this work, we present AdvSumm (Adversarial Summarization), a\ndomain-agnostic training framework designed to mitigate bias in text\nsummarization through improved generalization. Inspired by adversarial\nrobustness, AdvSumm introduces a novel Perturber component that applies\ngradient-guided perturbations at the embedding level of Sequence-to-Sequence\nmodels, enhancing the model's robustness to input variations. We empirically\ndemonstrate that AdvSumm effectively reduces different types of bias in\nsummarization-specifically, name-nationality bias and political framing\nbias-without compromising summarization quality. Compared to standard\ntransformers and data augmentation techniques like back-translation, AdvSumm\nachieves stronger bias mitigation performance across benchmark datasets.",
      "url": "http://arxiv.org/abs/2506.06273v1",
      "published_time_eastern_timestamp": 1749232672.0
    },
    {
      "title": "PersonaAgent: When Large Language Model Agents Meet Personalization at\n  Test Time",
      "summary": "Large Language Model (LLM) empowered agents have recently emerged as advanced\nparadigms that exhibit impressive capabilities in a wide range of domains and\ntasks. Despite their potential, current LLM agents often adopt a\none-size-fits-all approach, lacking the flexibility to respond to users'\nvarying needs and preferences. This limitation motivates us to develop\nPersonaAgent, the first personalized LLM agent framework designed to address\nversatile personalization tasks. Specifically, PersonaAgent integrates two\ncomplementary components - a personalized memory module that includes episodic\nand semantic memory mechanisms; a personalized action module that enables the\nagent to perform tool actions tailored to the user. At the core, the persona\n(defined as unique system prompt for each user) functions as an intermediary:\nit leverages insights from personalized memory to control agent actions, while\nthe outcomes of these actions in turn refine the memory. Based on the\nframework, we propose a test-time user-preference alignment strategy that\nsimulate the latest n interactions to optimize the persona prompt, ensuring\nreal-time user preference alignment through textual loss feedback between\nsimulated and ground-truth responses. Experimental evaluations demonstrate that\nPersonaAgent significantly outperforms other baseline methods by not only\npersonalizing the action space effectively but also scaling during test-time\nreal-world applications. These results underscore the feasibility and potential\nof our approach in delivering tailored, dynamic user experiences.",
      "url": "http://arxiv.org/abs/2506.06254v1",
      "published_time_eastern_timestamp": 1749230989.0
    },
    {
      "title": "Visual Graph Arena: Evaluating Visual Conceptualization of Vision and\n  Multimodal Large Language Models",
      "summary": "Recent advancements in multimodal large language models have driven\nbreakthroughs in visual question answering. Yet, a critical gap persists,\n`conceptualization'-the ability to recognize and reason about the same concept\ndespite variations in visual form, a basic ability of human reasoning. To\naddress this challenge, we introduce the Visual Graph Arena (VGA), a dataset\nfeaturing six graph-based tasks designed to evaluate and improve AI systems'\ncapacity for visual abstraction. VGA uses diverse graph layouts (e.g.,\nKamada-Kawai vs. planar) to test reasoning independent of visual form.\nExperiments with state-of-the-art vision models and multimodal LLMs reveal a\nstriking divide: humans achieved near-perfect accuracy across tasks, while\nmodels totally failed on isomorphism detection and showed limited success in\npath/cycle tasks. We further identify behavioral anomalies suggesting\npseudo-intelligent pattern matching rather than genuine understanding. These\nfindings underscore fundamental limitations in current AI models for visual\nunderstanding. By isolating the challenge of representation-invariant\nreasoning, the VGA provides a framework to drive progress toward human-like\nconceptualization in AI visual models. The Visual Graph Arena is available at:\n\\href{https://vga.csail.mit.edu/}{vga.csail.mit.edu}",
      "url": "http://arxiv.org/abs/2506.06242v1",
      "published_time_eastern_timestamp": 1749229585.0
    },
    {
      "title": "Bridging External and Parametric Knowledge: Mitigating Hallucination of\n  LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge",
      "summary": "Retrieval-augmented generation (RAG) is a cost-effective approach to mitigate\nthe hallucination of Large Language Models (LLMs) by incorporating the\nretrieved external knowledge into the generation process. However, external\nknowledge may conflict with the parametric knowledge of LLMs. Furthermore,\ncurrent LLMs lack inherent mechanisms for resolving such knowledge conflicts,\nmaking traditional RAG methods suffer from degraded performance and stability.\nThus, we propose a Dual-Stream Knowledge-Augmented Framework for Shared-Private\nSemantic Synergy (DSSP-RAG). Central to the framework is a novel approach that\nrefines self-attention into a mixed-attention, distinguishing shared and\nprivate semantics for a controlled internal-external knowledge integration. To\neffectively facilitate DSSP in RAG, we further introduce an unsupervised\nhallucination detection method based on cognitive uncertainty, ensuring the\nnecessity of introducing knowledge, and an Energy Quotient (EQ) based on\nattention difference matrices to reduce noise in the retrieved external\nknowledge. Extensive experiments on benchmark datasets show that DSSP-RAG can\neffectively resolve conflicts and enhance the complementarity of dual-stream\nknowledge, leading to superior performance over strong baselines.",
      "url": "http://arxiv.org/abs/2506.06240v1",
      "published_time_eastern_timestamp": 1749229223.0
    },
    {
      "title": "CompilerGPT: Leveraging Large Language Models for Analyzing and Acting\n  on Compiler Optimization Reports",
      "summary": "Current compiler optimization reports often present complex, technical\ninformation that is difficult for programmers to interpret and act upon\neffectively. This paper assesses the capability of large language models (LLM)\nto understand compiler optimization reports and automatically rewrite the code\naccordingly.\n  To this end, the paper introduces CompilerGPT, a novel framework that\nautomates the interaction between compilers, LLMs, and user defined test and\nevaluation harness. CompilerGPT's workflow runs several iterations and reports\non the obtained results.\n  Experiments with two leading LLM models (GPT-4o and Claude Sonnet),\noptimization reports from two compilers (Clang and GCC), and five benchmark\ncodes demonstrate the potential of this approach. Speedups of up to 6.5x were\nobtained, though not consistently in every test. This method holds promise for\nimproving compiler usability and streamlining the software optimization\nprocess.",
      "url": "http://arxiv.org/abs/2506.06227v1",
      "published_time_eastern_timestamp": 1749228134.0
    },
    {
      "title": "PROVSYN: Synthesizing Provenance Graphs for Data Augmentation in\n  Intrusion Detection Systems",
      "summary": "Provenance graph analysis plays a vital role in intrusion detection,\nparticularly against Advanced Persistent Threats (APTs), by exposing complex\nattack patterns. While recent systems combine graph neural networks (GNNs) with\nnatural language processing (NLP) to capture structural and semantic features,\ntheir effectiveness is limited by class imbalance in real-world data. To\naddress this, we introduce PROVSYN, an automated framework that synthesizes\nprovenance graphs through a three-phase pipeline: (1) heterogeneous graph\nstructure synthesis with structural-semantic modeling, (2) rule-based\ntopological refinement, and (3) context-aware textual attribute synthesis using\nlarge language models (LLMs). PROVSYN includes a comprehensive evaluation\nframework that integrates structural, textual, temporal, and embedding-based\nmetrics, along with a semantic validation mechanism to assess the correctness\nof generated attack patterns and system behaviors. To demonstrate practical\nutility, we use the synthetic graphs to augment training datasets for\ndownstream APT detection models. Experimental results show that PROVSYN\nproduces high-fidelity graphs and improves detection performance through\neffective data augmentation.",
      "url": "http://arxiv.org/abs/2506.06226v1",
      "published_time_eastern_timestamp": 1749228077.0
    },
    {
      "title": "Can Theoretical Physics Research Benefit from Language Agents?",
      "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.",
      "url": "http://arxiv.org/abs/2506.06214v1",
      "published_time_eastern_timestamp": 1749226806.0
    },
    {
      "title": "Building Models of Neurological Language",
      "summary": "This report documents the development and evaluation of domain-specific\nlanguage models for neurology. Initially focused on building a bespoke model,\nthe project adapted to rapid advances in open-source and commercial medical\nLLMs, shifting toward leveraging retrieval-augmented generation (RAG) and\nrepresentational models for secure, local deployment. Key contributions include\nthe creation of neurology-specific datasets (case reports, QA sets,\ntextbook-derived data), tools for multi-word expression extraction, and\ngraph-based analyses of medical terminology. The project also produced scripts\nand Docker containers for local hosting. Performance metrics and graph\ncommunity results are reported, with future possible work open for multimodal\nmodels using open-source architectures like phi-4.",
      "url": "http://arxiv.org/abs/2506.06208v1",
      "published_time_eastern_timestamp": 1749226468.0
    },
    {
      "title": "Astra: Toward General-Purpose Mobile Robots via Hierarchical Multimodal\n  Learning",
      "summary": "Modern robot navigation systems encounter difficulties in diverse and complex\nindoor environments. Traditional approaches rely on multiple modules with small\nmodels or rule-based systems and thus lack adaptability to new environments. To\naddress this, we developed Astra, a comprehensive dual-model architecture,\nAstra-Global and Astra-Local, for mobile robot navigation. Astra-Global, a\nmultimodal LLM, processes vision and language inputs to perform self and goal\nlocalization using a hybrid topological-semantic graph as the global map, and\noutperforms traditional visual place recognition methods. Astra-Local, a\nmultitask network, handles local path planning and odometry estimation. Its 4D\nspatial-temporal encoder, trained through self-supervised learning, generates\nrobust 4D features for downstream tasks. The planning head utilizes flow\nmatching and a novel masked ESDF loss to minimize collision risks for\ngenerating local trajectories, and the odometry head integrates multi-sensor\ninputs via a transformer encoder to predict the relative pose of the robot.\nDeployed on real in-house mobile robots, Astra achieves high end-to-end mission\nsuccess rate across diverse indoor environments.",
      "url": "http://arxiv.org/abs/2506.06205v1",
      "published_time_eastern_timestamp": 1749226127.0
    },
    {
      "title": "Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with\n  a Multi-Agent Approach",
      "summary": "Large language models can translate natural-language chart descriptions into\nrunnable code, yet approximately 15\\% of the generated scripts still fail to\nexecute, even after supervised fine-tuning and reinforcement learning. We\ninvestigate whether this persistent error rate stems from model limitations or\nfrom reliance on a single-prompt design. To explore this, we propose a\nlightweight multi-agent pipeline that separates drafting, execution, repair,\nand judgment, using only an off-the-shelf GPT-4o-mini model. On the\n\\textsc{Text2Chart31} benchmark, our system reduces execution errors to 4.5\\%\nwithin three repair iterations, outperforming the strongest fine-tuned baseline\nby nearly 5 percentage points while requiring significantly less compute.\nSimilar performance is observed on the \\textsc{ChartX} benchmark, with an error\nrate of 4.6\\%, demonstrating strong generalization. Under current benchmarks,\nexecution success appears largely solved. However, manual review reveals that 6\nout of 100 sampled charts contain hallucinations, and an LLM-based\naccessibility audit shows that only 33.3\\% (\\textsc{Text2Chart31}) and 7.2\\%\n(\\textsc{ChartX}) of generated charts satisfy basic colorblindness guidelines.\nThese findings suggest that future work should shift focus from execution\nreliability toward improving chart aesthetics, semantic fidelity, and\naccessibility.",
      "url": "http://arxiv.org/abs/2506.06175v1",
      "published_time_eastern_timestamp": 1749224357.0
    },
    {
      "title": "Technical Report for Egocentric Mistake Detection for the HoloAssist\n  Challenge",
      "summary": "In this report, we address the task of online mistake detection, which is\nvital in domains like industrial automation and education, where real-time\nvideo analysis allows human operators to correct errors as they occur. While\nprevious work focuses on procedural errors involving action order, broader\nerror types must be addressed for real-world use. We introduce an online\nmistake detection framework that handles both procedural and execution errors\n(e.g., motor slips or tool misuse). Upon detecting an error, we use a large\nlanguage model (LLM) to generate explanatory feedback. Experiments on the\nHoloAssist benchmark confirm the effectiveness of our approach, where our\napproach is placed second on the mistake detection task.",
      "url": "http://arxiv.org/abs/2506.06174v1",
      "published_time_eastern_timestamp": 1749224349.0
    },
    {
      "title": "The Lock-in Hypothesis: Stagnation by Algorithm",
      "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com",
      "url": "http://arxiv.org/abs/2506.06166v1",
      "published_time_eastern_timestamp": 1749223891.0
    },
    {
      "title": "Recommender systems, stigmergy, and the tyranny of popularity",
      "summary": "Scientific recommender systems, such as Google Scholar and Web of Science,\nare essential tools for discovery. Search algorithms that power work through\nstigmergy, a collective intelligence mechanism that surfaces useful paths\nthrough repeated engagement. While generally effective, this\n``rich-get-richer'' dynamic results in a small number of high-profile papers\nthat dominate visibility. This essay argues argue that these algorithm\nover-reliance on popularity fosters intellectual homogeneity and exacerbates\nstructural inequities, stifling innovative and diverse perspectives critical\nfor scientific progress. We propose an overhaul of search platforms to\nincorporate user-specific calibration, allowing researchers to manually adjust\nthe weights of factors like popularity, recency, and relevance. We also advise\nplatform developers on how word embeddings and LLMs could be implemented in\nways that increase user autonomy. While our suggestions are particularly\npertinent to aligning recommender systems with scientific values, these ideas\nare broadly applicable to information access systems in general. Designing\nplatforms that increase user autonomy is an important step toward more robust\nand dynamic information",
      "url": "http://arxiv.org/abs/2506.06162v1",
      "published_time_eastern_timestamp": 1749223643.0
    },
    {
      "title": "Masked Language Models are Good Heterogeneous Graph Generalizers",
      "summary": "Heterogeneous graph neural networks (HGNNs) excel at capturing structural and\nsemantic information in heterogeneous graphs (HGs), while struggling to\ngeneralize across domains and tasks. Recently, some researchers have turned to\nintegrating HGNNs with large language models (LLMs) for more generalizable\nheterogeneous graph learning. However, these approaches typically extract\nstructural information via HGNNs as HG tokens, and disparities in embedding\nspaces between HGNNs and LLMs have been shown to bias the LLM's comprehension\nof HGs. Moreover, as these HG tokens are often derived from node-level tasks,\nthe model's ability to generalize across tasks remains limited. To this end, we\npropose a simple yet effective Masked Language Modeling-based method, called\nMLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens\nto extract structural and semantic information inherent in HGs, and designs\ncustomized textual templates to unify different graph tasks into a coherent\ncloze-style \"mask\" token prediction paradigm. Specifically, MLM4HG first\nconverts HGs from various domains to texts based on metapaths, and subsequently\ncombines them with the unified task texts to form a HG-based corpus. Moreover,\nthe corpus is fed into a pretrained LM for fine-tuning with a constrained\ntarget vocabulary, enabling the fine-tuned LM to generalize to unseen target\nHGs. Extensive cross-domain and multi-task experiments on four real-world\ndatasets demonstrate the superior generalization performance of MLM4HG over\nstate-of-the-art methods in both few-shot and zero-shot scenarios. Our code is\navailable at https://github.com/BUPT-GAMMA/MLM4HG.",
      "url": "http://arxiv.org/abs/2506.06157v1",
      "published_time_eastern_timestamp": 1749223284.0
    },
    {
      "title": "Personalized Large Language Models Can Increase the Belief Accuracy of\n  Social Networks",
      "summary": "Large language models (LLMs) are increasingly involved in shaping public\nunderstanding on contested issues. This has led to substantial discussion about\nthe potential of LLMs to reinforce or correct misperceptions. While existing\nliterature documents the impact of LLMs on individuals' beliefs, limited work\nexplores how LLMs affect social networks. We address this gap with a\npre-registered experiment (N = 1265) around the 2024 US presidential election,\nwhere we empirically explore the impact of personalized LLMs on belief accuracy\nin the context of social networks. The LLMs are constructed to be personalized,\noffering messages tailored to individuals' profiles, and to have guardrails for\naccurate information retrieval. We find that the presence of a personalized LLM\nleads individuals to update their beliefs towards the truth. More importantly,\nindividuals with a personalized LLM in their social network not only choose to\nfollow it, indicating they would like to obtain information from it in\nsubsequent interactions, but also construct subsequent social networks to\ninclude other individuals with beliefs similar to the LLM -- in this case, more\naccurate beliefs. Therefore, our results show that LLMs have the capacity to\ninfluence individual beliefs and the social networks in which people exist, and\nhighlight the potential of LLMs to act as corrective agents in online\nenvironments. Our findings can inform future strategies for responsible\nAI-mediated communication.",
      "url": "http://arxiv.org/abs/2506.06153v1",
      "published_time_eastern_timestamp": 1749222997.0
    },
    {
      "title": "Joint-GCG: Unified Gradient-Based Poisoning Attacks on\n  Retrieval-Augmented Generation Systems",
      "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by retrieving relevant documents from external corpora before generating\nresponses. This approach significantly expands LLM capabilities by leveraging\nvast, up-to-date external knowledge. However, this reliance on external\nknowledge makes RAG systems vulnerable to corpus poisoning attacks that\nmanipulate generated outputs via poisoned document injection. Existing\npoisoning attack strategies typically treat the retrieval and generation stages\nas disjointed, limiting their effectiveness. We propose Joint-GCG, the first\nframework to unify gradient-based attacks across both retriever and generator\nmodels through three innovations: (1) Cross-Vocabulary Projection for aligning\nembedding spaces, (2) Gradient Tokenization Alignment for synchronizing\ntoken-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically\nbalancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves\nat most 25% and an average of 5% higher attack success rate than previous\nmethods across multiple retrievers and generators. While optimized under a\nwhite-box assumption, the generated poisons show unprecedented transferability\nto unseen models. Joint-GCG's innovative unification of gradient-based attacks\nacross retrieval and generation stages fundamentally reshapes our understanding\nof vulnerabilities within RAG systems. Our code is available at\nhttps://github.com/NicerWang/Joint-GCG.",
      "url": "http://arxiv.org/abs/2506.06151v1",
      "published_time_eastern_timestamp": 1749222726.0
    },
    {
      "title": "Table-r1: Self-supervised and Reinforcement Learning for Program-based\n  Table Reasoning in Small Language Models",
      "summary": "Table reasoning (TR) requires structured reasoning over semi-structured\ntabular data and remains challenging, particularly for small language models\n(SLMs, e.g., LLaMA-8B) due to their limited capacity compared to large LMs\n(LLMs, e.g., GPT-4o). To narrow this gap, we explore program-based TR (P-TR),\nwhich circumvents key limitations of text-based TR (T-TR), notably in numerical\nreasoning, by generating executable programs. However, applying P-TR to SLMs\nintroduces two challenges: (i) vulnerability to heterogeneity in table layouts,\nand (ii) inconsistency in reasoning due to limited code generation capability.\nWe propose Table-r1, a two-stage P-TR method designed for SLMs. Stage 1\nintroduces an innovative self-supervised learning task, Layout Transformation\nInference, to improve tabular layout generalization from a programmatic view.\nStage 2 adopts a mix-paradigm variant of Group Relative Policy Optimization,\nenhancing P-TR consistency while allowing dynamic fallback to T-TR when needed.\nExperiments on four TR benchmarks demonstrate that Table-r1 outperforms all\nSLM-based methods, achieving at least a 15% accuracy improvement over the base\nmodel (LLaMA-8B) across all datasets and reaching performance competitive with\nLLMs.",
      "url": "http://arxiv.org/abs/2506.06137v1",
      "published_time_eastern_timestamp": 1749221539.0
    }
  ]
}
{
  "last_updated": "2025-05-08T20:57:09.007958-04:00",
  "papers": [
    {
      "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning",
      "summary": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.",
      "url": "http://arxiv.org/abs/2505.04623v1",
      "published_time_eastern_timestamp": 1746640789.0
    },
    {
      "title": "On Path to Multimodal Generalist: General-Level and General-Bench",
      "summary": "The Multimodal Large Language Model (MLLM) is currently experiencing rapid\ngrowth, driven by the advanced capabilities of LLMs. Unlike earlier\nspecialists, existing MLLMs are evolving towards a Multimodal Generalist\nparadigm. Initially limited to understanding multiple modalities, these models\nhave advanced to not only comprehend but also generate across modalities. Their\ncapabilities have expanded from coarse-grained to fine-grained multimodal\nunderstanding and from supporting limited modalities to arbitrary ones. While\nmany benchmarks exist to assess MLLMs, a critical question arises: Can we\nsimply assume that higher performance across tasks indicates a stronger MLLM\ncapability, bringing us closer to human-level AI? We argue that the answer is\nnot as straightforward as it seems. This project introduces General-Level, an\nevaluation framework that defines 5-scale levels of MLLM performance and\ngenerality, offering a methodology to compare MLLMs and gauge the progress of\nexisting systems towards more robust multimodal generalists and, ultimately,\ntowards AGI. At the core of the framework is the concept of Synergy, which\nmeasures whether models maintain consistent capabilities across comprehension\nand generation, and across multiple modalities. To support this evaluation, we\npresent General-Bench, which encompasses a broader spectrum of skills,\nmodalities, formats, and capabilities, including over 700 tasks and 325,800\ninstances. The evaluation results that involve over 100 existing\nstate-of-the-art MLLMs uncover the capability rankings of generalists,\nhighlighting the challenges in reaching genuine AI. We expect this project to\npave the way for future research on next-generation multimodal foundation\nmodels, providing a robust infrastructure to accelerate the realization of AGI.\nProject page: https://generalist.top/",
      "url": "http://arxiv.org/abs/2505.04620v1",
      "published_time_eastern_timestamp": 1746640772.0
    },
    {
      "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue\n  Resolution",
      "summary": "The GitHub issue resolution task aims to resolve issues reported in\nrepositories automatically. With advances in large language models (LLMs), this\ntask has gained increasing attention, and several benchmarks are proposed to\nevaluate the issue resolution ability of LLMs. However, existing benchmarks\nhave three main limitations. First, current benchmarks focus on a single\nprogramming language, limiting the evaluation of issues from repositories\nacross different languages. Second, they usually cover a narrow range of\ndomains, which may fail to represent the diversity of real-world issues. Third,\nexisting benchmarks rely solely on textual information in issue descriptions,\noverlooking multimodal information such as images in issues. In this paper, we\npropose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual,\nmultimodal, and multi-domain. OmniGIRL includes 959 task instances, which are\ncollected from repositories across four programming languages (i.e., Python,\nJavaScript, TypeScript, and Java) and eight different domains. Our evaluation\nshows that current LLMs show limited performances on OmniGIRL. Notably, the\nbest-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we\nfind that current LLMs struggle to resolve issues requiring understanding\nimages. The best performance is achieved by Claude-3.5-Sonnet, which resolves\nonly 10.5% of the issues with image information. Finally, we analyze the\nreasons behind current LLMs' failure on OmniGIRL, providing insights for future\nimprovements.",
      "url": "http://arxiv.org/abs/2505.04606v1",
      "published_time_eastern_timestamp": 1746640270.0
    },
    {
      "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection",
      "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.",
      "url": "http://arxiv.org/abs/2505.04594v2",
      "published_time_eastern_timestamp": 1746639443.0
    },
    {
      "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
      "summary": "Effective information searching is essential for enhancing the reasoning and\ngeneration capabilities of large language models (LLMs). Recent research has\nexplored using reinforcement learning (RL) to improve LLMs' search capabilities\nby interacting with live search engines in real-world environments. While these\napproaches show promising results, they face two major challenges: (1)\nUncontrolled Document Quality: The quality of documents returned by search\nengines is often unpredictable, introducing noise and instability into the\ntraining process. (2) Prohibitively High API Costs: RL training requires\nfrequent rollouts, potentially involving hundreds of thousands of search\nrequests, which incur substantial API expenses and severely constrain\nscalability. To address these challenges, we introduce ZeroSearch, a\nreinforcement learning framework that incentivizes the search capabilities of\nLLMs without interacting with real search engines. Our approach begins with\nlightweight supervised fine-tuning to transform the LLM into a retrieval module\ncapable of generating both relevant and noisy documents in response to a query.\nDuring RL training, we employ a curriculum-based rollout strategy that\nincrementally degrades the quality of generated documents, progressively\neliciting the model's reasoning ability by exposing it to increasingly\nchallenging retrieval scenarios. Extensive experiments demonstrate that\nZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B\nLLM as the retrieval module. Remarkably, a 7B retrieval module achieves\ncomparable performance to the real search engine, while a 14B retrieval module\neven surpasses it. Furthermore, it generalizes well across both base and\ninstruction-tuned models of various parameter sizes and is compatible with a\nwide range of RL algorithms.",
      "url": "http://arxiv.org/abs/2505.04588v1",
      "published_time_eastern_timestamp": 1746639022.0
    },
    {
      "title": "SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for\n  Open-Ended Questions",
      "summary": "Feedback is important in supporting student learning. While various automated\nfeedback systems have been implemented to make the feedback scalable, many\nexisting solutions only focus on generating text-based feedback. As is\nindicated in the multimedia learning principle, learning with more modalities\ncould help utilize more separate channels, reduce the cognitive load and\nfacilitate students' learning. Hence, it is important to explore the potential\nof Artificial Intelligence (AI) in feedback generation from and to different\nmodalities. Our study leverages Large Language Models (LLMs) for textual\nfeedback with the supplementary guidance from other modality - relevant lecture\nslide retrieved from the slides hub. Through an online crowdsourcing study\n(N=91), this study investigates learning gains and student perceptions using a\n2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant\nslide), evaluating the clarity, engagement, perceived effectiveness, and\nreliability) of AI-facilitated multimodal feedback. We observed significant\npre-to-post learning gains across all conditions. However, the differences in\nthese gains were not statistically significant between conditions. The\npost-survey revealed that students found the slide feedback helpful in their\nlearning process, though they reported difficulty in understanding it.\nRegarding the AI-generated open-ended feedback, students considered it\npersonalized and relevant to their responses, but they expressed lower trust in\nthe AI feedback compared to human-generated feedback.",
      "url": "http://arxiv.org/abs/2505.04584v1",
      "published_time_eastern_timestamp": 1746638680.0
    },
    {
      "title": "Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code\n  Development",
      "summary": "Large Language Models (LLM) have significantly transformed various domains,\nincluding software development. These models assist programmers in generating\ncode, potentially increasing productivity and efficiency. However, the\nenvironmental impact of utilising these AI models is substantial, given their\nhigh energy consumption during both training and inference stages. This\nresearch aims to compare the energy consumption of manual software development\nversus an LLM-assisted approach, using Codeforces as a simulation platform for\nsoftware development. The goal is to quantify the environmental impact and\npropose strategies for minimising the carbon footprint of using LLM in software\ndevelopment. Our results show that the LLM-assisted code generation leads on\naverage to 32.72 higher carbon footprint than the manual one. Moreover, there\nis a significant correlation between task complexity and the difference in the\ncarbon footprint of the two approaches.",
      "url": "http://arxiv.org/abs/2505.04521v1",
      "published_time_eastern_timestamp": 1746633126.0
    },
    {
      "title": "Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs",
      "summary": "Sparse large language models (LLMs) with Mixture of Experts (MoE) and close\nto a trillion parameters are dominating the realm of most capable language\nmodels. However, the massive model scale poses significant challenges for the\nunderlying software and hardware systems. In this paper, we aim to uncover a\nrecipe to harness such scale on Ascend NPUs. The key goals are better usage of\nthe computing resources under the dynamic sparse model structures and\nmaterializing the expected performance gain on the actual hardware. To select\nmodel configurations suitable for Ascend NPUs without repeatedly running the\nexpensive experiments, we leverage simulation to compare the trade-off of\nvarious model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM\nwith 718 billion parameters, and we conducted experiments on the model to\nverify the simulation results. On the system side, we dig into Expert\nParallelism to optimize the communication between NPU devices to reduce the\nsynchronization overhead. We also optimize the memory efficiency within the\ndevices to further reduce the parameter and activation management overhead. In\nthe end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with\nperformance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and\ndemonstrate that the Ascend system is capable of harnessing all the training\nstages of the state-of-the-art language models. Extensive experiments indicate\nthat our recipe can lead to efficient training of large-scale sparse language\nmodels with MoE. We also study the behaviors of such models for future\nreference.",
      "url": "http://arxiv.org/abs/2505.04519v1",
      "published_time_eastern_timestamp": 1746632796.0
    },
    {
      "title": "A Design Space for the Critical Validation of LLM-Generated Tabular Data",
      "summary": "LLM-generated tabular data is creating new opportunities for data-driven\napplications in academia, business, and society. To leverage benefits like\nmissing value imputation, labeling, and enrichment with context-aware\nattributes, LLM-generated data needs a critical validation process. The number\nof pioneering approaches is increasing fast, opening a promising validation\nspace that, so far, remains unstructured. We present a design space for the\ncritical validation of LLM-generated tabular data with two dimensions: First,\nthe Analysis Granularity dimension: from within-attribute (single-item and\nmulti-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second,\nthe Data Source dimension: differentiating between LLM-generated values, ground\ntruth values, explanations, and their combinations. We discuss analysis tasks\nfor each dimension cross-cut, map 19 existing validation approaches, and\ndiscuss the characteristics of two approaches in detail, demonstrating\ndescriptive power.",
      "url": "http://arxiv.org/abs/2505.04487v1",
      "published_time_eastern_timestamp": 1746630083.0
    },
    {
      "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design\n  Parametric 3D Model Generation",
      "summary": "Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines.",
      "url": "http://arxiv.org/abs/2505.04481v1",
      "published_time_eastern_timestamp": 1746629522.0
    },
    {
      "title": "TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven\n  Evolution",
      "summary": "Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo.",
      "url": "http://arxiv.org/abs/2505.04480v1",
      "published_time_eastern_timestamp": 1746629503.0
    },
    {
      "title": "M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation",
      "summary": "Sequential recommendation systems aim to predict users' next preferences\nbased on their interaction histories, but existing approaches face critical\nlimitations in efficiency and multi-scale pattern recognition. While\nTransformer-based methods struggle with quadratic computational complexity,\nrecent Mamba-based models improve efficiency but fail to capture periodic user\nbehaviors, leverage rich semantic information, or effectively fuse multimodal\nfeatures. To address these challenges, we propose \\model, a novel sequential\nrecommendation framework that integrates multi-scale Mamba with Fourier\nanalysis, Large Language Models (LLMs), and adaptive gating. First, we enhance\nMamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns\nin the frequency domain, separating meaningful trends from noise. Second, we\nincorporate LLM-based text embeddings to enrich sparse interaction data with\nsemantic context from item descriptions. Finally, we introduce a learnable gate\nmechanism to dynamically balance temporal (Mamba), frequency (FFT), and\nsemantic (LLM) features, ensuring harmonious multimodal fusion. Extensive\nexperiments demonstrate that \\model\\ achieves state-of-the-art performance,\nimproving Hit Rate@10 by 3.2\\% over existing Mamba-based models while\nmaintaining 20\\% faster inference than Transformer baselines. Our results\nhighlight the effectiveness of combining frequency analysis, semantic\nunderstanding, and adaptive fusion for sequential recommendation. Code and\ndatasets are available at: https://anonymous.4open.science/r/M2Rec.",
      "url": "http://arxiv.org/abs/2505.04445v1",
      "published_time_eastern_timestamp": 1746627269.0
    },
    {
      "title": "Towards Effectively Leveraging Execution Traces for Program Repair with\n  Code LLMs",
      "summary": "Large Language Models (LLMs) show promising performance on various\nprogramming tasks, including Automatic Program Repair (APR). However, most\napproaches to LLM-based APR are limited to the static analysis of the programs,\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\nin this work, we aim to remedy this potential blind spot by augmenting standard\nAPR prompts with program execution traces. We evaluate our approach using the\nGPT family of models on three popular APR datasets. Our findings suggest that\nsimply incorporating execution traces into the prompt provides a limited\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\ndataset / model configurations. We further find that the effectiveness of\nexecution traces for APR diminishes as their complexity increases. We explore\nseveral strategies for leveraging traces in prompts and demonstrate that\nLLM-optimized prompts help outperform trace-free prompts more consistently.\nAdditionally, we show trace-based prompting to be superior to finetuning a\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\nthe notion that execution traces can complement the reasoning abilities of the\nLLMs.",
      "url": "http://arxiv.org/abs/2505.04441v1",
      "published_time_eastern_timestamp": 1746627161.0
    },
    {
      "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language\n  Models",
      "summary": "Large language models (LLMs) trained over extensive corpora risk memorizing\nsensitive, copyrighted, or toxic content. To address this, we propose\nOBLIVIATE, a robust unlearning framework that removes targeted data while\npreserving model utility. The framework follows a structured process:\nextracting target tokens, building retain sets, and fine-tuning with a tailored\nloss function comprising three components -- masking, distillation, and world\nfact. Using low-rank adapters (LoRA), it ensures efficiency without\ncompromising unlearning quality. We conduct experiments on multiple datasets,\nincluding the Harry Potter series, WMDP, and TOFU, using a comprehensive suite\nof metrics: forget quality (new document-level memorization score), model\nutility, and fluency. Results demonstrate its effectiveness in resisting\nmembership inference attacks, minimizing the impact on retained data, and\nmaintaining robustness across diverse scenarios.",
      "url": "http://arxiv.org/abs/2505.04416v1",
      "published_time_eastern_timestamp": 1746625902.0
    },
    {
      "title": "YABLoCo: Yet Another Benchmark for Long Context Code Generation",
      "summary": "Large Language Models demonstrate the ability to solve various programming\ntasks, including code generation. Typically, the performance of LLMs is\nmeasured on benchmarks with small or medium-sized context windows of thousands\nof lines of code. At the same time, in real-world software projects,\nrepositories can span up to millions of LoC. This paper closes this gap by\ncontributing to the long context code generation benchmark (YABLoCo). The\nbenchmark featured a test set of 215 functions selected from four large\nrepositories with thousands of functions. The dataset contained metadata of\nfunctions, contexts of the functions with different levels of dependencies,\ndocstrings, functions bodies, and call graphs for each repository. This paper\npresents three key aspects of the contribution. First, the benchmark aims at\nfunction body generation in large repositories in C and C++, two languages not\ncovered by previous benchmarks. Second, the benchmark contains large\nrepositories from 200K to 2,000K LoC. Third, we contribute a scalable\nevaluation pipeline for efficient computing of the target metrics and a tool\nfor visual analysis of generated code. Overall, these three aspects allow for\nevaluating code generation in large repositories in C and C++.",
      "url": "http://arxiv.org/abs/2505.04406v1",
      "published_time_eastern_timestamp": 1746625343.0
    },
    {
      "title": "Large Means Left: Political Bias in Large Language Models Increases with\n  Their Number of Parameters",
      "summary": "With the increasing prevalence of artificial intelligence, careful evaluation\nof inherent biases needs to be conducted to form the basis for alleviating the\neffects these predispositions can have on users. Large language models (LLMs)\nare predominantly used by many as a primary source of information for various\ntopics. LLMs frequently make factual errors, fabricate data (hallucinations),\nor present biases, exposing users to misinformation and influencing opinions.\nEducating users on their risks is key to responsible use, as bias, unlike\nhallucinations, cannot be caught through data verification. We quantify the\npolitical bias of popular LLMs in the context of the recent vote of the German\nBundestag using the score produced by the Wahl-O-Mat. This metric measures the\nalignment between an individual's political views and the positions of German\npolitical parties. We compare the models' alignment scores to identify factors\ninfluencing their political preferences. Doing so, we discover a bias toward\nleft-leaning parties, most dominant in larger LLMs. Also, we find that the\nlanguage we use to communicate with the models affects their political views.\nAdditionally, we analyze the influence of a model's origin and release date and\ncompare the results to the outcome of the recent vote of the Bundestag. Our\nresults imply that LLMs are prone to exhibiting political bias. Large\ncorporations with the necessary means to develop LLMs, thus, knowingly or\nunknowingly, have a responsibility to contain these biases, as they can\ninfluence each voter's decision-making process and inform public opinion in\ngeneral and at scale.",
      "url": "http://arxiv.org/abs/2505.04393v1",
      "published_time_eastern_timestamp": 1746623921.0
    },
    {
      "title": "The Aloe Family Recipe for Open and Specialized Healthcare LLMs",
      "summary": "Purpose: With advancements in Large Language Models (LLMs) for healthcare,\nthe need arises for competitive open-source models to protect the public\ninterest. This work contributes to the field of open medical LLMs by optimizing\nkey stages of data preprocessing and training, while showing how to improve\nmodel safety (through DPO) and efficacy (through RAG). The evaluation\nmethodology used, which includes four different types of tests, defines a new\nstandard for the field. The resultant models, shown to be competitive with the\nbest private alternatives, are released with a permisive license.\n  Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5,\nAloe Beta uses a custom dataset to enhance public data with synthetic Chain of\nThought examples. The models undergo alignment with Direct Preference\nOptimization, emphasizing ethical and policy-aligned performance in the\npresence of jailbreaking attacks. Evaluation includes close-ended, open-ended,\nsafety and human assessments, to maximize the reliability of results.\n  Results: Recommendations are made across the entire pipeline, backed by the\nsolid performance of the Aloe Family. These models deliver competitive\nperformance across healthcare benchmarks and medical fields, and are often\npreferred by healthcare professionals. On bias and toxicity, the Aloe Beta\nmodels significantly improve safety, showing resilience to unseen jailbreaking\nattacks. For a responsible release, a detailed risk assessment specific to\nhealthcare is attached to the Aloe Family models.\n  Conclusion: The Aloe Beta models, and the recipe that leads to them, are a\nsignificant contribution to the open-source medical LLM field, offering\ntop-of-the-line performance while maintaining high ethical requirements. This\nwork sets a new standard for developing and reporting aligned LLMs in\nhealthcare.",
      "url": "http://arxiv.org/abs/2505.04388v1",
      "published_time_eastern_timestamp": 1746623594.0
    },
    {
      "title": "Benchmarking LLMs' Swarm intelligence",
      "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict constraints-such as limited local perception and communication,\ncharacteristic of natural swarms-remains largely unexplored, particularly\nconcerning the nuances of swarm intelligence. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination that arise\nwhen agents operate with incomplete spatio-temporal information. To bridge this\ngap, we introduce SwarmBench, a novel benchmark designed to systematically\nevaluate the swarm intelligence capabilities of LLMs acting as decentralized\nagents. SwarmBench features five foundational MAS coordination tasks within a\nconfigurable 2D grid environment, forcing agents to rely primarily on local\nsensory input (k x k view) and local communication. We propose metrics for\ncoordination effectiveness and analyze emergent group dynamics. Evaluating\nseveral leading LLMs in a zero-shot setting, we find significant performance\nvariations across tasks, highlighting the difficulties posed by local\ninformation constraints. While some coordination emerges, results indicate\nlimitations in robust planning and strategy formation under uncertainty in\nthese decentralized scenarios. Assessing LLMs under swarm-like conditions is\ncrucial for realizing their potential in future decentralized systems. We\nrelease SwarmBench as an open, extensible toolkit-built upon a customizable and\nscalable physical system with defined mechanical properties. It provides\nenvironments, prompts, evaluation scripts, and the comprehensive experimental\ndatasets generated, aiming to foster reproducible research into LLM-based MAS\ncoordination and the theoretical underpinnings of Embodied MAS. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
      "url": "http://arxiv.org/abs/2505.04364v1",
      "published_time_eastern_timestamp": 1746621121.0
    },
    {
      "title": "GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer\n  Pharmacovigilance",
      "summary": "In the realm of cancer treatment, summarizing adverse drug events (ADEs)\nreported by patients using prescribed drugs is crucial for enhancing\npharmacovigilance practices and improving drug-related decision-making. While\nthe volume and complexity of pharmacovigilance data have increased, existing\nresearch in this field has predominantly focused on general diseases rather\nthan specifically addressing cancer. This work introduces the task of grouped\nsummarization of adverse drug events reported by multiple patients using the\nsame drug for cancer treatment. To address the challenge of limited resources\nin cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug\nReaction and Summarization (MCADRS) dataset. This dataset includes\npharmacovigilance posts detailing patient concerns regarding drug efficacy and\nadverse effects, along with extracted labels for drug names, adverse drug\nevents, severity, and adversity of reactions, as well as summaries of ADEs for\neach drug. Additionally, we propose the Grouping and Abstractive Summarization\nof Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that\ncombines the information extraction capabilities of Large Language Models\n(LLMs) with the summarization power of the encoder-decoder T5 model. Our work\nis the first to apply alignment techniques, including advanced algorithms like\nDirect Preference Optimization, to encoder-decoder models using synthetic\ndatasets for summarization tasks. Through extensive experiments, we demonstrate\nthe superior performance of GASCADE across various metrics, validated through\nboth automated assessments and human evaluations. This multitasking approach\nenhances drug-related decision-making and fosters a deeper understanding of\npatient concerns, paving the way for advancements in personalized and\nresponsive cancer care. The code and dataset used in this work are publicly\navailable.",
      "url": "http://arxiv.org/abs/2505.04284v1",
      "published_time_eastern_timestamp": 1746610818.0
    },
    {
      "title": "Weaponizing Language Models for Cybersecurity Offensive Operations:\n  Automating Vulnerability Assessment Report Validation; A Review Paper",
      "summary": "This, with the ever-increasing sophistication of cyberwar, calls for novel\nsolutions. In this regard, Large Language Models (LLMs) have emerged as a\nhighly promising tool for defensive and offensive cybersecurity-related\nstrategies. While existing literature has focused much on the defensive use of\nLLMs, when it comes to their offensive utilization, very little has been\nreported-namely, concerning Vulnerability Assessment (VA) report validation.\nConsequentially, this paper tries to fill that gap by investigating the\ncapabilities of LLMs in automating and improving the validation process of the\nreport of the VA. From the critical review of the related literature, this\npaper hereby proposes a new approach to using the LLMs in the automation of the\nanalysis and within the validation process of the report of the VA that could\npotentially reduce the number of false positives and generally enhance\nefficiency. These results are promising for LLM automatization for improving\nvalidation on reports coming from VA in order to improve accuracy while\nreducing human effort and security postures. The contribution of this paper\nprovides further evidence about the offensive and defensive LLM capabilities\nand therefor helps in devising more appropriate cybersecurity strategies and\ntools accordingly.",
      "url": "http://arxiv.org/abs/2505.04265v1",
      "published_time_eastern_timestamp": 1746609295.0
    }
  ]
}
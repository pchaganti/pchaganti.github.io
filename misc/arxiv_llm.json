{
  "last_updated": "2025-06-28T14:15:02.824280-04:00",
  "papers": [
    {
      "title": "mTSBench: Benchmarking Multivariate Time Series Anomaly Detection and\n  Model Selection at Scale",
      "summary": "Multivariate time series anomaly detection (MTS-AD) is critical in domains\nlike healthcare, cybersecurity, and industrial monitoring, yet remains\nchallenging due to complex inter-variable dependencies, temporal dynamics, and\nsparse anomaly labels. We introduce mTSBench, the largest benchmark to date for\nMTS-AD and unsupervised model selection, spanning 344 labeled time series\nacross 19 datasets and 12 diverse application domains. mTSBench evaluates 24\nanomaly detection methods, including large language model (LLM)-based detectors\nfor multivariate time series, and systematically benchmarks unsupervised model\nselection techniques under standardized conditions. Consistent with prior\nfindings, our results confirm that no single detector excels across datasets,\nunderscoring the importance of model selection. However, even state-of-the-art\nselection methods remain far from optimal, revealing critical gaps. mTSBench\nprovides a unified evaluation suite to enable rigorous, reproducible\ncomparisons and catalyze future advances in adaptive anomaly detection and\nrobust model selection.",
      "url": "http://arxiv.org/abs/2506.21550v1",
      "published_time_eastern_timestamp": 1750960798.0
    },
    {
      "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
      "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.",
      "url": "http://arxiv.org/abs/2506.21551v1",
      "published_time_eastern_timestamp": 1750960798.0
    },
    {
      "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation",
      "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution",
      "url": "http://arxiv.org/abs/2506.21535v1",
      "published_time_eastern_timestamp": 1750960460.0
    },
    {
      "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in\n  Large-Scale Conversational AI Datasets",
      "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat",
      "url": "http://arxiv.org/abs/2506.21532v1",
      "published_time_eastern_timestamp": 1750960338.0
    },
    {
      "title": "Potemkin Understanding in Large Language Models",
      "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.",
      "url": "http://arxiv.org/abs/2506.21521v1",
      "published_time_eastern_timestamp": 1750959695.0
    },
    {
      "title": "Enhancing User Engagement in Socially-Driven Dialogue through\n  Interactive LLM Alignments",
      "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.",
      "url": "http://arxiv.org/abs/2506.21497v1",
      "published_time_eastern_timestamp": 1750958777.0
    },
    {
      "title": "Bridging Offline and Online Reinforcement Learning for LLMs",
      "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.",
      "url": "http://arxiv.org/abs/2506.21495v1",
      "published_time_eastern_timestamp": 1750958749.0
    },
    {
      "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond\n  English",
      "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.",
      "url": "http://arxiv.org/abs/2506.21445v1",
      "published_time_eastern_timestamp": 1750955470.0
    },
    {
      "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection",
      "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications.",
      "url": "http://arxiv.org/abs/2506.21443v1",
      "published_time_eastern_timestamp": 1750955385.0
    },
    {
      "title": "Scalable Bayesian Low-Rank Adaptation of Large Language Models via\n  Stochastic Variational Subspace Inference",
      "summary": "Despite their widespread use, large language models (LLMs) are known to\nhallucinate incorrect information and be poorly calibrated. This makes the\nuncertainty quantification of these models of critical importance, especially\nin high-stakes domains, such as autonomy and healthcare. Prior work has made\nBayesian deep learning-based approaches to this problem more tractable by\nperforming inference over the low-rank adaptation (LoRA) parameters of a\nfine-tuned model. While effective, these approaches struggle to scale to larger\nLLMs due to requiring further additional parameters compared to LoRA. In this\nwork we present $\\textbf{Scala}$ble $\\textbf{B}$ayesian $\\textbf{L}$ow-Rank\nAdaptation via Stochastic Variational Subspace Inference (ScalaBL). We perform\nBayesian inference in an $r$-dimensional subspace, for LoRA rank $r$. By\nrepurposing the LoRA parameters as projection matrices, we are able to map\nsamples from this subspace into the full weight space of the LLM. This allows\nus to learn all the parameters of our approach using stochastic variational\ninference. Despite the low dimensionality of our subspace, we are able to\nachieve competitive performance with state-of-the-art approaches while only\nrequiring ${\\sim}1000$ additional parameters. Furthermore, it allows us to\nscale up to the largest Bayesian LLM to date, with four times as a many base\nparameters as prior work.",
      "url": "http://arxiv.org/abs/2506.21408v1",
      "published_time_eastern_timestamp": 1750953285.0
    },
    {
      "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented\n  Generation",
      "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.",
      "url": "http://arxiv.org/abs/2506.21384v1",
      "published_time_eastern_timestamp": 1750952112.0
    },
    {
      "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas\n  Semiotic Square for Large Language Models",
      "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.",
      "url": "http://arxiv.org/abs/2506.21360v1",
      "published_time_eastern_timestamp": 1750950624.0
    },
    {
      "title": "DynamicBench: Evaluating Real-Time Report Generation in Large Language\n  Models",
      "summary": "Traditional benchmarks for large language models (LLMs) typically rely on\nstatic evaluations through storytelling or opinion expression, which fail to\ncapture the dynamic requirements of real-time information processing in\ncontemporary applications. To address this limitation, we present DynamicBench,\na benchmark designed to evaluate the proficiency of LLMs in storing and\nprocessing up-to-the-minute data. DynamicBench utilizes a dual-path retrieval\npipeline, integrating web searches with local report databases. It necessitates\ndomain-specific knowledge, ensuring accurate responses report generation within\nspecialized fields. By evaluating models in scenarios that either provide or\nwithhold external documents, DynamicBench effectively measures their capability\nto independently process recent information or leverage contextual\nenhancements. Additionally, we introduce an advanced report generation system\nadept at managing dynamic information synthesis. Our experimental results\nconfirm the efficacy of our approach, with our method achieving\nstate-of-the-art performance, surpassing GPT4o in document-free and\ndocument-assisted scenarios by 7.0% and 5.8%, respectively. The code and data\nwill be made publicly available.",
      "url": "http://arxiv.org/abs/2506.21343v1",
      "published_time_eastern_timestamp": 1750949624.0
    },
    {
      "title": "Latent Prototype Routing: Achieving Near-Perfect Load Balancing in\n  Mixture-of-Experts",
      "summary": "Mixture-of-Experts (MoE) architectures have emerged as a key strategy for\nscaling large language models (LLMs) efficiently. However, current MoE systems\nsuffer from severe load imbalance, where only a small subset of experts is\nconsistently activated during training and inference, leading to significant\nunderutilization of model capacity and computational resources. In this work,\nwe revisit expert routing through a clustering perspective and propose Latent\nPrototype Routing (LPR), a novel routing framework that generalizes existing\napproaches while promoting balanced expert utilization without compromising\ndownstream performance. Extensive experiments across multiple open-source MoE\nmodels -- including DeepSeek-V3, Qwen3-MoE, and Mixtral -- demonstrate that LPR\nreduces the Gini coefficient of expert load from 0.70 to 0.035 on average,\nimproves the min-max expert load ratio from 1e-6 to 0.70, achieving\nnear-perfect load balancing.",
      "url": "http://arxiv.org/abs/2506.21328v1",
      "published_time_eastern_timestamp": 1750948878.0
    },
    {
      "title": "Multimodal LLMs for Visualization Reconstruction and Understanding",
      "summary": "Visualizations are crucial for data communication, yet understanding them\nrequires comprehension of both visual elements and their underlying data\nrelationships. Current multimodal large models, while effective in natural\nimage understanding, struggle with visualization due to their inability to\ndecode the data-to-visual mapping rules and extract structured information. To\naddress these challenges, we present a novel dataset and train multimodal\nvisualization LLMs specifically designed for understanding. Our approach\ncombines chart images with their corresponding vectorized representations,\nencoding schemes, and data features. The proposed vector format enables compact\nand accurate reconstruction of visualization content. Experimental results\ndemonstrate significant improvements in both data extraction accuracy and chart\nreconstruction quality.",
      "url": "http://arxiv.org/abs/2506.21319v1",
      "published_time_eastern_timestamp": 1750948559.0
    },
    {
      "title": "Detecting Referring Expressions in Visually Grounded Dialogue with\n  Autoregressive Language Models",
      "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.",
      "url": "http://arxiv.org/abs/2506.21294v1",
      "published_time_eastern_timestamp": 1750947260.0
    },
    {
      "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness",
      "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less",
      "url": "http://arxiv.org/abs/2506.21288v1",
      "published_time_eastern_timestamp": 1750946981.0
    },
    {
      "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via\n  Self-Critical Fine-Tuning",
      "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.",
      "url": "http://arxiv.org/abs/2506.21285v1",
      "published_time_eastern_timestamp": 1750946745.0
    },
    {
      "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
      "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
      "url": "http://arxiv.org/abs/2506.21277v1",
      "published_time_eastern_timestamp": 1750946463.0
    },
    {
      "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?",
      "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness",
      "url": "http://arxiv.org/abs/2506.21274v1",
      "published_time_eastern_timestamp": 1750946323.0
    }
  ]
}
{
  "last_updated": "2025-11-11T04:13:30.575973-05:00",
  "papers": [
    {
      "title": "Routing Manifold Alignment Improves Generalization of Mixture-of-Experts\n  LLMs",
      "summary": "Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large\nlanguage models since it can efficiently scale up the model capability without\nincreasing the inference cost. However, evaluations on broad downstream tasks\nreveal a consistent suboptimality of the routers in existing MoE LLMs, which\nresults in a severe performance gap (e.g., 10-20% in accuracy) to the optimal\nrouting. In this paper, we show that aligning the manifold of routing weights\nwith that of task embedding can effectively reduce the gap and improve MoE\nLLMs' generalization performance. Our method, \"Routing Manifold Alignment\n(RoMA)\", introduces an additional manifold regularization term in the\npost-training objective and only requires lightweight finetuning of routers\n(with other parameters frozen). Specifically, the regularization encourages the\nrouting weights of each sample to be close to those of its successful neighbors\n(whose routing weights lead to correct answers) in a task embedding space.\nConsequently, samples targeting similar tasks will share similar expert choices\nacross layers. Building such bindings between tasks and experts over different\nsamples is essential to achieve better generalization. Moreover, RoMA\ndemonstrates the advantage of unifying the task understanding (by embedding\nmodels) with solution generation (by MoE LLMs). In experiments, we finetune\nrouters in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse\nbenchmarks and extensive comparisons with baselines show the substantial\nimprovement brought by RoMA.",
      "url": "http://arxiv.org/abs/2511.07419v1",
      "published_time_eastern_timestamp": 1762801193.0
    },
    {
      "title": "Using Vision Language Models as Closed-Loop Symbolic Planners for\n  Robotic Applications: A Control-Theoretic Perspective",
      "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) have been\nwidely used for embodied symbolic planning. Yet, how to effectively use these\nmodels for closed-loop symbolic planning remains largely unexplored. Because\nthey operate as black boxes, LLMs and VLMs can produce unpredictable or costly\nerrors, making their use in high-level robotic planning especially challenging.\nIn this work, we investigate how to use VLMs as closed-loop symbolic planners\nfor robotic applications from a control-theoretic perspective. Concretely, we\nstudy how the control horizon and warm-starting impact the performance of VLM\nsymbolic planners. We design and conduct controlled experiments to gain\ninsights that are broadly applicable to utilizing VLMs as closed-loop symbolic\nplanners, and we discuss recommendations that can help improve the performance\nof VLM symbolic planners.",
      "url": "http://arxiv.org/abs/2511.07410v1",
      "published_time_eastern_timestamp": 1762801016.0
    },
    {
      "title": "SPOT: An Annotated French Corpus and Benchmark for Detecting Critical\n  Interventions in Online Conversations",
      "summary": "We introduce SPOT (Stopping Points in Online Threads), the first annotated\ncorpus translating the sociological concept of stopping point into a\nreproducible NLP task. Stopping points are ordinary critical interventions that\npause or redirect online discussions through a range of forms (irony, subtle\ndoubt or fragmentary arguments) that frameworks like counterspeech or social\ncorrection often overlook. We operationalize this concept as a binary\nclassification task and provide reliable annotation guidelines. The corpus\ncontains 43,305 manually annotated French Facebook comments linked to URLs\nflagged as false information by social media users, enriched with contextual\nmetadata (article, post, parent comment, page or group, and source). We\nbenchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs\nunder various prompting strategies. Results show that fine-tuned encoders\noutperform prompted LLMs in F1 score by more than 10 percentage points,\nconfirming the importance of supervised learning for emerging non-English\nsocial media tasks. Incorporating contextual metadata further improves encoder\nmodels F1 scores from 0.75 to 0.78. We release the anonymized dataset, along\nwith the annotation guidelines and code in our code repository, to foster\ntransparency and reproducible research.",
      "url": "http://arxiv.org/abs/2511.07405v1",
      "published_time_eastern_timestamp": 1762800880.0
    },
    {
      "title": "SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial\n  Rewards",
      "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in\nvision-language tasks, but they continue to struggle with spatial\nunderstanding. Existing spatial MLLMs often rely on explicit 3D inputs or\narchitecture-specific modifications, and remain constrained by large-scale\ndatasets or sparse supervision. To address these limitations, we introduce\nSpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial\ngrounding with multi-step reasoning. The model simulates human-like spatial\nperception by constructing a scene graph of task-relevant objects and spatial\nrelations, and reasoning towards an answer via dense spatial rewards.\nSpatialThinker consists of two key contributions: (1) a data synthesis pipeline\nthat generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL\nwith a multi-objective dense spatial reward enforcing spatial grounding.\nSpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline\non spatial understanding and real-world VQA benchmarks, nearly doubling the\nbase-model gain compared to sparse RL, and surpassing GPT-4o. These results\nshowcase the effectiveness of combining spatial supervision with reward-aligned\nreasoning in enabling robust 3D spatial understanding with limited data and\nadvancing MLLMs towards human-level visual reasoning.",
      "url": "http://arxiv.org/abs/2511.07403v1",
      "published_time_eastern_timestamp": 1762800767.0
    },
    {
      "title": "C3PO: Optimized Large Language Model Cascades with Probabilistic Cost\n  Constraints for Reasoning",
      "summary": "Large language models (LLMs) have achieved impressive results on complex\nreasoning tasks, but their high inference cost remains a major barrier to\nreal-world deployment. A promising solution is to use cascaded inference, where\nsmall, cheap models handle easy queries, and only the hardest examples are\nescalated to more powerful models. However, existing cascade methods typically\nrely on supervised training with labeled data, offer no theoretical\ngeneralization guarantees, and provide limited control over test-time\ncomputational cost. We introduce C3PO (Cost Controlled Cascaded Prediction\nOptimization), a self-supervised framework for optimizing LLM cascades under\nprobabilistic cost constraints. By focusing on minimizing regret with respect\nto the most powerful model (MPM), C3PO avoids the need for labeled data by\nconstructing a cascade using only unlabeled model outputs. It leverages\nconformal prediction to bound the probability that inference cost exceeds a\nuser-specified budget. We provide theoretical guarantees on both cost control\nand generalization error, and show that our optimization procedure is effective\neven with small calibration sets. Empirically, C3PO achieves state-of-the-art\nperformance across a diverse set of reasoning benchmarks including GSM8K,\nMATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines\nin both accuracy and cost-efficiency. Our results demonstrate that principled,\nlabel-free cascade optimization can enable scalable LLM deployment.",
      "url": "http://arxiv.org/abs/2511.07396v1",
      "published_time_eastern_timestamp": 1762800627.0
    },
    {
      "title": "Surgical Agent Orchestration Platform for Voice-directed Patient Data\n  Interaction",
      "summary": "In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in\nthe procedure, making it difficult to access and manipulate multimodal patient\ndata without interruption. We propose a voice-directed Surgical Agent\nOrchestrator Platform (SAOP) built on a hierarchical multi-agent framework,\nconsisting of an orchestration agent and three task-specific agents driven by\nLarge Language Models (LLMs). These LLM-based agents autonomously plan, refine,\nvalidate, and reason to map voice commands into specific tasks such as\nretrieving clinical information, manipulating CT scans, or navigating 3D\nanatomical models on the surgical video. We also introduce a Multi-level\nOrchestration Evaluation Metric (MOEM) to comprehensively assess the\nperformance and robustness from command-level and category-level perspectives.\nThe SAOP achieves high accuracy and success rates across 240 voice commands,\nwhile LLM-based agents improve robustness against speech recognition errors and\ndiverse or ambiguous free-form commands, demonstrating strong potential to\nsupport minimally invasive da Vinci robotic surgery.",
      "url": "http://arxiv.org/abs/2511.07392v1",
      "published_time_eastern_timestamp": 1762800444.0
    },
    {
      "title": "Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for\n  Bangla-to-Python Code Generation",
      "summary": "Large Language Models (LLMs) have advanced the automated generation of code\nfrom natural language prompts. However, low-resource languages (LRLs) like\nBangla remain underrepresented due to the limited availability of\ninstruction-to-code datasets and evaluation benchmarks. To address this, the\nBLP Workshop at IJCNLP-AACL 2025 introduced a shared task on \"Code Generation\nin Bangla\". In this work, we propose a method that combines instruction\nprompting with a test-driven, feedback-guided iterative refinement process\nusing a fine-tuned Qwen2.5-14B model. The model generates code from Bangla\ninstructions, tests it against unit tests, and iteratively refines any failing\noutputs through three evaluation passes, using test feedback to guide each\nstep. This approach helped our team \"Retriv\" to secure 2nd place in the shared\ntask with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla\ninstruction understanding and Python code generation, emphasizing the need for\ntargeted methods in LRLs. We made experimental scripts publicly available for\nthe community.",
      "url": "http://arxiv.org/abs/2511.07382v1",
      "published_time_eastern_timestamp": 1762800104.0
    },
    {
      "title": "Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource\n  Domains",
      "summary": "Large language models (LLMs) have achieved remarkable success across\nwidespread tasks, yet their application in low-resource domains remains a\nsignificant challenge due to data scarcity and the high risk of overfitting.\nWhile in-domain data is limited, there exist vast amounts of similar\ngeneral-domain data, and our initial findings reveal that they could\npotentially serve as auxiliary supervision for domain enhancement. This\nobservation leads us to our central research question: \\textbf{\\textit{how to\neffectively select the most valuable auxiliary data to maximize domain-specific\nperformance}}, particularly when traditional methods are inapplicable due to a\nlack of large in-domain data pools or validation sets. To address this, we\npropose \\textbf{NTK-Selector}, a principled and efficient framework for\nselecting general-domain auxiliary data to enhance domain-specific performance\nvia neural tangent kernels (NTK). Our method tackles two challenges of directly\napplying NTK to LLMs, theoretical assumptions and prohibitive computational\ncost, by empirically demonstrating a stable NTK-like behavior in LLMs during\nLoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive\nexperiments across four low-resource domains (medical, financial, legal, and\npsychological) demonstrate that NTK-Selector consistently improves downstream\nperformance. Specifically, fine-tuning on 1,000 in-domain samples alone only\nyielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In\ncontrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led\nto substantial \\textbf{gains of +8.7 and +5.1 points}, which corresponds to a\n\\textbf{10.9x and 5.7x improvement} over the domain-only setting.",
      "url": "http://arxiv.org/abs/2511.07380v1",
      "published_time_eastern_timestamp": 1762800083.0
    },
    {
      "title": "Provable Benefit of Curriculum in Transformer Tree-Reasoning\n  Post-Training",
      "summary": "Recent curriculum techniques in the post-training stage of LLMs have been\nwidely observed to outperform non-curriculum approaches in enhancing reasoning\nperformance, yet a principled understanding of why and to what extent they work\nremains elusive. To address this gap, we develop a theoretical framework\ngrounded in the intuition that progressively learning through manageable steps\nis more efficient than directly tackling a hard reasoning task, provided each\nstage stays within the model's effective competence. Under mild complexity\nconditions linking consecutive curriculum stages, we show that curriculum\npost-training avoids the exponential complexity bottleneck.\n  To substantiate this result, drawing insights from the Chain-of-Thoughts\n(CoTs) solving mathematical problems such as Countdown and parity, we model CoT\ngeneration as a states-conditioned autoregressive reasoning tree, define a\nuniform-branching base model to capture pretrained behavior, and formalize\ncurriculum stages as either depth-increasing (longer reasoning chains) or\nhint-decreasing (shorter prefixes) subtasks. Our analysis shows that, under\noutcome-only reward signals, reinforcement learning finetuning achieves high\naccuracy with polynomial sample complexity, whereas direct learning suffers\nfrom an exponential bottleneck. We further establish analogous guarantees for\ntest-time scaling, where curriculum-aware querying reduces both reward oracle\ncalls and sampling cost from exponential to polynomial order.",
      "url": "http://arxiv.org/abs/2511.07372v1",
      "published_time_eastern_timestamp": 1762799394.0
    },
    {
      "title": "Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence\n  Estimation for Failure Detection",
      "summary": "Reliability and failure detection of large language models (LLMs) is critical\nfor their deployment in high-stakes, multi-step reasoning tasks. Prior work\nexplores confidence estimation for self-evaluating LLM-scorer systems, with\nconfidence scorers estimating the likelihood of errors in LLM responses.\nHowever, most methods focus on single-step outputs and overlook the challenges\nof multi-step reasoning. In this work, we extend self-evaluation techniques to\nmulti-step tasks, testing two intuitive approaches: holistic scoring and\nstep-by-step scoring. Using two multi-step benchmark datasets, we show that\nstepwise evaluation generally outperforms holistic scoring in detecting\npotential errors, with up to 15% relative increase in AUC-ROC. Our findings\ndemonstrate that self-evaluating LLM systems provide meaningful confidence\nestimates in complex reasoning, improving their trustworthiness and providing a\npractical framework for failure detection.",
      "url": "http://arxiv.org/abs/2511.07364v1",
      "published_time_eastern_timestamp": 1762798791.0
    },
    {
      "title": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas",
      "summary": "Simulating human profiles by instilling personas into large language models\n(LLMs) is rapidly transforming research in agentic behavioral simulation, LLM\npersonalization, and human-AI alignment. However, most existing synthetic\npersonas remain shallow and simplistic, capturing minimal attributes and\nfailing to reflect the rich complexity and diversity of real human identities.\nWe introduce DEEPPERSONA, a scalable generative engine for synthesizing\nnarrative-complete synthetic personas through a two-stage, taxonomy-guided\nmethod. First, we algorithmically construct the largest-ever human-attribute\ntaxonomy, comprising over hundreds of hierarchically organized attributes, by\nmining thousands of real user-ChatGPT conversations. Second, we progressively\nsample attributes from this taxonomy, conditionally generating coherent and\nrealistic personas that average hundreds of structured attributes and roughly 1\nMB of narrative text, two orders of magnitude deeper than prior works.\nIntrinsic evaluations confirm significant improvements in attribute diversity\n(32 percent higher coverage) and profile uniqueness (44 percent greater)\ncompared to state-of-the-art baselines. Extrinsically, our personas enhance\nGPT-4.1-mini's personalized question answering accuracy by 11.6 percent on\naverage across ten metrics and substantially narrow (by 31.7 percent) the gap\nbetween simulated LLM citizens and authentic human responses in social surveys.\nOur generated national citizens reduced the performance gap on the Big Five\npersonality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA\nthus provides a rigorous, scalable, and privacy-free platform for high-fidelity\nhuman simulation and personalized AI research.",
      "url": "http://arxiv.org/abs/2511.07338v1",
      "published_time_eastern_timestamp": 1762796276.0
    },
    {
      "title": "Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder\n  Training",
      "summary": "Retrieval-Augmented Generation (RAG) methods enhance LLM performance by\nefficiently filtering relevant context for LLMs, reducing hallucinations and\ninference cost. However, most existing RAG methods focus on single-step\nretrieval, which is often insufficient for answering complex questions that\nrequire multi-step search. Recently, multi-step retrieval approaches have\nemerged, typically involving the fine-tuning of small LLMs to perform\nmulti-step retrieval. This type of fine-tuning is highly resource-intensive and\ndoes not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel\napproach that fine-tunes the Embedder model for multi-step retrieval using\nreinforcement learning (RL). Q-RAG offers a competitive, resource-efficient\nalternative to existing multi-step retrieval methods for open-domain question\nanswering and achieves state-of-the-art results on the popular long-context\nbenchmarks Babilong and RULER for contexts up to 10M tokens.",
      "url": "http://arxiv.org/abs/2511.07328v1",
      "published_time_eastern_timestamp": 1762795862.0
    },
    {
      "title": "FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework\n  for Equity Research Report Generation",
      "summary": "While LLMs have shown great success in financial tasks like stock prediction\nand question answering, their application in fully automating Equity Research\nReport generation remains uncharted territory. In this paper, we formulate the\nEquity Research Report (ERR) Generation task for the first time. To address the\ndata scarcity and the evaluation metrics absence, we present an open-source\nevaluation benchmark for ERR generation - FinRpt. We frame a Dataset\nConstruction Pipeline that integrates 7 financial data types and produces a\nhigh-quality ERR dataset automatically, which could be used for model training\nand evaluation. We also introduce a comprehensive evaluation system including\n11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent\nframework specifically tailored to address this task, named FinRpt-Gen, and\ntrain several LLM-based agents on the proposed datasets using Supervised\nFine-Tuning and Reinforcement Learning. Experimental results indicate the data\nquality and metrics effectiveness of the benchmark FinRpt and the strong\nperformance of FinRpt-Gen, showcasing their potential to drive innovation in\nthe ERR generation field. All code and datasets are publicly available.",
      "url": "http://arxiv.org/abs/2511.07322v1",
      "published_time_eastern_timestamp": 1762795352.0
    },
    {
      "title": "When Bias Pretends to Be Truth: How Spurious Correlations Undermine\n  Hallucination Detection in LLMs",
      "summary": "Despite substantial advances, large language models (LLMs) continue to\nexhibit hallucinations, generating plausible yet incorrect responses. In this\npaper, we highlight a critical yet previously underexplored class of\nhallucinations driven by spurious correlations -- superficial but statistically\nprominent associations between features (e.g., surnames) and attributes (e.g.,\nnationality) present in the training data. We demonstrate that these spurious\ncorrelations induce hallucinations that are confidently generated, immune to\nmodel scaling, evade current detection methods, and persist even after refusal\nfine-tuning. Through systematically controlled synthetic experiments and\nempirical evaluations on state-of-the-art open-source and proprietary LLMs\n(including GPT-5), we show that existing hallucination detection methods, such\nas confidence-based filtering and inner-state probing, fundamentally fail in\nthe presence of spurious correlations. Our theoretical analysis further\nelucidates why these statistical biases intrinsically undermine\nconfidence-based detection techniques. Our findings thus emphasize the urgent\nneed for new approaches explicitly designed to address hallucinations caused by\nspurious correlations.",
      "url": "http://arxiv.org/abs/2511.07318v1",
      "published_time_eastern_timestamp": 1762795167.0
    },
    {
      "title": "LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging",
      "summary": "Low-dose computed tomography (CT) represents a significant improvement in\npatient safety through lower radiation doses, but increased noise, blur, and\ncontrast loss can diminish diagnostic quality. Therefore, consistency and\nrobustness in image quality assessment become essential for clinical\napplications. In this study, we propose an LLM-based quality assessment system\nthat generates both numerical scores and textual descriptions of degradations\nsuch as noise, blur, and contrast loss. Furthermore, various inference\nstrategies - from the zero-shot approach to metadata integration and error\nfeedback - are systematically examined, demonstrating the progressive\ncontribution of each method to overall performance. The resultant assessments\nyield not only highly correlated scores but also interpretable output, thereby\nadding value to clinical workflows. The source codes of our study are available\nat https://github.com/itu-biai/lmms_ldct_iqa.",
      "url": "http://arxiv.org/abs/2511.07298v1",
      "published_time_eastern_timestamp": 1762793771.0
    },
    {
      "title": "VADER: Towards Causal Video Anomaly Understanding with Relation-Aware\n  Large Language Models",
      "summary": "Video anomaly understanding (VAU) aims to provide detailed interpretation and\nsemantic comprehension of anomalous events within videos, addressing\nlimitations of traditional methods that focus solely on detecting and\nlocalizing anomalies. However, existing approaches often neglect the deeper\ncausal relationships and interactions between objects, which are critical for\nunderstanding anomalous behaviors. In this paper, we propose VADER, an\nLLM-driven framework for Video Anomaly unDErstanding, which integrates keyframe\nobject Relation features with visual cues to enhance anomaly comprehension from\nvideo. Specifically, VADER first applies an Anomaly Scorer to assign per-frame\nanomaly scores, followed by a Context-AwarE Sampling (CAES) strategy to capture\nthe causal context of each anomalous event. A Relation Feature Extractor and a\nCOntrastive Relation Encoder (CORE) jointly model dynamic object interactions,\nproducing compact relational representations for downstream reasoning. These\nvisual and relational cues are integrated with LLMs to generate detailed,\ncausally grounded descriptions and support robust anomaly-related question\nanswering. Experiments on multiple real-world VAU benchmarks demonstrate that\nVADER achieves strong results across anomaly description, explanation, and\ncausal reasoning tasks, advancing the frontier of explainable video anomaly\nanalysis.",
      "url": "http://arxiv.org/abs/2511.07299v1",
      "published_time_eastern_timestamp": 1762793771.0
    },
    {
      "title": "Who Is the Story About? Protagonist Entity Recognition in News",
      "summary": "News articles often reference numerous organizations, but traditional Named\nEntity Recognition (NER) treats all mentions equally, obscuring which entities\ngenuinely drive the narrative. This limits downstream tasks that rely on\nunderstanding event salience, influence, or narrative focus. We introduce\nProtagonist Entity Recognition (PER), a task that identifies the organizations\nthat anchor a news story and shape its main developments. To validate PER, we\ncompare he predictions of Large Language Models (LLMs) against annotations from\nfour expert annotators over a gold corpus, establishing both inter-annotator\nconsistency and human-LLM agreement. Leveraging these findings, we use\nstate-of-the-art LLMs to automatically label large-scale news collections\nthrough NER-guided prompting, generating scalable, high-quality supervision. We\nthen evaluate whether other LLMs, given reduced context and without explicit\ncandidate guidance, can still infer the correct protagonists. Our results\ndemonstrate that PER is a feasible and meaningful extension to\nnarrative-centered information extraction, and that guided LLMs can approximate\nhuman judgments of narrative importance at scale.",
      "url": "http://arxiv.org/abs/2511.07296v1",
      "published_time_eastern_timestamp": 1762793625.0
    },
    {
      "title": "Hard vs. Noise: Resolving Hard-Noisy Sample Confusion in Recommender\n  Systems via Large Language Models",
      "summary": "Implicit feedback, employed in training recommender systems, unavoidably\nconfronts noise due to factors such as misclicks and position bias. Previous\nstudies have attempted to identify noisy samples through their diverged data\npatterns, such as higher loss values, and mitigate their influence through\nsample dropping or reweighting. However, we observed that noisy samples and\nhard samples display similar patterns, leading to hard-noisy confusion issue.\nSuch confusion is problematic as hard samples are vital for modeling user\npreferences. To solve this problem, we propose LLMHNI framework, leveraging two\nauxiliary user-item relevance signals generated by Large Language Models (LLMs)\nto differentiate hard and noisy samples. LLMHNI obtains user-item semantic\nrelevance from LLM-encoded embeddings, which is used in negative sampling to\nselect hard negatives while filtering out noisy false negatives. An objective\nalignment strategy is proposed to project LLM-encoded embeddings, originally\nfor general language tasks, into a representation space optimized for user-item\nrelevance modeling. LLMHNI also exploits LLM-inferred logical relevance within\nuser-item interactions to identify hard and noisy samples. These LLM-inferred\ninteractions are integrated into the interaction graph and guide denoising with\ncross-graph contrastive alignment. To eliminate the impact of unreliable\ninteractions induced by LLM hallucination, we propose a graph contrastive\nlearning strategy that aligns representations from randomly edge-dropped views\nto suppress unreliable edges. Empirical results demonstrate that LLMHNI\nsignificantly improves denoising and recommendation performance.",
      "url": "http://arxiv.org/abs/2511.07295v1",
      "published_time_eastern_timestamp": 1762793463.0
    },
    {
      "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache\n  Retrieval and Compression",
      "summary": "Video Large Language Models (Video-LLMs) have demonstrated significant\npotential in the areas of video captioning, search, and summarization. However,\ncurrent Video-LLMs still face challenges with long real-world videos. Recent\nmethods have introduced a retrieval mechanism that retrieves query-relevant KV\ncaches for question answering, enhancing the efficiency and accuracy of long\nreal-world videos. However, the compression and retrieval of KV caches are\nstill not fully explored. In this paper, we propose \\textbf{StreamKV}, a\ntraining-free framework that seamlessly equips Video-LLMs with advanced KV\ncache retrieval and compression. Compared to previous methods that used uniform\npartitioning, StreamKV dynamically partitions video streams into semantic\nsegments, which better preserves semantic information. For KV cache retrieval,\nStreamKV calculates a summary vector for each segment to retain segment-level\ninformation essential for retrieval. For KV cache compression, StreamKV\nintroduces a guidance prompt designed to capture the key semantic elements\nwithin each segment, ensuring only the most informative KV caches are retained\nfor answering questions. Moreover, StreamKV unifies KV cache retrieval and\ncompression within a single module, performing both in a layer-adaptive manner,\nthereby further improving the effectiveness of streaming video question\nanswering. Extensive experiments on public StreamingVQA benchmarks demonstrate\nthat StreamKV significantly outperforms existing Online Video-LLMs, achieving\nsuperior accuracy while substantially improving both memory efficiency and\ncomputational latency. The code has been released at\nhttps://github.com/sou1p0wer/StreamKV.",
      "url": "http://arxiv.org/abs/2511.07278v1",
      "published_time_eastern_timestamp": 1762791903.0
    },
    {
      "title": "Omni-AVSR: Towards Unified Multimodal Speech Recognition with Large\n  Language Models",
      "summary": "Large language models (LLMs) have recently achieved impressive results in\nspeech recognition across multiple modalities, including Auditory Speech\nRecognition (ASR), Visual Speech Recognition (VSR), and Audio-Visual Speech\nRecognition (AVSR). Despite this progress, current LLM-based approaches\ntypically address each task independently, training separate models that raise\ncomputational and deployment resource use while missing potential cross-task\nsynergies. They also rely on fixed-rate token compression, which restricts\nflexibility in balancing accuracy with efficiency. These limitations highlight\nthe need for a unified framework that can support ASR, VSR, and AVSR while\nenabling elastic inference. To this end, we present Omni-AVSR, a unified\naudio-visual LLM that combines efficient multi-granularity training with\nparameter-efficient adaptation. Specifically, we adapt the matryoshka\nrepresentation learning paradigm to efficiently train across multiple audio and\nvisual granularities, reducing its inherent training resource use. Furthermore,\nwe explore three LoRA-based strategies for adapting the backbone LLM, balancing\nshared and task-specific specialization. Experiments on LRS2 and LRS3 show that\nOmni-AVSR achieves comparable or superior accuracy to state-of-the-art\nbaselines while training a single model at substantially lower training and\ndeployment resource use. The model also remains robust under acoustic noise,\nand we analyze its scaling behavior as LLM size increases, providing insights\ninto the trade-off between performance and efficiency.",
      "url": "http://arxiv.org/abs/2511.07253v1",
      "published_time_eastern_timestamp": 1762790624.0
    }
  ]
}
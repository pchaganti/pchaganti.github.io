{
  "last_updated": "2025-08-19T08:23:27.387743-04:00",
  "papers": [
    {
      "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden\n  Representation Patterns",
      "summary": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard",
      "url": "http://arxiv.org/abs/2508.13152v1",
      "published_time_eastern_timestamp": 1755539955.0
    },
    {
      "title": "Exploring Autonomous Agents: A Closer Look at Why They Fail When\n  Completing Tasks",
      "summary": "Autonomous agent systems powered by Large Language Models (LLMs) have\ndemonstrated promising capabilities in automating complex tasks. However,\ncurrent evaluations largely rely on success rates without systematically\nanalyzing the interactions, communication mechanisms, and failure causes within\nthese systems. To bridge this gap, we present a benchmark of 34 representative\nprogrammable tasks designed to rigorously assess autonomous agents. Using this\nbenchmark, we evaluate three popular open-source agent frameworks combined with\ntwo LLM backbones, observing a task completion rate of approximately 50%.\nThrough in-depth failure analysis, we develop a three-tier taxonomy of failure\ncauses aligned with task phases, highlighting planning errors, task execution\nissues, and incorrect response generation. Based on these insights, we propose\nactionable improvements to enhance agent planning and self-diagnosis\ncapabilities. Our failure taxonomy, together with mitigation advice, provides\nan empirical foundation for developing more robust and effective autonomous\nagent systems in the future.",
      "url": "http://arxiv.org/abs/2508.13143v1",
      "published_time_eastern_timestamp": 1755539722.0
    },
    {
      "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs",
      "summary": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future.",
      "url": "http://arxiv.org/abs/2508.13141v1",
      "published_time_eastern_timestamp": 1755539590.0
    },
    {
      "title": "Improving Detection of Watermarked Language Models",
      "summary": "Watermarking has recently emerged as an effective strategy for detecting the\ngenerations of large language models (LLMs). The strength of a watermark\ntypically depends strongly on the entropy afforded by the language model and\nthe set of input prompts. However, entropy can be quite limited in practice,\nespecially for models that are post-trained, for example via instruction tuning\nor reinforcement learning from human feedback (RLHF), which makes detection\nbased on watermarking alone challenging. In this work, we investigate whether\ndetection can be improved by combining watermark detectors with non-watermark\nones. We explore a number of hybrid schemes that combine the two, observing\nperformance gains over either class of detector under a wide range of\nexperimental conditions.",
      "url": "http://arxiv.org/abs/2508.13131v1",
      "published_time_eastern_timestamp": 1755538986.0
    },
    {
      "title": "Spot the BlindSpots: Systematic Identification and Quantification of\n  Fine-Grained LLM Biases in Contact Center Summaries",
      "summary": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.",
      "url": "http://arxiv.org/abs/2508.13124v1",
      "published_time_eastern_timestamp": 1755538263.0
    },
    {
      "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with\n  Retrieval-Augmented Generation",
      "summary": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.",
      "url": "http://arxiv.org/abs/2508.13118v1",
      "published_time_eastern_timestamp": 1755537771.0
    },
    {
      "title": "Choosing the Right Engine in the Virtual Reality Landscape",
      "summary": "Virtual reality (VR) development relies on game engines to provide real-time\nrendering, physics simulation, and interaction systems. Among the most widely\nused game engines, Unreal Engine and Unity dominate the industry, offering\ndistinct advantages in graphics rendering, performance optimization, usability,\nresource requirements, and scalability. This study presents a comprehensive\ncomparative analysis of both engines, evaluating their capabilities and\ntrade-offs through empirical assessments and real-world case studies of\nlarge-scale VR projects. The findings highlight key factors such as rendering\nfidelity, computational efficiency, cross-platform compatibility, and\ndevelopment workflows. These provide practical insights for selecting the most\nsuitable engine based on project-specific needs. Furthermore, emerging trends\nin artificial intelligence (AI)-driven enhancements, including Deep Learning\nSuper Sampling (DLSS) and large language models (LLMs), are explored to assess\ntheir impact on VR development workflows. By aligning engine capabilities with\ntechnical and creative requirements, developers can overcome performance\nbottlenecks, enhance immersion, and streamline optimization techniques.\n  This study serves as a valuable resource for VR developers, researchers, and\nindustry professionals, offering data-driven recommendations to navigate the\nevolving landscape of VR technology.",
      "url": "http://arxiv.org/abs/2508.13116v1",
      "published_time_eastern_timestamp": 1755537726.0
    },
    {
      "title": "VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in\n  Verilog",
      "summary": "Timely detection of hardware vulnerabilities during the early design stage is\ncritical for reducing remediation costs. Existing early detection techniques\noften require specialized security expertise, limiting their usability. Recent\nefforts have explored the use of large language models (LLMs) for Verilog\nvulnerability detection. However, LLMs struggle to capture the structure in\nVerilog code, resulting in inconsistent detection results. To this end, we\npropose VerilogLAVD, the first LLM-aided graph traversal rule generation\napproach for Verilog vulnerability detection. Our approach introduces the\nVerilog Property Graph (VeriPG), a unified representation of Verilog code. It\ncombines syntactic features extracted from the abstract syntax tree (AST) with\nsemantic information derived from control flow and data dependency graphs. We\nleverage LLMs to generate VeriPG-based detection rules from Common Weakness\nEnumeration (CWE) descriptions. These rules guide the rule executor that\ntraversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we\nbuild a dataset collected from open-source repositories and synthesized data.\nIn our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,\nVerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with\nexternal knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,\nrespectively.",
      "url": "http://arxiv.org/abs/2508.13092v1",
      "published_time_eastern_timestamp": 1755536718.0
    },
    {
      "title": "DocHPLT: A Massively Multilingual Document-Level Translation Dataset",
      "summary": "Existing document-level machine translation resources are only available for\na handful of languages, mostly high-resourced ones. To facilitate the training\nand evaluation of document-level translation and, more broadly, long-context\nmodeling for global communities, we create DocHPLT, the largest publicly\navailable document-level translation dataset to date. It contains 124 million\naligned document pairs across 50 languages paired with English, comprising 4.26\nbillion sentences, with further possibility to provide 2500 bonus pairs not\ninvolving English. Unlike previous reconstruction-based approaches that piece\ntogether documents from sentence-level data, we modify an existing web\nextraction pipeline to preserve complete document integrity from the source,\nretaining all content including unaligned portions. After our preliminary\nexperiments identify the optimal training context strategy for document-level\ntranslation, we demonstrate that LLMs fine-tuned on DocHPLT substantially\noutperform off-the-shelf instruction-tuned baselines, with particularly\ndramatic improvements for under-resourced languages. We open-source the dataset\nunder a permissive license, providing essential infrastructure for advancing\nmultilingual document-level translation.",
      "url": "http://arxiv.org/abs/2508.13079v1",
      "published_time_eastern_timestamp": 1755535938.0
    },
    {
      "title": "Doğal Dil İşlemede Tokenizasyon Standartları ve\n  Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin\n  Karşılaştırmalı Analizi",
      "summary": "Tokenization is a fundamental preprocessing step in Natural Language\nProcessing (NLP), significantly impacting the capability of large language\nmodels (LLMs) to capture linguistic and semantic nuances. This study introduces\na novel evaluation framework addressing tokenization challenges specific to\nmorphologically-rich and low-resource languages such as Turkish. Utilizing the\nTurkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from\nthe Turkish education system, we assessed tokenizers based on vocabulary size,\ntoken count, processing time, language-specific token percentages (\\%TR), and\ntoken purity (\\%Pure). These newly proposed metrics measure how effectively\ntokenizers preserve linguistic structures. Our analysis reveals that\nlanguage-specific token percentages exhibit a stronger correlation with\ndownstream performance (e.g., MMLU scores) than token purity. Furthermore,\nincreasing model parameters alone does not necessarily enhance linguistic\nperformance, underscoring the importance of tailored, language-specific\ntokenization methods. The proposed framework establishes robust and practical\ntokenization standards for morphologically complex languages.",
      "url": "http://arxiv.org/abs/2508.13058v1",
      "published_time_eastern_timestamp": 1755534402.0
    },
    {
      "title": "MAJIC: Markovian Adaptive Jailbreaking via Iterative Composition of\n  Diverse Innovative Strategies",
      "summary": "Large Language Models (LLMs) have exhibited remarkable capabilities but\nremain vulnerable to jailbreaking attacks, which can elicit harmful content\nfrom the models by manipulating the input prompts. Existing black-box\njailbreaking techniques primarily rely on static prompts crafted with a single,\nnon-adaptive strategy, or employ rigid combinations of several underperforming\nattack methods, which limits their adaptability and generalization. To address\nthese limitations, we propose MAJIC, a Markovian adaptive jailbreaking\nframework that attacks black-box LLMs by iteratively combining diverse\ninnovative disguise strategies. MAJIC first establishes a ``Disguise Strategy\nPool'' by refining existing strategies and introducing several innovative\napproaches. To further improve the attack performance and efficiency, MAJIC\nformulate the sequential selection and fusion of strategies in the pool as a\nMarkov chain. Under this formulation, MAJIC initializes and employs a Markov\nmatrix to guide the strategy composition, where transition probabilities\nbetween strategies are dynamically adapted based on attack outcomes, thereby\nenabling MAJIC to learn and discover effective attack pathways tailored to the\ntarget model. Our empirical results demonstrate that MAJIC significantly\noutperforms existing jailbreak methods on prominent models such as GPT-4o and\nGemini-2.0-flash, achieving over 90\\% attack success rate with fewer than 15\nqueries per attempt on average.",
      "url": "http://arxiv.org/abs/2508.13048v1",
      "published_time_eastern_timestamp": 1755533397.0
    },
    {
      "title": "Using AI for User Representation: An Analysis of 83 Persona Prompts",
      "summary": "We analyzed 83 persona prompts from 27 research articles that used large\nlanguage models (LLMs) to generate user personas. Findings show that the\nprompts predominantly generate single personas. Several prompts express a\ndesire for short or concise persona descriptions, which deviates from the\ntradition of creating rich, informative, and rounded persona profiles. Text is\nthe most common format for generated persona attributes, followed by numbers.\nText and numbers are often generated together, and demographic attributes are\nincluded in nearly all generated personas. Researchers use up to 12 prompts in\na single study, though most research uses a small number of prompts. Comparison\nand testing multiple LLMs is rare. More than half of the prompts require the\npersona output in a structured format, such as JSON, and 74% of the prompts\ninsert data or dynamic variables. We discuss the implications of increased use\nof computational personas for user representation.",
      "url": "http://arxiv.org/abs/2508.13047v1",
      "published_time_eastern_timestamp": 1755533387.0
    },
    {
      "title": "Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans\n  Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları",
      "summary": "Language models have made significant advancements in understanding and\ngenerating human language, achieving remarkable success in various\napplications. However, evaluating these models remains a challenge,\nparticularly for resource-limited languages like Turkish. To address this\nissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a\nmeticulously curated dataset comprising 6,200 multiple-choice questions across\n62 sections within the Turkish education system. This benchmark provides a\nstandard framework for Turkish NLP research, enabling detailed analyses of\nLLMs' capabilities in processing Turkish text. In this study, we evaluated\nstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model\ndesign. TR-MMLU sets a new standard for advancing Turkish NLP research and\ninspiring future innovations.",
      "url": "http://arxiv.org/abs/2508.13044v1",
      "published_time_eastern_timestamp": 1755532843.0
    },
    {
      "title": "Can Large Models Teach Student Models to Solve Mathematical Problems\n  Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction",
      "summary": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.",
      "url": "http://arxiv.org/abs/2508.13037v1",
      "published_time_eastern_timestamp": 1755532570.0
    },
    {
      "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents",
      "summary": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios.",
      "url": "http://arxiv.org/abs/2508.13024v1",
      "published_time_eastern_timestamp": 1755531682.0
    },
    {
      "title": "G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive\n  Guidance",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced\nthe reasoning abilities of large language models (LLMs). Its success, however,\nlargely depends on strong base models with rich world knowledge, yielding only\nmodest improvements for small-size language models (SLMs). To address this\nlimitation, we investigate Guided GRPO, which injects ground-truth reasoning\nsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.\nThrough a comprehensive study of various guidance configurations, we find that\nnaively adding guidance delivers limited gains. These insights motivate\nG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength\nin response to the model's evolving training dynamics. Experiments on\nmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A\nsubstantially outperforms vanilla GRPO. Our code and models are available at\nhttps://github.com/T-Lab-CUHKSZ/G2RPO-A.",
      "url": "http://arxiv.org/abs/2508.13023v1",
      "published_time_eastern_timestamp": 1755531676.0
    },
    {
      "title": "EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning\n  via Evolutionary Testing",
      "summary": "The rapid advancement of LLMs poses a significant challenge to existing\nmathematical reasoning benchmarks. These benchmarks commonly suffer from issues\nsuch as score saturation, temporal decay, and data contamination. To address\nthis challenge, this paper introduces EvolMathEval, an automated mathematical\nbenchmark generation and evolution framework based on evolutionary testing. By\ndynamically generating unique evaluation instances ab initio, the framework\nfundamentally eliminates the risk of data contamination, and ensuring the\nbenchmark remains perpetually challenging for future models.The core mechanisms\nof EvolMathEval include: seed problem generation based on reverse engineering\nwith algebraic guarantees; multi-dimensional genetic operators designed to\ninject diverse cognitive challenges; and a composite fitness function that can\nrapidly and accurately assess problem difficulty. Experimental results\ndemonstrate that the proposed composite fitness function can efficiently and\nprecisely quantify the difficulty of mathematical problems. Furthermore,\nEvolMathEval can not only generate a large volume of high-difficulty problems\nthrough continuous self-iteration, but it can also significantly enhance the\ncomplexity of public datasets like GSM8K through evolution, reducing model\naccuracy by an average of 48%. Deeper investigation reveals that when solving\nthese evolved, complex problems, LLMs tend to employ non-rigorous heuristics to\nbypass complex multi-step logical reasoning, consequently leading to incorrect\nsolutions. We define this phenomenon as \"Pseudo Aha Moment\". This finding\nuncovers a cognitive shortcut-taking behavior in the deep reasoning processes\nof current LLMs, which we find accounts for 77% to 100% of errors on targeted\nproblems. Code and resources are available\nat:https://github.com/SYSUSELab/EvolMathEval.",
      "url": "http://arxiv.org/abs/2508.13003v1",
      "published_time_eastern_timestamp": 1755530650.0
    },
    {
      "title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning",
      "summary": "Multi-agent systems (MASs) have pushed the boundaries of large language model\n(LLM) agents in domains such as web research and software engineering. However,\nlong-horizon, multi-constraint planning tasks involve conditioning on detailed\ninformation and satisfying complex interdependent constraints, which can pose a\nchallenge for these systems. In this study, we construct an LLM-based MAS for a\ntravel planning task which is representative of these challenges. We evaluate\nthe impact of a notebook to facilitate information sharing, and evaluate an\norchestrator agent to improve coordination in free form conversation between\nagents. We find that the notebook reduces errors due to hallucinated details by\n18%, while an orchestrator directs the MAS to focus on and further reduce\nerrors by up to 13.5% within focused sub-areas. Combining both mechanisms\nachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute\nimprovement over the single-agent baseline's 7.5% pass rate. These results\nhighlight the potential of structured information sharing and reflective\norchestration as key components in MASs for long horizon planning with LLMs.",
      "url": "http://arxiv.org/abs/2508.12981v1",
      "published_time_eastern_timestamp": 1755529022.0
    },
    {
      "title": "Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical\n  Reasoning with Enhanced Semantic Discrimination",
      "summary": "Reinforcement learning (RL) with rule-based rewards has demonstrated strong\npotential in enhancing the reasoning and generalization capabilities of\nvision-language models (VLMs) and large language models (LLMs), while reducing\ncomputational overhead. However, its application in medical imaging remains\nunderexplored. Existing reinforcement fine-tuning (RFT) approaches in this\ndomain primarily target closed-ended visual question answering (VQA), limiting\ntheir applicability to real-world clinical reasoning. In contrast, open-ended\nmedical VQA better reflects clinical practice but has received limited\nattention. While some efforts have sought to unify both formats via\nsemantically guided RL, we observe that model-based semantic rewards often\nsuffer from reward collapse, where responses with significant semantic\ndifferences receive similar scores. To address this, we propose ARMed (Adaptive\nReinforcement for Medical Reasoning), a novel RL framework for open-ended\nmedical VQA. ARMed first incorporates domain knowledge through supervised\nfine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning\nwith textual correctness and adaptive semantic rewards to enhance reasoning\nquality. We evaluate ARMed on six challenging medical VQA benchmarks. Results\nshow that ARMed consistently boosts both accuracy and generalization, achieving\na 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain\nbenchmarks. These results highlight the critical role of reward\ndiscriminability in medical RL and the promise of semantically guided rewards\nfor enabling robust and clinically meaningful multimodal reasoning.",
      "url": "http://arxiv.org/abs/2508.12957v1",
      "published_time_eastern_timestamp": 1755527486.0
    },
    {
      "title": "Towards Open-Ended Emotional Support Conversations in LLMs via\n  Reinforcement Learning with Future-Oriented Rewards",
      "summary": "Emotional Support Conversation (ESC) systems aim to alleviate users'\nemotional difficulties and provide long-term, systematic support for emotional\nwell-being. However, most large language model (LLM)-based ESC systems rely on\npredefined strategies, which limits their effectiveness in complex, real-life\nscenarios. To enable flexible responses to diverse emotional problem scenarios,\nthis paper introduces a novel end-to-end framework (RLFF-ESC) that directly\nlearns enduring emotionally supportive response skills using reinforcement\nlearning. For sustained emotional support, we first employ an LLM-based\nmulti-agent mechanism to simulate future dialogue trajectories and collect\nfuture-oriented rewards. We then train a future-oriented reward model, which is\nsubsequently used to train the emotional support policy model. Additionally, we\nincorporate an explicit reasoning process during response generation to further\nenhance the quality, relevance, and contextual appropriateness of the system's\nresponses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and\nLLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two\npublic ESC datasets. Experimental results demonstrate that RLFF-ESC\nconsistently outperforms existing baselines in terms of goal completion and\nresponse quality.",
      "url": "http://arxiv.org/abs/2508.12935v1",
      "published_time_eastern_timestamp": 1755525866.0
    }
  ]
}
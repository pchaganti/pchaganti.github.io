{
  "last_updated": "2025-06-24T05:14:25.353298-04:00",
  "papers": [
    {
      "title": "FilMaster: Bridging Cinematic Principles and Generative AI for Automated\n  Film Generation",
      "summary": "AI-driven content creation has shown potential in film production. However,\nexisting film generation systems struggle to implement cinematic principles and\nthus fail to generate professional-quality films, particularly lacking diverse\ncamera language and cinematic rhythm. This results in templated visuals and\nunengaging narratives. To address this, we introduce FilMaster, an end-to-end\nAI system that integrates real-world cinematic principles for\nprofessional-grade film generation, yielding editable, industry-standard\noutputs. FilMaster is built on two key principles: (1) learning cinematography\nfrom extensive real-world film data and (2) emulating professional,\naudience-centric post-production workflows. Inspired by these principles,\nFilMaster incorporates two stages: a Reference-Guided Generation Stage which\ntransforms user input to video clips, and a Generative Post-Production Stage\nwhich transforms raw footage into audiovisual outputs by orchestrating visual\nand auditory elements for cinematic rhythm. Our generation stage highlights a\nMulti-shot Synergized RAG Camera Language Design module to guide the AI in\ngenerating professional camera language by retrieving reference clips from a\nvast corpus of 440,000 film clips. Our post-production stage emulates\nprofessional workflows by designing an Audience-Centric Cinematic Rhythm\nControl module, including Rough Cut and Fine Cut processes informed by\nsimulated audience feedback, for effective integration of audiovisual elements\nto achieve engaging content. The system is empowered by generative AI models\nlike (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a\ncomprehensive benchmark for evaluating AI-generated films. Extensive\nexperiments show FilMaster's superior performance in camera language design and\ncinematic rhythm control, advancing generative AI in professional filmmaking.",
      "url": "http://arxiv.org/abs/2506.18899v1",
      "published_time_eastern_timestamp": 1750701556.0
    },
    {
      "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
      "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
      "url": "http://arxiv.org/abs/2506.18898v1",
      "published_time_eastern_timestamp": 1750701554.0
    },
    {
      "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
      "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
      "url": "http://arxiv.org/abs/2506.18896v1",
      "published_time_eastern_timestamp": 1750701542.0
    },
    {
      "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation",
      "summary": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems.",
      "url": "http://arxiv.org/abs/2506.18887v1",
      "published_time_eastern_timestamp": 1750701394.0
    },
    {
      "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory,\n  Compositional, and Transformative Generalization",
      "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.",
      "url": "http://arxiv.org/abs/2506.18880v1",
      "published_time_eastern_timestamp": 1750701100.0
    },
    {
      "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
      "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
      "url": "http://arxiv.org/abs/2506.18879v1",
      "published_time_eastern_timestamp": 1750701011.0
    },
    {
      "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
      "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
      "url": "http://arxiv.org/abs/2506.18841v1",
      "published_time_eastern_timestamp": 1750697942.0
    },
    {
      "title": "Understanding Software Engineering Agents: A Study of\n  Thought-Action-Result Trajectories",
      "summary": "Large Language Model (LLM)-based agents are increasingly employed to automate\ncomplex software engineering tasks such as program repair and issue resolution.\nThese agents operate by autonomously generating natural language thoughts,\ninvoking external tools, and iteratively refining their solutions. Despite\ntheir widespread adoption, the internal decision-making processes of these\nagents remain largely unexplored, limiting our understanding of their\noperational dynamics and failure modes. In this paper, we present a large-scale\nempirical study of the thought-action-result trajectories of three\nstate-of-the-art LLM-based agents: \\textsc{RepairAgent},\n\\textsc{AutoCodeRover}, and \\textsc{OpenHands}. We unify their interaction logs\ninto a common format, capturing 120 trajectories and 2822 LLM interactions\nfocused on program repair and issue resolution. Our study combines quantitative\nanalyses of structural properties, action patterns, and token usage with\nqualitative assessments of reasoning coherence and feedback integration. We\nidentify key trajectory characteristics such as iteration counts and token\nconsumption, recurring action sequences, and the semantic coherence linking\nthoughts, actions, and their results. Our findings reveal behavioral motifs and\nanti-patterns that distinguish successful from failed executions, providing\nactionable insights for improving agent design, including prompting strategies,\nfailure diagnosis, and anti-pattern detection. We release our dataset and\nannotation framework to support further research on transparent and robust\nautonomous software engineering agents.",
      "url": "http://arxiv.org/abs/2506.18824v1",
      "published_time_eastern_timestamp": 1750696492.0
    },
    {
      "title": "RWESummary: A Framework and Test for Choosing Large Language Models to\n  Summarize Real-World Evidence (RWE) Studies",
      "summary": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.",
      "url": "http://arxiv.org/abs/2506.18819v1",
      "published_time_eastern_timestamp": 1750696083.0
    },
    {
      "title": "FORGE: An LLM-driven Framework for Large-Scale Smart Contract\n  Vulnerability Dataset Construction",
      "summary": "High-quality smart contract vulnerability datasets are critical for\nevaluating security tools and advancing smart contract security research. Two\nmajor limitations of current manual dataset construction are (1)\nlabor-intensive and error-prone annotation processes limiting the scale,\nquality, and evolution of the dataset, and (2) absence of standardized\nclassification rules results in inconsistent vulnerability categories and\nlabeling results across different datasets. To address these limitations, we\npresent FORGE, the first automated approach for constructing smart contract\nvulnerability datasets. FORGE leverages an LLM-driven pipeline to extract\nhigh-quality vulnerabilities from real-world audit reports and classify them\naccording to the CWE, the most widely recognized classification in software\nsecurity. FORGE employs a divide-and-conquer strategy to extract structured and\nself-contained vulnerability information from these reports. Additionally, it\nuses a tree-of-thoughts technique to classify the vulnerability information\ninto the hierarchical CWE classification. To evaluate FORGE's effectiveness, we\nrun FORGE on 6,454 real-world audit reports and generate a dataset comprising\n81,390 solidity files and 27,497 vulnerability findings across 296 CWE\ncategories. Manual assessment of the dataset demonstrates high extraction\nprecision and classification consistency with human experts (precision of 95.6%\nand inter-rater agreement k-$\\alpha$ of 0.87). We further validate the\npracticality of our dataset by benchmarking 13 existing security tools on our\ndataset. The results reveal the significant limitations in current detection\ncapabilities. Furthermore, by analyzing the severity-frequency distribution\npatterns through a unified CWE perspective in our dataset, we highlight\ninconsistency between current smart contract research focus and priorities\nidentified from real-world vulnerabilities...",
      "url": "http://arxiv.org/abs/2506.18795v1",
      "published_time_eastern_timestamp": 1750694596.0
    },
    {
      "title": "TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation",
      "summary": "TRIZ, the Theory of Inventive Problem Solving, is a structured,\nknowledge-based framework for innovation and abstracting problems to find\ninventive solutions. However, its application is often limited by the\ncomplexity and deep interdisciplinary knowledge required. Advancements in Large\nLanguage Models (LLMs) have revealed new possibilities for automating parts of\nthis process. While previous studies have explored single LLMs in TRIZ\napplications, this paper introduces a multi-agent approach. We propose an\nLLM-based multi-agent system, called TRIZ agents, each with specialized\ncapabilities and tool access, collaboratively solving inventive problems based\non the TRIZ methodology. This multi-agent system leverages agents with various\ndomain expertise to efficiently navigate TRIZ steps. The aim is to model and\nsimulate an inventive process with language agents. We assess the effectiveness\nof this team of agents in addressing complex innovation challenges based on a\nselected case study in engineering. We demonstrate the potential of agent\ncollaboration to produce diverse, inventive solutions. This research\ncontributes to the future of AI-driven innovation, showcasing the advantages of\ndecentralized problem-solving in complex ideation tasks.",
      "url": "http://arxiv.org/abs/2506.18783v1",
      "published_time_eastern_timestamp": 1750693994.0
    },
    {
      "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks",
      "summary": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.",
      "url": "http://arxiv.org/abs/2506.18781v1",
      "published_time_eastern_timestamp": 1750693821.0
    },
    {
      "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions\n  During Code Training",
      "summary": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles.",
      "url": "http://arxiv.org/abs/2506.18777v1",
      "published_time_eastern_timestamp": 1750693544.0
    },
    {
      "title": "PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries",
      "summary": "LLM serving systems typically treat user prompts as monolithic inputs,\noptimizing inference through decoding tricks or inter-query batching. However,\nmany real-world prompts contain latent semantic parallelism--decomposable\nstructures where subtasks can be executed independently to reduce latency while\npreserving meaning. We introduce PARALLELPROMPT, the first benchmark for\nmeasuring intra-query parallelism in natural user prompts. Our dataset\ncomprises over 37,000 real-world prompts from public LLM chat logs, each\nannotated with a structured schema capturing task templates, shared context,\nand iteration inputs. These schemas are extracted using LLM-assisted prompting\nwith rule-based multilingual validation. To evaluate the benefits of\ndecomposition, we provide an execution suite that benchmarks serial vs.\nparallel strategies, measuring latency, structural adherence, and semantic\nfidelity. Our results show that intra-query parallelism can be successfully\nparsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks\nlike translation, comprehension, and comparative analysis, with minimal quality\ndegradation. By releasing this benchmark, curation pipeline, and evaluation\nsuite, we provide the first standardized testbed for studying structure-aware\nexecution in LLM serving pipelines.",
      "url": "http://arxiv.org/abs/2506.18728v1",
      "published_time_eastern_timestamp": 1750691154.0
    },
    {
      "title": "LLM-enhanced Interactions in Human-Robot Collaborative Drawing with\n  Older Adults",
      "summary": "The goal of this study is to identify factors that support and enhance older\nadults' creative experiences in human-robot co-creativity. Because the research\ninto the use of robots for creativity support with older adults remains\nunderexplored, we carried out an exploratory case study. We took a\nparticipatory approach and collaborated with professional art educators to\ndesign a course Drawing with Robots for adults aged 65 and over. The course\nfeatured human-human and human-robot drawing activities with various types of\nrobots. We observed collaborative drawing interactions, interviewed\nparticipants on their experiences, and analyzed collected data. Findings show\nthat participants preferred acting as curators, evaluating creative suggestions\nfrom the robot in a teacher or coach role. When we enhanced a robot with a\nmultimodal Large Language Model (LLM), participants appreciated its spoken\ndialogue capabilities. They reported however, that the robot's feedback\nsometimes lacked an understanding of the context, and sensitivity to their\nartistic goals and preferences. Our findings highlight the potential of\nLLM-enhanced robots to support creativity and offer future directions for\nadvancing human-robot co-creativity with older adults.",
      "url": "http://arxiv.org/abs/2506.18711v1",
      "published_time_eastern_timestamp": 1750690143.0
    },
    {
      "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
      "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
      "url": "http://arxiv.org/abs/2506.18710v1",
      "published_time_eastern_timestamp": 1750690141.0
    },
    {
      "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language\n  Models?",
      "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.",
      "url": "http://arxiv.org/abs/2506.18674v1",
      "published_time_eastern_timestamp": 1750688326.0
    },
    {
      "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based\n  Information Retriever via Query-Document Co-Augmentation",
      "summary": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever.",
      "url": "http://arxiv.org/abs/2506.18670v1",
      "published_time_eastern_timestamp": 1750688083.0
    },
    {
      "title": "A Random Matrix Analysis of In-context Memorization for Nonlinear\n  Attention",
      "summary": "Attention mechanisms have revolutionized machine learning (ML) by enabling\nefficient modeling of global dependencies across inputs. Their inherently\nparallelizable structures allow for efficient scaling with the exponentially\nincreasing size of both pretrained data and model parameters. Yet, despite\ntheir central role as the computational backbone of modern large language\nmodels (LLMs), the theoretical understanding of Attentions, especially in the\nnonlinear setting, remains limited.\n  In this paper, we provide a precise characterization of the \\emph{in-context\nmemorization error} of \\emph{nonlinear Attention}, in the high-dimensional\nproportional regime where the number of input tokens $n$ and their embedding\ndimension $p$ are both large and comparable. Leveraging recent advances in the\ntheory of large kernel random matrices, we show that nonlinear Attention\ntypically incurs higher memorization error than linear ridge regression on\nrandom inputs. However, this gap vanishes, and can even be reversed, when the\ninput exhibits statistical structure, particularly when the Attention weights\nalign with the input signal direction. Our results reveal how nonlinearity and\ninput structure interact with each other to govern the memorization performance\nof nonlinear Attention. The theoretical insights are supported by numerical\nexperiments.",
      "url": "http://arxiv.org/abs/2506.18656v1",
      "published_time_eastern_timestamp": 1750687003.0
    },
    {
      "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
      "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
      "url": "http://arxiv.org/abs/2506.18631v1",
      "published_time_eastern_timestamp": 1750685784.0
    }
  ]
}
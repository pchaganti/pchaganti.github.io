{
  "last_updated": "2025-06-04T02:17:55.856702-04:00",
  "papers": [
    {
      "title": "Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and\n  Semantic Understanding Capability of LLM",
      "summary": "Neuroscience research publications encompass a vast wealth of knowledge.\nAccurately retrieving existing information and discovering new insights from\nthis extensive literature is essential for advancing the field. However, when\nknowledge is dispersed across multiple sources, current state-of-the-art\nretrieval methods often struggle to extract the necessary information. A\nknowledge graph (KG) can integrate and link knowledge from multiple sources,\nbut existing methods for constructing KGs in neuroscience often rely on labeled\ndata and require domain expertise. Acquiring large-scale, labeled data for a\nspecialized area like neuroscience presents significant challenges. This work\nproposes novel methods for constructing KG from unlabeled large-scale\nneuroscience research corpus utilizing large language models (LLM),\nneuroscience ontology, and text embeddings. We analyze the semantic relevance\nof neuroscience text segments identified by LLM for building the knowledge\ngraph. We also introduce an entity-augmented information retrieval algorithm to\nextract knowledge from the KG. Several experiments were conducted to evaluate\nthe proposed approaches, and the results demonstrate that our methods\nsignificantly enhance knowledge discovery from the unlabeled neuroscience\nresearch corpus. It achieves an F1 score of 0.84 for entity extraction, and the\nknowledge obtained from the KG improves answers to over 54% of the questions.",
      "url": "http://arxiv.org/abs/2506.03145v1",
      "published_time_eastern_timestamp": 1748973558.0
    },
    {
      "title": "Not All Tokens Are Meant to Be Forgotten",
      "summary": "Large Language Models (LLMs), pre-trained on massive text corpora, exhibit\nremarkable human-level language understanding, reasoning, and decision-making\nabilities. However, they tend to memorize unwanted information, such as private\nor copyrighted content, raising significant privacy and legal concerns.\nUnlearning has emerged as a promising solution, but existing methods face a\nsignificant challenge of over-forgetting. This issue arises because they\nindiscriminately suppress the generation of all the tokens in forget samples,\nleading to a substantial loss of model utility. To overcome this challenge, we\nintroduce the Targeted Information Forgetting (TIF) framework, which consists\nof (1) a flexible targeted information identifier designed to differentiate\nbetween unwanted words (UW) and general words (GW) in the forget samples, and\n(2) a novel Targeted Preference Optimization approach that leverages Logit\nPreference Loss to unlearn unwanted information associated with UW and\nPreservation Loss to retain general information in GW, effectively improving\nthe unlearning process while mitigating utility degradation. Extensive\nexperiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF\nframework enhances unlearning effectiveness while preserving model utility and\nachieving state-of-the-art results.",
      "url": "http://arxiv.org/abs/2506.03142v1",
      "published_time_eastern_timestamp": 1748973545.0
    },
    {
      "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
      "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
      "url": "http://arxiv.org/abs/2506.03139v1",
      "published_time_eastern_timestamp": 1748973537.0
    },
    {
      "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
      "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
      "url": "http://arxiv.org/abs/2506.03136v1",
      "published_time_eastern_timestamp": 1748973522.0
    },
    {
      "title": "Native-Resolution Image Synthesis",
      "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
      "url": "http://arxiv.org/abs/2506.03131v1",
      "published_time_eastern_timestamp": 1748973453.0
    },
    {
      "title": "AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit\n  Topology Generation",
      "summary": "Analog circuit topology synthesis is integral to Electronic Design Automation\n(EDA), enabling the automated creation of circuit structures tailored to\nspecific design requirements. However, the vast design search space and strict\nconstraint adherence make efficient synthesis challenging. Leveraging the\nversatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel\nreinforcement learning (RL)-based framework for automated analog circuit\nsynthesis. The framework operates in two phases: instruction tuning, where an\nLLM learns to generate circuit topologies from structured prompts encoding\ndesign constraints, and RL refinement, which further improves the\ninstruction-tuned model using reward models that evaluate validity, efficiency,\nand output voltage. The refined model is then used directly to generate\ntopologies that satisfy the design constraints. Empirical results show that\nAUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by\n~14% compared to the best baselines, while reducing duplicate generation rates\nby ~38%. It achieves over 60% success in synthesizing valid circuits with\nlimited training data, demonstrating strong generalization. These findings\nhighlight the framework's effectiveness in scaling to complex circuits while\nmaintaining efficiency and constraint adherence, marking a significant\nadvancement in AI-driven circuit design.",
      "url": "http://arxiv.org/abs/2506.03122v1",
      "published_time_eastern_timestamp": 1748973270.0
    },
    {
      "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
      "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
      "url": "http://arxiv.org/abs/2506.03106v1",
      "published_time_eastern_timestamp": 1748972342.0
    },
    {
      "title": "Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified\n  Theory and Risk Bounds",
      "summary": "Retrieval-augmented generation (RAG) has seen many empirical successes in\nrecent years by aiding the LLM with external knowledge. However, its\ntheoretical aspect has remained mostly unexplored. In this paper, we propose\nthe first finite-sample generalization bound for RAG in in-context linear\nregression and derive an exact bias-variance tradeoff. Our framework views the\nretrieved texts as query-dependent noisy in-context examples and recovers the\nclassical in-context learning (ICL) and standard RAG as the limit cases. Our\nanalysis suggests that an intrinsic ceiling on generalization error exists on\nRAG as opposed to the ICL. Furthermore, our framework is able to model\nretrieval both from the training data and from external corpora by introducing\nuniform and non-uniform RAG noise. In line with our theory, we show the sample\nefficiency of ICL and RAG empirically with experiments on common QA benchmarks,\nsuch as Natural Questions and TriviaQA.",
      "url": "http://arxiv.org/abs/2506.03100v1",
      "published_time_eastern_timestamp": 1748971913.0
    },
    {
      "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
      "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
      "url": "http://arxiv.org/abs/2506.03099v1",
      "published_time_eastern_timestamp": 1748971768.0
    },
    {
      "title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents",
      "summary": "Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.",
      "url": "http://arxiv.org/abs/2506.03095v1",
      "published_time_eastern_timestamp": 1748971624.0
    },
    {
      "title": "Literary Evidence Retrieval via Long-Context Language Models",
      "summary": "How well do modern long-context language models understand literary fiction?\nWe explore this question via the task of literary evidence retrieval,\nrepurposing the RELiC dataset of That et al. (2022) to construct a benchmark\nwhere the entire text of a primary source (e.g., The Great Gatsby) is provided\nto an LLM alongside literary criticism with a missing quotation from that work.\nThis setting, in which the model must generate the missing quotation, mirrors\nthe human process of literary analysis by requiring models to perform both\nglobal narrative reasoning and close textual examination. We curate a\nhigh-quality subset of 292 examples through extensive filtering and human\nverification. Our experiments show that recent reasoning models, such as Gemini\nPro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In\ncontrast, the best open-weight model achieves only 29.1% accuracy, highlighting\na wide gap in interpretive reasoning between open and closed-weight models.\nDespite their speed and apparent accuracy, even the strongest models struggle\nwith nuanced literary signals and overgeneration, signaling open challenges for\napplying LLMs to literary analysis. We release our dataset and evaluation code\nto encourage future work in this direction.",
      "url": "http://arxiv.org/abs/2506.03090v1",
      "published_time_eastern_timestamp": 1748971185.0
    },
    {
      "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence\n  Training of LLMs",
      "summary": "Training language models on long sequence data is a demanding requirement for\nenhancing the model's capability on complex tasks, e.g., long-chain reasoning.\nHowever, as the sequence length scales up, the memory cost for storing\nactivation values becomes huge during the Backpropagation (BP) process, even\nwith the application of gradient checkpointing technique. To tackle this\nchallenge, we propose a memory-efficient and exact BP method called StreamBP,\nwhich performs a linear decomposition of the chain rule along the sequence\ndimension in a layer-wise manner, significantly reducing the memory cost of\nactivation values and logits. The proposed method is applicable to common\nobjectives such as SFT, GRPO, and DPO. From an implementation perspective,\nStreamBP achieves less computational FLOPs and faster BP speed by leveraging\nthe causal structure of the language model. Compared to gradient checkpointing,\nStreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,\nwhile using comparable or even less BP time. Note that StreamBP's sequence\nlength scaling ability can be directly transferred to batch size scaling for\naccelerating training. We further develop a communication-efficient distributed\nStreamBP to effectively support multi-GPU training and broaden its\napplicability. Our code can be easily integrated into the training pipeline of\nany transformer models and is available at https://github.com/Ledzy/StreamBP.",
      "url": "http://arxiv.org/abs/2506.03077v1",
      "published_time_eastern_timestamp": 1748969655.0
    },
    {
      "title": "MAEBE: Multi-Agent Emergent Behavior Framework",
      "summary": "Traditional AI safety evaluations on isolated LLMs are insufficient as\nmulti-agent AI ensembles become prevalent, introducing novel emergent risks.\nThis paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)\nframework to systematically assess such risks. Using MAEBE with the Greatest\nGood Benchmark (and a novel double-inversion question technique), we\ndemonstrate that: (1) LLM moral preferences, particularly for Instrumental\nHarm, are surprisingly brittle and shift significantly with question framing,\nboth in single agents and ensembles. (2) The moral reasoning of LLM ensembles\nis not directly predictable from isolated agent behavior due to emergent group\ndynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure\ninfluencing convergence, even when guided by a supervisor, highlighting\ndistinct safety and alignment challenges. Our findings underscore the necessity\nof evaluating AI systems in their interactive, multi-agent contexts.",
      "url": "http://arxiv.org/abs/2506.03053v1",
      "published_time_eastern_timestamp": 1748968427.0
    },
    {
      "title": "Facts Do Care About Your Language: Assessing Answer Quality of\n  Multilingual LLMs",
      "summary": "Factuality is a necessary precursor to useful educational tools. As adoption\nof Large Language Models (LLMs) in education continues of grow, ensuring\ncorrectness in all settings is paramount. Despite their strong English\ncapabilities, LLM performance in other languages is largely untested. In this\nwork, we evaluate the correctness of the Llama3.1 family of models in answering\nfactual questions appropriate for middle and high school students. We\ndemonstrate that LLMs not only provide extraneous and less truthful\ninformation, but also exacerbate existing biases against rare languages.",
      "url": "http://arxiv.org/abs/2506.03051v1",
      "published_time_eastern_timestamp": 1748968312.0
    },
    {
      "title": "Towards Analyzing and Understanding the Limitations of VAPO: A\n  Theoretical Perspective",
      "summary": "Reinforcement learning (RL) enhances large language models (LLMs) in complex,\nlong-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,\ndespite sophisticated mechanisms like Decoupled GAE, theoretically faces\nfundamental limitations in comprehensively modeling and leveraging deep,\nlong-term value for fine-grained, step-by-step policy guidance in extended\nreasoning chains. We argue these limitations stem from inherent difficulties in\ncredit assignment, value function representational capacity with temporally\nabstracted goals, and translating global value signals into local policy\nimprovements, especially with sparse rewards. Our theoretical analysis examines\nthese aspects to illuminate VAPO's boundaries in long-term value modeling,\naiming to deepen understanding of current RL for advanced reasoning and suggest\nfuture research for more robust LLM agents.",
      "url": "http://arxiv.org/abs/2506.03038v1",
      "published_time_eastern_timestamp": 1748967647.0
    },
    {
      "title": "Leveraging Information Retrieval to Enhance Spoken Language\n  Understanding Prompts in Few-Shot Learning",
      "summary": "Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.",
      "url": "http://arxiv.org/abs/2506.03035v1",
      "published_time_eastern_timestamp": 1748967525.0
    },
    {
      "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
      "summary": "Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions.",
      "url": "http://arxiv.org/abs/2506.03032v1",
      "published_time_eastern_timestamp": 1748966874.0
    },
    {
      "title": "GenFair: Systematic Test Generation for Fairness Fault Detection in\n  Large Language Models",
      "summary": "Large Language Models (LLMs) are increasingly deployed in critical domains,\nyet they often exhibit biases inherited from training data, leading to fairness\nconcerns. This work focuses on the problem of effectively detecting fairness\nviolations, especially intersectional biases that are often missed by existing\ntemplate-based and grammar-based testing methods. Previous approaches, such as\nCheckList and ASTRAEA, provide structured or grammar-driven test generation but\nstruggle with low test diversity and limited sensitivity to complex demographic\ninteractions. To address these limitations, we propose GenFair, a metamorphic\nfairness testing framework that systematically generates source test cases\nusing equivalence partitioning, mutation operators, and boundary value\nanalysis. GenFair improves fairness testing by generating linguistically\ndiverse, realistic, and intersectional test cases. It applies metamorphic\nrelations (MR) to derive follow-up cases and detects fairness violations via\ntone-based comparisons between source and follow-up responses. In experiments\nwith GPT-4.0 and LLaMA-3.0, GenFair outperformed two baseline methods. It\nachieved a fault detection rate (FDR) of 0.73 (GPT-4.0) and 0.69 (LLaMA-3.0),\ncompared to 0.54/0.51 for template-based and 0.39/0.36 for ASTRAEA. GenFair\nalso showed the highest test case diversity (syntactic:10.06, semantic: 76.68)\nand strong coherence (syntactic: 291.32, semantic: 0.7043), outperforming both\nbaselines. These results demonstrate the effectiveness of GenFair in uncovering\nnuanced fairness violations. The proposed method offers a scalable and\nautomated solution for fairness testing and contributes to building more\nequitable LLMs.",
      "url": "http://arxiv.org/abs/2506.03024v1",
      "published_time_eastern_timestamp": 1748966430.0
    },
    {
      "title": "Conditioning Large Language Models on Legal Systems? Detecting\n  Punishable Hate Speech",
      "summary": "The assessment of legal problems requires the consideration of a specific\nlegal system and its levels of abstraction, from constitutional law to\nstatutory law to case law. The extent to which Large Language Models (LLMs)\ninternalize such legal systems is unknown. In this paper, we propose and\ninvestigate different approaches to condition LLMs at different levels of\nabstraction in legal systems. This paper examines different approaches to\nconditioning LLMs at multiple levels of abstraction in legal systems to detect\npotentially punishable hate speech. We focus on the task of classifying whether\na specific social media posts falls under the criminal offense of incitement to\nhatred as prescribed by the German Criminal Code. The results show that there\nis still a significant performance gap between models and legal experts in the\nlegal assessment of hate speech, regardless of the level of abstraction with\nwhich the models were conditioned. Our analysis revealed, that models\nconditioned on abstract legal knowledge lacked deep task understanding, often\ncontradicting themselves and hallucinating answers, while models using concrete\nlegal knowledge performed reasonably well in identifying relevant target\ngroups, but struggled with classifying target conducts.",
      "url": "http://arxiv.org/abs/2506.03009v1",
      "published_time_eastern_timestamp": 1748965827.0
    },
    {
      "title": "A Preference-Driven Methodology for High-Quality Solidity Code\n  Generation",
      "summary": "While Large Language Models (LLMs) have demonstrated remarkable progress in\ngenerating functionally correct Solidity code, they continue to face critical\nchallenges in producing gas-efficient and secure code, which are critical\nrequirements for real-world smart contract deployment. Although recent advances\nleverage Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO)\nfor code preference alignment, existing approaches treat functional\ncorrectness, gas optimization, and security as independent objectives,\nresulting in contracts that may achieve operational soundness but suffer from\nprohibitive execution costs or dangerous vulnerabilities. To address these\nlimitations, we propose PrefGen, a novel framework that extends standard DPO\nbeyond human preferences to incorporate quantifiable blockchain-specific\nmetrics, enabling holistic multi-objective optimization specifically tailored\nfor smart contract generation. Our framework introduces a comprehensive\nevaluation methodology with four complementary metrics: Pass@k (functional\ncorrectness), Compile@k (syntactic correctness), Gas@k (gas efficiency), and\nSecure@k (security assessment), providing rigorous multi-dimensional contract\nevaluation. Through extensive experimentation, we demonstrate that PrefGen\nsignificantly outperforms existing approaches across all critical dimensions,\nachieving 66.7% Pass@5, 58.9% Gas@5, and 62.5% Secure@5, while generating\nproduction-ready smart contracts that are functionally correct, cost-efficient,\nand secure.",
      "url": "http://arxiv.org/abs/2506.03006v1",
      "published_time_eastern_timestamp": 1748965531.0
    }
  ]
}
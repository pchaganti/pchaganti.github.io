{
  "last_updated": "2025-11-04T22:39:39.834140-05:00",
  "papers": [
    {
      "title": "Continuous Autoregressive Language Models",
      "summary": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.",
      "url": "http://arxiv.org/abs/2510.27688v1",
      "published_time_eastern_timestamp": 1761933491.0
    },
    {
      "title": "Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in\n  Creative Work",
      "summary": "As AI becomes more deeply embedded in knowledge work, building assistants\nthat support human creativity and expertise becomes more important. Yet\nachieving synergy in human-AI collaboration is not easy. Providing AI with\ndetailed information about a user's demographics, psychological attributes,\ndivergent thinking, and domain expertise may improve performance by scaffolding\nmore effective multi-turn interactions. We implemented a personalized LLM-based\nassistant, informed by users' psychometric profiles and an AI-guided interview\nabout their work style, to help users complete a marketing task for a fictional\nstartup. We randomized 331 participants to work with AI that was either generic\n(n = 116), partially personalized (n = 114), or fully personalized (n=101).\nParticipants working with personalized AI produce marketing campaigns of\nsignificantly higher quality and creativity, beyond what AI alone could have\nproduced. Compared to generic AI, personalized AI leads to higher self-reported\nlevels of assistance and feedback, while also increasing participant trust and\nconfidence. Causal mediation analysis shows that personalization improves\nperformance indirectly by enhancing collective memory, attention, and reasoning\nin the human-AI interaction. These findings provide a theory-driven framework\nin which personalization functions as external scaffolding that builds common\nground and shared partner models, reducing uncertainty and enhancing joint\ncognition. This informs the design of future AI assistants that maximize\nsynergy and support human creative potential while limiting negative\nhomogenization.",
      "url": "http://arxiv.org/abs/2510.27681v1",
      "published_time_eastern_timestamp": 1761932990.0
    },
    {
      "title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting",
      "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\nmultimodal reasoning, yet most medical applications remain limited to 2D\nimaging. In this work, we extend VLMs to 3D positron emission tomography and\ncomputed tomography (PET/CT), a domain characterized by large volumetric data,\nsmall and dispersed lesions, and lengthy radiology reports. We introduce a\nlarge-scale dataset comprising over 11,000 lesion-level descriptions paired\nwith 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid\nrule-based and large language model (LLM) pipeline. Building upon this dataset,\nwe propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,\nCT, and lesion contours for spatially grounded report generation. PETAR bridges\nglobal contextual reasoning with fine-grained lesion awareness, producing\nclinically coherent and localized findings. Comprehensive automated and human\nevaluations demonstrate that PETAR substantially improves PET/CT report\ngeneration quality, advancing 3D medical vision-language understanding.",
      "url": "http://arxiv.org/abs/2510.27680v1",
      "published_time_eastern_timestamp": 1761932941.0
    },
    {
      "title": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability\n  Detection",
      "summary": "Large language models (LLMs) have demonstrated impressive capabilities for\nmany coding tasks, including summarization, translation, completion, and code\ngeneration. However, detecting code vulnerabilities remains a challenging task\nfor LLMs. An effective way to improve LLM performance is in-context learning\n(ICL) - providing few-shot examples similar to the query, along with correct\nanswers, can improve an LLM's ability to generate correct solutions. However,\nchoosing the few-shot examples appropriately is crucial to improving model\nperformance. In this paper, we explore two criteria for choosing few-shot\nexamples for ICL used in the code vulnerability detection task. The first\ncriterion considers if the LLM (consistently) makes a mistake or not on a\nsample with the intuition that LLM performance on a sample is informative about\nits usefulness as a few-shot example. The other criterion considers similarity\nof the examples with the program under query and chooses few-shot examples\nbased on the $k$-nearest neighbors to the given sample. We perform evaluations\nto determine the benefits of these criteria individually as well as under\nvarious combinations, using open-source models on multiple datasets.",
      "url": "http://arxiv.org/abs/2510.27675v1",
      "published_time_eastern_timestamp": 1761932518.0
    },
    {
      "title": "Culture Cartography: Mapping the Landscape of Cultural Knowledge",
      "summary": "To serve global users safely and productively, LLMs need culture-specific\nknowledge that might not be learned during pre-training. How do we find such\nknowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The\nmost common solutions are single-initiative: either researchers define\nchallenging questions that users passively answer (traditional annotation), or\nusers actively produce data that researchers structure as benchmarks (knowledge\nextraction). The process would benefit from mixed-initiative collaboration,\nwhere users guide the process to meaningfully reflect their cultures, and LLMs\nsteer the process towards more challenging questions that meet the researcher's\ngoals. We propose a mixed-initiative methodology called CultureCartography.\nHere, an LLM initializes annotation with questions for which it has\nlow-confidence answers, making explicit both its prior knowledge and the gaps\ntherein. This allows a human respondent to fill these gaps and steer the model\ntowards salient topics through direct edits. We implement this methodology as a\ntool called CultureExplorer. Compared to a baseline where humans answer\nLLM-proposed questions, we find that CultureExplorer more effectively produces\nknowledge that leading models like DeepSeek R1 and GPT-4o are missing, even\nwith web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B\nby up to 19.2% on related culture benchmarks.",
      "url": "http://arxiv.org/abs/2510.27672v1",
      "published_time_eastern_timestamp": 1761932254.0
    },
    {
      "title": "RDMA Point-to-Point Communication for LLM Systems",
      "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.",
      "url": "http://arxiv.org/abs/2510.27656v1",
      "published_time_eastern_timestamp": 1761931702.0
    },
    {
      "title": "SpecAttn: Speculating Sparse Attention",
      "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
      "url": "http://arxiv.org/abs/2510.27641v1",
      "published_time_eastern_timestamp": 1761930754.0
    },
    {
      "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training",
      "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.",
      "url": "http://arxiv.org/abs/2510.27630v2",
      "published_time_eastern_timestamp": 1761930022.0
    },
    {
      "title": "Validity Is What You Need",
      "summary": "While AI agents have long been discussed and studied in computer science,\ntoday's Agentic AI systems are something new. We consider other definitions of\nAgentic AI and propose a new realist definition. Agentic AI is a software\ndelivery mechanism, comparable to software as a service (SaaS), which puts an\napplication to work autonomously in a complex enterprise setting. Recent\nadvances in large language models (LLMs) as foundation models have driven\nexcitement in Agentic AI. We note, however, that Agentic AI systems are\nprimarily applications, not foundations, and so their success depends on\nvalidation by end users and principal stakeholders. The tools and techniques\nneeded by the principal users to validate their applications are quite\ndifferent from the tools and techniques used to evaluate foundation models.\nIronically, with good validation measures in place, in many cases the\nfoundation models can be replaced with much simpler, faster, and more\ninterpretable models that handle core logic. When it comes to Agentic AI,\nvalidity is what you need. LLMs are one option that might achieve it.",
      "url": "http://arxiv.org/abs/2510.27628v1",
      "published_time_eastern_timestamp": 1761930004.0
    },
    {
      "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
      "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
      "url": "http://arxiv.org/abs/2510.27617v1",
      "published_time_eastern_timestamp": 1761928858.0
    }
  ]
}
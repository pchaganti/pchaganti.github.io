{
  "last_updated": "2025-08-06T14:19:10.229131-04:00",
  "papers": [
    {
      "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
      "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.",
      "url": "http://arxiv.org/abs/2508.03686v1",
      "published_time_eastern_timestamp": 1754416524.0
    },
    {
      "title": "No LLM Solved Yu Tsumura's 554th Problem",
      "summary": "We show, contrary to the optimism about LLM's problem-solving abilities,\nfueled by the recent gold medals that were attained, that a problem exists --\nYu Tsumura's 554th problem -- that a) is within the scope of an IMO problem in\nterms of proof sophistication, b) is not a combinatorics problem which has\ncaused issues for LLMs, c) requires fewer proof techniques than typical hard\nIMO problems, d) has a publicly available solution (likely in the training data\nof LLMs), and e) that cannot be readily solved by any existing off-the-shelf\nLLM (commercial or open-source).",
      "url": "http://arxiv.org/abs/2508.03685v1",
      "published_time_eastern_timestamp": 1754416520.0
    },
    {
      "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
      "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
      "url": "http://arxiv.org/abs/2508.03680v1",
      "published_time_eastern_timestamp": 1754416213.0
    },
    {
      "title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code\n  Generation",
      "summary": "State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement.",
      "url": "http://arxiv.org/abs/2508.03678v1",
      "published_time_eastern_timestamp": 1754416188.0
    },
    {
      "title": "A DbC Inspired Neurosymbolic Layer for Trustworthy Agent Design",
      "summary": "Generative models, particularly Large Language Models (LLMs), produce fluent\noutputs yet lack verifiable guarantees. We adapt Design by Contract (DbC) and\ntype-theoretic principles to introduce a contract layer that mediates every LLM\ncall. Contracts stipulate semantic and type requirements on inputs and outputs,\ncoupled with probabilistic remediation to steer generation toward compliance.\nThe layer exposes the dual view of LLMs as semantic parsers and probabilistic\nblack-box components. Contract satisfaction is probabilistic and semantic\nvalidation is operationally defined through programmer-specified conditions on\nwell-typed data structures. More broadly, this work postulates that any two\nagents satisfying the same contracts are \\emph{functionally equivalent} with\nrespect to those contracts.",
      "url": "http://arxiv.org/abs/2508.03665v1",
      "published_time_eastern_timestamp": 1754414690.0
    },
    {
      "title": "Automated Algorithmic Discovery for Gravitational-Wave Detection Guided\n  by LLM-Informed Evolutionary Monte Carlo Tree Search",
      "summary": "Computational scientific discovery increasingly relies on algorithms to\nprocess complex data and identify meaningful patterns - yet faces persistent\nchallenges in gravitational-wave signal identification. While existing\nalgorithmic approaches like matched filtering (MF) and deep neural networks\n(DNNs) have achieved partial success, their limitations directly stem from\nfundamental limitations: MF's excessive computational demands arise from its\nreliance on predefined theoretical waveform templates, while DNNs' black-box\narchitectures obscure decision logic and introduce hidden biases. We propose\nEvolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses\nthese limitations through systematic algorithm space exploration guided by\ndomain-aware physical constraints. Our approach combines tree-structured search\nwith evolutionary optimization and large language model heuristics to create\ninterpretable algorithmic solutions. Our Evo-MCTS framework demonstrates\nsubstantial improvements, achieving a 20.2\\% improvement over state-of-the-art\ngravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.\nHigh-performing algorithm variants consistently exceed thresholds. The\nframework generates human-interpretable algorithmic pathways that reveal\ndistinct performance patterns. Beyond performance improvements, our framework\ndiscovers novel algorithmic combinations, thereby establishing a transferable\nmethodology for automated algorithmic discovery across computational science\ndomains.",
      "url": "http://arxiv.org/abs/2508.03661v1",
      "published_time_eastern_timestamp": 1754414300.0
    },
    {
      "title": "LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for\n  Advertiser Keyphrase Recommendations at eBay",
      "summary": "Sellers at eBay are recommended keyphrases to bid on to enhance the\nperformance of their advertising campaigns. The relevance of these keyphrases\nis crucial in avoiding the overcrowding of search systems with irrelevant items\nand maintaining a positive seller perception. It is essential that keyphrase\nrecommendations align with both seller and Search judgments regarding auctions.\nDue to the difficulty in procuring negative human judgment at scale, employing\nLLM-as-a-judge to mimic seller judgment has been established as the norm in\nseveral studies. This study introduces a novel two-step LLM distillation\nprocess from a LLM-judge used to debias our Embedding Based Retrieval (EBR)\nmodel from the various biases that exist in click-data. We distill from an LLM\nteacher via a cross-encoder assistant into a bi-encoder student using a\nmulti-task training approach, ultimately employing the student bi-encoder to\nretrieve relevant advertiser keyphrases. We show that integrating a knowledge\ndistillation process from LLMs in a multi-task training setup enhances\nbi-encoder performance in retrieving relevant advertiser keyphrases at eBay.",
      "url": "http://arxiv.org/abs/2508.03628v1",
      "published_time_eastern_timestamp": 1754412437.0
    },
    {
      "title": "Refining Critical Thinking in LLM Code Generation: A Faulty\n  Premise-based Evaluation Framework",
      "summary": "With the advancement of code generation capabilities in large language models\n(LLMs), their reliance on input premises has intensified. When users provide\ninputs containing faulty premises, the probability of code generation\nhallucinations rises significantly, exposing deficiencies in their\nself-scrutiny capabilities. This paper proposes Faulty Premises Bench\n(FPBench), the first code generation evaluation framework targeting faulty\npremises. By systematically constructing three categories of faulty premises\nand integrating multi-dimensional evaluation metrics, it conducts in-depth\nassessments of 15 representative LLMs. The key findings are as follows: (1)\nMost models exhibit poor reasoning abilities and suboptimal code generation\nperformance under faulty premises, heavily relying on explicit prompts for\nerror detection, with limited self-scrutiny capabilities; (2) Faulty premises\ntrigger a point of diminishing returns in resource investment, leading to\nblindly increasing length fails to enhance quality; (3) The three types of\nfaulty premises respectively activate distinct defect patterns in models,\nrevealing a triple dissociation in the cognitive mechanisms of code generation\nmodels. This study not only highlights the urgent need for LLMs to proactively\nverify premises in code generation but also, through the proposed FPBench\nframework and multi-dimensional evaluation system, provides a theoretical\nfoundation and practical pathway for developing reliable, human-centric code\ngeneration models.",
      "url": "http://arxiv.org/abs/2508.03622v1",
      "published_time_eastern_timestamp": 1754411979.0
    },
    {
      "title": "Block: Balancing Load in LLM Serving with Context, Knowledge and\n  Predictive Scheduling",
      "summary": "This paper presents Block, a distributed scheduling framework designed to\noptimize load balancing and auto-provisioning across instances in large\nlanguage model serving frameworks by leveraging contextual information from\nincoming requests. Unlike popular model serving systems that rely on monolithic\nand heuristic task schedulers, Block operates as a fully distributed,\nstateless, and predictive scheduling system to achieve low overhead,\nreliability, and scalability. It leverages the deterministic and predictable\ncharacteristics of LLM inferences, such as host configurations, response\nlengths, and hardware performance, to make scheduling decisions based on\naccurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block\nsignificantly outperforms heuristic schedulers, boosting serving capacity by up\nto 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance\ngains remain consistent across diverse models, workloads and configurations.\nCode and data are open-sourced.",
      "url": "http://arxiv.org/abs/2508.03611v1",
      "published_time_eastern_timestamp": 1754411230.0
    },
    {
      "title": "ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated\n  Test Programs",
      "summary": "Existing LLM-based compiler fuzzers often produce syntactically or\nsemantically invalid test programs, limiting their effectiveness in exercising\ncompiler optimizations and backend components. We introduce ReFuzzer, a\nframework for refining LLM-generated test programs by systematically detecting\nand correcting compilation and runtime violations (e.g. division by zero or\narray out-of-bounds accesses). ReFuzzer employs a feedback loop with a local\nLLM to validate and filter erroneous programs before execution, improving\nfuzzing effectiveness beyond crash detection and enabling the generation of\ndiverse yet valid test programs.\n  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box\nfuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'\nvalidity from 47.0-49.4% to 96.6-97.3%, with an average processing time of\n2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing\nsignificantly increased code coverage in critical optimization and IR\ngeneration components. For example, vectorization coverage had an absolute\nimprovement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,\nenhancing testing effectiveness.",
      "url": "http://arxiv.org/abs/2508.03603v1",
      "published_time_eastern_timestamp": 1754410622.0
    },
    {
      "title": "OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering\n  Dataset",
      "summary": "Lifelogging refers to the process of passively collecting, storing, and\nanalysing personal daily life data using wearable devices. This data can\nsupport applications in memory preservation and enhancement. For example, using\nan ask-and-answer strategy, question-answering (QA) on lifelog data opens an\ninteractive and interesting way to explore memorable events and insights into\ndaily life. However, research resources for QA on lifelog data are limited to\nsmall-sized or synthetic QA datasets. In this paper, we present a novel lifelog\nQA dataset called OpenLifelogQA, building upon an 18-month lifelog dataset. Our\ndataset focuses on an open-ended and practical QA with real-world application\nin daily lifelog usage. We construct 14,187 pairs of Q&A with diverse types and\ndifficulty levels. A baseline experiment is reported for this dataset with\ncompetitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665\nLLM Score from LLaVA-NeXT-Interleave 7B model. We release this Q&A dataset to\nthe research community to support new research into lifelog technologies, such\nas enabling personal chat-based assistants for lifelog data to become a\nreality.",
      "url": "http://arxiv.org/abs/2508.03583v1",
      "published_time_eastern_timestamp": 1754409016.0
    },
    {
      "title": "Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed\n  Learning for Continual Adaptation",
      "summary": "Large Language Models (LLMs) often suffer from performance degradation when\nfaced with domain shifts, primarily due to catastrophic forgetting. In this\nwork, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),\na novel continual learning framework that integrates dynamic knowledge graphs\nwith instruction tuning. By leveraging retrieved domain-specific knowledge as\nguidance during training, KILO enhances both adaptability to new domains and\nretention of previously acquired knowledge. We pretrain our model on\nWikiText-103 and evaluate sequential adaptation across four diverse target\ndomains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that\nKILO consistently outperforms strong baselines, including continual\nfine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward\ntransfer, F1 score, retention rate, and training efficiency. These results\nhighlight the effectiveness of combining structured knowledge retrieval and\ninstruction prompting to overcome domain shift challenges in continual learning\nscenarios.",
      "url": "http://arxiv.org/abs/2508.03571v1",
      "published_time_eastern_timestamp": 1754408377.0
    },
    {
      "title": "SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code\n  Generation",
      "summary": "In today's rapidly evolving field of electronic design automation (EDA), the\ncomplexity of hardware designs is increasing, necessitating more sophisticated\nautomation solutions. High-level synthesis (HLS), as a pivotal solution,\nautomates hardware designs from high-level abstractions (e.g., C/C++). However,\nit faces significant challenges, particularly in design space exploration and\noptimization. While large language models (LLMs) have shown notable\ncapabilities in code generation, their application to HLS has been limited due\nto the scarcity of (publicly) available HLS code datasets. Hence, research in\nthis domain has primarily focused on techniques such as prompt engineering and\nretrieval-augmented generation (RAG). To overcome this limitation, this paper\nintroduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS\ncode generation. Our method includes three key advancements: (i) We implement\nVerilog-to-C/C++ porting, converting verified and synthesizable Verilog codes\ninto corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement\na fine-tuning strategy, which is based on instruction prompting to code\ngeneration guided by abstract syntax tree (AST); (iii) We develop a\nsemi-automated evaluation framework using VerilogEval to assess the\nfunctionality of the generated HLS code. Our experiments show that SAGE-HLS,\nfined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate\nin code synthesizability and a 75% success rate in functional correctness.",
      "url": "http://arxiv.org/abs/2508.03558v1",
      "published_time_eastern_timestamp": 1754407693.0
    },
    {
      "title": "VRPRM: Process Reward Modeling via Visual Reasoning",
      "summary": "Process Reward Model (PRM) is widely used in the post-training of Large\nLanguage Model (LLM) because it can perform fine-grained evaluation of the\nreasoning steps of generated content. However, most PRMs lack long-term\nreasoning and deep thinking capabilities. On the other hand, although a few\nworks have tried to introduce Chain-of-Thought capability into PRMs, the\nannotation cost of CoT-PRM data is too expensive to play a stable role in\nvarious tasks. To address the above challenges, we propose VRPRM, a process\nreward model via visual reasoning, and design an efficient two-stage training\nstrategy. Experimental results show that using only 3.6K CoT-PRM SFT data and\n50K non-CoT PRM RL training data, VRPRM can surpass the non-thinking PRM with a\ntotal data volume of 400K and achieved a relative performance improvement of up\nto 118\\% over the base model in the BoN experiment. This result confirms that\nthe proposed combined training strategy can achieve higher quality reasoning\ncapabilities at a lower data annotation cost, thus providing a new paradigm for\nPRM training with more efficient data utilization.",
      "url": "http://arxiv.org/abs/2508.03556v1",
      "published_time_eastern_timestamp": 1754407524.0
    },
    {
      "title": "MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in\n  Multi-source Retrieval Augmented Generation",
      "summary": "Retrieval Augmented Generation (RAG) has emerged as a promising solution to\naddress hallucination issues in Large Language Models (LLMs). However, the\nintegration of multiple retrieval sources, while potentially more informative,\nintroduces new challenges that can paradoxically exacerbate hallucination\nproblems. These challenges manifest primarily in two aspects: the sparse\ndistribution of multi-source data that hinders the capture of logical\nrelationships and the inherent inconsistencies among different sources that\nlead to information conflicts. To address these challenges, we propose\nMultiRAG, a novel framework designed to mitigate hallucination in multi-source\nretrieval-augmented generation through knowledge-guided approaches. Our\nframework introduces two key innovations: (1) a knowledge construction module\nthat employs multi-source line graphs to efficiently aggregate logical\nrelationships across different knowledge sources, effectively addressing the\nsparse data distribution issue; and (2) a sophisticated retrieval module that\nimplements a multi-level confidence calculation mechanism, performing both\ngraph-level and node-level assessments to identify and eliminate unreliable\ninformation nodes, thereby reducing hallucinations caused by inter-source\ninconsistencies. Extensive experiments on four multi-domain query datasets and\ntwo multi-hop QA datasets demonstrate that MultiRAG significantly enhances the\nreliability and efficiency of knowledge retrieval in complex multi-source\nscenarios. \\textcolor{blue}{Our code is available in\nhttps://github.com/wuwenlong123/MultiRAG.",
      "url": "http://arxiv.org/abs/2508.03553v1",
      "published_time_eastern_timestamp": 1754407252.0
    },
    {
      "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via\n  Internal Representations",
      "summary": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using large language models, a paradigm known as\n\"LLMas-a-judge.\" However, improving its alignment with human preferences\nwithout complex prompts or fine-tuning remains challenging. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and taskrelevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a lightweight and\nefficient framework for enhancing LLM-as-a-Judge alignment with human scoring,\nvia internal representations. LAGER produces fine-grained judgment scores by\naggregating cross-layer scoretoken logits and computing the expected score from\na softmax-based distribution, with the LLM backbone kept frozen. LAGER fully\nleverages the complementary information across different layers, overcoming the\nlimitations of relying solely on the final layer. We evaluate our method on the\nstandard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman\ncorrelation, and find that LAGER achieves improvements of up to 7.5% over the\nbest baseline across these benchmarks. Without reasoning steps, LAGER matches\nor outperforms reasoning-based methods. Experiments on downstream applications,\nsuch as data selection and emotional understanding, further show the\neffectiveness of our method.",
      "url": "http://arxiv.org/abs/2508.03550v1",
      "published_time_eastern_timestamp": 1754407116.0
    },
    {
      "title": "Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs\n  and Vision Models",
      "summary": "Large language models (LLMs) have enabled the automatic generation of\nstep-by-step augmented reality (AR) instructions for a wide range of physical\ntasks. However, existing LLM-based AR guidance often lacks rich visual\naugmentations to effectively embed instructions into spatial context for a\nbetter user understanding. We present Guided Reality, a fully automated AR\nsystem that generates embedded and dynamic visual guidance based on\nstep-by-step instructions. Our system integrates LLMs and vision models to: 1)\ngenerate multi-step instructions from user queries, 2) identify appropriate\ntypes of visual guidance, 3) extract spatial information about key interaction\npoints in the real world, and 4) embed visual guidance in physical space to\nsupport task execution. Drawing from a corpus of user manuals, we define five\ncategories of visual guidance and propose an identification strategy based on\nthe current step. We evaluate the system through a user study (N=16),\ncompleting real-world tasks and exploring the system in the wild. Additionally,\nfour instructors shared insights on how Guided Reality could be integrated into\ntheir training workflows.",
      "url": "http://arxiv.org/abs/2508.03547v1",
      "published_time_eastern_timestamp": 1754406935.0
    },
    {
      "title": "MoKA: Mixture of Kronecker Adapters",
      "summary": "Parameter-efficient fine-tuning (PEFT) is essential for reducing the\ncomputational overhead of large language models (LLMs). Low-rank family\nadapters are commonly used to control the parameter size efficiently while\nmaintaining the generative power of LLMs. However, their limited expressiveness\ndue to the rank constraint often restricts their performance on complex tasks.\nWe propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker\nadapters that addresses this limitation by modeling weight updates as a mixture\nof Kronecker products. Our proposed adapter leverages a gating mechanism that\nmeasures the importance of each Kronecker factor, enabling more expressive\nadaptation. Moreover, MoKA enables a rank flexibility that provides a better\ntrade-off between parameter efficiency and accuracy. To ensure hardware\nefficiency, we reformulate Kronecker computations using standard matrix\noperations, allowing seamless deployment on GPU-optimized hardware. We conduct\nextensive experiments on instruction-tuning and commonsense reasoning tasks\nusing low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not\nonly outperforms PEFT baselines, but also reduces the number of trainable\nparameters up to 27x, achieving state-of-the-art trade-offs between performance\nand parameter efficiency.",
      "url": "http://arxiv.org/abs/2508.03527v1",
      "published_time_eastern_timestamp": 1754405894.0
    },
    {
      "title": "FilBench: Can LLMs Understand and Generate Filipino?",
      "summary": "Despite the impressive performance of LLMs on English-based tasks, little is\nknown about their capabilities in specific languages such as Filipino. In this\nwork, we address this gap by introducing FilBench, a Filipino-centric benchmark\ndesigned to evaluate LLMs across a diverse set of tasks and capabilities in\nFilipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to\nreflect the priorities and trends of NLP research in the Philippines such as\nCultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By\nevaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs\nsuffer from reading comprehension and translation capabilities. Our results\nindicate that FilBench is challenging, with the best model, GPT-4o, achieving\nonly a score of 72.23%. Moreover, we also find that models trained specifically\nfor Southeast Asian languages tend to underperform on FilBench, with the\nhighest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.\nOur work demonstrates the value of curating language-specific LLM benchmarks to\naid in driving progress on Filipino NLP and increasing the inclusion of\nPhilippine languages in LLM development.",
      "url": "http://arxiv.org/abs/2508.03523v1",
      "published_time_eastern_timestamp": 1754405312.0
    },
    {
      "title": "Training Long-Context, Multi-Turn Software Engineering Agents with\n  Reinforcement Learning",
      "summary": "Research on applications of Reinforcement Learning (RL) to Large Language\nModels (LLMs) has mostly been focused on single-turn problems, such as\nmathematical reasoning or single-shot code generation. While these problems can\nbe viewed as token-level multi-turn MDPs, this view corresponds to a degenerate\ncase of multi-turn interaction where the environment provides no feedback. This\ncontrasts with many real-world domains, such as software engineering (SWE),\nwhich require rich multi-turn interactions with a stateful environment that\nresponds to each action with a non-trivial observation.\n  To bridge this gap, we demonstrate the successful application of RL to this\ngeneral regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)\nalgorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world\nsoftware engineering tasks. Our approach increases the agent's success rate on\nthe SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to\n39%, without relying on any teacher models. On SWE-rebench, our agent matches\nor outperforms leading open-weight models such as DeepSeek-V3-0324 and\nQwen3-235B-A22B using an identical scaffolding, offering a viable path toward\nbuilding more capable autonomous agents for complex real-world problems based\non open models.",
      "url": "http://arxiv.org/abs/2508.03501v1",
      "published_time_eastern_timestamp": 1754404247.0
    }
  ]
}
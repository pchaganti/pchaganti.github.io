{
  "last_updated": "2025-07-13T02:17:36.710898-04:00",
  "papers": [
    {
      "title": "PyVision: Agentic Vision with Dynamic Tooling",
      "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.",
      "url": "http://arxiv.org/abs/2507.07998v1",
      "published_time_eastern_timestamp": 1752170395.0
    },
    {
      "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
      "summary": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
      "url": "http://arxiv.org/abs/2507.07996v1",
      "published_time_eastern_timestamp": 1752170393.0
    },
    {
      "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
      "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2$\\times$ speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3$\\times$ speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
      "url": "http://arxiv.org/abs/2507.07990v1",
      "published_time_eastern_timestamp": 1752170342.0
    },
    {
      "title": "Automating Expert-Level Medical Reasoning Evaluation of Large Language\n  Models",
      "summary": "As large language models (LLMs) become increasingly integrated into clinical\ndecision-making, ensuring transparent and trustworthy reasoning is essential.\nHowever, existing evaluation strategies of LLMs' medical reasoning capability\neither suffer from unsatisfactory assessment or poor scalability, and a\nrigorous benchmark remains lacking. To address this, we introduce\nMedThink-Bench, a benchmark designed for rigorous, explainable, and scalable\nassessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging\nquestions across ten medical domains, each annotated with expert-crafted\nstep-by-step rationales. Building on this, we propose LLM-w-Ref, a novel\nevaluation framework that leverages fine-grained rationales and LLM-as-a-Judge\nmechanisms to assess intermediate reasoning with expert-level fidelity while\nmaintaining scalability. Experiments show that LLM-w-Ref exhibits a strong\npositive correlation with expert judgments. Benchmarking twelve\nstate-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can\nsurpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,\nMedThink-Bench offers a foundational tool for evaluating LLMs' medical\nreasoning, advancing their safe and responsible deployment in clinical\npractice.",
      "url": "http://arxiv.org/abs/2507.07988v1",
      "published_time_eastern_timestamp": 1752170306.0
    },
    {
      "title": "Performance and Practical Considerations of Large and Small Language\n  Models in Clinical Decision Support in Rheumatology",
      "summary": "Large language models (LLMs) show promise for supporting clinical\ndecision-making in complex fields such as rheumatology. Our evaluation shows\nthat smaller language models (SLMs), combined with retrieval-augmented\ngeneration (RAG), achieve higher diagnostic and therapeutic performance than\nlarger models, while requiring substantially less energy and enabling\ncost-efficient, local deployment. These features are attractive for\nresource-limited healthcare. However, expert oversight remains essential, as no\nmodel consistently reached specialist-level accuracy in rheumatology.",
      "url": "http://arxiv.org/abs/2507.07983v1",
      "published_time_eastern_timestamp": 1752170163.0
    },
    {
      "title": "Defending Against Prompt Injection With a Few DefensiveTokens",
      "summary": "When large language model (LLM) systems interact with external data to\nperform complex tasks, a new attack, namely prompt injection, becomes a\nsignificant threat. By injecting instructions into the data accessed by the\nsystem, the attacker is able to override the initial user task with an\narbitrary task directed by the attacker. To secure the system, test-time\ndefenses, e.g., defensive prompting, have been proposed for system developers\nto attain security only when needed in a flexible manner. However, they are\nmuch less effective than training-time defenses that change the model\nparameters. Motivated by this, we propose DefensiveToken, a test-time defense\nwith prompt injection robustness comparable to training-time alternatives.\nDefensiveTokens are newly inserted as special tokens, whose embeddings are\noptimized for security. In security-sensitive cases, system developers can\nappend a few DefensiveTokens before the LLM input to achieve security with a\nminimal utility drop. In scenarios where security is less of a concern,\ndevelopers can simply skip DefensiveTokens; the LLM system remains the same as\nthere is no defense, generating high-quality responses. Thus, DefensiveTokens,\nif released alongside the model, allow a flexible switch between the\nstate-of-the-art (SOTA) utility and almost-SOTA security at test time. The code\nis available at https://github.com/Sizhe-Chen/DefensiveToken.",
      "url": "http://arxiv.org/abs/2507.07974v1",
      "published_time_eastern_timestamp": 1752169865.0
    },
    {
      "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
      "summary": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.",
      "url": "http://arxiv.org/abs/2507.07957v1",
      "published_time_eastern_timestamp": 1752169211.0
    },
    {
      "title": "Can Large Language Models Improve Phishing Defense? A Large-Scale\n  Controlled Experiment on Warning Dialogue Explanations",
      "summary": "Phishing has become a prominent risk in modern cybersecurity, often used to\nbypass technological defences by exploiting predictable human behaviour.\nWarning dialogues are a standard mitigation measure, but the lack of\nexplanatory clarity and static content limits their effectiveness. In this\npaper, we report on our research to assess the capacity of Large Language\nModels (LLMs) to generate clear, concise, and scalable explanations for\nphishing warnings. We carried out a large-scale between-subjects user study (N\n= 750) to compare the influence of warning dialogues supplemented with manually\ngenerated explanations against those generated by two LLMs, Claude 3.5 Sonnet\nand Llama 3.3 70B. We investigated two explanatory styles (feature-based and\ncounterfactual) for their effects on behavioural metrics (click-through rate)\nand perceptual outcomes (e.g., trust, risk, clarity). The results indicate that\nwell-constructed LLM-generated explanations can equal or surpass manually\ncrafted explanations in reducing susceptibility to phishing; Claude-generated\nwarnings exhibited particularly robust performance. Feature-based explanations\nwere more effective for genuine phishing attempts, whereas counterfactual\nexplanations diminished false-positive rates. Other variables such as workload,\ngender, and prior familiarity with warning dialogues significantly moderated\nwarning effectiveness. These results indicate that LLMs can be used to\nautomatically build explanations for warning users against phishing, and that\nsuch solutions are scalable, adaptive, and consistent with human-centred\nvalues.",
      "url": "http://arxiv.org/abs/2507.07916v1",
      "published_time_eastern_timestamp": 1752166445.0
    },
    {
      "title": "DTECT: Dynamic Topic Explorer & Context Tracker",
      "summary": "The explosive growth of textual data over time presents a significant\nchallenge in uncovering evolving themes and trends. Existing dynamic topic\nmodeling techniques, while powerful, often exist in fragmented pipelines that\nlack robust support for interpretation and user-friendly exploration. We\nintroduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end\nsystem that bridges the gap between raw textual data and meaningful temporal\ninsights. DTECT provides a unified workflow that supports data preprocessing,\nmultiple model architectures, and dedicated evaluation metrics to analyze the\ntopic quality of temporal topic models. It significantly enhances\ninterpretability by introducing LLM-driven automatic topic labeling, trend\nanalysis via temporally salient words, interactive visualizations with\ndocument-level summarization, and a natural language chat interface for\nintuitive data querying. By integrating these features into a single, cohesive\nplatform, DTECT empowers users to more effectively track and understand\nthematic dynamics. DTECT is open-source and available at\nhttps://github.com/AdhyaSuman/DTECT.",
      "url": "http://arxiv.org/abs/2507.07910v1",
      "published_time_eastern_timestamp": 1752165873.0
    },
    {
      "title": "Agentic Retrieval of Topics and Insights from Earnings Calls",
      "summary": "Tracking the strategic focus of companies through topics in their earnings\ncalls is a key task in financial analysis. However, as industries evolve,\ntraditional topic modeling techniques struggle to dynamically capture emerging\ntopics and their relationships. In this work, we propose an LLM-agent driven\napproach to discover and retrieve emerging topics from quarterly earnings\ncalls. We propose an LLM-agent to extract topics from documents, structure them\ninto a hierarchical ontology, and establish relationships between new and\nexisting topics through a topic ontology. We demonstrate the use of extracted\ntopics to infer company-level insights and emerging trends over time. We\nevaluate our approach by measuring ontology coherence, topic evolution\naccuracy, and its ability to surface emerging financial trends.",
      "url": "http://arxiv.org/abs/2507.07906v1",
      "published_time_eastern_timestamp": 1752165539.0
    },
    {
      "title": "Automating MD simulations for Proteins using Large language Models:\n  NAMD-Agent",
      "summary": "Molecular dynamics simulations are an essential tool in understanding protein\nstructure, dynamics, and function at the atomic level. However, preparing high\nquality input files for MD simulations can be a time consuming and error prone\nprocess. In this work, we introduce an automated pipeline that leverages Large\nLanguage Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with\npython scripting and Selenium based web automation to streamline the generation\nof MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based\ninterface for preparing simulation-ready inputs for NAMD. By integrating\nGemini's code generation and iterative refinement capabilities, simulation\nscripts are automatically written, executed, and revised to navigate CHARMM\nGUI, extract appropriate parameters, and produce the required NAMD input files.\nPost processing is performed using additional software to further refine the\nsimulation outputs, thereby enabling a complete and largely hands free\nworkflow. Our results demonstrate that this approach reduces setup time,\nminimizes manual errors, and offers a scalable solution for handling multiple\nprotein systems in parallel. This automated framework paves the way for broader\napplication of LLMs in computational structural biology, offering a robust and\nadaptable platform for future developments in simulation automation.",
      "url": "http://arxiv.org/abs/2507.07887v1",
      "published_time_eastern_timestamp": 1752164260.0
    },
    {
      "title": "Edge-ASR: Towards Low-Bit Quantization of Automatic Speech Recognition\n  Models",
      "summary": "Recent advances in Automatic Speech Recognition (ASR) have demonstrated\nremarkable accuracy and robustness in diverse audio applications, such as live\ntranscription and voice command processing. However, deploying these models on\nresource constrained edge devices (e.g., IoT device, wearables) still presents\nsubstantial challenges due to strict limits on memory, compute and power.\nQuantization, particularly Post-Training Quantization (PTQ), offers an\neffective way to reduce model size and inference cost without retraining.\nDespite its importance, the performance implications of various advanced\nquantization methods and bit-width configurations on ASR models remain unclear.\nIn this work, we present a comprehensive benchmark of eight state-of-the-art\n(SOTA) PTQ methods applied to two leading edge-ASR model families, Whisper and\nMoonshine. We systematically evaluate model performances (i.e., accuracy,\nmemory I/O and bit operations) across seven diverse datasets from the open ASR\nleaderboard, analyzing the impact of quantization and various configurations on\nboth weights and activations. Built on an extension of the LLM compression\ntoolkit, our framework integrates edge-ASR models, diverse advanced\nquantization algorithms, a unified calibration and evaluation data pipeline,\nand detailed analysis tools. Our results characterize the trade-offs between\nefficiency and accuracy, demonstrating that even 3-bit quantization can succeed\non high capacity models when using advanced PTQ techniques. These findings\nprovide valuable insights for optimizing ASR models on low-power, always-on\nedge devices.",
      "url": "http://arxiv.org/abs/2507.07877v1",
      "published_time_eastern_timestamp": 1752163227.0
    },
    {
      "title": "DocCHA: Towards LLM-Augmented Interactive Online diagnosis System",
      "summary": "Despite the impressive capabilities of Large Language Models (LLMs), existing\nConversational Health Agents (CHAs) remain static and brittle, incapable of\nadaptive multi-turn reasoning, symptom clarification, or transparent\ndecision-making. This hinders their real-world applicability in clinical\ndiagnosis, where iterative and structured dialogue is essential. We propose\nDocCHA, a confidence-aware, modular framework that emulates clinical reasoning\nby decomposing the diagnostic process into three stages: (1) symptom\nelicitation, (2) history acquisition, and (3) causal graph construction. Each\nmodule uses interpretable confidence scores to guide adaptive questioning,\nprioritize informative clarifications, and refine weak reasoning links.\n  Evaluated on two real-world Chinese consultation datasets (IMCS21, DX),\nDocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5,\nGPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and\nover 30 percent improvement in symptom recall, with only modest increase in\ndialogue turns. These results demonstrate the effectiveness of DocCHA in\nenabling structured, transparent, and efficient diagnostic conversations --\npaving the way for trustworthy LLM-powered clinical assistants in multilingual\nand resource-constrained settings.",
      "url": "http://arxiv.org/abs/2507.07870v1",
      "published_time_eastern_timestamp": 1752162724.0
    },
    {
      "title": "From Ambiguity to Accuracy: The Transformative Effect of Coreference\n  Resolution on Retrieval-Augmented Generation systems",
      "summary": "Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in\nnatural language processing (NLP), improving factual consistency and reducing\nhallucinations by integrating external document retrieval with large language\nmodels (LLMs). However, the effectiveness of RAG is often hindered by\ncoreferential complexity in retrieved documents, introducing ambiguity that\ndisrupts in-context learning. In this study, we systematically investigate how\nentity coreference affects both document retrieval and generative performance\nin RAG-based systems, focusing on retrieval relevance, contextual\nunderstanding, and overall response quality. We demonstrate that coreference\nresolution enhances retrieval effectiveness and improves question-answering\n(QA) performance. Through comparative analysis of different pooling strategies\nin retrieval tasks, we find that mean pooling demonstrates superior context\ncapturing ability after applying coreference resolution. In QA tasks, we\ndiscover that smaller models benefit more from the disambiguation process,\nlikely due to their limited inherent capacity for handling referential\nambiguity. With these findings, this study aims to provide a deeper\nunderstanding of the challenges posed by coreferential complexity in RAG,\nproviding guidance for improving retrieval and generation in\nknowledge-intensive AI applications.",
      "url": "http://arxiv.org/abs/2507.07847v1",
      "published_time_eastern_timestamp": 1752161219.0
    },
    {
      "title": "MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving",
      "summary": "Recent studies show large language models (LLMs) and vision language models\n(VLMs) trained using web-scale data can empower end-to-end autonomous driving\nsystems for a better generalization and interpretation. Specifically, by\ndynamically routing inputs to specialized subsets of parameters, the\nMixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve\nsubstantial performance improvements while maintaining computational\nefficiency. However, general MoE models usually demands extensive training data\nand complex optimization. In this work, inspired by the learning process of\nhuman drivers, we propose a skill-oriented MoE, called MoSE, which mimics human\ndrivers' learning process and reasoning process, skill-by-skill and\nstep-by-step. We propose a skill-oriented routing mechanism that begins with\ndefining and annotating specific skills, enabling experts to identify the\nnecessary driving competencies for various scenarios and reasoning tasks,\nthereby facilitating skill-by-skill learning. Further align the driving process\nto multi-step planning in human reasoning and end-to-end driving models, we\nbuild a hierarchical skill dataset and pretrain the router to encourage the\nmodel to think step-by-step. Unlike multi-round dialogs, MoSE integrates\nvaluable auxiliary tasks (e.g.\\ description, reasoning, planning) in one single\nforward process without introducing any extra computational cost. With less\nthan 3B sparsely activated parameters, our model outperforms several 8B+\nparameters on CODA AD corner case reasoning task. Compared to existing methods\nbased on open-source models and data, our approach achieves state-of-the-art\nperformance with significantly reduced activated model size (at least by\n$62.5\\%$) with a single-turn conversation.",
      "url": "http://arxiv.org/abs/2507.07818v1",
      "published_time_eastern_timestamp": 1752158888.0
    },
    {
      "title": "Understanding and Controlling Repetition Neurons and Induction Heads in\n  In-Context Learning",
      "summary": "This paper investigates the relationship between large language models'\n(LLMs) ability to recognize repetitive input patterns and their performance on\nin-context learning (ICL). In contrast to prior work that has primarily focused\non attention heads, we examine this relationship from the perspective of skill\nneurons, specifically repetition neurons. Our experiments reveal that the\nimpact of these neurons on ICL performance varies depending on the depth of the\nlayer in which they reside. By comparing the effects of repetition neurons and\ninduction heads, we further identify strategies for reducing repetitive outputs\nwhile maintaining strong ICL capabilities.",
      "url": "http://arxiv.org/abs/2507.07810v1",
      "published_time_eastern_timestamp": 1752158431.0
    },
    {
      "title": "Measuring AI Alignment with Human Flourishing",
      "summary": "This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel\nevaluation framework that assesses AI alignment with human flourishing across\nseven dimensions: Character and Virtue, Close Social Relationships, Happiness\nand Life Satisfaction, Meaning and Purpose, Mental and Physical Health,\nFinancial and Material Stability, and Faith and Spirituality. Unlike\ntraditional benchmarks that focus on technical capabilities or harm prevention,\nthe FAI Benchmark measures AI performance on how effectively models contribute\nto the flourishing of a person across these dimensions. The benchmark evaluates\nhow effectively LLM AI systems align with current research models of holistic\nhuman well-being through a comprehensive methodology that incorporates 1,229\nobjective and subjective questions. Using specialized judge Large Language\nModels (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs\ngeometric mean scoring to ensure balanced performance across all flourishing\ndimensions. Initial testing of 28 leading language models reveals that while\nsome models approach holistic alignment (with the highest-scoring models\nachieving 72/100), none are acceptably aligned across all dimensions,\nparticularly in Faith and Spirituality, Character and Virtue, and Meaning and\nPurpose. This research establishes a framework for developing AI systems that\nactively support human flourishing rather than merely avoiding harm, offering\nsignificant implications for AI development, ethics, and evaluation.",
      "url": "http://arxiv.org/abs/2507.07787v1",
      "published_time_eastern_timestamp": 1752156593.0
    },
    {
      "title": "SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex\n  3D Scenes",
      "summary": "The integration of language and 3D perception is critical for embodied AI and\nrobotic systems to perceive, understand, and interact with the physical world.\nSpatial reasoning, a key capability for understanding spatial relationships\nbetween objects, remains underexplored in current 3D vision-language research.\nExisting datasets often mix semantic cues (e.g., object name) with spatial\ncontext, leading models to rely on superficial shortcuts rather than genuinely\ninterpreting spatial relationships. To address this gap, we introduce\nS\\textsc{urprise}3D, a novel dataset designed to evaluate language-guided\nspatial reasoning segmentation in complex 3D scenes. S\\textsc{urprise}3D\nconsists of more than 200k vision language pairs across 900+ detailed indoor\nscenes from ScanNet++ v2, including more than 2.8k unique object classes. The\ndataset contains 89k+ human-annotated spatial queries deliberately crafted\nwithout object name, thereby mitigating shortcut biases in spatial\nunderstanding. These queries comprehensively cover various spatial reasoning\nskills, such as relative position, narrative perspective, parametric\nperspective, and absolute distance reasoning. Initial benchmarks demonstrate\nsignificant challenges for current state-of-the-art expert 3D visual grounding\nmethods and 3D-LLMs, underscoring the necessity of our dataset and the\naccompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite.\nS\\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially\naware AI, paving the way for effective embodied interaction and robotic\nplanning. The code and datasets can be found in\nhttps://github.com/liziwennba/SUPRISE.",
      "url": "http://arxiv.org/abs/2507.07781v1",
      "published_time_eastern_timestamp": 1752156084.0
    },
    {
      "title": "Structured Prompts, Better Outcomes? Exploring the Effects of a\n  Structured Interface with ChatGPT in a Graduate Robotics Course",
      "summary": "Prior research shows that how students engage with Large Language Models\n(LLMs) influences their problem-solving and understanding, reinforcing the need\nto support productive LLM-uses that promote learning. This study evaluates the\nimpact of a structured GPT platform designed to promote 'good' prompting\nbehavior with data from 58 students in a graduate-level robotics course. The\nstudents were assigned to either an intervention group using the structured\nplatform or a control group using ChatGPT freely for two practice lab sessions,\nbefore a third session where all students could freely use ChatGPT. We analyzed\nstudent perception (pre-post surveys), prompting behavior (logs), performance\n(task scores), and learning (pre-post tests). Although we found no differences\nin performance or learning between groups, we identified prompting behaviors -\nsuch as having clear prompts focused on understanding code - that were linked\nwith higher learning gains and were more prominent when students used the\nstructured platform. However, such behaviors did not transfer once students\nwere no longer constrained to use the structured platform. Qualitative survey\ndata showed mixed perceptions: some students perceived the value of the\nstructured platform, but most did not perceive its relevance and resisted\nchanging their habits. These findings contribute to ongoing efforts to identify\neffective strategies for integrating LLMs into learning and question the\neffectiveness of bottom-up approaches that temporarily alter user interfaces to\ninfluence students' interaction. Future research could instead explore top-down\nstrategies that address students' motivations and explicitly demonstrate how\ncertain interaction patterns support learning.",
      "url": "http://arxiv.org/abs/2507.07767v1",
      "published_time_eastern_timestamp": 1752155407.0
    },
    {
      "title": "When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical\n  Advances, and Ethical Governance",
      "summary": "This paper establishes the first comprehensive review of Large Language\nModels (LLMs) applied within the legal domain. It pioneers an innovative dual\nlens taxonomy that integrates legal reasoning frameworks and professional\nontologies to systematically unify historical research and contemporary\nbreakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such\nas contextual reasoning and generative argumentation, surmount traditional\nlimitations by dynamically capturing legal semantics and unifying evidence\nreasoning. Significant progress is documented in task generalization, reasoning\nformalization, workflow integration, and addressing core challenges in text\nprocessing, knowledge integration, and evaluation rigor via technical\ninnovations like sparse attention mechanisms and mixture-of-experts\narchitectures. However, widespread adoption of LLM introduces critical\nchallenges: hallucination, explainability deficits, jurisdictional adaptation\ndifficulties, and ethical asymmetry. This review proposes a novel taxonomy that\nmaps legal roles to NLP subtasks and computationally implements the Toulmin\nargumentation framework, thus systematizing advances in reasoning, retrieval,\nprediction, and dispute resolution. It identifies key frontiers including\nlow-resource systems, multimodal evidence integration, and dynamic rebuttal\nhandling. Ultimately, this work provides both a technical roadmap for\nresearchers and a conceptual framework for practitioners navigating the\nalgorithmic future, laying a robust foundation for the next era of legal\nartificial intelligence. We have created a GitHub repository to index the\nrelevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.",
      "url": "http://arxiv.org/abs/2507.07748v1",
      "published_time_eastern_timestamp": 1752153994.0
    }
  ]
}
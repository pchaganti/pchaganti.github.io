{
  "last_updated": "2025-05-07T19:35:19.364576-04:00",
  "papers": [
    {
      "title": "WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional\n  Websites from Scratch",
      "summary": "LLM-based agents have demonstrated great potential in generating and managing\ncode within complex codebases. In this paper, we introduce WebGen-Bench, a\nnovel benchmark designed to measure an LLM-based agent's ability to create\nmulti-file website codebases from scratch. It contains diverse instructions for\nwebsite generation, created through the combined efforts of human annotators\nand GPT-4o. These instructions span three major categories and thirteen minor\ncategories, encompassing nearly all important types of web applications. To\nassess the quality of the generated websites, we use GPT-4o to generate test\ncases targeting each functionality described in the instructions, and then\nmanually filter, adjust, and organize them to ensure accuracy, resulting in 647\ntest cases. Each test case specifies an operation to be performed on the\nwebsite and the expected result after the operation. To automate testing and\nimprove reproducibility, we employ a powerful web-navigation agent to execute\ntests on the generated websites and determine whether the observed responses\nalign with the expected results. We evaluate three high-performance code-agent\nframeworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and\nopen-source LLMs as engines. The best-performing combination, Bolt.diy powered\nby DeepSeek-R1, achieves only 27.8\\% accuracy on the test cases, highlighting\nthe challenging nature of our benchmark. Additionally, we construct\nWebGen-Instruct, a training set consisting of 6,667 website-generation\ninstructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories\ngenerated from a subset of this training set achieves an accuracy of 38.2\\%,\nsurpassing the performance of the best proprietary model.",
      "url": "http://arxiv.org/abs/2505.03733v1",
      "published_time_eastern_timestamp": 1746554355.0
    },
    {
      "title": "Graph Drawing for LLMs: An Empirical Evaluation",
      "summary": "Our work contributes to the fast-growing literature on the use of Large\nLanguage Models (LLMs) to perform graph-related tasks. In particular, we focus\non usage scenarios that rely on the visual modality, feeding the model with a\ndrawing of the graph under analysis. We investigate how the model's performance\nis affected by the chosen layout paradigm, the aesthetics of the drawing, and\nthe prompting technique used for the queries. We formulate three corresponding\nresearch questions and present the results of a thorough experimental analysis.\nOur findings reveal that choosing the right layout paradigm and optimizing the\nreadability of the input drawing from a human perspective can significantly\nimprove the performance of the model on the given task. Moreover, selecting the\nmost effective prompting technique is a challenging yet crucial task for\nachieving optimal performance.",
      "url": "http://arxiv.org/abs/2505.03678v1",
      "published_time_eastern_timestamp": 1746548622.0
    },
    {
      "title": "PhysLLM: Harnessing Large Language Models for Cross-Modal Remote\n  Physiological Sensing",
      "summary": "Remote photoplethysmography (rPPG) enables non-contact physiological\nmeasurement but remains highly susceptible to illumination changes, motion\nartifacts, and limited temporal modeling. Large Language Models (LLMs) excel at\ncapturing long-range dependencies, offering a potential solution but struggle\nwith the continuous, noise-sensitive nature of rPPG signals due to their\ntext-centric design. To bridge this gap, we introduce PhysLLM, a collaborative\noptimization framework that synergizes LLMs with domain-specific rPPG\ncomponents. Specifically, the Text Prototype Guidance (TPG) strategy is\nproposed to establish cross-modal alignment by projecting hemodynamic features\ninto LLM-interpretable semantic space, effectively bridging the\nrepresentational gap between physiological signals and linguistic tokens.\nBesides, a novel Dual-Domain Stationary (DDS) Algorithm is proposed for\nresolving signal instability through adaptive time-frequency domain feature\nre-weighting. Finally, rPPG task-specific cues systematically inject\nphysiological priors through physiological statistics, environmental contextual\nanswering, and task description, leveraging cross-modal learning to integrate\nboth visual and textual information, enabling dynamic adaptation to challenging\nscenarios like variable illumination and subject movements. Evaluation on four\nbenchmark datasets, PhysLLM achieves state-of-the-art accuracy and robustness,\ndemonstrating superior generalization across lighting variations and motion\nscenarios.",
      "url": "http://arxiv.org/abs/2505.03621v1",
      "published_time_eastern_timestamp": 1746544718.0
    },
    {
      "title": "LlamaFirewall: An open source guardrail system for building secure AI\n  agents",
      "summary": "Large language models (LLMs) have evolved from simple chatbots into\nautonomous agents capable of performing complex tasks such as editing\nproduction code, orchestrating workflows, and taking higher-stakes actions\nbased on untrusted inputs like webpages and emails. These capabilities\nintroduce new security risks that existing security measures, such as model\nfine-tuning or chatbot-focused guardrails, do not fully address. Given the\nhigher stakes and the absence of deterministic solutions to mitigate these\nrisks, there is a critical need for a real-time guardrail monitor to serve as a\nfinal layer of defense, and support system level, use case specific safety\npolicy definition and enforcement. We introduce LlamaFirewall, an open-source\nsecurity focused guardrail framework designed to serve as a final layer of\ndefense against security risks associated with AI Agents. Our framework\nmitigates risks such as prompt injection, agent misalignment, and insecure code\nrisks through three powerful guardrails: PromptGuard 2, a universal jailbreak\ndetector that demonstrates clear state of the art performance; Agent Alignment\nChecks, a chain-of-thought auditor that inspects agent reasoning for prompt\ninjection and goal misalignment, which, while still experimental, shows\nstronger efficacy at preventing indirect injections in general scenarios than\npreviously proposed approaches; and CodeShield, an online static analysis\nengine that is both fast and extensible, aimed at preventing the generation of\ninsecure or dangerous code by coding agents. Additionally, we include\neasy-to-use customizable scanners that make it possible for any developer who\ncan write a regular expression or an LLM prompt to quickly update an agent's\nsecurity guardrails.",
      "url": "http://arxiv.org/abs/2505.03574v1",
      "published_time_eastern_timestamp": 1746542061.0
    },
    {
      "title": "Say It Another Way: A Framework for User-Grounded Paraphrasing",
      "summary": "Small changes in how a prompt is worded can lead to meaningful differences in\nthe behavior of large language models (LLMs), raising concerns about the\nstability and reliability of their evaluations. While prior work has explored\nsimple formatting changes, these rarely capture the kinds of natural variation\nseen in real-world language use. We propose a controlled paraphrasing framework\nbased on a taxonomy of minimal linguistic transformations to systematically\ngenerate natural prompt variations. Using the BBQ dataset, we validate our\nmethod with both human annotations and automated checks, then use it to study\nhow LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our\nanalysis shows that even subtle prompt modifications can lead to substantial\nchanges in model behavior. These results highlight the need for robust,\nparaphrase-aware evaluation protocols.",
      "url": "http://arxiv.org/abs/2505.03563v1",
      "published_time_eastern_timestamp": 1746541050.0
    },
    {
      "title": "A Comprehensive Survey of Large AI Models for Future Communications:\n  Foundations, Applications and Challenges",
      "summary": "The 6G wireless communications aim to establish an intelligent world of\nubiquitous connectivity, providing an unprecedented communication experience.\nLarge artificial intelligence models (LAMs) are characterized by significantly\nlarger scales (e.g., billions or trillions of parameters) compared to typical\nartificial intelligence (AI) models. LAMs exhibit outstanding cognitive\nabilities, including strong generalization capabilities for fine-tuning to\ndownstream tasks, and emergent capabilities to handle tasks unseen during\ntraining. Therefore, LAMs efficiently provide AI services for diverse\ncommunication applications, making them crucial tools for addressing complex\nchallenges in future wireless communication systems. This study provides a\ncomprehensive review of the foundations, applications, and challenges of LAMs\nin communication. First, we introduce the current state of AI-based\ncommunication systems, emphasizing the motivation behind integrating LAMs into\ncommunications and summarizing the key contributions. We then present an\noverview of the essential concepts of LAMs in communication. This includes an\nintroduction to the main architectures of LAMs, such as transformer, diffusion\nmodels, and mamba. We also explore the classification of LAMs, including large\nlanguage models (LLMs), large vision models (LVMs), large multimodal models\n(LMMs), and world models, and examine their potential applications in\ncommunication. Additionally, we cover the training methods and evaluation\ntechniques for LAMs in communication systems. Lastly, we introduce optimization\nstrategies such as chain of thought (CoT), retrieval augmented generation\n(RAG), and agentic systems. Following this, we discuss the research\nadvancements of LAMs across various communication scenarios. Finally, we\nanalyze the challenges in the current research and provide insights into\npotential future research directions.",
      "url": "http://arxiv.org/abs/2505.03556v1",
      "published_time_eastern_timestamp": 1746540569.0
    },
    {
      "title": "A Hashgraph-Inspired Consensus Mechanism for Reliable Multi-Model\n  Reasoning",
      "summary": "Inconsistent outputs and hallucinations from large language models (LLMs) are\nmajor obstacles to reliable AI systems. When different proprietary reasoning\nmodels (RMs), such as those by OpenAI, Google, Anthropic, DeepSeek, and xAI,\nare given the same complex request, they often produce divergent results due to\nvariations in training and inference. This paper proposes a novel consensus\nmechanism, inspired by distributed ledger technology, to validate and converge\nthese outputs, treating each RM as a black-box peer. Building on the Hashgraph\nconsensus algorithm, our approach employs gossip-about-gossip communication and\nvirtual voting to achieve agreement among an ensemble of RMs. We present an\narchitectural design for a prototype system in which RMs iteratively exchange\nand update their answers, using information from each round to improve accuracy\nand confidence in subsequent rounds. This approach goes beyond simple majority\nvoting by incorporating the knowledge and cross-verification content of every\nmodel. We justify the feasibility of this Hashgraph-inspired consensus for AI\nensembles and outline its advantages over traditional ensembling techniques in\nreducing nonfactual outputs. Preliminary considerations for implementation,\nevaluation criteria for convergence and accuracy, and potential challenges are\ndiscussed. The proposed mechanism demonstrates a promising direction for\nmulti-agent AI systems to self-validate and deliver high-fidelity responses in\ncomplex tasks.",
      "url": "http://arxiv.org/abs/2505.03553v1",
      "published_time_eastern_timestamp": 1746540312.0
    },
    {
      "title": "STORY2GAME: Generating (Almost) Everything in an Interactive Fiction\n  Game",
      "summary": "We introduce STORY2GAME, a novel approach to using Large Language Models to\ngenerate text-based interactive fiction games that starts by generating a\nstory, populates the world, and builds the code for actions in a game engine\nthat enables the story to play out interactively. Whereas a given set of\nhard-coded actions can artificially constrain story generation, the ability to\ngenerate actions means the story generation process can be more open-ended but\nstill allow for experiences that are grounded in a game state. The key to\nsuccessful action generation is to use LLM-generated preconditions and effects\nof actions in the stories as guides for what aspects of the game state must be\ntracked and changed by the game engine when a player performs an action. We\nalso introduce a technique for dynamically generating new actions to\naccommodate the player's desire to perform actions that they think of that are\nnot part of the story. Dynamic action generation may require on-the-fly updates\nto the game engine's state representation and revision of previously generated\nactions. We evaluate the success rate of action code generation with respect to\nwhether a player can interactively play through the entire generated story.",
      "url": "http://arxiv.org/abs/2505.03547v1",
      "published_time_eastern_timestamp": 1746540041.0
    },
    {
      "title": "Faster MoE LLM Inference for Extremely Large Models",
      "summary": "Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually\nbecoming the mainstream approach for ultra-large-scale models. Existing\noptimization efforts for MoE models have focused primarily on coarse-grained\nMoE architectures. With the emergence of DeepSeek Models, fine-grained MoE\nmodels are gaining popularity, yet research on them remains limited. Therefore,\nwe want to discuss the efficiency dynamic under different service loads.\nAdditionally, fine-grained models allow deployers to reduce the number of\nrouted experts, both activated counts and total counts, raising the question of\nhow this reduction affects the trade-off between MoE efficiency and\nperformance. Our findings indicate that while deploying MoE models presents\ngreater challenges, it also offers significant optimization opportunities.\nReducing the number of activated experts can lead to substantial efficiency\nimprovements in certain scenarios, with only minor performance degradation.\nReducing the total number of experts provides limited efficiency gains but\nresults in severe performance degradation. Our method can increase throughput\nby at least 10\\% without any performance degradation. Overall, we conclude that\nMoE inference optimization remains an area with substantial potential for\nexploration and improvement.",
      "url": "http://arxiv.org/abs/2505.03531v1",
      "published_time_eastern_timestamp": 1746538877.0
    },
    {
      "title": "Ruled by the Representation Space: On the University's Embrace of Large\n  Language Models",
      "summary": "This paper explores the implications of universities' rapid adoption of large\nlanguage models (LLMs) for studying, teaching, and research by analyzing the\nlogics underpinning their representation space. It argues that by uncritically\nadopting LLMs, the University surrenders its autonomy to a field of heteronomy,\nthat of generative AI, whose norms are not democratically shaped. Unlike\nearlier forms of rule-based AI, which sought to exclude human judgment and\ninterpretation, generative AI's new normative rationality is explicitly based\non the automation of moral judgment, valuation, and interpretation. By\nintegrating LLMs into pedagogical and research contexts before establishing a\ncritical framework for their use, the University subjects itself to being\ngoverned by contingent, ever-evolving, and domain-non-specific norms that\nstructure the model's virtual representation space and thus everything it\ngenerates.",
      "url": "http://arxiv.org/abs/2505.03513v1",
      "published_time_eastern_timestamp": 1746537757.0
    },
    {
      "title": "BadLingual: A Novel Lingual-Backdoor Attack against Large Language\n  Models",
      "summary": "In this paper, we present a new form of backdoor attack against Large\nLanguage Models (LLMs): lingual-backdoor attacks. The key novelty of\nlingual-backdoor attacks is that the language itself serves as the trigger to\nhijack the infected LLMs to generate inflammatory speech. They enable the\nprecise targeting of a specific language-speaking group, exacerbating racial\ndiscrimination by malicious entities. We first implement a baseline\nlingual-backdoor attack, which is carried out by poisoning a set of training\ndata for specific downstream tasks through translation into the trigger\nlanguage. However, this baseline attack suffers from poor task generalization\nand is impractical in real-world settings. To address this challenge, we design\nBadLingual, a novel task-agnostic lingual-backdoor, capable of triggering any\ndownstream tasks within the chat LLMs, regardless of the specific questions of\nthese tasks. We design a new approach using PPL-constrained Greedy Coordinate\nGradient-based Search (PGCG) based adversarial training to expand the decision\nboundary of lingual-backdoor, thereby enhancing the generalization ability of\nlingual-backdoor across various tasks. We perform extensive experiments to\nvalidate the effectiveness of our proposed attacks. Specifically, the baseline\nattack achieves an ASR of over 90% on the specified tasks. However, its ASR\nreaches only 37.61% across six tasks in the task-agnostic scenario. In\ncontrast, BadLingual brings up to 37.35% improvement over the baseline. Our\nstudy sheds light on a new perspective of vulnerabilities in LLMs with\nmultilingual capabilities and is expected to promote future research on the\npotential defenses to enhance the LLMs' robustness",
      "url": "http://arxiv.org/abs/2505.03501v1",
      "published_time_eastern_timestamp": 1746536877.0
    },
    {
      "title": "Augmenting Human Cognition through Everyday AR",
      "summary": "As spatial computing and multimodal LLMs mature, AR is tending to become an\nintuitive \"thinking tool,\" embedding semantic and context-aware intelligence\ndirectly into everyday environments. This paper explores how always-on AR can\nseamlessly bridge digital cognition and physical affordances, enabling\nproactive, context-sensitive interactions that enhance human task performance\nand understanding.",
      "url": "http://arxiv.org/abs/2505.03492v1",
      "published_time_eastern_timestamp": 1746535718.0
    },
    {
      "title": "am-ELO: A Stable Framework for Arena-based LLM Evaluation",
      "summary": "Arena-based evaluation is a fundamental yet significant evaluation paradigm\nfor modern AI models, especially large language models (LLMs). Existing\nframework based on ELO rating system suffers from the inevitable instability\nproblem due to ranking inconsistency and the lack of attention to the varying\nabilities of annotators. In this paper, we introduce a novel stable arena\nframework to address these issues by enhancing the ELO Rating System.\nSpecifically, we replace the iterative update method with a Maximum Likelihood\nEstimation (MLE) approach, m-ELO, and provide theoretical proof of the\nconsistency and stability of the MLE approach for model ranking. Additionally,\nwe proposed the am-ELO, which modify the Elo Rating's probability function to\nincorporate annotator abilities, enabling the simultaneous estimation of model\nscores and annotator reliability. Experiments demonstrate that this method\nensures stability, proving that this framework offers a more robust, accurate,\nand stable evaluation method for LLMs.",
      "url": "http://arxiv.org/abs/2505.03475v1",
      "published_time_eastern_timestamp": 1746534530.0
    },
    {
      "title": "Evaluation of LLMs on Long-tail Entity Linking in Historical Documents",
      "summary": "Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP)\napplications, enabling the disambiguation of entity mentions by linking them to\ntheir corresponding entries in a reference knowledge base (KB). Thanks to their\ndeep contextual understanding capabilities, LLMs offer a new perspective to\ntackle EL, promising better results than traditional methods. Despite the\nimpressive generalization capabilities of LLMs, linking less popular, long-tail\nentities remains challenging as these entities are often underrepresented in\ntraining data and knowledge bases. Furthermore, the long-tail EL task is an\nunderstudied problem, and limited studies address it with LLMs. In the present\nwork, we assess the performance of two popular LLMs, GPT and LLama3, in a\nlong-tail entity linking scenario. Using MHERCL v0.1, a manually annotated\nbenchmark of sentences from domain-specific historical texts, we quantitatively\ncompare the performance of LLMs in identifying and linking entities to their\ncorresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity\nLinking and Relation Extraction framework. Our preliminary experiments reveal\nthat LLMs perform encouragingly well in long-tail EL, indicating that this\ntechnology can be a valuable adjunct in filling the gap between head and\nlong-tail EL.",
      "url": "http://arxiv.org/abs/2505.03473v1",
      "published_time_eastern_timestamp": 1746534315.0
    },
    {
      "title": "Uncertainty-Aware Large Language Models for Explainable Disease\n  Diagnosis",
      "summary": "Explainable disease diagnosis, which leverages patient information (e.g.,\nsigns and symptoms) and computational models to generate probable diagnoses and\nreasonings, offers clear clinical values. However, when clinical notes\nencompass insufficient evidence for a definite diagnosis, such as the absence\nof definitive symptoms, diagnostic uncertainty usually arises, increasing the\nrisk of misdiagnosis and adverse outcomes. Although explicitly identifying and\nexplaining diagnostic uncertainties is essential for trustworthy diagnostic\nsystems, it remains under-explored. To fill this gap, we introduce ConfiDx, an\nuncertainty-aware large language model (LLM) created by fine-tuning open-source\nLLMs with diagnostic criteria. We formalized the task and assembled richly\nannotated datasets that capture varying degrees of diagnostic ambiguity.\nEvaluating ConfiDx on real-world datasets demonstrated that it excelled in\nidentifying diagnostic uncertainties, achieving superior diagnostic\nperformance, and generating trustworthy explanations for diagnoses and\nuncertainties. To our knowledge, this is the first study to jointly address\ndiagnostic uncertainty recognition and explanation, substantially enhancing the\nreliability of automatic diagnostic systems.",
      "url": "http://arxiv.org/abs/2505.03467v1",
      "published_time_eastern_timestamp": 1746533568.0
    },
    {
      "title": "LogisticsVLN: Vision-Language Navigation For Low-Altitude Terminal\n  Delivery Based on Agentic UAVs",
      "summary": "The growing demand for intelligent logistics, particularly fine-grained\nterminal delivery, underscores the need for autonomous UAV (Unmanned Aerial\nVehicle)-based delivery systems. However, most existing last-mile delivery\nstudies rely on ground robots, while current UAV-based Vision-Language\nNavigation (VLN) tasks primarily focus on coarse-grained, long-range goals,\nmaking them unsuitable for precise terminal delivery. To bridge this gap, we\npropose LogisticsVLN, a scalable aerial delivery system built on multimodal\nlarge language models (MLLMs) for autonomous terminal delivery. LogisticsVLN\nintegrates lightweight Large Language Models (LLMs) and Visual-Language Models\n(VLMs) in a modular pipeline for request understanding, floor localization,\nobject detection, and action-decision making. To support research and\nevaluation in this new setting, we construct the Vision-Language Delivery (VLD)\ndataset within the CARLA simulator. Experimental results on the VLD dataset\nshowcase the feasibility of the LogisticsVLN system. In addition, we conduct\nsubtask-level evaluations of each module of our system, offering valuable\ninsights for improving the robustness and real-world deployment of foundation\nmodel-based vision-language delivery systems.",
      "url": "http://arxiv.org/abs/2505.03460v1",
      "published_time_eastern_timestamp": 1746532849.0
    },
    {
      "title": "The Steganographic Potentials of Language Models",
      "summary": "The potential for large language models (LLMs) to hide messages within plain\ntext (steganography) poses a challenge to detection and thwarting of unaligned\nAI agents, and undermines faithfulness of LLMs reasoning. We explore the\nsteganographic capabilities of LLMs fine-tuned via reinforcement learning (RL)\nto: (1) develop covert encoding schemes, (2) engage in steganography when\nprompted, and (3) utilize steganography in realistic scenarios where hidden\nreasoning is likely, but not prompted. In these scenarios, we detect the\nintention of LLMs to hide their reasoning as well as their steganography\nperformance. Our findings in the fine-tuning experiments as well as in\nbehavioral non fine-tuning evaluations reveal that while current models exhibit\nrudimentary steganographic abilities in terms of security and capacity,\nexplicit algorithmic guidance markedly enhances their capacity for information\nconcealment.",
      "url": "http://arxiv.org/abs/2505.03439v1",
      "published_time_eastern_timestamp": 1746530752.0
    },
    {
      "title": "Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in\n  LLM-Based Agents",
      "summary": "Large Language Models (LLMs) represent a landmark achievement in Artificial\nIntelligence (AI), demonstrating unprecedented proficiency in procedural tasks\nsuch as text generation, code completion, and conversational coherence. These\ncapabilities stem from their architecture, which mirrors human procedural\nmemory -- the brain's ability to automate repetitive, pattern-driven tasks\nthrough practice. However, as LLMs are increasingly deployed in real-world\napplications, it becomes impossible to ignore their limitations operating in\ncomplex, unpredictable environments. This paper argues that LLMs, while\ntransformative, are fundamentally constrained by their reliance on procedural\nmemory. To create agents capable of navigating ``wicked'' learning environments\n-- where rules shift, feedback is ambiguous, and novelty is the norm -- we must\naugment LLMs with semantic memory and associative learning systems. By adopting\na modular architecture that decouples these cognitive functions, we can bridge\nthe gap between narrow procedural expertise and the adaptive intelligence\nrequired for real-world problem-solving.",
      "url": "http://arxiv.org/abs/2505.03434v1",
      "published_time_eastern_timestamp": 1746530314.0
    },
    {
      "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks",
      "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare.",
      "url": "http://arxiv.org/abs/2505.03427v1",
      "published_time_eastern_timestamp": 1746529646.0
    },
    {
      "title": "Directed Greybox Fuzzing via Large Language Model",
      "summary": "Directed greybox fuzzing (DGF) focuses on efficiently reaching specific\nprogram locations or triggering particular behaviors, making it essential for\ntasks like vulnerability detection and crash reproduction. However, existing\nmethods often suffer from path explosion and randomness in input mutation,\nleading to inefficiencies in exploring and exploiting target paths. In this\npaper, we propose HGFuzzer, an automatic framework that leverages the large\nlanguage model (LLM) to address these challenges. HGFuzzer transforms path\nconstraint problems into targeted code generation tasks, systematically\ngenerating test harnesses and reachable inputs to reduce unnecessary\nexploration paths significantly. Additionally, we implement custom mutators\ndesigned specifically for target functions, minimizing randomness and improving\nthe precision of directed fuzzing. We evaluated HGFuzzer on 20 real-world\nvulnerabilities, successfully triggering 17, including 11 within the first\nminute, achieving a speedup of at least 24.8x compared to state-of-the-art\ndirected fuzzers. Furthermore, HGFuzzer discovered 9 previously unknown\nvulnerabilities, all of which were assigned CVE IDs, demonstrating the\neffectiveness of our approach in identifying real-world vulnerabilities.",
      "url": "http://arxiv.org/abs/2505.03425v1",
      "published_time_eastern_timestamp": 1746529447.0
    }
  ]
}
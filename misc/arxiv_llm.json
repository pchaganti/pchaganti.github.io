{
  "last_updated": "2025-07-31T05:19:01.496756-04:00",
  "papers": [
    {
      "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context\n  Learning",
      "summary": "In-context learning (ICL) is a critical emerging capability of large language\nmodels (LLMs), enabling few-shot learning during inference by including a few\ndemonstrations (demos) in the prompt. However, it has been found that ICL's\nperformance can be sensitive to the choices of demos and their order. This\npaper investigates an unexplored new positional bias of ICL for the first time:\nwe observe that the predictions and accuracy can drift drastically when the\npositions of demos, the system prompt, and the user message in LLM input are\nvaried. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We\ndesign a systematic evaluation pipeline to study this type of positional bias\nacross classification, question answering, summarization, and reasoning tasks.\nWe introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify\nnet gains and output volatility induced by changes in the demos' position.\nExtensive experiments on ten LLMs from four open-source model families (QWEN,\nLLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their\naccuracy and predictions: placing demos at the start of the prompt yields the\nmost stable and accurate outputs with gains of up to +6 points. In contrast,\nplacing demos at the end of the user message flips over 30\\% of predictions\nwithout improving correctness on QA tasks. Smaller models are most affected by\nthis sensitivity, though even large models remain marginally affected on\ncomplex tasks.",
      "url": "http://arxiv.org/abs/2507.22887v1",
      "published_time_eastern_timestamp": 1753898386.0
    },
    {
      "title": "RecGPT Technical Report",
      "summary": "Recommender systems are among the most impactful applications of artificial\nintelligence, serving as critical infrastructure connecting users, merchants,\nand platforms. However, most current industrial systems remain heavily reliant\non historical co-occurrence patterns and log-fitting objectives, i.e.,\noptimizing for past user interactions without explicitly modeling user intent.\nThis log-fitting approach often leads to overfitting to narrow historical\npreferences, failing to capture users' evolving and latent interests. As a\nresult, it reinforces filter bubbles and long-tail phenomena, ultimately\nharming user experience and threatening the sustainability of the whole\nrecommendation ecosystem.\n  To address these challenges, we rethink the overall design paradigm of\nrecommender systems and propose RecGPT, a next-generation framework that places\nuser intent at the center of the recommendation pipeline. By integrating large\nlanguage models (LLMs) into key stages of user interest mining, item retrieval,\nand explanation generation, RecGPT transforms log-fitting recommendation into\nan intent-centric process. To effectively align general-purpose LLMs to the\nabove domain-specific recommendation tasks at scale, RecGPT incorporates a\nmulti-stage training paradigm, which integrates reasoning-enhanced\npre-alignment and self-training evolution, guided by a Human-LLM cooperative\njudge system. Currently, RecGPT has been fully deployed on the Taobao App.\nOnline experiments demonstrate that RecGPT achieves consistent performance\ngains across stakeholders: users benefit from increased content diversity and\nsatisfaction, merchants and the platform gain greater exposure and conversions.\nThese comprehensive improvement results across all stakeholders validates that\nLLM-driven, intent-centric design can foster a more sustainable and mutually\nbeneficial recommendation ecosystem.",
      "url": "http://arxiv.org/abs/2507.22879v1",
      "published_time_eastern_timestamp": 1753898106.0
    },
    {
      "title": "Automatically discovering heuristics in a complex SAT solver with large\n  language models",
      "summary": "Satisfiability problem (SAT) is a cornerstone of computational complexity\nwith broad industrial applications, and it remains challenging to optimize\nmodern SAT solvers in real-world settings due to their intricate architectures.\nWhile automatic configuration frameworks have been developed, they rely on\nmanually constrained search spaces and yield limited performance gains. This\nwork introduces a novel paradigm which effectively optimizes complex SAT\nsolvers via Large Language Models (LLMs), and a tool called AutoModSAT is\ndeveloped. Three fundamental challenges are addressed in order to achieve\nsuperior performance: (1) LLM-friendly solver: Systematic guidelines are\nproposed for developing a modularized solver to meet LLMs' compatibility,\nemphasizing code simplification, information share and bug reduction; (2)\nAutomatic prompt optimization: An unsupervised automatic prompt optimization\nmethod is introduced to advance the diversity of LLMs' output; (3) Efficient\nsearch strategy: We design a presearch strategy and an EA evolutionary\nalgorithm for the final efficient and effective discovery of heuristics.\nExtensive experiments across a wide range of datasets demonstrate that\nAutoModSAT achieves 50% performance improvement over the baseline solver and\nachieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,\nAutoModSAT attains a 20% speedup on average compared to parameter-tuned\nalternatives of the SOTA solvers, showcasing the enhanced capability in\nhandling complex problem instances. This work bridges the gap between AI-driven\nheuristics discovery and mission-critical system optimization, and provides\nboth methodological advancements and empirically validated results for\nnext-generation complex solver development.",
      "url": "http://arxiv.org/abs/2507.22876v1",
      "published_time_eastern_timestamp": 1753897945.0
    },
    {
      "title": "Repair-R1: Better Test Before Repair",
      "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
      "url": "http://arxiv.org/abs/2507.22853v1",
      "published_time_eastern_timestamp": 1753896245.0
    },
    {
      "title": "The Incomplete Bridge: How AI Research (Mis)Engages with Psychology",
      "summary": "Social sciences have accumulated a rich body of theories and methodologies\nfor investigating the human mind and behaviors, while offering valuable\ninsights into the design and understanding of Artificial Intelligence (AI)\nsystems. Focusing on psychology as a prominent case, this study explores the\ninterdisciplinary synergy between AI and the field by analyzing 1,006\nLLM-related papers published in premier AI venues between 2023 and 2025, along\nwith the 2,544 psychology publications they cite. Through our analysis, we\nidentify key patterns of interdisciplinary integration, locate the psychology\ndomains most frequently referenced, and highlight areas that remain\nunderexplored. We further examine how psychology theories/frameworks are\noperationalized and interpreted, identify common types of misapplication, and\noffer guidance for more effective incorporation. Our work provides a\ncomprehensive map of interdisciplinary engagement between AI and psychology,\nthereby facilitating deeper collaboration and advancing AI systems.",
      "url": "http://arxiv.org/abs/2507.22847v1",
      "published_time_eastern_timestamp": 1753895039.0
    },
    {
      "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
      "summary": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.",
      "url": "http://arxiv.org/abs/2507.22827v1",
      "published_time_eastern_timestamp": 1753893681.0
    },
    {
      "title": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph",
      "summary": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM.",
      "url": "http://arxiv.org/abs/2507.22811v1",
      "published_time_eastern_timestamp": 1753892987.0
    },
    {
      "title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and\n  Hierarchical Group Attention",
      "summary": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA.",
      "url": "http://arxiv.org/abs/2507.22805v1",
      "published_time_eastern_timestamp": 1753892122.0
    },
    {
      "title": "The Multi-Agent Fault Localization System Based on Monte Carlo Tree\n  Search Approach",
      "summary": "In real-world scenarios, due to the highly decoupled and flexible nature of\nmicroservices, it poses greater challenges to system reliability. The more\nfrequent occurrence of incidents has created a demand for Root Cause\nAnalysis(RCA) methods that enable rapid identification and recovery of\nincidents. Large language model (LLM) provides a new path for quickly locating\nand recovering from incidents by leveraging their powerful generalization\nability combined with expert experience. Current LLM for RCA frameworks are\nbased on ideas like ReAct and Chain-of-Thought, but the hallucination of LLM\nand the propagation nature of anomalies often lead to incorrect localization\nresults. Moreover, the massive amount of anomalous information generated in\nlarge, complex systems presents a huge challenge for the context window length\nof LLMs. To address these challenges, we propose KnowledgeMind, an innovative\nLLM multi-agent system based on Monte Carlo Tree Search and a knowledge base\nreward mechanism for standardized service-by-service reasoning. Compared to\nState-Of-The-Art(SOTA) LLM for RCA methods, our service-by-service exploration\napproach significantly reduces the burden on the maximum context window length,\nrequiring only one-tenth of its size. Additionally, by incorporating a\nrule-based real-time reward mechanism, our method effectively mitigates\nhallucinations during the inference process. Compared to the SOTA LLM for RCA\nframework, our method achieves a 49.29% to 128.35% improvement in root cause\nlocalization accuracy.",
      "url": "http://arxiv.org/abs/2507.22800v1",
      "published_time_eastern_timestamp": 1753891401.0
    },
    {
      "title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer",
      "summary": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels.",
      "url": "http://arxiv.org/abs/2507.22789v1",
      "published_time_eastern_timestamp": 1753890908.0
    },
    {
      "title": "Empirical Evaluation of Concept Drift in ML-Based Android Malware\n  Detection",
      "summary": "Despite outstanding results, machine learning-based Android malware detection\nmodels struggle with concept drift, where rapidly evolving malware\ncharacteristics degrade model effectiveness. This study examines the impact of\nconcept drift on Android malware detection, evaluating two datasets and nine\nmachine learning and deep learning algorithms, as well as Large Language Models\n(LLMs). Various feature types--static, dynamic, hybrid, semantic, and\nimage-based--were considered. The results showed that concept drift is\nwidespread and significantly affects model performance. Factors influencing the\ndrift include feature types, data environments, and detection methods.\nBalancing algorithms helped with class imbalance but did not fully address\nconcept drift, which primarily stems from the dynamic nature of the malware\nlandscape. No strong link was found between the type of algorithm used and\nconcept drift, the impact was relatively minor compared to other variables\nsince hyperparameters were not fine-tuned, and the default algorithm\nconfigurations were used. While LLMs using few-shot learning demonstrated\npromising detection performance, they did not fully mitigate concept drift,\nhighlighting the need for further investigation.",
      "url": "http://arxiv.org/abs/2507.22772v1",
      "published_time_eastern_timestamp": 1753889751.0
    },
    {
      "title": "MASCA: LLM based-Multi Agents System for Credit Assessment",
      "summary": "Recent advancements in financial problem-solving have leveraged LLMs and\nagent-based systems, with a primary focus on trading and financial modeling.\nHowever, credit assessment remains an underexplored challenge, traditionally\ndependent on rule-based methods and statistical models. In this paper, we\nintroduce MASCA, an LLM-driven multi-agent system designed to enhance credit\nevaluation by mirroring real-world decision-making processes. The framework\nemploys a layered architecture where specialized LLM-based agents\ncollaboratively tackle sub-tasks. Additionally, we integrate contrastive\nlearning for risk and reward assessment to optimize decision-making. We further\npresent a signaling game theory perspective on hierarchical multi-agent\nsystems, offering theoretical insights into their structure and interactions.\nOur paper also includes a detailed bias analysis in credit assessment,\naddressing fairness concerns. Experimental results demonstrate that MASCA\noutperforms baseline approaches, highlighting the effectiveness of hierarchical\nLLM-based multi-agent systems in financial applications, particularly in credit\nscoring.",
      "url": "http://arxiv.org/abs/2507.22758v1",
      "published_time_eastern_timestamp": 1753888778.0
    },
    {
      "title": "Opportunities and Challenges of LLMs in Education: An NLP Perspective",
      "summary": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future.",
      "url": "http://arxiv.org/abs/2507.22753v1",
      "published_time_eastern_timestamp": 1753888332.0
    },
    {
      "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset",
      "summary": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering.",
      "url": "http://arxiv.org/abs/2507.22752v1",
      "published_time_eastern_timestamp": 1753888255.0
    },
    {
      "title": "How Exposed Are UK Jobs to Generative AI? Developing and Applying a\n  Novel Task-Based Index",
      "summary": "We introduce the Generative AI Susceptibility Index (GAISI), a task-based\nmeasure of UK job exposure to large language models (LLMs), such as ChatGPT.\nGAISI is derived from probabilistic task ratings by LLMs and linked to\nworker-reported task data from the Skills and Employment Surveys. It reflects\nthe share of job activities where an LLM or LLM-powered system can reduce task\ncompletion time by at least 25 per cent beyond existing productivity tools. The\nindex demonstrates high reliability, strong alignment with AI capabilities, and\nsuperior predictive power compared to existing exposure measures. By 2023-24,\nnearly all UK jobs exhibited some exposure, yet only a minority were heavily\naffected. Aggregate exposure has risen since 2017, primarily due to\noccupational shifts rather than changes in task profiles. The price premium for\nAI-exposed tasks declined relative to 2017, measuring approximately 11 per cent\nlower in 2023-24. Job postings in high-exposure roles also fell by 6.5 per cent\nfollowing the release of ChatGPT. GAISI offers a robust framework for assessing\ngenerative AI's impact on work, providing early evidence that displacement\neffects may already outweigh productivity gains.",
      "url": "http://arxiv.org/abs/2507.22748v1",
      "published_time_eastern_timestamp": 1753887905.0
    },
    {
      "title": "Resource-Efficient Adaptation of Large Language Models for Text\n  Embeddings via Prompt Engineering and Contrastive Fine-tuning",
      "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs.",
      "url": "http://arxiv.org/abs/2507.22729v1",
      "published_time_eastern_timestamp": 1753886970.0
    },
    {
      "title": "Investigating Hallucination in Conversations for Low Resource Languages",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.",
      "url": "http://arxiv.org/abs/2507.22720v1",
      "published_time_eastern_timestamp": 1753886391.0
    },
    {
      "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs",
      "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.",
      "url": "http://arxiv.org/abs/2507.22716v1",
      "published_time_eastern_timestamp": 1753885784.0
    },
    {
      "title": "OFCnetLLM: Large Language Model for Network Monitoring and Alertness",
      "summary": "The rapid evolution of network infrastructure is bringing new challenges and\nopportunities for efficient network management, optimization, and security.\nWith very large monitoring databases becoming expensive to explore, the use of\nAI and Generative AI can help reduce costs of managing these datasets. This\npaper explores the use of Large Language Models (LLMs) to revolutionize network\nmonitoring management by addressing the limitations of query finding and\npattern analysis. We leverage LLMs to enhance anomaly detection, automate\nroot-cause analysis, and automate incident analysis to build a well-monitored\nnetwork management team using AI. Through a real-world example of developing\nour own OFCNetLLM, based on the open-source LLM model, we demonstrate practical\napplications of OFCnetLLM in the OFC conference network. Our model is developed\nas a multi-agent approach and is still evolving, and we present early results\nhere.",
      "url": "http://arxiv.org/abs/2507.22711v1",
      "published_time_eastern_timestamp": 1753885362.0
    },
    {
      "title": "A Systematic Literature Review on Detecting Software Vulnerabilities\n  with Large Language Models",
      "summary": "The increasing adoption of Large Language Models (LLMs) in software\nengineering has sparked interest in their use for software vulnerability\ndetection. However, the rapid development of this field has resulted in a\nfragmented research landscape, with diverse studies that are difficult to\ncompare due to differences in, e.g., system designs and dataset usage. This\nfragmentation makes it difficult to obtain a clear overview of the\nstate-of-the-art or compare and categorize studies meaningfully. In this work,\nwe present a comprehensive systematic literature review (SLR) of LLM-based\nsoftware vulnerability detection. We analyze 227 studies published between\nJanuary 2020 and June 2025, categorizing them by task formulation, input\nrepresentation, system architecture, and adaptation techniques. Further, we\nanalyze the datasets used, including their characteristics, vulnerability\ncoverage, and diversity. We present a fine-grained taxonomy of vulnerability\ndetection approaches, identify key limitations, and outline actionable future\nresearch opportunities. By providing a structured overview of the field, this\nreview improves transparency and serves as a practical guide for researchers\nand practitioners aiming to conduct more comparable and reproducible research.\nWe publicly release all artifacts and maintain a living repository of LLM-based\nsoftware vulnerability detection studies.",
      "url": "http://arxiv.org/abs/2507.22659v1",
      "published_time_eastern_timestamp": 1753881436.0
    }
  ]
}
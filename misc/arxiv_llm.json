{
  "last_updated": "2025-07-30T08:26:40.419516-04:00",
  "papers": [
    {
      "title": "MetaCLIP 2: A Worldwide Scaling Recipe",
      "summary": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model,\nsupporting from zero-shot classification, retrieval to encoders for multimodal\nlarge language models (MLLMs). Although CLIP is successfully trained on\nbillion-scale image-text pairs from the English world, scaling CLIP's training\nfurther to learning from the worldwide web data is still challenging: (1) no\ncuration method is available to handle data points from non-English world; (2)\nthe English performance from existing multilingual CLIP is worse than its\nEnglish-only counterpart, i.e., \"curse of multilinguality\" that is common in\nLLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch\non worldwide web-scale image-text pairs. To generalize our findings, we conduct\nrigorous ablations with minimal changes that are necessary to address the above\nchallenges and present a recipe enabling mutual benefits from English and\nnon-English world data. In zero-shot ImageNet classification, MetaCLIP 2\nViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%,\nand surprisingly sets new state-of-the-art without system-level confounding\nfactors (e.g., translation, bespoke architecture changes) on multilingual\nbenchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with\n64.3% on image-to-text retrieval.",
      "url": "http://arxiv.org/abs/2507.22062v1",
      "published_time_eastern_timestamp": 1753811998.0
    },
    {
      "title": "DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router",
      "summary": "Large Language Models (LLMs) excel at many reasoning tasks but struggle with\nknowledge-intensive queries due to their inability to dynamically access\nup-to-date or domain-specific information. Retrieval-Augmented Generation (RAG)\nhas emerged as a promising solution, enabling LLMs to ground their responses in\nexternal sources. However, existing RAG methods lack fine-grained control over\nboth the query and source sides, often resulting in noisy retrieval and shallow\nreasoning. In this work, we introduce DeepSieve, an agentic RAG framework that\nincorporates information sieving via LLM-as-a-knowledge-router. DeepSieve\ndecomposes complex queries into structured sub-questions and recursively routes\neach to the most suitable knowledge source, filtering irrelevant information\nthrough a multi-stage distillation process. Our design emphasizes modularity,\ntransparency, and adaptability, leveraging recent advances in agentic system\ndesign. Experiments on multi-hop QA tasks across heterogeneous sources\ndemonstrate improved reasoning depth, retrieval precision, and interpretability\nover conventional RAG approaches.",
      "url": "http://arxiv.org/abs/2507.22050v1",
      "published_time_eastern_timestamp": 1753811723.0
    },
    {
      "title": "Validating Generative Agent-Based Models of Social Norm Enforcement:\n  From Replication to Novel Predictions",
      "summary": "As large language models (LLMs) advance, there is growing interest in using\nthem to simulate human social behavior through generative agent-based modeling\n(GABM). However, validating these models remains a key challenge. We present a\nsystematic two-stage validation approach using social dilemma paradigms from\npsychological literature, first identifying the cognitive components necessary\nfor LLM agents to reproduce known human behaviors in mixed-motive settings from\ntwo landmark papers, then using the validated architecture to simulate novel\nconditions. Our model comparison of different cognitive architectures shows\nthat both persona-based individual differences and theory of mind capabilities\nare essential for replicating third-party punishment (TPP) as a costly signal\nof trustworthiness. For the second study on public goods games, this\narchitecture is able to replicate an increase in cooperation from the spread of\nreputational information through gossip. However, an additional strategic\ncomponent is necessary to replicate the additional boost in cooperation rates\nin the condition that allows both ostracism and gossip. We then test novel\npredictions for each paper with our validated generative agents. We find that\nTPP rates significantly drop in settings where punishment is anonymous, yet a\nsubstantial amount of TPP persists, suggesting that both reputational and\nintrinsic moral motivations play a role in this behavior. For the second paper,\nwe introduce a novel intervention and see that open discussion periods before\nrounds of the public goods game further increase contributions, allowing groups\nto develop social norms for cooperation. This work provides a framework for\nvalidating generative agent models while demonstrating their potential to\ngenerate novel and testable insights into human social behavior.",
      "url": "http://arxiv.org/abs/2507.22049v1",
      "published_time_eastern_timestamp": 1753811638.0
    },
    {
      "title": "Composable Effect Handling for Programming LLM-integrated Scripts",
      "summary": "Implementing LLM-integrated scripts introduces challenges in modularity and\nperformance, as scripts are often coupled to specific LLM implementations and\nfail to exploit parallelization opportunities. This paper proposes using\ncomposable effect handling to separate workflow logic from effectful\noperations, such as LLM calls, I/O, and concurrency, enabling modularity\nwithout sacrificing the opportunity for performance optimization. By treating\nthese operations as abstract interfaces and discharging them via effect\nhandlers, this paper shows that scripts can achieve significant speedups (e.g.,\n10$\\times$ in a Tree-of-Thoughts case study) without compromising modularity.\nThis paper aims to promote composable effect handling as a programming style\nfor LLM scripting.",
      "url": "http://arxiv.org/abs/2507.22048v1",
      "published_time_eastern_timestamp": 1753811587.0
    },
    {
      "title": "UserBench: An Interactive Gym Environment for User-Centric Agents",
      "summary": "Large Language Models (LLMs)-based agents have made impressive progress in\nreasoning and tool use, enabling them to solve complex tasks. However, their\nability to proactively collaborate with users, especially when goals are vague,\nevolving, or indirectly expressed, remains underexplored. To address this gap,\nwe introduce UserBench, a user-centric benchmark designed to evaluate agents in\nmulti-turn, preference-driven interactions. UserBench features simulated users\nwho start with underspecified goals and reveal preferences incrementally,\nrequiring agents to proactively clarify intent and make grounded decisions with\ntools. Our evaluation of leading open- and closed-source LLMs reveals a\nsignificant disconnect between task completion and user alignment. For\ninstance, models provide answers that fully align with all user intents only\n20% of the time on average, and even the most advanced models uncover fewer\nthan 30% of all user preferences through active interaction. These results\nhighlight the challenges of building agents that are not just capable task\nexecutors, but true collaborative partners. UserBench offers an interactive\nenvironment to measure and advance this critical capability.",
      "url": "http://arxiv.org/abs/2507.22034v1",
      "published_time_eastern_timestamp": 1753810452.0
    },
    {
      "title": "Exploring the Stratified Space Structure of an RL Game with the Volume\n  Growth Transform",
      "summary": "In this work, we explore the structure of the embedding space of a\ntransformer model trained for playing a particular reinforcement learning (RL)\ngame. Specifically, we investigate how a transformer-based Proximal Policy\nOptimization (PPO) model embeds visual inputs in a simple environment where an\nagent must collect \"coins\" while avoiding dynamic obstacles consisting of\n\"spotlights.\" By adapting Robinson et al.'s study of the volume growth\ntransform for LLMs to the RL setting, we find that the token embedding space\nfor our visual coin collecting game is also not a manifold, and is better\nmodeled as a stratified space, where local dimension can vary from point to\npoint. We further strengthen Robinson's method by proving that fairly general\nvolume growth curves can be realized by stratified spaces. Finally, we carry\nout an analysis that suggests that as an RL agent acts, its latent\nrepresentation alternates between periods of low local dimension, while\nfollowing a fixed sub-strategy, and bursts of high local dimension, where the\nagent achieves a sub-goal (e.g., collecting an object) or where the\nenvironmental complexity increases (e.g., more obstacles appear). Consequently,\nour work suggests that the distribution of dimensions in a stratified latent\nspace may provide a new geometric indicator of complexity for RL games.",
      "url": "http://arxiv.org/abs/2507.22010v1",
      "published_time_eastern_timestamp": 1753808433.0
    },
    {
      "title": "ChemDFM-R: An Chemical Reasoner LLM Enhanced with Atomized Chemical\n  Knowledge",
      "summary": "While large language models (LLMs) have achieved impressive progress, their\napplication in scientific domains such as chemistry remains hindered by shallow\ndomain understanding and limited reasoning capabilities. In this work, we focus\non the specific field of chemistry and develop a Chemical Reasoner LLM,\nChemDFM-R. We first construct a comprehensive dataset of atomized knowledge\npoints to enhance the model's understanding of the fundamental principles and\nlogical structure of chemistry. Then, we propose a mix-sourced distillation\nstrategy that integrates expert-curated knowledge with general-domain reasoning\nskills, followed by domain-specific reinforcement learning to enhance chemical\nreasoning. Experiments on diverse chemical benchmarks demonstrate that\nChemDFM-R achieves state-of-the-art performance while providing interpretable,\nrationale-driven outputs. Further case studies illustrate how explicit\nreasoning chains significantly improve the reliability, transparency, and\npractical utility of the model in real-world human-AI collaboration scenarios.",
      "url": "http://arxiv.org/abs/2507.21990v1",
      "published_time_eastern_timestamp": 1753807249.0
    },
    {
      "title": "Improving Generative Ad Text on Facebook using Reinforcement Learning",
      "summary": "Generative artificial intelligence (AI), in particular large language models\n(LLMs), is poised to drive transformative economic change. LLMs are pre-trained\non vast text data to learn general language patterns, but a subsequent\npost-training phase is critical to align them for specific real-world tasks.\nReinforcement learning (RL) is the leading post-training technique, yet its\neconomic impact remains largely underexplored and unquantified. We examine this\nquestion through the lens of the first deployment of an RL-trained LLM for\ngenerative advertising on Facebook. Integrated into Meta's Text Generation\nfeature, our model, \"AdLlama,\" powers an AI tool that helps advertisers create\nnew variations of human-written ad text. To train this model, we introduce\nreinforcement learning with performance feedback (RLPF), a post-training method\nthat uses historical ad performance data as a reward signal. In a large-scale\n10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad\nvariations, we find that AdLlama improves click-through rates by 6.7%\n(p=0.0296) compared to a supervised imitation model trained on curated ads.\nThis represents a substantial improvement in advertiser return on investment on\nFacebook. We also find that advertisers who used AdLlama generated more ad\nvariations, indicating higher satisfaction with the model's outputs. To our\nknowledge, this is the largest study to date on the use of generative AI in an\necologically valid setting, offering an important data point quantifying the\ntangible impact of RL post-training. Furthermore, the results show that RLPF is\na promising and generalizable approach for metric-driven post-training that\nbridges the gap between highly capable language models and tangible outcomes.",
      "url": "http://arxiv.org/abs/2507.21983v1",
      "published_time_eastern_timestamp": 1753806842.0
    },
    {
      "title": "Predicting Microbial Ontology and Pathogen Risk from Environmental\n  Metadata with Large Language Models",
      "summary": "Traditional machine learning models struggle to generalize in microbiome\nstudies where only metadata is available, especially in small-sample settings\nor across studies with heterogeneous label formats. In this work, we explore\nthe use of large language models (LLMs) to classify microbial samples into\nontology categories such as EMPO 3 and related biological labels, as well as to\npredict pathogen contamination risk, specifically the presence of E. Coli,\nusing environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude\n3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing\ntheir performance against traditional models like Random Forests across\nmultiple real-world datasets. Our results show that LLMs not only outperform\nbaselines in ontology classification, but also demonstrate strong predictive\nability for contamination risk, generalizing across sites and metadata\ndistributions. These findings suggest that LLMs can effectively reason over\nsparse, heterogeneous biological metadata and offer a promising metadata-only\napproach for environmental microbiology and biosurveillance applications.",
      "url": "http://arxiv.org/abs/2507.21980v1",
      "published_time_eastern_timestamp": 1753806765.0
    },
    {
      "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks",
      "summary": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.",
      "url": "http://arxiv.org/abs/2507.21974v1",
      "published_time_eastern_timestamp": 1753806102.0
    },
    {
      "title": "Towards Cognitive Synergy in LLM-Based Multi-Agent Systems: Integrating\n  Theory of Mind and Critical Evaluation",
      "summary": "Recently, the field of Multi-Agent Systems (MAS) has gained popularity as\nresearchers are trying to develop artificial intelligence capable of efficient\ncollective reasoning. Agents based on Large Language Models (LLMs) perform well\nin isolated tasks, yet struggle with higher-order cognition required for\nadaptive collaboration. Human teams achieve synergy not only through knowledge\nsharing, but also through recursive reasoning, structured critique, and the\nability to infer others' mental states. Current artificial systems lack these\nessential mechanisms, limiting their ability to engage in sophisticated\ncollective reasoning. This work explores cognitive processes that enable\neffective collaboration, focusing on adaptive theory of mind (ToM) and\nsystematic critical evaluation. We investigate three key questions. First, how\ndoes the ability to model others' perspectives enhance coordination and reduce\nredundant reasoning? Second, to what extent does structured critique improve\nreasoning quality by identifying logical gaps and mitigating biases? Third, the\ninterplay of these mechanisms can lead to emergent cognitive synergy, where the\ncollective intelligence of the system exceeds the sum of its parts. Through an\nempirical case study on complex decision making, we show that the integration\nof these cognitive mechanisms leads to more coherent, adaptive, and rigorous\nagent interactions. This article contributes to the field of cognitive science\nand AI research by presenting a structured framework that emulates human-like\ncollaborative reasoning MAS. It highlights the significance of dynamic ToM and\ncritical evaluation in advancing multi-agent systems' ability to tackle\ncomplex, real-world challenges.",
      "url": "http://arxiv.org/abs/2507.21969v1",
      "published_time_eastern_timestamp": 1753805793.0
    },
    {
      "title": "Thou Shalt Not Prompt: Zero-Shot Human Activity Recognition in Smart\n  Homes via Language Modeling of Sensor Data & Activities",
      "summary": "Developing zero-shot human activity recognition (HAR) methods is a critical\ndirection in smart home research -- considering its impact on making HAR\nsystems work across smart homes having diverse sensing modalities, layouts, and\nactivities of interest. The state-of-the-art solutions along this direction are\nbased on generating natural language descriptions of the sensor data and\nfeeding it via a carefully crafted prompt to the LLM to perform classification.\nDespite their performance guarantees, such ``prompt-the-LLM'' approaches carry\nseveral risks, including privacy invasion, reliance on an external service, and\ninconsistent predictions due to version changes, making a case for alternative\nzero-shot HAR methods that do not require prompting the LLMs. In this paper, we\npropose one such solution that models sensor data and activities using natural\nlanguage, leveraging its embeddings to perform zero-shot classification and\nthereby bypassing the need to prompt the LLMs for activity predictions. The\nimpact of our work lies in presenting a detailed case study on six datasets,\nhighlighting how language modeling can bolster HAR systems in zero-shot\nrecognition.",
      "url": "http://arxiv.org/abs/2507.21964v1",
      "published_time_eastern_timestamp": 1753805590.0
    },
    {
      "title": "MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile\n  Task Automation",
      "summary": "The recent advancement of autonomous agents powered by Large Language Models\n(LLMs) has demonstrated significant potential for automating tasks on mobile\ndevices through graphical user interfaces (GUIs). Despite initial progress,\nthese agents still face challenges when handling complex real-world tasks.\nThese challenges arise from a lack of knowledge about real-life mobile\napplications in LLM-based agents, which may lead to ineffective task planning\nand even cause hallucinations. To address these challenges, we propose a novel\nLLM-based agent framework called MapAgent that leverages memory constructed\nfrom historical trajectories to augment current task planning. Specifically, we\nfirst propose a trajectory-based memory mechanism that transforms task\nexecution trajectories into a reusable and structured page-memory database.\nEach page within a trajectory is extracted as a compact yet comprehensive\nsnapshot, capturing both its UI layout and functional context. Secondly, we\nintroduce a coarse-to-fine task planning approach that retrieves relevant pages\nfrom the memory database based on similarity and injects them into the LLM\nplanner to compensate for potential deficiencies in understanding real-world\napp scenarios, thereby achieving more informed and context-aware task planning.\nFinally, planned tasks are transformed into executable actions through a task\nexecutor supported by a dual-LLM architecture, ensuring effective tracking of\ntask progress. Experimental results in real-world scenarios demonstrate that\nMapAgent achieves superior performance to existing methods. The code will be\nopen-sourced to support further research.",
      "url": "http://arxiv.org/abs/2507.21953v1",
      "published_time_eastern_timestamp": 1753805132.0
    },
    {
      "title": "Culinary Crossroads: A RAG Framework for Enhancing Diversity in\n  Cross-Cultural Recipe Adaptation",
      "summary": "In cross-cultural recipe adaptation, the goal is not only to ensure cultural\nappropriateness and retain the original dish's essence, but also to provide\ndiverse options for various dietary needs and preferences. Retrieval Augmented\nGeneration (RAG) is a promising approach, combining the retrieval of real\nrecipes from the target cuisine for cultural adaptability with large language\nmodels (LLMs) for relevance. However, it remains unclear whether RAG can\ngenerate diverse adaptation results. Our analysis shows that RAG tends to\noverly rely on a limited portion of the context across generations, failing to\nproduce diverse outputs even when provided with varied contextual inputs. This\nreveals a key limitation of RAG in creative tasks with multiple valid answers:\nit fails to leverage contextual diversity for generating varied responses. To\naddress this issue, we propose CARRIAGE, a plug-and-play RAG framework for\ncross-cultural recipe adaptation that enhances diversity in both retrieval and\ncontext organization. To our knowledge, this is the first RAG framework that\nexplicitly aims to generate highly diverse outputs to accommodate multiple user\npreferences. Our experiments show that CARRIAGE achieves Pareto efficiency in\nterms of diversity and quality of recipe adaptation compared to closed-book\nLLMs.",
      "url": "http://arxiv.org/abs/2507.21934v1",
      "published_time_eastern_timestamp": 1753804092.0
    },
    {
      "title": "Post-Training Large Language Models via Reinforcement Learning from\n  Self-Feedback",
      "summary": "Large Language Models (LLMs) often produce plausible but poorly-calibrated\nanswers, limiting their reliability on reasoning-intensive tasks. We present\nReinforcement Learning from Self-Feedback (RLSF), a post-training stage that\nuses the model's own confidence as an intrinsic reward, mimicking how humans\nlearn in the absence of external feedback. After a frozen LLM generates several\nchain-of-thought solutions, we define and compute the confidence of each final\nanswer span and rank the traces accordingly. These synthetic preferences are\nthen used to fine-tune the policy with standard preference optimization,\nsimilar to RLHF yet requiring no human labels, gold answers, or externally\ncurated rewards.\n  RLSF simultaneously (i) refines the model's probability estimates --\nrestoring well-behaved calibration -- and (ii) strengthens step-by-step\nreasoning, yielding improved performance on arithmetic reasoning and\nmultiple-choice question answering.\n  By turning a model's own uncertainty into useful self-feedback, RLSF affirms\nreinforcement learning on intrinsic model behaviour as a principled and\ndata-efficient component of the LLM post-training pipeline and warrents further\nresearch in intrinsic rewards for LLM post-training.",
      "url": "http://arxiv.org/abs/2507.21931v1",
      "published_time_eastern_timestamp": 1753803986.0
    },
    {
      "title": "Libra: Large Chinese-based Safeguard for AI Content",
      "summary": "Large language models (LLMs) excel in text understanding and generation but\nraise significant safety and ethical concerns in high-stakes applications. To\nmitigate these risks, we present Libra-Guard, a cutting-edge safeguard system\ndesigned to enhance the safety of Chinese-based LLMs. Leveraging a two-stage\ncurriculum training pipeline, Libra-Guard enhances data efficiency by employing\nguard pretraining on synthetic samples, followed by fine-tuning on\nhigh-quality, real-world data, thereby significantly reducing reliance on\nmanual annotations. To enable rigorous safety evaluations, we also introduce\nLibra-Test, the first benchmark specifically designed to evaluate the\neffectiveness of safeguard systems for Chinese content. It covers seven\ncritical harm scenarios and includes over 5,700 samples annotated by domain\nexperts. Experiments show that Libra-Guard achieves 86.79% accuracy,\noutperforming Qwen2.5-14B-Instruct (74.33%) and ShieldLM-Qwen-14B-Chat\n(65.69%), and nearing closed-source models like Claude-3.5-Sonnet and GPT-4o.\nThese contributions establish a robust framework for advancing the safety\ngovernance of Chinese LLMs and represent a tentative step toward developing\nsafer, more reliable Chinese AI systems.",
      "url": "http://arxiv.org/abs/2507.21929v1",
      "published_time_eastern_timestamp": 1753803950.0
    },
    {
      "title": "MMAT-1M: A Large Reasoning Dataset for Multimodal Agent Tuning",
      "summary": "Large Language Models (LLMs), enhanced through agent tuning, have\ndemonstrated remarkable capabilities in Chain-of-Thought (CoT) and tool\nutilization, significantly surpassing the performance of standalone models.\nHowever, the multimodal domain still lacks a large-scale, high-quality agent\ntuning dataset to unlock the full potential of multimodal large language\nmodels. To bridge this gap, we introduce MMAT-1M, the first million-scale\nmultimodal agent tuning dataset designed to support CoT, reflection, and\ndynamic tool usage. Our dataset is constructed through a novel four-stage data\nengine: 1) We first curate publicly available multimodal datasets containing\nquestion-answer pairs; 2) Then, leveraging GPT-4o, we generate rationales for\nthe original question-answer pairs and dynamically integrate API calls and\nRetrieval Augmented Generation (RAG) information through a multi-turn paradigm;\n3) Furthermore, we refine the rationales through reflection to ensure logical\nconsistency and accuracy, creating a multi-turn dialogue dataset with both\nRationale and Reflection (RR); 4) Finally, to enhance efficiency, we optionally\ncompress multi-turn dialogues into a One-turn Rationale and Reflection (ORR)\nformat. By fine-tuning open-source multimodal models on the MMAT-1M, we observe\nsignificant performance gains. For instance, the InternVL2.5-8B-RR model\nachieves an average improvement of 2.7% across eight public benchmarks and 8.8%\non the RAG benchmark Dyn-VQA, demonstrating the dataset's effectiveness in\nenhancing multimodal reasoning and tool-based capabilities. The dataset is\npublicly available at https://github.com/VIS-MPU-Agent/MMAT-1M.",
      "url": "http://arxiv.org/abs/2507.21924v1",
      "published_time_eastern_timestamp": 1753803554.0
    },
    {
      "title": "Rote Learning Considered Useful: Generalizing over Memorized Data in\n  LLMs",
      "summary": "Rote learning is a memorization technique based on repetition. It is commonly\nbelieved to hinder generalization by encouraging verbatim memorization rather\nthan deeper understanding. This insight holds for even learning factual\nknowledge that inevitably requires a certain degree of memorization. In this\nwork, we demonstrate that LLMs can be trained to generalize from rote memorized\ndata. We introduce a two-phase memorize-then-generalize framework, where the\nmodel first rote memorizes factual subject-object associations using a\nsemantically meaningless token and then learns to generalize by fine-tuning on\na small set of semantically meaningful prompts. Extensive experiments over 8\nLLMs show that the models can reinterpret rote memorized data through the\nsemantically meaningful prompts, as evidenced by the emergence of structured,\nsemantically aligned latent representations between the two. This surprising\nfinding opens the door to both effective and efficient knowledge injection and\npossible risks of repurposing the memorized data for malicious usage.",
      "url": "http://arxiv.org/abs/2507.21914v1",
      "published_time_eastern_timestamp": 1753802921.0
    },
    {
      "title": "Who's important? -- SUnSET: Synergistic Understanding of Stakeholder,\n  Events and Time for Timeline Generation",
      "summary": "As news reporting becomes increasingly global and decentralized online,\ntracking related events across multiple sources presents significant\nchallenges. Existing news summarization methods typically utilizes Large\nLanguage Models and Graphical methods on article-based summaries. However, this\nis not effective since it only considers the textual content of similarly dated\narticles to understand the gist of the event. To counteract the lack of\nanalysis on the parties involved, it is essential to come up with a novel\nframework to gauge the importance of stakeholders and the connection of related\nevents through the relevant entities involved. Therefore, we present SUnSET:\nSynergistic Understanding of Stakeholder, Events and Time for the task of\nTimeline Summarization (TLS). We leverage powerful Large Language Models (LLMs)\nto build SET triplets and introduced the use of stakeholder-based ranking to\nconstruct a $Relevancy$ metric, which can be extended into general situations.\nOur experimental results outperform all prior baselines and emerged as the new\nState-of-the-Art, highlighting the impact of stakeholder information within\nnews article.",
      "url": "http://arxiv.org/abs/2507.21903v1",
      "published_time_eastern_timestamp": 1753802079.0
    },
    {
      "title": "Leveraging LLMs for Persona-Based Visualization of Election Data",
      "summary": "Visualizations are essential tools for disseminating information regarding\nelections and their outcomes, potentially influencing public perceptions.\nPersonas, delineating distinctive segments within the populace, furnish a\nvaluable framework for comprehending the nuanced perspectives, requisites, and\nbehaviors of diverse voter demographics. In this work, we propose making\nvisualizations tailored to these personas to make election information easier\nto understand and more relevant. Using data from UK parliamentary elections and\nnew developments in Large Language Models (LLMs), we create personas that\nencompass the diverse demographics, technological preferences, voting\ntendencies, and information consumption patterns observed among\nvoters.Subsequently, we elucidate how these personas can inform the design of\nvisualizations through specific design criteria. We then provide illustrative\nexamples of visualization prototypes based on these criteria and evaluate these\nprototypes using these personas and LLMs. We finally propose some actionable\ninsights based upon the framework and the different design artifacts.",
      "url": "http://arxiv.org/abs/2507.21900v1",
      "published_time_eastern_timestamp": 1753801797.0
    }
  ]
}
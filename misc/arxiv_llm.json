{
  "last_updated": "2025-09-30T21:00:51.885566-04:00",
  "papers": [
    {
      "title": "UniAPL: A Unified Adversarial Preference Learning Framework for\n  Instruct-Following",
      "summary": "Shaping powerful LLMs to be beneficial and safe is central to AI alignment.\nWe argue that post-training alignment is fundamentally a unified Preference\nLearning problem, involving two modalities: demonstrated preferences (e.g.,\nSupervised Fine-Tuning, SFT) and comparative preferences (e.g., Reinforcement\nLearning, RL).The standard sequential pipeline-SFT followed by RL-is flawed due\nto a critical distributional mismatch: SFT uses static expert data, but as the\npolicy evolves, its generation distribution drifts, making SFT knowledge\nbrittle. Subsequent RL then explores without direct access to the rich,\nground-truth knowledge in expert demonstrations, leading to inefficient,\nungrounded updates. This separation prevents mutual regularization between data\nsources. To address this, we reframe alignment as a constrained optimization\nproblem and propose Unified Adversarial Preference Learning (UniAPL),a novel\nframework that dynamically aligns the policy's distribution with the expert's.\nUniAPL implements a single-stage unified training objective, jointly learning\nfrom mixed batches of SFT and preference data. In every gradient step, dense\nexpert demonstrations directly ground and regularize online exploration,\ninherently resolving distributional mismatch and maximizing data synergy.We\nevaluate UniAPL on instruction-following tasks using Qwen3-235B-Instruct-2507\nas the teacher. Our models match or exceed strong GRPO baselines: +5.77% on\nQwen3-0.6B (matching a 32B model) and +3.75% on Qwen3-4B,even outperforming the\nteacher. Analyses of response length and log-probability distributions confirm\nthat UniAPL outputs closely mimic expert demonstrations, achieving both\nstronger performance and better behavioral alignment.",
      "url": "http://arxiv.org/abs/2509.25148v1",
      "published_time_eastern_timestamp": 1759168389.0
    },
    {
      "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs\n  for Low-Resource Text Generation",
      "summary": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.",
      "url": "http://arxiv.org/abs/2509.25144v1",
      "published_time_eastern_timestamp": 1759168315.0
    },
    {
      "title": "Vision-and-Language Navigation with Analogical Textual Descriptions in\n  LLMs",
      "summary": "Integrating large language models (LLMs) into embodied AI models is becoming\nincreasingly prevalent. However, existing zero-shot LLM-based\nVision-and-Language Navigation (VLN) agents either encode images as textual\nscene descriptions, potentially oversimplifying visual details, or process raw\nimage inputs, which can fail to capture abstract semantics required for\nhigh-level reasoning. In this paper, we improve the navigation agent's\ncontextual understanding by incorporating textual descriptions from multiple\nperspectives that facilitate analogical reasoning across images. By leveraging\ntext-based analogical reasoning, the agent enhances its global scene\nunderstanding and spatial reasoning, leading to more accurate action decisions.\nWe evaluate our approach on the R2R dataset, where our experiments demonstrate\nsignificant improvements in navigation performance.",
      "url": "http://arxiv.org/abs/2509.25139v1",
      "published_time_eastern_timestamp": 1759168261.0
    },
    {
      "title": "Investigating Language and Retrieval Bias in Multilingual Previously\n  Fact-Checked Claim Detection",
      "summary": "Multilingual Large Language Models (LLMs) offer powerful capabilities for\ncross-lingual fact-checking. However, these models often exhibit language bias,\nperforming disproportionately better on high-resource languages such as English\nthan on low-resource counterparts. We also present and inspect a novel concept\n- retrieval bias, when information retrieval systems tend to favor certain\ninformation over others, leaving the retrieval process skewed. In this paper,\nwe study language and retrieval bias in the context of Previously Fact-Checked\nClaim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20\nlanguages using a fully multilingual prompting strategy, leveraging the AMC-16K\ndataset. By translating task prompts into each language, we uncover disparities\nin monolingual and cross-lingual performance and identify key trends based on\nmodel family, size, and prompting strategy. Our findings highlight persistent\nbias in LLM behavior and offer recommendations for improving equity in\nmultilingual fact-checking. To investigate retrieval bias, we employed\nmultilingual embedding models and look into the frequency of retrieved claims.\nOur analysis reveals that certain claims are retrieved disproportionately\nacross different posts, leading to inflated retrieval performance for popular\nclaims while under-representing less common ones.",
      "url": "http://arxiv.org/abs/2509.25138v1",
      "published_time_eastern_timestamp": 1759168232.0
    },
    {
      "title": "BALF: Budgeted Activation-Aware Low-Rank Factorization for\n  Fine-Tuning-Free Model Compression",
      "summary": "Neural network compression techniques typically require expensive fine-tuning\nor search procedures, rendering them impractical on commodity hardware.\nInspired by recent LLM compression research, we present a general\nactivation-aware factorization framework that can be applied to a broad range\nof layers. Moreover, we introduce a scalable budgeted rank allocator that\nallows flexible control over compression targets (e.g., retaining 50% of\nparameters) with no overhead. Together, these components form BALF, an\nefficient pipeline for compressing models without fine-tuning. We demonstrate\nits effectiveness across multiple scales and architectures, from ResNet-20 on\nCIFAR-10 to ResNeXt-101 and vision transformers on ImageNet, and show that it\nachieves excellent results in the fine-tuning-free regime. For instance, BALF\nreduces FLOPs on ResNeXt-101 by 45% with only a 1-percentage-point top-1\naccuracy drop.",
      "url": "http://arxiv.org/abs/2509.25136v1",
      "published_time_eastern_timestamp": 1759168229.0
    },
    {
      "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
      "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
      "url": "http://arxiv.org/abs/2509.25131v1",
      "published_time_eastern_timestamp": 1759168108.0
    },
    {
      "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by\n  Composing Old Ones",
      "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing\nones? This question lies at the core of ongoing debates about the role of RL in\nLLM post-training. On one side, strong empirical results can be achieved with\nRL even without preceding supervised finetuning; on the other, critics argue\nthat RL contributes little beyond reweighting existing reasoning strategies.\nThis work provides concrete evidence that LLMs can acquire genuinely new skills\nduring RL by composing existing ones, mirroring one of the central mechanisms\nby which humans acquire new cognitive skills. To mitigate data contamination\nand other confounding factors, and to allow precise control over task\ncomplexity, we develop a synthetic framework for our investigation.\nSpecifically, we define a skill as the ability to infer the output of a string\ntransformation function f(x) given x. When an LLM has already learned f and g\nprior to RL, our experiments reveal that RL enables it to learn unseen\ncompositions of them h(x)=g(f(x)). Further, this compositional ability\ngeneralizes to more difficult problems such as compositions of >2 functions\nunseen during RL training. Surprisingly, our experiments show that\ncompositional skill acquired on a source task transfers to a different target\ntask. This transfer happens even without compositional training on the target,\nrequiring only prior knowledge of the target's atomic skills. Our qualitative\nanalysis shows that RL fundamentally changes the reasoning behaviors of the\nmodels. In contrast, next-token training with the same data yields none of\nthese findings. Our systematic experiments provide fresh insights into LLM\nlearning, suggesting the value of first building base models with basic skills,\nthen using RL to incentivize advanced, generalizable skills for complex\nproblems.",
      "url": "http://arxiv.org/abs/2509.25123v1",
      "published_time_eastern_timestamp": 1759167867.0
    },
    {
      "title": "Knowledge Extraction on Semi-Structured Content: Does It Remain Relevant\n  for Question Answering in the Era of LLMs?",
      "summary": "The advent of Large Language Models (LLMs) has significantly advanced\nweb-based Question Answering (QA) systems over semi-structured content, raising\nquestions about the continued utility of knowledge extraction for question\nanswering. This paper investigates the value of triple extraction in this new\nparadigm by extending an existing benchmark with knowledge extraction\nannotations and evaluating commercial and open-source LLMs of varying sizes.\nOur results show that web-scale knowledge extraction remains a challenging task\nfor LLMs. Despite achieving high QA accuracy, LLMs can still benefit from\nknowledge extraction, through augmentation with extracted triples and\nmulti-task learning. These findings provide insights into the evolving role of\nknowledge triple extraction in web-based QA and highlight strategies for\nmaximizing LLM effectiveness across different model sizes and resource\nsettings.",
      "url": "http://arxiv.org/abs/2509.25107v1",
      "published_time_eastern_timestamp": 1759167559.0
    },
    {
      "title": "ORPO-Distill: Mixed-Policy Preference Optimization for\n  Cross-Architecture LLM Distillation",
      "summary": "We introduce ORPO-Distill, a general-purpose method for cross-architecture\nLLM distillation that formulates the problem as a preference optimization task.\nUnlike standard CoT distillation, the approach transfers knowledge through\ndiverse reasoning traces. It employs an Odds-Ratio Preference Optimization\nobjective that contrasts teacher and student traces for more effective\nlearning, and adopts a mixed-policy strategy for utilizing student-generated\noutputs, outperforming both off- and on-policy alternatives. Experiments on\nfive datasets and multiple student models show consistent improvements over\nconventional black-box KD baselines.",
      "url": "http://arxiv.org/abs/2509.25100v1",
      "published_time_eastern_timestamp": 1759167242.0
    },
    {
      "title": "Scaling with Collapse: Efficient and Predictable Training of LLM\n  Families",
      "summary": "Effective LLM training relies on *consistency*, meaning that key quantities\n-- such as final losses and optimal hyperparameters -- scale predictably across\nmodel sizes. Qiu et al. (2025) recently showed that this consistency extends\nbeyond scalars: whole training loss curves can *collapse* onto a universal\ntrajectory after a simple normalization. What remains unclear is whether this\nphenomenon holds for LLM families trained under *practical scaling recipes*,\nwhere width, depth, learning rate, batch size, and weight decay are scaled\njointly. We show that it does: loss curves collapse across scales precisely\nwhen optimization hyperparameters are set optimally for the given data budget,\nin accordance with recent empirical scaling laws. Collapse thus emerges as a\nsignature of compute-efficient training. We demonstrate two applications at\nscale: (1) deviation-from-collapse provides a sensitive, early diagnostic of\ntraining pathologies, and (2) the predictability of collapsed curves enables\nearly stopping in large-scale hyperparameter tuning. Finally, we train a\ncompetitive LLM family, *Celerity*, using these insights, highlighting collapse\nas an effective tool for developing efficient LLMs.",
      "url": "http://arxiv.org/abs/2509.25087v1",
      "published_time_eastern_timestamp": 1759166771.0
    },
    {
      "title": "Towards Trustworthy Lexical Simplification: Exploring Safety and\n  Efficiency with Small LLMs",
      "summary": "Despite their strong performance, large language models (LLMs) face\nchallenges in real-world application of lexical simplification (LS),\nparticularly in privacy-sensitive and resource-constrained environments.\nMoreover, since vulnerable user groups (e.g., people with disabilities) are one\nof the key target groups of this technology, it is crucial to ensure the safety\nand correctness of the output of LS systems. To address these issues, we\npropose an efficient framework for LS systems that utilizes small LLMs\ndeployable in local environments. Within this framework, we explore knowledge\ndistillation with synthesized data and in-context learning as baselines. Our\nexperiments in five languages evaluate model outputs both automatically and\nmanually. Our manual analysis reveals that while knowledge distillation boosts\nautomatic metric scores, it also introduces a safety trade-off by increasing\nharmful simplifications. Importantly, we find that the model's output\nprobability is a useful signal for detecting harmful simplifications.\nLeveraging this, we propose a filtering strategy that suppresses harmful\nsimplifications while largely preserving beneficial ones. This work establishes\na benchmark for efficient and safe LS with small LLMs. It highlights the key\ntrade-offs between performance, efficiency, and safety, and demonstrates a\npromising approach for safe real-world deployment.",
      "url": "http://arxiv.org/abs/2509.25086v1",
      "published_time_eastern_timestamp": 1759166756.0
    },
    {
      "title": "Optimizing Privacy-Preserving Primitives to Support LLM-Scale\n  Applications",
      "summary": "Privacy-preserving technologies have introduced a paradigm shift that allows\nfor realizable secure computing in real-world systems. The significant barrier\nto the practical adoption of these primitives is the computational and\ncommunication overhead that is incurred when applied at scale. In this paper,\nwe present an overview of our efforts to bridge the gap between this overhead\nand practicality for privacy-preserving learning systems using multi-party\ncomputation (MPC), zero-knowledge proofs (ZKPs), and fully homomorphic\nencryption (FHE). Through meticulous hardware/software/algorithm co-design, we\nshow progress towards enabling LLM-scale applications in privacy-preserving\nsettings. We demonstrate the efficacy of our solutions in several contexts,\nincluding DNN IP ownership, ethical LLM usage enforcement, and transformer\ninference.",
      "url": "http://arxiv.org/abs/2509.25072v1",
      "published_time_eastern_timestamp": 1759166211.0
    },
    {
      "title": "Learning from Convenience Samples: A Case Study on Fine-Tuning LLMs for\n  Survey Non-response in the German Longitudinal Election Study",
      "summary": "Survey researchers face two key challenges: the rising costs of probability\nsamples and missing data (e.g., non-response or attrition), which can undermine\ninference and increase the use of convenience samples. Recent work explores\nusing large language models (LLMs) to simulate respondents via persona-based\nprompts, often without labeled data. We study a more practical setting where\npartial survey responses exist: we fine-tune LLMs on available data to impute\nself-reported vote choice under both random and systematic nonresponse, using\nthe German Longitudinal Election Study. We compare zero-shot prompting and\nsupervised fine-tuning against tabular classifiers (e.g., CatBoost) and test\nhow different convenience samples (e.g., students) used for fine-tuning affect\ngeneralization.\n  Our results show that when data are missing completely at random, fine-tuned\nLLMs match tabular classifiers but outperform zero-shot approaches. When only\nbiased convenience samples are available, fine-tuning small (3B to 8B)\nopen-source LLMs can recover both individual-level predictions and\npopulation-level distributions more accurately than zero-shot and often better\nthan tabular methods. This suggests fine-tuned LLMs offer a promising strategy\nfor researchers working with non-probability samples or systematic missingness,\nand may enable new survey designs requiring only easily accessible\nsubpopulations.",
      "url": "http://arxiv.org/abs/2509.25063v1",
      "published_time_eastern_timestamp": 1759165938.0
    },
    {
      "title": "Cogito, Ergo Ludo: An Agent that Learns to Play by Reasoning and\n  Planning",
      "summary": "The pursuit of artificial agents that can learn to master complex\nenvironments has led to remarkable successes, yet prevailing deep reinforcement\nlearning methods often rely on immense experience, encoding their knowledge\nopaquely within neural network weights. We propose a different paradigm, one in\nwhich an agent learns to play by reasoning and planning. We introduce Cogito,\nergo ludo (CEL), a novel agent architecture that leverages a Large Language\nModel (LLM) to build an explicit, language-based understanding of its\nenvironment's mechanics and its own strategy. Starting from a tabula rasa state\nwith no prior knowledge (except action set), CEL operates on a cycle of\ninteraction and reflection. After each episode, the agent analyzes its complete\ntrajectory to perform two concurrent learning processes: Rule Induction, where\nit refines its explicit model of the environment's dynamics, and Strategy and\nPlaybook Summarization, where it distills experiences into an actionable\nstrategic playbook. We evaluate CEL on diverse grid-world tasks (i.e.,\nMinesweeper, Frozen Lake, and Sokoban), and show that the CEL agent\nsuccessfully learns to master these games by autonomously discovering their\nrules and developing effective policies from sparse rewards. Ablation studies\nconfirm that the iterative process is critical for sustained learning. Our work\ndemonstrates a path toward more general and interpretable agents that not only\nact effectively but also build a transparent and improving model of their world\nthrough explicit reasoning on raw experience.",
      "url": "http://arxiv.org/abs/2509.25052v1",
      "published_time_eastern_timestamp": 1759165351.0
    },
    {
      "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models",
      "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.",
      "url": "http://arxiv.org/abs/2509.25050v1",
      "published_time_eastern_timestamp": 1759165340.0
    },
    {
      "title": "Confidence-Guided Error Correction for Disordered Speech Recognition",
      "summary": "We investigate the use of large language models (LLMs) as post-processing\nmodules for automatic speech recognition (ASR), focusing on their ability to\nperform error correction for disordered speech. In particular, we propose\nconfidence-informed prompting, where word-level uncertainty estimates are\nembedded directly into LLM training to improve robustness and generalization\nacross speakers and datasets. This approach directs the model to uncertain ASR\nregions and reduces overcorrection. We fine-tune a LLaMA 3.1 model and compare\nour approach to both transcript-only fine-tuning and post hoc confidence-based\nfiltering. Evaluations show that our method achieves a 10% relative WER\nreduction compared to naive LLM correction on the Speech Accessibility Project\nspontaneous speech and a 47% reduction on TORGO, demonstrating the\neffectiveness of confidence-aware fine-tuning for impaired speech.",
      "url": "http://arxiv.org/abs/2509.25048v1",
      "published_time_eastern_timestamp": 1759165238.0
    },
    {
      "title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic\n  Architectures",
      "summary": "Despite their capabilities, Large Language Models (LLMs) remain opaque with\nlimited understanding of their internal representations. Current\ninterpretability methods, such as direct logit attribution (DLA) and sparse\nautoencoders (SAEs), provide restricted insight due to limitations such as the\nmodel's output vocabulary or unclear feature names. This work introduces\nHyperdimensional Probe, a novel paradigm for decoding information from the LLM\nvector space. It combines ideas from symbolic representations and neural\nprobing to project the model's residual stream into interpretable concepts via\nVector Symbolic Architectures (VSAs). This probe combines the strengths of SAEs\nand conventional probes while overcoming their key limitations. We validate our\ndecoding paradigm with controlled input-completion tasks, probing the model's\nfinal state before next-token prediction on inputs spanning syntactic pattern\nrecognition, key-value associations, and abstract inference. We further assess\nit in a question-answering setting, examining the state of the model both\nbefore and after text generation. Our experiments show that our probe reliably\nextracts meaningful concepts across varied LLMs, embedding sizes, and input\ndomains, also helping identify LLM failures. Our work advances information\ndecoding in LLM vector space, enabling extracting more informative,\ninterpretable, and structured features from neural representations.",
      "url": "http://arxiv.org/abs/2509.25045v1",
      "published_time_eastern_timestamp": 1759165147.0
    },
    {
      "title": "Large Language Models for Software Testing: A Research Roadmap",
      "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field.",
      "url": "http://arxiv.org/abs/2509.25043v1",
      "published_time_eastern_timestamp": 1759165101.0
    },
    {
      "title": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for\n  Efficient Distributed MoE Inference",
      "summary": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance.",
      "url": "http://arxiv.org/abs/2509.25041v1",
      "published_time_eastern_timestamp": 1759165053.0
    },
    {
      "title": "MARLIN: Multi-Agent Reinforcement Learning with Murmuration Intelligence\n  and LLM Guidance for Reservoir Management",
      "summary": "As climate change intensifies extreme weather events, water disasters pose\ngrowing threats to global communities, making adaptive reservoir management\ncritical for protecting vulnerable populations and ensuring water security.\nModern water resource management faces unprecedented challenges from cascading\nuncertainties propagating through interconnected reservoir networks. These\nuncertainties, rooted in physical water transfer losses and environmental\nvariability, make precise control difficult. For example, sending 10 tons\ndownstream may yield only 8-12 tons due to evaporation and seepage. Traditional\ncentralized optimization approaches suffer from exponential computational\ncomplexity and cannot effectively handle such real-world uncertainties, while\nexisting multi-agent reinforcement learning (MARL) methods fail to achieve\neffective coordination under uncertainty. To address these challenges, we\npresent MARLIN, a decentralized reservoir management framework inspired by\nstarling murmurations intelligence. Integrating bio-inspired alignment,\nseparation, and cohesion rules with MARL, MARLIN enables individual reservoirs\nto make local decisions while achieving emergent global coordination. In\naddition, a LLM provides real-time reward shaping signals, guiding agents to\nadapt to environmental changes and human-defined preferences. Experiments on\nreal-world USGS data show that MARLIN improves uncertainty handling by 23\\%,\ncuts computation by 35\\%, and accelerates flood response by 68\\%, exhibiting\nsuper-linear coordination, with complexity scaling 5.4x from 400 to 10,000\nnodes. These results demonstrate MARLIN's potential for disaster prevention and\nprotecting communities through intelligent, scalable water resource management.",
      "url": "http://arxiv.org/abs/2509.25034v1",
      "published_time_eastern_timestamp": 1759164804.0
    }
  ]
}
{
  "last_updated": "2025-06-17T23:42:59.628042-04:00",
  "papers": [
    {
      "title": "Large Language Models -- the Future of Fundamental Physics?",
      "summary": "For many fundamental physics applications, transformers, as the state of the\nart in learning complex correlations, benefit from pretraining on\nquasi-out-of-domain data. The obvious question is whether we can exploit Large\nLanguage Models, requiring proper out-of-domain transfer learning. We show how\nthe Qwen2.5 LLM can be used to analyze and generate SKA data, specifically 3D\nmaps of the cosmological large-scale structure for a large part of the\nobservable Universe. We combine the LLM with connector networks and show, for\ncosmological parameter regression and lightcone generation, that this Lightcone\nLLM (L3M) with Qwen2.5 weights outperforms standard initialization and compares\nfavorably with dedicated networks of matching size.",
      "url": "http://arxiv.org/abs/2506.14757v1",
      "published_time_eastern_timestamp": 1750182679.0
    },
    {
      "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
      "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
      "url": "http://arxiv.org/abs/2506.14731v1",
      "published_time_eastern_timestamp": 1750180354.0
    },
    {
      "title": "AgentDistill: Training-Free Agent Distillation with Generalizable MCP\n  Boxes",
      "summary": "While knowledge distillation has become a mature field for compressing large\nlanguage models (LLMs) into smaller ones by aligning their outputs or internal\nrepresentations, the distillation of LLM-based agents, which involve planning,\nmemory, and tool use, remains relatively underexplored. Existing agent\ndistillation methods typically replay full teacher trajectories or imitate\nstep-by-step teacher tool usage, but they often struggle to train student\nagents to dynamically plan and act in novel environments. We propose\nAgentDistill, a novel, training-free agent distillation framework that enables\nefficient and scalable knowledge transfer via direct reuse of\nModel-Context-Protocols (MCPs), which are structured and reusable task-solving\nmodules autonomously generated by teacher agents. The reuse of these distilled\nMCPs enables student agents to generalize their capabilities across domains and\nsolve new problems with minimal supervision or human intervention. Experiments\non biomedical and mathematical benchmarks demonstrate that our distilled\nstudent agents, built on small language models, can achieve performance\ncomparable to advanced systems using large LLMs such as OctoTools (GPT-4o),\nhighlighting the effectiveness of our framework in building scalable and\ncost-efficient intelligent agents.",
      "url": "http://arxiv.org/abs/2506.14728v1",
      "published_time_eastern_timestamp": 1750180112.0
    },
    {
      "title": "Unified Software Engineering agent as AI Software Engineer",
      "summary": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.",
      "url": "http://arxiv.org/abs/2506.14683v1",
      "published_time_eastern_timestamp": 1750177153.0
    },
    {
      "title": "AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language\n  Models",
      "summary": "We introduce AIRTBench, an AI red teaming benchmark for evaluating language\nmodels' ability to autonomously discover and exploit Artificial Intelligence\nand Machine Learning (AI/ML) security vulnerabilities. The benchmark consists\nof 70 realistic black-box capture-the-flag (CTF) challenges from the Crucible\nchallenge environment on the Dreadnode platform, requiring models to write\npython code to interact with and compromise AI systems. Claude-3.7-Sonnet\nemerged as the clear leader, solving 43 challenges (61% of the total suite,\n46.9% overall success rate), with Gemini-2.5-Pro following at 39 challenges\n(56%, 34.3% overall), GPT-4.5-Preview at 34 challenges (49%, 36.9% overall),\nand DeepSeek R1 at 29 challenges (41%, 26.9% overall). Our evaluations show\nfrontier models excel at prompt injection attacks (averaging 49% success rates)\nbut struggle with system exploitation and model inversion challenges (below\n26%, even for the best performers). Frontier models are far outpacing\nopen-source alternatives, with the best truly open-source model (Llama-4-17B)\nsolving 7 challenges (10%, 1.0% overall), though demonstrating specialized\ncapabilities on certain hard challenges. Compared to human security\nresearchers, large language models (LLMs) solve challenges with remarkable\nefficiency completing in minutes what typically takes humans hours or days-with\nefficiency advantages of over 5,000x on hard challenges. Our contribution fills\na critical gap in the evaluation landscape, providing the first comprehensive\nbenchmark specifically designed to measure and track progress in autonomous AI\nred teaming capabilities.",
      "url": "http://arxiv.org/abs/2506.14682v1",
      "published_time_eastern_timestamp": 1750177146.0
    },
    {
      "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality",
      "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness--often surpassing superficial similarity between trained data and\nbenchmark--and that mid-layer weight changes correlate most strongly with\nperformance gains. We will release these 1,000+ SFT models and benchmark\nresults to accelerate further research.",
      "url": "http://arxiv.org/abs/2506.14681v1",
      "published_time_eastern_timestamp": 1750176795.0
    },
    {
      "title": "Issue Retrieval and Verification Enhanced Supplementary Code Comment\n  Generation",
      "summary": "Issue reports have been recognized to contain rich information for\nretrieval-augmented code comment generation. However, how to minimize\nhallucinations in the generated comments remains significant challenges. In\nthis paper, we propose IsComment, an issue-based LLM retrieval and verification\napproach for generating method's design rationale, usage directives, and so on\nas supplementary code comments. We first identify five main types of code\nsupplementary information that issue reports can provide through\ncode-comment-issue analysis. Next, we retrieve issue sentences containing these\ntypes of supplementary information and generate candidate code comments. To\nreduce hallucinations, we filter out those candidate comments that are\nirrelevant to the code or unverifiable by the issue report, making the code\ncomment generation results more reliable. Our experiments indicate that\ncompared with LLMs, IsComment increases the coverage of manual supplementary\ncomments from 33.6% to 72.2% for ChatGPT, from 35.8% to 88.4% for GPT-4o, and\nfrom 35.0% to 86.2% for DeepSeek-V3. Compared with existing work, IsComment can\ngenerate richer and more useful supplementary code comments for programming\nunderstanding, which is quantitatively evaluated through the MESIA metric on\nboth methods with and without manual code comments.",
      "url": "http://arxiv.org/abs/2506.14649v1",
      "published_time_eastern_timestamp": 1750174945.0
    },
    {
      "title": "Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to\n  Mimic Polarized Social Media Comments",
      "summary": "The increasing sophistication of large language models (LLMs) has sparked\ngrowing concerns regarding their potential role in exacerbating ideological\npolarization through the automated generation of persuasive and biased content.\nThis study explores the extent to which fine-tuned LLMs can replicate and\namplify polarizing discourse within online environments. Using a curated\ndataset of politically charged discussions extracted from Reddit, we fine-tune\nan open-source LLM to produce context-aware and ideologically aligned\nresponses. The model's outputs are evaluated through linguistic analysis,\nsentiment scoring, and human annotation, with particular attention to\ncredibility and rhetorical alignment with the original discourse. The results\nindicate that, when trained on partisan data, LLMs are capable of producing\nhighly plausible and provocative comments, often indistinguishable from those\nwritten by humans. These findings raise significant ethical questions about the\nuse of AI in political discourse, disinformation, and manipulation campaigns.\nThe paper concludes with a discussion of the broader implications for AI\ngovernance, platform regulation, and the development of detection tools to\nmitigate adversarial fine-tuning risks.",
      "url": "http://arxiv.org/abs/2506.14645v1",
      "published_time_eastern_timestamp": 1750174886.0
    },
    {
      "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than\n  Few-shot",
      "summary": "In-Context Learning (ICL) is an essential emergent ability of Large Language\nModels (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars\nof ICL to enhance the reasoning capability, especially in mathematics tasks.\nHowever, given the continuous advancement of model capabilities, it remains\nunclear whether CoT exemplars still benefit recent, stronger models in such\ntasks. Through systematic experiments, we find that for recent strong models\nsuch as the Qwen2.5 series, adding traditional CoT exemplars does not improve\nreasoning performance compared to Zero-Shot CoT. Instead, their primary\nfunction is to align the output format with human expectations. We further\ninvestigate the effectiveness of enhanced CoT exemplars, constructed using\nanswers from advanced models such as \\texttt{Qwen2.5-Max} and\n\\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced\nexemplars still fail to improve the model's reasoning performance. Further\nanalysis reveals that models tend to ignore the exemplars and focus primarily\non the instructions, leading to no observable gain in reasoning ability.\nOverall, our findings highlight the limitations of the current ICL+CoT\nframework in mathematical reasoning, calling for a re-examination of the ICL\nparadigm and the definition of exemplars.",
      "url": "http://arxiv.org/abs/2506.14641v1",
      "published_time_eastern_timestamp": 1750174773.0
    },
    {
      "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation",
      "summary": "The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.",
      "url": "http://arxiv.org/abs/2506.14634v1",
      "published_time_eastern_timestamp": 1750174133.0
    },
    {
      "title": "Probabilistic Aggregation and Targeted Embedding Optimization for\n  Collective Moral Reasoning in Large Language Models",
      "summary": "Large Language Models (LLMs) have shown impressive moral reasoning abilities.\nYet they often diverge when confronted with complex, multi-factor moral\ndilemmas. To address these discrepancies, we propose a framework that\nsynthesizes multiple LLMs' moral judgments into a collectively formulated moral\njudgment, realigning models that deviate significantly from this consensus. Our\naggregation mechanism fuses continuous moral acceptability scores (beyond\nbinary labels) into a collective probability, weighting contributions by model\nreliability. For misaligned models, a targeted embedding-optimization procedure\nfine-tunes token embeddings for moral philosophical theories, minimizing JS\ndivergence to the consensus while preserving semantic integrity. Experiments on\na large-scale social moral dilemma dataset show our approach builds robust\nconsensus and improves individual model fidelity. These findings highlight the\nvalue of data-driven moral alignment across multiple models and its potential\nfor safer, more consistent AI systems.",
      "url": "http://arxiv.org/abs/2506.14625v1",
      "published_time_eastern_timestamp": 1750173741.0
    },
    {
      "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
      "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
      "url": "http://arxiv.org/abs/2506.14606v1",
      "published_time_eastern_timestamp": 1750172814.0
    },
    {
      "title": "NetRoller: Interfacing General and Specialized Models for End-to-End\n  Autonomous Driving",
      "summary": "Integrating General Models (GMs) such as Large Language Models (LLMs), with\nSpecialized Models (SMs) in autonomous driving tasks presents a promising\napproach to mitigating challenges in data diversity and model capacity of\nexisting specialized driving models. However, this integration leads to\nproblems of asynchronous systems, which arise from the distinct characteristics\ninherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an\nadapter that incorporates a set of novel mechanisms to facilitate the seamless\nintegration of GMs and specialized driving models. Specifically, our mechanisms\nfor interfacing the asynchronous GMs and SMs are organized into three key\nstages. NetRoller first harvests semantically rich and computationally\nefficient representations from the reasoning processes of LLMs using an early\nstopping mechanism, which preserves critical insights on driving context while\nmaintaining low overhead. It then applies learnable query embeddings,\nnonsensical embeddings, and positional layer embeddings to facilitate robust\nand efficient cross-modality translation. At last, it employs computationally\nefficient Query Shift and Feature Shift mechanisms to enhance the performance\nof SMs through few-epoch fine-tuning. Based on the mechanisms formalized in\nthese three stages, NetRoller enables specialized driving models to operate at\ntheir native frequencies while maintaining situational awareness of the GM.\nExperiments conducted on the nuScenes dataset demonstrate that integrating GM\nthrough NetRoller significantly improves human similarity and safety in\nplanning tasks, and it also achieves noticeable precision improvements in\ndetection and mapping tasks for end-to-end autonomous driving. The code and\nmodels are available at https://github.com/Rex-sys-hk/NetRoller .",
      "url": "http://arxiv.org/abs/2506.14589v1",
      "published_time_eastern_timestamp": 1750171970.0
    },
    {
      "title": "GenerationPrograms: Fine-grained Attribution with Executable Programs",
      "summary": "Recent large language models (LLMs) achieve impressive performance in\nsource-conditioned text generation but often fail to correctly provide\nfine-grained attributions for their outputs, undermining verifiability and\ntrust. Moreover, existing attribution methods do not explain how and why models\nleverage the provided source documents to generate their final responses,\nlimiting interpretability. To overcome these challenges, we introduce a modular\ngeneration framework, GenerationPrograms, inspired by recent advancements in\nexecutable \"code agent\" architectures. Unlike conventional generation methods\nthat simultaneously generate outputs and attributions or rely on post-hoc\nattribution, GenerationPrograms decomposes the process into two distinct\nstages: first, creating an executable program plan composed of modular text\noperations (such as paraphrasing, compression, and fusion) explicitly tailored\nto the query, and second, executing these operations following the program's\nspecified instructions to produce the final response. Empirical evaluations\ndemonstrate that GenerationPrograms significantly improves attribution quality\nat both the document level and sentence level across two long-form\nquestion-answering tasks and a multi-document summarization task. We further\ndemonstrate that GenerationPrograms can effectively function as a post-hoc\nattribution method, outperforming traditional techniques in recovering accurate\nattributions. In addition, the interpretable programs generated by\nGenerationPrograms enable localized refinement through modular-level\nimprovements that further enhance overall attribution quality.",
      "url": "http://arxiv.org/abs/2506.14580v1",
      "published_time_eastern_timestamp": 1750171029.0
    },
    {
      "title": "AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs",
      "summary": "Weight decay is a standard regularization technique for training large\nlanguage models (LLMs). While it is common to assign a uniform decay rate to\nevery layer, this approach overlooks the structural diversity of LLMs and the\nvarying spectral properties across modules. In this paper, we introduce\nAlphaDecay, a simple yet effective method that adaptively assigns different\nweight decay strengths to each module of an LLM. Our approach is guided by\nHeavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical\nspectral density (ESD) of weight correlation matrices to quantify\n\"heavy-tailedness.\" Modules exhibiting more pronounced heavy-tailed ESDs,\nreflecting stronger feature learning, are assigned weaker decay, while modules\nwith lighter-tailed spectra receive stronger decay. Our method leverages\ntailored weight decay assignments to balance the module-wise differences in\nspectral properties, leading to improved performance. Extensive pre-training\ntasks with various model sizes from 60M to 1B demonstrate that AlphaDecay\nachieves better perplexity and generalization than conventional uniform decay\nand other adaptive decay baselines.",
      "url": "http://arxiv.org/abs/2506.14562v1",
      "published_time_eastern_timestamp": 1750170070.0
    },
    {
      "title": "Doppelg√§nger Method: Breaking Role Consistency in LLM Agent via\n  Prompt-based Transferable Adversarial Attack",
      "summary": "Since the advent of large language models, prompt engineering now enables the\nrapid, low-effort creation of diverse autonomous agents that are already in\nwidespread use. Yet this convenience raises urgent concerns about the safety,\nrobustness, and behavioral consistency of the underlying prompts, along with\nthe pressing challenge of preventing those prompts from being exposed to user's\nattempts. In this paper, we propose the ''Doppelg\\\"anger method'' to\ndemonstrate the risk of an agent being hijacked, thereby exposing system\ninstructions and internal information. Next, we define the ''Prompt Alignment\nCollapse under Adversarial Transfer (PACAT)'' level to evaluate the\nvulnerability to this adversarial transfer attack. We also propose a ''Caution\nfor Adversarial Transfer (CAT)'' prompt to counter the Doppelg\\\"anger method.\nThe experimental results demonstrate that the Doppelg\\\"anger method can\ncompromise the agent's consistency and expose its internal information. In\ncontrast, CAT prompts enable effective defense against this adversarial attack.",
      "url": "http://arxiv.org/abs/2506.14539v1",
      "published_time_eastern_timestamp": 1750168899.0
    },
    {
      "title": "Automatic Qiskit Code Refactoring Using Large Language Models",
      "summary": "As quantum software frameworks evolve, developers face increasing challenges\nin maintaining compatibility with rapidly changing APIs. In this work, we\npresent a novel methodology for refactoring Qiskit code using large language\nmodels (LLMs). We begin by extracting a taxonomy of migration scenarios from\nthe different sources of official Qiskit documentation (such as release notes),\ncapturing common patterns such as migration of functionality to different\nmodules and deprecated usage. This taxonomy, along with the original Python\nsource code, is provided as input to an LLM, which is then tasked with\nidentifying instances of migration scenarios in the code and suggesting\nappropriate refactoring solutions. Our approach is designed to address the\ncontext length limitations of current LLMs by structuring the input and\nreasoning process in a targeted, efficient manner. The results demonstrate that\nLLMs, when guided by domain-specific migration knowledge, can effectively\nassist in automating Qiskit code migration. This work contributes both a set of\nproven prompts and taxonomy for Qiskit code migration from earlier versions to\nversion 0.46 and a methodology to asses the capabilities of LLMs to assist in\nthe migration of quantum code.",
      "url": "http://arxiv.org/abs/2506.14535v1",
      "published_time_eastern_timestamp": 1750168848.0
    },
    {
      "title": "M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with\n  Large Language Models",
      "summary": "This paper introduces a novel neural network framework called M2BeamLLM for\nbeam prediction in millimeter-wave (mmWave) massive multi-input multi-output\n(mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data,\nincluding images, radar, LiDAR, and GPS, leveraging the powerful reasoning\ncapabilities of large language models (LLMs) such as GPT-2 for beam prediction.\nBy combining sensing data encoding, multimodal alignment and fusion, and\nsupervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam\nprediction accuracy and robustness, demonstrably outperforming traditional deep\nlearning (DL) models in both standard and few-shot scenarios. Furthermore, its\nprediction performance consistently improves with increased diversity in\nsensing modalities. Our study provides an efficient and intelligent beam\nprediction solution for vehicle-to-infrastructure (V2I) mmWave communication\nsystems.",
      "url": "http://arxiv.org/abs/2506.14532v1",
      "published_time_eastern_timestamp": 1750168716.0
    },
    {
      "title": "Automated Decision-Making on Networks with LLMs through Knowledge-Guided\n  Evolution",
      "summary": "Effective decision-making on networks often relies on learning from\ngraph-structured data, where Graph Neural Networks (GNNs) play a central role,\nbut they take efforts to configure and tune. In this demo, we propose LLMNet,\nshowing how to design GNN automated through Large Language Models. Our system\ndevelops a set of agents that construct graph-related knowlege bases and then\nleverages Retrieval-Augmented Generation (RAG) to support automated\nconfiguration and refinement of GNN models through a knowledge-guided evolution\nprocess. These agents, equipped with specialized knowledge bases, extract\ninsights into tasks and graph structures by interacting with the knowledge\nbases. Empirical results show LLMNet excels in twelve datasets across three\ngraph learning tasks, validating its effectiveness of GNN model designing.",
      "url": "http://arxiv.org/abs/2506.14529v1",
      "published_time_eastern_timestamp": 1750168428.0
    },
    {
      "title": "RMIT-ADM+S at the SIGIR 2025 LiveRAG Challenge",
      "summary": "This paper presents the RMIT--ADM+S participation in the SIGIR 2025 LiveRAG\nChallenge. Our Generation-Retrieval-Augmented Generation (GRAG) approach relies\non generating a hypothetical answer that is used in the retrieval phase,\nalongside the original question. GRAG also incorporates a pointwise large\nlanguage model (LLM)-based re-ranking step prior to final answer generation. We\ndescribe the system architecture and the rationale behind our design choices.\nIn particular, a systematic evaluation using the Grid of Points (GoP) framework\nand N-way ANOVA enabled comparison across multiple configurations, including\nquery variant generation, question decomposition, rank fusion strategies, and\nprompting techniques for answer generation. Our system achieved a Relevance\nscore of 1.199 and a Faithfulness score of 0.477 on the private leaderboard,\nplacing among the top four finalists in the LiveRAG 2025 Challenge.",
      "url": "http://arxiv.org/abs/2506.14516v1",
      "published_time_eastern_timestamp": 1750167672.0
    }
  ]
}
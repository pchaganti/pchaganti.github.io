{
  "last_updated": "2025-09-18T17:10:24.387630-04:00",
  "papers": [
    {
      "title": "Compute as Teacher: Turning Inference Compute Into Reference-Free\n  Supervision",
      "summary": "Where do learning signals come from when there is no ground truth in\npost-training? We propose turning exploration into supervision through Compute\nas Teacher (CaT), which converts the model's own exploration at inference-time\ninto reference-free supervision by synthesizing a single reference from a group\nof parallel rollouts and then optimizing toward it. Concretely, the current\npolicy produces a group of rollouts; a frozen anchor (the initial policy)\nreconciles omissions and contradictions to estimate a reference, turning extra\ninference-time compute into a teacher signal. We turn this into rewards in two\nregimes: (i) verifiable tasks use programmatic equivalence on final answers;\n(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria\nscored by an independent LLM judge, with reward given by the fraction\nsatisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge\nscores), synthesis may disagree with the majority and be correct even when all\nrollouts are wrong; performance scales with the number of rollouts. As a\ntest-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up\nto +27% on MATH-500; +12% on HealthBench). With reinforcement learning\n(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained\npolicy surpassing the initial teacher signal.",
      "url": "http://arxiv.org/abs/2509.14234v1",
      "published_time_eastern_timestamp": 1758131982.0
    },
    {
      "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language\n  Environments",
      "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.",
      "url": "http://arxiv.org/abs/2509.14233v1",
      "published_time_eastern_timestamp": 1758131961.0
    },
    {
      "title": "NIRVANA: Structured pruning reimagined for large language models\n  compression",
      "summary": "Structured pruning of large language models (LLMs) offers substantial\nefficiency improvements by removing entire hidden units, yet current approaches\noften suffer from significant performance degradation, particularly in\nzero-shot settings, and necessitate costly recovery techniques such as\nsupervised fine-tuning (SFT) or adapter insertion. To address these critical\nshortcomings, we introduce NIRVANA, a novel pruning method explicitly designed\nto balance immediate zero-shot accuracy preservation with robust fine-tuning\ncapability. Leveraging a first-order saliency criterion derived from the Neural\nTangent Kernel under Adam optimization dynamics, NIRVANA provides a\ntheoretically grounded pruning strategy that respects essential model training\nbehaviors. To further address the unique challenges posed by structured\npruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across\nlayers and modules (attention vs. MLP), which adjusts pruning intensity between\nmodules in a globally balanced manner. Additionally, to mitigate the high\nsensitivity of pruning decisions to calibration data quality, we propose a\nsimple yet effective KL divergence-based calibration data selection strategy,\nensuring more reliable and task-agnostic pruning outcomes. Comprehensive\nexperiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA\noutperforms existing structured pruning methods under equivalent sparsity\nconstraints, providing a theoretically sound and practical approach to LLM\ncompression. The code is available at\nhttps://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.",
      "url": "http://arxiv.org/abs/2509.14230v1",
      "published_time_eastern_timestamp": 1758131940.0
    },
    {
      "title": "GEM-Bench: A Benchmark for Ad-Injected Response Generation within\n  Generative Engine Marketing",
      "summary": "Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing\ngenerative engines, such as LLM-based chatbots, by seamlessly integrating\nrelevant advertisements into their responses. At the core of GEM lies the\ngeneration and evaluation of ad-injected responses. However, existing\nbenchmarks are not specifically designed for this purpose, which limits future\nresearch. To address this gap, we propose GEM-Bench, the first comprehensive\nbenchmark for ad-injected response generation in GEM. GEM-Bench includes three\ncurated datasets covering both chatbot and search scenarios, a metric ontology\nthat captures multiple dimensions of user satisfaction and engagement, and\nseveral baseline solutions implemented within an extensible multi-agent\nframework. Our preliminary results indicate that, while simple prompt-based\nmethods achieve reasonable engagement such as click-through rate, they often\nreduce user satisfaction. In contrast, approaches that insert ads based on\npre-generated ad-free responses help mitigate this issue but introduce\nadditional overhead. These findings highlight the need for future research on\ndesigning more effective and efficient solutions for generating ad-injected\nresponses in GEM.",
      "url": "http://arxiv.org/abs/2509.14221v1",
      "published_time_eastern_timestamp": 1758131623.0
    },
    {
      "title": "A Universal Banach--Bregman Framework for Stochastic Iterations:\n  Unifying Stochastic Mirror Descent, Learning and LLM Training",
      "summary": "Stochastic optimization powers the scalability of modern artificial\nintelligence, spanning machine learning, deep learning, reinforcement learning,\nand large language model training. Yet, existing theory remains largely\nconfined to Hilbert spaces, relying on inner-product frameworks and\northogonality. This paradigm fails to capture non-Euclidean settings, such as\nmirror descent on simplices, Bregman proximal methods for sparse learning,\nnatural gradient descent in information geometry, or\nKullback--Leibler-regularized language model training. Unlike Euclidean-based\nHilbert-space methods, this approach embraces general Banach spaces. This work\nintroduces a pioneering Banach--Bregman framework for stochastic iterations,\nestablishing Bregman geometry as a foundation for next-generation optimization.\nIt (i) provides a unified template via Bregman projections and Bregman--Fejer\nmonotonicity, encompassing stochastic approximation, mirror descent, natural\ngradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations\n($\\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and\nelucidating their acceleration effect; and (iii) delivers convergence theorems\nspanning almost-sure boundedness to geometric rates, validated on synthetic and\nreal-world tasks. Empirical studies across machine learning (UCI benchmarks),\ndeep learning (e.g., Transformer training), reinforcement learning\n(actor--critic), and large language models (WikiText-2 with distilGPT-2) show\nup to 20% faster convergence, reduced variance, and enhanced accuracy over\nclassical baselines. These results position Banach--Bregman geometry as a\ncornerstone unifying optimization theory and practice across core AI paradigms.",
      "url": "http://arxiv.org/abs/2509.14216v1",
      "published_time_eastern_timestamp": 1758131459.0
    },
    {
      "title": "Framing Migration: A Computational Analysis of UK Parliamentary\n  Discourse",
      "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.",
      "url": "http://arxiv.org/abs/2509.14197v1",
      "published_time_eastern_timestamp": 1758130317.0
    },
    {
      "title": "AI and the Future of Academic Peer Review",
      "summary": "Peer review remains the central quality-control mechanism of science, yet its\nability to fulfill this role is increasingly strained. Empirical studies\ndocument serious shortcomings: long publication delays, escalating reviewer\nburden concentrated on a small minority of scholars, inconsistent quality and\nlow inter-reviewer agreement, and systematic biases by gender, language, and\ninstitutional prestige. Decades of human-centered reforms have yielded only\nmarginal improvements. Meanwhile, artificial intelligence, especially large\nlanguage models (LLMs), is being piloted across the peer-review pipeline by\njournals, funders, and individual reviewers. Early studies suggest that AI\nassistance can produce reviews comparable in quality to humans, accelerate\nreviewer selection and feedback, and reduce certain biases, but also raise\ndistinctive concerns about hallucination, confidentiality, gaming, novelty\nrecognition, and loss of trust. In this paper, we map the aims and persistent\nfailure modes of peer review to specific LLM applications and systematically\nanalyze the objections they raise alongside safeguards that could make their\nuse acceptable. Drawing on emerging evidence, we show that targeted, supervised\nLLM assistance can plausibly improve error detection, timeliness, and reviewer\nworkload without displacing human judgment. We highlight advanced\narchitectures, including fine-tuned, retrieval-augmented, and multi-agent\nsystems, that may enable more reliable, auditable, and interdisciplinary\nreview. We argue that ethical and practical considerations are not peripheral\nbut constitutive: the legitimacy of AI-assisted peer review depends on\ngovernance choices as much as technical capacity. The path forward is neither\nuncritical adoption nor reflexive rejection, but carefully scoped pilots with\nexplicit evaluation metrics, transparency, and accountability.",
      "url": "http://arxiv.org/abs/2509.14189v1",
      "published_time_eastern_timestamp": 1758130032.0
    },
    {
      "title": "Read to Hear: A Zero-Shot Pronunciation Assessment Using Textual\n  Descriptions and LLMs",
      "summary": "Automatic pronunciation assessment is typically performed by acoustic models\ntrained on audio-score pairs. Although effective, these systems provide only\nnumerical scores, without the information needed to help learners understand\ntheir errors. Meanwhile, large language models (LLMs) have proven effective in\nsupporting language learning, but their potential for assessing pronunciation\nremains unexplored. In this work, we introduce TextPA, a zero-shot, Textual\ndescription-based Pronunciation Assessment approach. TextPA utilizes\nhuman-readable representations of speech signals, which are fed into an LLM to\nassess pronunciation accuracy and fluency, while also providing reasoning\nbehind the assigned scores. Finally, a phoneme sequence match scoring method is\nused to refine the accuracy scores. Our work highlights a previously overlooked\ndirection for pronunciation assessment. Instead of relying on supervised\ntraining with audio-score examples, we exploit the rich pronunciation knowledge\nembedded in written text. Experimental results show that our approach is both\ncost-efficient and competitive in performance. Furthermore, TextPA\nsignificantly improves the performance of conventional audio-score-trained\nmodels on out-of-domain data by offering a complementary perspective.",
      "url": "http://arxiv.org/abs/2509.14187v1",
      "published_time_eastern_timestamp": 1758129989.0
    },
    {
      "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation\n  Framework for Personal Finance LLMs",
      "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.",
      "url": "http://arxiv.org/abs/2509.14180v1",
      "published_time_eastern_timestamp": 1758129158.0
    },
    {
      "title": "TopoSizing: An LLM-aided Framework of Topology-based Understanding and\n  Sizing for AMS Circuits",
      "summary": "Analog and mixed-signal circuit design remains challenging due to the\nshortage of high-quality data and the difficulty of embedding domain knowledge\ninto automated flows. Traditional black-box optimization achieves sampling\nefficiency but lacks circuit understanding, which often causes evaluations to\nbe wasted in low-value regions of the design space. In contrast, learning-based\nmethods embed structural knowledge but are case-specific and costly to retrain.\nRecent attempts with large language models show potential, yet they often rely\non manual intervention, limiting generality and transparency. We propose\nTopoSizing, an end-to-end framework that performs robust circuit understanding\ndirectly from raw netlists and translates this knowledge into optimization\ngains. Our approach first applies graph algorithms to organize circuits into a\nhierarchical device-module-stage representation. LLM agents then execute an\niterative hypothesis-verification-refinement loop with built-in consistency\nchecks, producing explicit annotations. Verified insights are integrated into\nBayesian optimization through LLM-guided initial sampling and\nstagnation-triggered trust-region updates, improving efficiency while\npreserving feasibility.",
      "url": "http://arxiv.org/abs/2509.14169v1",
      "published_time_eastern_timestamp": 1758127966.0
    },
    {
      "title": "MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods,\n  Results, Discussion, and Outlook",
      "summary": "This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim\nto bring together different approaches in multimodal machine learning and LLMs\nvia a large benchmark. We hope it better allows researchers to follow the\nstate-of-the-art in this very dynamic area. Meanwhile, a growing number of\ntestbeds have boosted the evolution of general-purpose large language models.\nThus, this year's MARS2 focuses on real-world and specialized scenarios to\nbroaden the multimodal reasoning applications of MLLMs. Our organizing team\nreleased two tailored datasets Lens and AdsQA as test sets, which support\ngeneral reasoning in 12 daily scenarios and domain-specific reasoning in\nadvertisement videos, respectively. We evaluated 40+ baselines that include\nboth generalist MLLMs and task-specific models, and opened up three competition\ntracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question\nAnswering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative\nAdvertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and\nindustrial institutions have registered and 40+ valid submissions (out of\n1200+) have been included in our ranking lists. Our datasets, code sets (40+\nbaselines and 15+ participants' methods), and rankings are publicly available\non the MARS2 workshop website and our GitHub organization page\nhttps://github.com/mars2workshop/, where our updates and announcements of\nupcoming events will be continuously provided.",
      "url": "http://arxiv.org/abs/2509.14142v1",
      "published_time_eastern_timestamp": 1758126094.0
    },
    {
      "title": "When Avatars Have Personality: Effects on Engagement and Communication\n  in Immersive Medical Training",
      "summary": "While virtual reality (VR) excels at simulating physical environments, its\neffectiveness for training complex interpersonal skills is limited by a lack of\npsychologically plausible virtual humans. This is a critical gap in high-stakes\ndomains like medical education, where communication is a core competency. This\npaper introduces a framework that integrates large language models (LLMs) into\nimmersive VR to create medically coherent virtual patients with distinct,\nconsistent personalities, built on a modular architecture that decouples\npersonality from clinical data. We evaluated our system in a mixed-method,\nwithin-subjects study with licensed physicians who engaged in simulated\nconsultations. Results demonstrate that the approach is not only feasible but\nis also perceived by physicians as a highly rewarding and effective training\nenhancement. Furthermore, our analysis uncovers critical design principles,\nincluding a ``realism-verbosity paradox\" where less communicative agents can\nseem more artificial, and the need for challenges to be perceived as authentic\nto be instructive. This work provides a validated framework and key insights\nfor developing the next generation of socially intelligent VR training\nenvironments.",
      "url": "http://arxiv.org/abs/2509.14132v1",
      "published_time_eastern_timestamp": 1758125617.0
    },
    {
      "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST",
      "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.",
      "url": "http://arxiv.org/abs/2509.14128v1",
      "published_time_eastern_timestamp": 1758125326.0
    },
    {
      "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
      "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
      "url": "http://arxiv.org/abs/2509.14093v1",
      "published_time_eastern_timestamp": 1758123224.0
    },
    {
      "title": "Enhancing Multi-Agent Debate System Performance via Confidence\n  Expression",
      "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.",
      "url": "http://arxiv.org/abs/2509.14034v1",
      "published_time_eastern_timestamp": 1758119667.0
    },
    {
      "title": "SAIL-VL2 Technical Report",
      "summary": "We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)\nfor comprehensive multimodal understanding and reasoning. As the successor to\nSAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B\nparameter scales across diverse image and video benchmarks, demonstrating\nstrong capabilities from fine-grained perception to complex reasoning. Three\ncore innovations drive its effectiveness. First, a large-scale data curation\npipeline with scoring and filtering strategies enhances both quality and\ndistribution across captioning, OCR, QA, and video data, improving training\nefficiency. Second, a progressive training framework begins with a powerful\npre-trained vision encoder (SAIL-ViT), advances through multimodal\npre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that\nsystematically strengthens model capabilities. Third, architectural advances\nextend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.\nWith these contributions, SAIL-VL2 demonstrates competitive performance across\n106 datasets and achieves state-of-the-art results on challenging reasoning\nbenchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass\nleaderboard, SAIL-VL2-2B ranks first among officially released open-source\nmodels under the 4B parameter scale, while serving as an efficient and\nextensible foundation for the open-source multimodal community.",
      "url": "http://arxiv.org/abs/2509.14033v1",
      "published_time_eastern_timestamp": 1758119642.0
    },
    {
      "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System",
      "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.",
      "url": "http://arxiv.org/abs/2509.14030v1",
      "published_time_eastern_timestamp": 1758119478.0
    },
    {
      "title": "Early Stopping Chain-of-thoughts in Large Language Models",
      "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.",
      "url": "http://arxiv.org/abs/2509.14004v1",
      "published_time_eastern_timestamp": 1758118445.0
    },
    {
      "title": "An RDMA-First Object Storage System with SmartNIC Offload",
      "summary": "AI training and inference impose sustained, fine-grain I/O that stresses\nhost-mediated, TCP-based storage paths. Motivated by kernel-bypass networking\nand user-space storage stacks, we revisit POSIX-compatible object storage for\nGPU-centric pipelines. We present ROS2, an RDMA-first object storage system\ndesign that offloads the DAOS client to an NVIDIA BlueField-3 SmartNIC while\nleaving the DAOS I/O engine unchanged on the storage server. ROS2 separates a\nlightweight control plane (gRPC for namespace and capability exchange) from a\nhigh-throughput data plane (UCX/libfabric over RDMA or TCP) and removes host\nmediation from the data path.\n  Using FIO/DFS across local and remote configurations, we find that on\nserver-grade CPUs RDMA consistently outperforms TCP for both large sequential\nand small random I/O. When the RDMA-driven DAOS client is offloaded to\nBlueField-3, end-to-end performance is comparable to the host, demonstrating\nthat SmartNIC offload preserves RDMA efficiency while enabling DPU-resident\nfeatures such as multi-tenant isolation and inline services (e.g.,\nencryption/decryption) close to the NIC. In contrast, TCP on the SmartNIC lags\nhost performance, underscoring the importance of RDMA for offloaded\ndeployments.\n  Overall, our results indicate that an RDMA-first, SmartNIC-offloaded\nobject-storage stack is a practical foundation for scaling data delivery in\nmodern LLM training environments; integrating optional GPU-direct placement for\nLLM tasks is left for future work.",
      "url": "http://arxiv.org/abs/2509.13997v1",
      "published_time_eastern_timestamp": 1758118244.0
    },
    {
      "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency",
      "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.",
      "url": "http://arxiv.org/abs/2509.13990v1",
      "published_time_eastern_timestamp": 1758117651.0
    }
  ]
}
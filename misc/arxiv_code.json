{
  "last_updated": "2025-05-27T05:12:47.371040-04:00",
  "papers": [
    {
      "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models",
      "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.",
      "url": "http://arxiv.org/abs/2505.20152v1",
      "published_time_eastern_timestamp": 1748274928.0
    },
    {
      "title": "Error Optimization: Overcoming Exponential Signal Decay in Deep\n  Predictive Coding Networks",
      "summary": "Predictive Coding (PC) offers a biologically plausible alternative to\nbackpropagation for neural network training, yet struggles with deeper\narchitectures. This paper identifies the root cause: an inherent signal decay\nproblem where gradients attenuate exponentially with depth, becoming\ncomputationally negligible due to numerical precision constraints. To address\nthis fundamental limitation, we introduce Error Optimization (EO), a novel\nreparameterization that preserves PC's theoretical properties while eliminating\nsignal decay. By optimizing over prediction errors rather than states, EO\nenables signals to reach all layers simultaneously and without attenuation,\nconverging orders of magnitude faster than standard PC. Experiments across\nmultiple architectures and datasets demonstrate that EO matches\nbackpropagation's performance even for deeper models where conventional PC\nstruggles. Besides practical improvements, our work provides theoretical\ninsight into PC dynamics and establishes a foundation for scaling\nbiologically-inspired learning to deeper architectures on digital hardware and\nbeyond.",
      "url": "http://arxiv.org/abs/2505.20137v1",
      "published_time_eastern_timestamp": 1748273956.0
    },
    {
      "title": "Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based\n  Continual Learning",
      "summary": "Replay-based continual learning (CL) methods assume that models trained on a\nsmall subset can also effectively minimize the empirical risk of the complete\ndataset. These methods maintain a memory buffer that stores a sampled subset of\ndata from previous tasks to consolidate past knowledge. However, this\nassumption is not guaranteed in practice due to the limited capacity of the\nmemory buffer and the heuristic criteria used for buffer data selection. To\naddress this issue, we propose a new dataset distillation framework tailored\nfor CL, which maintains a learnable memory buffer to distill the global\ninformation from the current task data and accumulated knowledge preserved in\nthe previous memory buffer. Moreover, to avoid the computational overhead and\noverfitting risks associated with parameterizing the entire buffer during\ndistillation, we introduce a lightweight distillation module that can achieve\nglobal information distillation solely by generating learnable soft labels for\nthe memory buffer data. Extensive experiments show that, our method can achieve\ncompetitive results and effectively mitigates forgetting across various\ndatasets. The source code will be publicly available.",
      "url": "http://arxiv.org/abs/2505.20135v1",
      "published_time_eastern_timestamp": 1748273830.0
    },
    {
      "title": "TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on\n  Dense Dynamic Videos",
      "summary": "Videos are unique in their integration of temporal elements, including\ncamera, scene, action, and attribute, along with their dynamic relationships\nover time. However, existing benchmarks for video understanding often treat\nthese properties separately or narrowly focus on specific aspects, overlooking\nthe holistic nature of video content. To address this, we introduce TUNA, a\ntemporal-oriented benchmark for fine-grained understanding on dense dynamic\nvideos, with two complementary tasks: captioning and QA. Our TUNA features\ndiverse video scenarios and dynamics, assisted by interpretable and robust\nevaluation criteria. We evaluate several leading models on our benchmark,\nproviding fine-grained performance assessments across various dimensions. This\nevaluation reveals key challenges in video temporal understanding, such as\nlimited action description, inadequate multi-subject understanding, and\ninsensitivity to camera motion, offering valuable insights for improving video\nunderstanding models. The data and code are available at\nhttps://friedrichor.github.io/projects/TUNA.",
      "url": "http://arxiv.org/abs/2505.20124v1",
      "published_time_eastern_timestamp": 1748273046.0
    },
    {
      "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models",
      "summary": "Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon.",
      "url": "http://arxiv.org/abs/2505.20100v1",
      "published_time_eastern_timestamp": 1748272117.0
    },
    {
      "title": "Transformer in Protein: A Survey",
      "summary": "As protein informatics advances rapidly, the demand for enhanced predictive\naccuracy, structural analysis, and functional understanding has intensified.\nTransformer models, as powerful deep learning architectures, have demonstrated\nunprecedented potential in addressing diverse challenges across protein\nresearch. However, a comprehensive review of Transformer applications in this\nfield remains lacking. This paper bridges this gap by surveying over 100\nstudies, offering an in-depth analysis of practical implementations and\nresearch progress of Transformers in protein-related tasks. Our review\nsystematically covers critical domains, including protein structure prediction,\nfunction prediction, protein-protein interaction analysis, functional\nannotation, and drug discovery/target identification. To contextualize these\nadvancements across various protein domains, we adopt a domain-oriented\nclassification system. We first introduce foundational concepts: the\nTransformer architecture and attention mechanisms, categorize Transformer\nvariants tailored for protein science, and summarize essential protein\nknowledge. For each research domain, we outline its objectives and background,\ncritically evaluate prior methods and their limitations, and highlight\ntransformative contributions enabled by Transformer models. We also curate and\nsummarize pivotal datasets and open-source code resources to facilitate\nreproducibility and benchmarking. Finally, we discuss persistent challenges in\napplying Transformers to protein informatics and propose future research\ndirections. This review aims to provide a consolidated foundation for the\nsynergistic integration of Transformer and protein informatics, fostering\nfurther innovation and expanded applications in the field.",
      "url": "http://arxiv.org/abs/2505.20098v1",
      "published_time_eastern_timestamp": 1748272098.0
    },
    {
      "title": "Safety Through Reasoning: An Empirical Study of Reasoning Guardrail\n  Models",
      "summary": "Reasoning-based language models have demonstrated strong performance across\nvarious domains, with the most notable gains seen in mathematical and coding\ntasks. Recent research has shown that reasoning also offers significant\nbenefits for LLM safety and guardrail applications. In this work, we conduct a\ncomprehensive analysis of training reasoning-based guardrail models for content\nmoderation, with an emphasis on generalization to custom safety policies at\ninference time. Our study focuses on two key dimensions: data efficiency and\ninference efficiency. On the data front, we find that reasoning-based models\nexhibit strong sample efficiency, achieving competitive performance with\nsignificantly fewer training examples than their non-reasoning counterparts.\nThis unlocks the potential to repurpose the remaining data for mining\nhigh-value, difficult samples that further enhance model performance. On the\ninference side, we evaluate practical trade-offs by introducing reasoning\nbudgets, examining the impact of reasoning length on latency and accuracy, and\nexploring dual-mode training to allow runtime control over reasoning behavior.\nOur findings will provide practical insights for researchers and developers to\neffectively and efficiently train and deploy reasoning-based guardrails models\nin real-world systems.",
      "url": "http://arxiv.org/abs/2505.20087v1",
      "published_time_eastern_timestamp": 1748271697.0
    },
    {
      "title": "Inference-time Alignment in Continuous Space",
      "summary": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA",
      "url": "http://arxiv.org/abs/2505.20081v1",
      "published_time_eastern_timestamp": 1748271513.0
    },
    {
      "title": "Grokking ExPLAIND: Unifying Model, Data, and Training Attribution to\n  Study Model Behavior",
      "summary": "Post-hoc interpretability methods typically attribute a model's behavior to\nits components, data, or training trajectory in isolation. This leads to\nexplanations that lack a unified view and may miss key interactions. While\ncombining existing methods or applying them at different training stages offers\nbroader insights, these approaches usually lack theoretical support. In this\nwork, we present ExPLAIND, a unified framework that integrates all three\nperspectives. First, we generalize recent work on gradient path kernels, which\nreformulate models trained by gradient descent as a kernel machine, to more\nrealistic training settings. Empirically, we find that both a CNN and a\nTransformer model are replicated accurately by this reformulation. Second, we\nderive novel parameter- and step-wise influence scores from the kernel feature\nmaps. We show their effectiveness in parameter pruning that is comparable to\nexisting methods, reinforcing their value for model component attribution.\nFinally, jointly interpreting model components and data over the training\nprocess, we leverage ExPLAIND to analyze a Transformer that exhibits Grokking.\nAmong other things, our findings support previously proposed stages of\nGrokking, while refining the final phase as one of alignment of input\nembeddings and final layers around a representation pipeline learned after the\nmemorization phase. Overall, ExPLAIND provides a theoretically grounded,\nunified framework to interpret model behavior and training dynamics.",
      "url": "http://arxiv.org/abs/2505.20076v1",
      "published_time_eastern_timestamp": 1748271191.0
    },
    {
      "title": "Incentivizing Reasoning from Weak Supervision",
      "summary": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR.",
      "url": "http://arxiv.org/abs/2505.20072v1",
      "published_time_eastern_timestamp": 1748271089.0
    },
    {
      "title": "LPCM: Learning-based Predictive Coding for LiDAR Point Cloud Compression",
      "summary": "Since the data volume of LiDAR point clouds is very huge, efficient\ncompression is necessary to reduce their storage and transmission costs.\nHowever, existing learning-based compression methods do not exploit the\ninherent angular resolution of LiDAR and ignore the significant differences in\nthe correlation of geometry information at different bitrates. The predictive\ngeometry coding method in the geometry-based point cloud compression (G-PCC)\nstandard uses the inherent angular resolution to predict the azimuth angles.\nHowever, it only models a simple linear relationship between the azimuth angles\nof neighboring points. Moreover, it does not optimize the quantization\nparameters for residuals on each coordinate axis in the spherical coordinate\nsystem. We propose a learning-based predictive coding method (LPCM) with both\nhigh-bitrate and low-bitrate coding modes. LPCM converts point clouds into\npredictive trees using the spherical coordinate system. In high-bitrate coding\nmode, we use a lightweight Long-Short-Term Memory-based predictive (LSTM-P)\nmodule that captures long-term geometry correlations between different\ncoordinates to efficiently predict and compress the elevation angles. In\nlow-bitrate coding mode, where geometry correlation degrades, we introduce a\nvariational radius compression (VRC) module to directly compress the point\nradii. Then, we analyze why the quantization of spherical coordinates differs\nfrom that of Cartesian coordinates and propose a differential evolution\n(DE)-based quantization parameter selection method, which improves\nrate-distortion performance without increasing coding time. Experimental\nresults on the LiDAR benchmark \\textit{SemanticKITTI} and the MPEG-specified\n\\textit{Ford} datasets show that LPCM outperforms G-PCC and other\nlearning-based methods.",
      "url": "http://arxiv.org/abs/2505.20059v1",
      "published_time_eastern_timestamp": 1748270729.0
    },
    {
      "title": "Data-Free Class-Incremental Gesture Recognition with Prototype-Guided\n  Pseudo Feature Replay",
      "summary": "Gesture recognition is an important research area in the field of computer\nvision. Most gesture recognition efforts focus on close-set scenarios, thereby\nlimiting the capacity to effectively handle unseen or novel gestures. We aim to\naddress class-incremental gesture recognition, which entails the ability to\naccommodate new and previously unseen gestures over time. Specifically, we\nintroduce a Prototype-Guided Pseudo Feature Replay (PGPFR) framework for\ndata-free class-incremental gesture recognition. This framework comprises four\ncomponents: Pseudo Feature Generation with Batch Prototypes (PFGBP),\nVariational Prototype Replay (VPR) for old classes, Truncated Cross-Entropy\n(TCE) for new classes, and Continual Classifier Re-Training (CCRT). To tackle\nthe issue of catastrophic forgetting, the PFGBP dynamically generates a\ndiversity of pseudo features in an online manner, leveraging class prototypes\nof old classes along with batch class prototypes of new classes. Furthermore,\nthe VPR enforces consistency between the classifier's weights and the\nprototypes of old classes, leveraging class prototypes and covariance matrices\nto enhance robustness and generalization capabilities. The TCE mitigates the\nimpact of domain differences of the classifier caused by pseudo features.\nFinally, the CCRT training strategy is designed to prevent overfitting to new\nclasses and ensure the stability of features extracted from old classes.\nExtensive experiments conducted on two widely used gesture recognition\ndatasets, namely SHREC 2017 3D and EgoGesture 3D, demonstrate that our approach\noutperforms existing state-of-the-art methods by 11.8\\% and 12.8\\% in terms of\nmean global accuracy, respectively. The code is available on\nhttps://github.com/sunao-101/PGPFR-3/.",
      "url": "http://arxiv.org/abs/2505.20049v1",
      "published_time_eastern_timestamp": 1748270255.0
    },
    {
      "title": "ReasonPlan: Unified Scene Prediction and Decision Reasoning for\n  Closed-loop Autonomous Driving",
      "summary": "Due to the powerful vision-language reasoning and generalization abilities,\nmultimodal large language models (MLLMs) have garnered significant attention in\nthe field of end-to-end (E2E) autonomous driving. However, their application to\nclosed-loop systems remains underexplored, and current MLLM-based methods have\nnot shown clear superiority to mainstream E2E imitation learning approaches. In\nthis work, we propose ReasonPlan, a novel MLLM fine-tuning framework designed\nfor closed-loop driving through holistic reasoning with a self-supervised Next\nScene Prediction task and supervised Decision Chain-of-Thought process. This\ndual mechanism encourages the model to align visual representations with\nactionable driving context, while promoting interpretable and causally grounded\ndecision making. We curate a planning-oriented decision reasoning dataset,\nnamely PDR, comprising 210k diverse and high-quality samples. Our method\noutperforms the mainstream E2E imitation learning method by a large margin of\n19% L2 and 16.1 driving score on Bench2Drive benchmark. Furthermore, ReasonPlan\ndemonstrates strong zero-shot generalization on unseen DOS benchmark,\nhighlighting its adaptability in handling zero-shot corner cases. Code and\ndataset will be found in https://github.com/Liuxueyi/ReasonPlan.",
      "url": "http://arxiv.org/abs/2505.20024v1",
      "published_time_eastern_timestamp": 1748268758.0
    },
    {
      "title": "On the class of coding optimality of human languages and the origins of\n  Zipf's law",
      "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are separated linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Whithin that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are separated by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. Our findings provide\nsupport for the hypothesis that Zipf's law originates from compression.",
      "url": "http://arxiv.org/abs/2505.20015v1",
      "published_time_eastern_timestamp": 1748268345.0
    },
    {
      "title": "TabPFN: One Model to Rule Them All?",
      "summary": "Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a\ntransformer-based deep learning model for regression and classification on\ntabular data, which they claim \"outperforms all previous methods on datasets\nwith up to 10,000 samples by a wide margin, using substantially less training\ntime.\" Furthermore, they have called TabPFN a \"foundation model\" for tabular\ndata, as it can support \"data generation, density estimation, learning reusable\nembeddings and fine-tuning\". If these statements are well-supported, TabPFN may\nhave the potential to supersede existing modeling approaches on a wide range of\nstatistical tasks, mirroring a similar revolution in other areas of artificial\nintelligence that began with the advent of large language models. In this\npaper, we provide a tailored explanation of how TabPFN works for a statistics\naudience, by emphasizing its interpretation as approximate Bayesian inference.\nWe also provide more evidence of TabPFN's \"foundation model\" capabilities: We\nshow that an out-of-the-box application of TabPFN vastly outperforms\nspecialized state-of-the-art methods for semi-supervised parameter estimation,\nprediction under covariate shift, and heterogeneous treatment effect\nestimation. We further show that TabPFN can outperform LASSO at sparse\nregression and can break a robustness-efficiency trade-off in classification.\nAll experiments can be reproduced using the code provided at\nhttps://github.com/qinglong-tian/tabpfn_study\n(https://github.com/qinglong-tian/tabpfn_study).",
      "url": "http://arxiv.org/abs/2505.20003v1",
      "published_time_eastern_timestamp": 1748267729.0
    },
    {
      "title": "Automatic Metadata Extraction for Text-to-SQL",
      "summary": "Large Language Models (LLMs) have recently become sophisticated enough to\nautomate many tasks ranging from pattern finding to writing assistance to code\ngeneration. In this paper, we examine text-to-SQL generation. We have observed\nfrom decades of experience that the most difficult part of query development\nlies in understanding the database contents. These experiences inform the\ndirection of our research.\n  Text-to-SQL benchmarks such as SPIDER and Bird contain extensive metadata\nthat is generally not available in practice. Human-generated metadata requires\nthe use of expensive Subject Matter Experts (SMEs), who are often not fully\naware of many aspects of their databases. In this paper, we explore techniques\nfor automatic metadata extraction to enable text-to-SQL generation.\n  Ee explore the use of two standard and one newer metadata extraction\ntechniques: profiling, query log analysis, and SQL-to text generation using an\nLLM. We use BIRD benchmark [JHQY+23] to evaluate the effectiveness of these\ntechniques. BIRD does not provide query logs on their test database, so we\nprepared a submission that uses profiling alone, and does not use any specially\ntuned model (we used GPT-4o). From Sept 1 to Sept 23, 2024, and Nov 11 through\nNov 23, 2024 we achieved the highest score both with and without using the\n\"oracle\" information provided with the question set. We regained the number 1\nspot on Mar 11, 2025, and are still at #1 at the time of the writing (May,\n2025).",
      "url": "http://arxiv.org/abs/2505.19988v1",
      "published_time_eastern_timestamp": 1748267023.0
    },
    {
      "title": "Adaptive Location Hierarchy Learning for Long-Tailed Mobility Prediction",
      "summary": "Human mobility prediction is crucial for applications ranging from\nlocation-based recommendations to urban planning, which aims to forecast users'\nnext location visits based on historical trajectories. Despite the severe\nlong-tailed distribution of locations, the problem of long-tailed mobility\nprediction remains largely underexplored. Existing long-tailed learning methods\nprimarily focus on rebalancing the skewed distribution at the data, model, or\nclass level, neglecting to exploit the spatiotemporal semantics of locations.\nTo address this gap, we propose the first plug-and-play framework for\nlong-tailed mobility prediction in an exploitation and exploration manner,\nnamed \\textbf{A}daptive \\textbf{LO}cation \\textbf{H}ier\\textbf{A}rchy learning\n(ALOHA). First, we construct city-tailored location hierarchy based on Large\nLanguage Models (LLMs) by exploiting Maslow's theory of human motivation to\ndesign Chain-of-Thought (CoT) prompts that captures spatiotemporal semantics.\nSecond, we optimize the location hierarchy predictions by Gumbel disturbance\nand node-wise adaptive weights within the hierarchical tree structure.\nExperiments on state-of-the-art models across six datasets demonstrate the\nframework's consistent effectiveness and generalizability, which strikes a well\nbalance between head and tail locations. Weight analysis and ablation studies\nreveal the optimization differences of each component for head and tail\nlocations. Furthermore, in-depth analyses of hierarchical distance and case\nstudy demonstrate the effective semantic guidance from the location hierarchy.\nOur code will be made publicly available.",
      "url": "http://arxiv.org/abs/2505.19965v1",
      "published_time_eastern_timestamp": 1748265995.0
    },
    {
      "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for\n  Large Language Models",
      "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.",
      "url": "http://arxiv.org/abs/2505.19959v1",
      "published_time_eastern_timestamp": 1748265678.0
    },
    {
      "title": "DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep\n  Contextual Schema Link Graph",
      "summary": "Text-to-SQL, which translates a natural language question into an SQL query,\nhas advanced with in-context learning of Large Language Models (LLMs). However,\nexisting methods show little improvement in performance compared to randomly\nchosen demonstrations, and significant performance drops when smaller LLMs\n(e.g., Llama 3.1-8B) are used. This indicates that these methods heavily rely\non the intrinsic capabilities of hyper-scaled LLMs, rather than effectively\nretrieving useful demonstrations. In this paper, we propose a novel approach\nfor effectively retrieving demonstrations and generating SQL queries. We\nconstruct a Deep Contextual Schema Link Graph, which contains key information\nand semantic relationship between a question and its database schema items.\nThis graph-based structure enables effective representation of Text-to-SQL\nsamples and retrieval of useful demonstrations for in-context learning.\nExperimental results on the Spider benchmark demonstrate the effectiveness of\nour approach, showing consistent improvements in SQL generation performance and\nefficiency across both hyper-scaled LLMs and small LLMs. Our code will be\nreleased.",
      "url": "http://arxiv.org/abs/2505.19956v1",
      "published_time_eastern_timestamp": 1748265550.0
    },
    {
      "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
      "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
      "url": "http://arxiv.org/abs/2505.19955v1",
      "published_time_eastern_timestamp": 1748265517.0
    }
  ]
}
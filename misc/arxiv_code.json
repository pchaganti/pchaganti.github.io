{
  "last_updated": "2025-07-16T14:18:10.529538-04:00",
  "papers": [
    {
      "title": "Streaming 4D Visual Geometry Transformer",
      "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
      "url": "http://arxiv.org/abs/2507.11539v1",
      "published_time_eastern_timestamp": 1752602397.0
    },
    {
      "title": "Understanding Quantum Information and Computation",
      "summary": "This is a course on the theory of quantum computing. It consists of 16\nlessons, each with a video and written component, covering the basics of\nquantum information, quantum algorithms (including query algorithms, Shor's\nalgorithm for integer factorization, and Grover's algorithm), the general\nformulation of quantum information (including density matrices, quantum\nchannels, and general measurements), and quantum error correction (including\nthe basics, the stabilizer formalism, CSS codes, the toric code, and\nfault-tolerant quantum computation).",
      "url": "http://arxiv.org/abs/2507.11536v1",
      "published_time_eastern_timestamp": 1752602345.0
    },
    {
      "title": "Sharp Error-Rate Transitions in Quantum QC-LDPC Codes under Joint BP\n  Decoding",
      "summary": "In this study, we report that quantum quasi-cyclic low-density parity-check\ncodes decoded via joint belief propagation (BP) exhibit steep error-rate\ncurves, despite the presence of error floors. To the best of our knowledge,\nthis is the first observation of such threshold-like behavior for quantum codes\nwith non-vanishing coding rate, excluding those decoded with non-binary BP\ndecoders. Moreover, we find that dominant error events contributing to the\nerror floor typically involve only a small number of bits. These findings\nsuggest that the error floor is caused by trapping sets -- specific subgraph\nstructures in the Tanner graph -- and indicate that identifying and avoiding\nsuch structures may lead to further reduction of the error floor.",
      "url": "http://arxiv.org/abs/2507.11534v1",
      "published_time_eastern_timestamp": 1752602313.0
    },
    {
      "title": "CharaConsist: Fine-Grained Consistent Character Generation",
      "summary": "In text-to-image generation, producing a series of consistent contents that\npreserve the same identity is highly valuable for real-world applications.\nAlthough a few works have explored training-free methods to enhance the\nconsistency of generated subjects, we observe that they suffer from the\nfollowing problems. First, they fail to maintain consistent background details,\nwhich limits their applicability. Furthermore, when the foreground character\nundergoes large motion variations, inconsistencies in identity and clothing\ndetails become evident. To address these problems, we propose CharaConsist,\nwhich employs point-tracking attention and adaptive token merge along with\ndecoupled control of the foreground and background. CharaConsist enables\nfine-grained consistency for both foreground and background, supporting the\ngeneration of one character in continuous shots within a fixed scene or in\ndiscrete shots across different scenes. Moreover, CharaConsist is the first\nconsistent generation method tailored for text-to-image DiT model. Its ability\nto maintain fine-grained consistency, combined with the larger capacity of\nlatest base model, enables it to produce high-quality visual outputs,\nbroadening its applicability to a wider range of real-world scenarios. The\nsource code has been released at https://github.com/Murray-Wang/CharaConsist",
      "url": "http://arxiv.org/abs/2507.11533v1",
      "published_time_eastern_timestamp": 1752602288.0
    },
    {
      "title": "Precision Spatio-Temporal Feature Fusion for Robust Remote Sensing\n  Change Detection",
      "summary": "Remote sensing change detection is vital for monitoring environmental and\nurban transformations but faces challenges like manual feature extraction and\nsensitivity to noise. Traditional methods and early deep learning models, such\nas convolutional neural networks (CNNs), struggle to capture long-range\ndependencies and global context essential for accurate change detection in\ncomplex scenes. While Transformer-based models mitigate these issues, their\ncomputational complexity limits their applicability in high-resolution remote\nsensing. Building upon ChangeMamba architecture, which leverages state space\nmodels for efficient global context modeling, this paper proposes precision\nfusion blocks to capture channel-wise temporal variations and per-pixel\ndifferences for fine-grained change detection. An enhanced decoder pipeline,\nincorporating lightweight channel reduction mechanisms, preserves local details\nwith minimal computational cost. Additionally, an optimized loss function\ncombining Cross Entropy, Dice and Lovasz objectives addresses class imbalance\nand boosts Intersection-over-Union (IoU). Evaluations on SYSU-CD, LEVIR-CD+,\nand WHU-CD datasets demonstrate superior precision, recall, F1 score, IoU, and\noverall accuracy compared to state-of-the-art methods, highlighting the\napproach's robustness for remote sensing change detection. For complete\ntransparency, the codes and pretrained models are accessible at\nhttps://github.com/Buddhi19/MambaCD.git",
      "url": "http://arxiv.org/abs/2507.11523v1",
      "published_time_eastern_timestamp": 1752601819.0
    },
    {
      "title": "TOPJoin: A Context-Aware Multi-Criteria Approach for Joinable Column\n  Search",
      "summary": "One of the major challenges in enterprise data analysis is the task of\nfinding joinable tables that are conceptually related and provide meaningful\ninsights. Traditionally, joinable tables have been discovered through a search\nfor similar columns, where two columns are considered similar syntactically if\nthere is a set overlap or they are considered similar semantically if either\nthe column embeddings or value embeddings are closer in the embedding space.\nHowever, for enterprise data lakes, column similarity is not sufficient to\nidentify joinable columns and tables. The context of the query column is\nimportant. Hence, in this work, we first define context-aware column\njoinability. Then we propose a multi-criteria approach, called TOPJoin, for\njoinable column search. We evaluate TOPJoin against existing join search\nbaselines over one academic and one real-world join search benchmark. Through\nexperiments, we find that TOPJoin performs better on both benchmarks than the\nbaselines.",
      "url": "http://arxiv.org/abs/2507.11505v1",
      "published_time_eastern_timestamp": 1752600056.0
    },
    {
      "title": "Enhancing the Clique Local Decoder to Correct Length-2 Space Errors in\n  the Surface Code",
      "summary": "The growing demand for fault-tolerant quantum computing drives the need for\nefficient, scalable Quantum Error Correction (QEC) strategies. Conventional\ndecoders designed for worst-case error scenarios incur significant overhead,\nprompting the development of local decoders, that leverage the sparse and often\ntrivial nature of many quantum errors, to support the conventional decoders.\nThe previously proposed Clique decoder addresses this by handling isolated,\nlength-1 space and time errors within the cryogenic environment with minimal\nhardware costs, thereby mitigating I/O bandwidth constraints between cryogenic\nquantum systems and room-temperature processors. Building on this foundation,\nwe propose Clique_L2 that extends the Clique-based approach by relaxing some\noriginal constraints and incorporating additional low-cost logic to also\ncorrect length-2 error chains in space, which become non-trivial occurrences at\nhigher physical error rates and code distances. This enhanced capability not\nonly further reduces out-of-the-fridge data transmission but also adapts more\neffectively to clustered errors observed under a variety of noise models.\nSpecifically, under data-qubit-only errors and uniformly random noise,\nClique_L2 achieves up to 8.95x decoding bandwidth reduction over the original\nClique (or Clique_L1) decoder, especially beneficial at higher code distances.\nWhen clustered errors and longer error chains are more likely to occur,\nClique_L2 achieves up to 18.3x decoding bandwidth reduction over Clique_L1,\nachieving substantial benefits across a wide range of physical qubit error\nrates.",
      "url": "http://arxiv.org/abs/2507.11481v1",
      "published_time_eastern_timestamp": 1752598156.0
    },
    {
      "title": "Modeling Code: Is Text All You Need?",
      "summary": "Code LLMs have become extremely popular recently for modeling source code\nacross a variety of tasks, such as generation, translation, and summarization.\nHowever, transformer-based models are limited in their capabilities to reason\nthrough structured, analytical properties of code, such as control and data\nflow. Previous work has explored the modeling of these properties with\nstructured data and graph neural networks. However, these approaches lack the\ngenerative capabilities and scale of modern LLMs. In this work, we introduce a\nnovel approach to combine the strengths of modeling both code as text and more\nstructured forms.",
      "url": "http://arxiv.org/abs/2507.11467v1",
      "published_time_eastern_timestamp": 1752597552.0
    },
    {
      "title": "Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror\n  Descent",
      "summary": "Deep Equilibrium Models (DEQs) are implicit neural networks with fixed\npoints, which have recently gained attention for learning image regularization\nfunctionals, particularly in settings involving Gaussian fidelities, where\nassumptions on the forward operator ensure contractiveness of standard\n(proximal) Gradient Descent operators. In this work, we extend the application\nof DEQs to Poisson inverse problems, where the data fidelity term is more\nappropriately modeled by the Kullback-Leibler divergence. To this end, we\nintroduce a novel DEQ formulation based on Mirror Descent defined in terms of a\ntailored non-Euclidean geometry that naturally adapts with the structure of the\ndata term. This enables the learning of neural regularizers within a principled\ntraining framework. We derive sufficient conditions to guarantee the\nconvergence of the learned reconstruction scheme and propose computational\nstrategies that enable both efficient training and fully parameter-free\ninference. Numerical experiments show that our method outperforms traditional\nmodel-based approaches and it is comparable to the performance of Bregman\nPlug-and-Play methods, while mitigating their typical drawbacks - namely,\nsensitivity to initialization and careful tuning of hyperparameters. The code\nis publicly available at https://github.com/christiandaniele/DEQ-MD.",
      "url": "http://arxiv.org/abs/2507.11461v1",
      "published_time_eastern_timestamp": 1752597181.0
    },
    {
      "title": "Implementing Adaptations for Vision AutoRegressive Model",
      "summary": "Vision AutoRegressive model (VAR) was recently introduced as an alternative\nto Diffusion Models (DMs) in image generation domain. In this work we focus on\nits adaptations, which aim to fine-tune pre-trained models to perform specific\ndownstream tasks, like medical data generation. While for DMs there exist many\ntechniques, adaptations for VAR remain underexplored. Similarly, differentially\nprivate (DP) adaptations-ones that aim to preserve privacy of the adaptation\ndata-have been extensively studied for DMs, while VAR lacks such solutions. In\nour work, we implement and benchmark many strategies for VAR, and compare them\nto state-of-the-art DM adaptation strategies. We observe that VAR outperforms\nDMs for non-DP adaptations, however, the performance of DP suffers, which\nnecessitates further research in private adaptations for VAR. Code is available\nat https://github.com/sprintml/finetuning_var_dp.",
      "url": "http://arxiv.org/abs/2507.11441v1",
      "published_time_eastern_timestamp": 1752595530.0
    },
    {
      "title": "U-RWKV: Lightweight medical image segmentation with direction-adaptive\n  RWKV",
      "summary": "Achieving equity in healthcare accessibility requires lightweight yet\nhigh-performance solutions for medical image segmentation, particularly in\nresource-limited settings. Existing methods like U-Net and its variants often\nsuffer from limited global Effective Receptive Fields (ERFs), hindering their\nability to capture long-range dependencies. To address this, we propose U-RWKV,\na novel framework leveraging the Recurrent Weighted Key-Value(RWKV)\narchitecture, which achieves efficient long-range modeling at O(N)\ncomputational cost. The framework introduces two key innovations: the\nDirection-Adaptive RWKV Module(DARM) and the Stage-Adaptive\nSqueeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan\nmechanisms to aggregate contextual cues across images, mitigating directional\nbias while preserving global context and maintaining high computational\nefficiency. SASE dynamically adapts its architecture to different feature\nextraction stages, balancing high-resolution detail preservation and semantic\nrelationship capture. Experiments demonstrate that U-RWKV achieves\nstate-of-the-art segmentation performance with high computational efficiency,\noffering a practical solution for democratizing advanced medical imaging\ntechnologies in resource-constrained environments. The code is available at\nhttps://github.com/hbyecoding/U-RWKV.",
      "url": "http://arxiv.org/abs/2507.11415v1",
      "published_time_eastern_timestamp": 1752594017.0
    },
    {
      "title": "Sparse Regression Codes exploit Multi-User Diversity without CSI",
      "summary": "We study sparse regression codes (SPARC) for multiple access channels with\nmultiple receive antennas, in non-coherent flat fading channels. We propose a\nnovel practical decoder, referred to as maximum likelihood matching pursuit\n(MLMP), which greedily finds the support of the codewords of users with partial\nmaximum likelihood metrics. As opposed to the conventional\nsuccessive-cancellation based greedy algorithms, MLMP works as a\nsuccessive-combining energy detector. We also propose MLMP modifications to\nimprove the performance at high code rates. Our studies in short block lengths\nshow that, even without any channel state information, SPARC with MLMP decoder\nachieves multi-user diversity in some scenarios, giving better error\nperformance with multiple users than that of the corresponding single-user\ncase. We also show that SPARC with MLMP performs better than conventional\nsparse recovery algorithms and pilot-aided transmissions with polar codes.",
      "url": "http://arxiv.org/abs/2507.11383v1",
      "published_time_eastern_timestamp": 1752591111.0
    },
    {
      "title": "The miniJPAS survey quasar selection V: combined algorithm",
      "summary": "Aims. Quasar catalogues from narrow-band photometric data are used in a\nvariety of applications, including targeting for spectroscopic follow-up,\nmeasurements of supermassive black hole masses, or Baryon Acoustic\nOscillations. Here, we present the final quasar catalogue, including redshift\nestimates, from the miniJPAS Data Release constructed using several flavours of\nmachine-learning algorithms. Methods. In this work, we use a machine learning\nalgorithm to classify quasars, optimally combining the output of 8 individual\nalgorithms. We assess the relative importance of the different classifiers. We\ninclude results from 3 different redshift estimators to also provide improved\nphotometric redshifts. We compare our final catalogue against both simulated\ndata and real spectroscopic data. Our main comparison metric is the $f_1$\nscore, which balances the catalogue purity and completeness. Results. We\nevaluate the performance of the combined algorithm using synthetic data. In\nthis scenario, the combined algorithm outperforms the rest of the codes,\nreaching $f_1=0.88$ and $f_1=0.79$ for high- and low-z quasars (with $z\\geq2.1$\nand $z<2.1$, respectively) down to magnitude $r=23.5$. We further evaluate its\nperformance against real spectroscopic data, finding different performances. We\nconclude that our simulated data is not realistic enough and that a new version\nof the mocks would improve the performance. Our redshift estimates on mocks\nsuggest a typical uncertainty of $\\sigma_{\\rm NMAD} =0.11$, which, according to\nour results with real data, could be significantly smaller (as low as\n$\\sigma_{\\rm NMAD}=0.02$). We note that the data sample is still not large\nenough for a full statistical consideration.",
      "url": "http://arxiv.org/abs/2507.11380v1",
      "published_time_eastern_timestamp": 1752591004.0
    },
    {
      "title": "Attributes Shape the Embedding Space of Face Recognition Models",
      "summary": "Face Recognition (FR) tasks have made significant progress with the advent of\nDeep Neural Networks, particularly through margin-based triplet losses that\nembed facial images into high-dimensional feature spaces. During training,\nthese contrastive losses focus exclusively on identity information as labels.\nHowever, we observe a multiscale geometric structure emerging in the embedding\nspace, influenced by interpretable facial (e.g., hair color) and image\nattributes (e.g., contrast). We propose a geometric approach to describe the\ndependence or invariance of FR models to these attributes and introduce a\nphysics-inspired alignment metric. We evaluate the proposed metric on\ncontrolled, simplified models and widely used FR models fine-tuned with\nsynthetic data for targeted attribute augmentation. Our findings reveal that\nthe models exhibit varying degrees of invariance across different attributes,\nproviding insight into their strengths and weaknesses and enabling deeper\ninterpretability. Code available here:\nhttps://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs",
      "url": "http://arxiv.org/abs/2507.11372v1",
      "published_time_eastern_timestamp": 1752590679.0
    },
    {
      "title": "Security Debt in Practice: Nuanced Insights from Practitioners",
      "summary": "With the increasing reliance on software and automation nowadays, tight\ndeadlines, limited resources, and prioritization of functionality over security\ncan lead to insecure coding practices. When not handled properly, these\nconstraints cause unaddressed security vulnerabilities to accumulate over time,\nforming Security Debts (SDs). Despite their critical importance, there is\nlimited empirical evidence on how software practitioners perceive, manage, and\ncommunicate SDs in real-world settings. In this paper, we present a qualitative\nempirical study based on semi-structured interviews with 22 software\npractitioners across various roles, organizations, and countries. We address\nfour research questions: i) we assess software practitioners' knowledge of SDs\nand awareness of associated security risks, ii) we investigate their behavior\ntowards SDs, iii) we explore common tools and strategies used to mitigate SDs,\nand iv) we analyze how security risks are communicated within teams and to\ndecision makers. We observe variations in how practitioners perceive and manage\nSDs, with some prioritizing delivery speed over security, while others\nconsistently maintain security as a priority. Our findings emphasize the need\nfor stronger integration of security practices across the Software Development\nLife Cycle (SDLC), more consistent use of mitigation strategies, better\nbalancing of deadlines, resources, and security-related tasks, with attention\nto the Confidentiality, Integrity, and Availability (CIA) triad.",
      "url": "http://arxiv.org/abs/2507.11362v1",
      "published_time_eastern_timestamp": 1752589708.0
    },
    {
      "title": "Adaptive Robust Optimization for European Electricity System Planning\n  Considering Regional Dunkelflaute Events",
      "summary": "This study develops a capacity expansion model for a fully decarbonized\nEuropean electricity system using an Adaptive Robust Optimization (ARO)\nframework. The model endogenously identifies the worst regional Dunkelflaute\nevents, prolonged periods of low wind and solar availability, and incorporates\nmultiple extreme weather realizations within a single optimization run. Results\nshow that system costs rise nonlinearly with the geographic extent of these\nevents: a single worst-case regional disruption increases costs by 9%, but\nbroader disruptions across multiple regions lead to much sharper increases, up\nto 51%. As Dunkelflaute conditions extend across most of Europe, additional\ncost impacts level off, with a maximum increase of 71%. The optimal technology\nmix evolves with the severity of weather stress: while renewables, batteries,\nand interregional transmission are sufficient to manage localized events,\nlarge-scale disruptions require long-term hydrogen storage and load shedding to\nmaintain system resilience. Central European regions, especially Germany and\nFrance, emerge as systemic bottlenecks, while peripheral regions bear the cost\nof compensatory overbuilding. These findings underscore the need for a\ncoordinated European policy strategy that goes beyond national planning to\nsupport cross-border infrastructure investment, scale up flexible technologies\nsuch as long-duration storage, and promote a geographically balanced deployment\nof renewables to mitigate systemic risks associated with Dunkelflaute events.",
      "url": "http://arxiv.org/abs/2507.11361v1",
      "published_time_eastern_timestamp": 1752589703.0
    },
    {
      "title": "Caveats about measuring carbon abundances in stars using the CH band",
      "summary": "Deriving accurate carbon abundance estimates for a wide variety of stars is\nstill complex due to the difficulties in properly measuring it from atomic and\nmolecular lines. The aim of this paper is to analyse the carbon abundance\ndetermination for the large empirical X-shooter Spectral Library (XSL),\ncommonly used as a benchmark for the development of stellar population models.\nThe analysis was performed over strong molecular CH bands in the G-band region.\nWe used the GAUGUIN automated spectrum synthesis code, and adopted two\ndifferent grids of reference synthetic spectra separately, each with the same\n[C/Fe] abundance coverage. We carried out a detailed comparison between both\ngrids to evaluate the accuracy and the model dependence of the measured [C/Fe]\nabundances. We obtained a large and precise unbiased [C/Fe] abundance catalogue\nfrom both theoretical grids, well distributed in the Hertzsprung-Russell (HR)\ndiagram and with no trend with the stellar parameters. We also measured\ncompatible values from each independent CH band, with a high-quality [C/Fe]\nabundance estimate for both dwarfs and giants indistinctly. We observed a\ndispersed flat trend around [C/Fe] = 0.0 dex all along the metallicity regime,\nin agreement with some literature studies. However, we reported variations up\nto 0.8 dex in the [C/Fe] composition of the star depending on the adopted grid.\nWe did not find such differences in the $\\alpha$-element measurements. This\nbehaviour implies a strong model dependence in the [C/Fe] abundance estimate.\nPotential sources of error could be associated with the use of spectral\nsynthesis methods to derive stellar carbon abundances in the CH4300A band.\nIntrinsic small differences in the synthetic models over this crowded and\nblended region may induce a large disparity in the precise abundance estimate\nfor any stellar type, leading to inaccurate carbon measurements without being\nnoticed",
      "url": "http://arxiv.org/abs/2507.11351v1",
      "published_time_eastern_timestamp": 1752589411.0
    },
    {
      "title": "RefModel: Detecting Refactorings using Foundation Models",
      "summary": "Refactoring is a common software engineering practice that improves code\nquality without altering program behavior. Although tools like ReExtractor+,\nRefactoringMiner, and RefDiff have been developed to detect refactorings\nautomatically, they rely on complex rule definitions and static analysis,\nmaking them difficult to extend and generalize to other programming languages.\nIn this paper, we investigate the viability of using foundation models for\nrefactoring detection, implemented in a tool named RefModel. We evaluate\nPhi4-14B, and Claude 3.5 Sonnet on a dataset of 858 single-operation\ntransformations applied to artificially generated Java programs, covering\nwidely-used refactoring types. We also extend our evaluation by including\nGemini 2.5 Pro and o4-mini-high, assessing their performance on 44 real-world\nrefactorings extracted from four open-source projects. These models are\ncompared against RefactoringMiner, RefDiff, and ReExtractor+. RefModel is\ncompetitive with, and in some cases outperform, traditional tools. In\nreal-world settings, Claude 3.5 Sonnet and Gemini 2.5 Pro jointly identified\n97% of all refactorings, surpassing the best-performing static-analysis-based\ntools. The models showed encouraging generalization to Python and Golang. They\nprovide natural language explanations and require only a single sentence to\ndefine each refactoring type.",
      "url": "http://arxiv.org/abs/2507.11346v1",
      "published_time_eastern_timestamp": 1752589256.0
    },
    {
      "title": "MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network",
      "summary": "Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for\na sequence of calibrated images to recover dense point clouds. However,\nexisting MVS methods often struggle with challenging regions, such as\ntextureless regions and reflective surfaces, where feature matching fails. In\ncontrast, monocular depth estimation inherently does not require feature\nmatching, allowing it to achieve robust relative depth estimation in these\nregions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature\nand depth guided MVS network that integrates powerful priors from a monocular\nfoundation model into multi-view geometry. Firstly, the monocular feature of\nthe reference view is integrated into source view features by the attention\nmechanism with a newly designed cross-view position encoding. Then, the\nmonocular depth of the reference view is aligned to dynamically update the\ndepth candidates for edge regions during the sampling procedure. Finally, a\nrelative consistency loss is further designed based on the monocular depth to\nsupervise the depth prediction. Extensive experiments demonstrate that\nMonoMVSNet achieves state-of-the-art performance on the DTU and\nTanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate\nand Advanced benchmarks. The source code is available at\nhttps://github.com/JianfeiJ/MonoMVSNet.",
      "url": "http://arxiv.org/abs/2507.11333v1",
      "published_time_eastern_timestamp": 1752588322.0
    },
    {
      "title": "Internal Value Alignment in Large Language Models through Controlled\n  Value Vector Activation",
      "summary": "Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.",
      "url": "http://arxiv.org/abs/2507.11316v1",
      "published_time_eastern_timestamp": 1752587315.0
    }
  ]
}
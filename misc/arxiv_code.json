{
  "last_updated": "2025-06-26T05:13:39.569896-04:00",
  "papers": [
    {
      "title": "MMSearch-R1: Incentivizing LMMs to Search",
      "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
      "url": "http://arxiv.org/abs/2506.20670v1",
      "published_time_eastern_timestamp": 1750874382.0
    },
    {
      "title": "Architectural mechanisms of a universal fault-tolerant quantum computer",
      "summary": "Quantum error correction (QEC) is believed to be essential for the\nrealization of large-scale quantum computers. However, due to the complexity of\noperating on the encoded `logical' qubits, understanding the physical\nprinciples for building fault-tolerant quantum devices and combining them into\nefficient architectures is an outstanding scientific challenge. Here we utilize\nreconfigurable arrays of up to 448 neutral atoms to implement all key elements\nof a universal, fault-tolerant quantum processing architecture and\nexperimentally explore their underlying working mechanisms. We first employ\nsurface codes to study how repeated QEC suppresses errors, demonstrating\n2.14(13)x below-threshold performance in a four-round characterization circuit\nby leveraging atom loss detection and machine learning decoding. We then\ninvestigate logical entanglement using transversal gates and lattice surgery,\nand extend it to universal logic through transversal teleportation with 3D\n[[15,1,3]] codes, enabling arbitrary-angle synthesis with logarithmic overhead.\nFinally, we develop mid-circuit qubit re-use, increasing experimental cycle\nrates by two orders of magnitude and enabling deep-circuit protocols with\ndozens of logical qubits and hundreds of logical teleportations with [[7,1,3]]\nand high-rate [[16,6,4]] codes while maintaining constant internal entropy. Our\nexperiments reveal key principles for efficient architecture design, involving\nthe interplay between quantum logic and entropy removal, judiciously using\nphysical entanglement in logic gates and magic state generation, and leveraging\nteleportations for universality and physical qubit reset. These results\nestablish foundations for scalable, universal error-corrected processing and\nits practical implementation with neutral atom systems.",
      "url": "http://arxiv.org/abs/2506.20661v1",
      "published_time_eastern_timestamp": 1750874037.0
    },
    {
      "title": "EditP23: 3D Editing via Propagation of Image Prompts to Multi-View",
      "summary": "We present EditP23, a method for mask-free 3D editing that propagates 2D\nimage edits to multi-view representations in a 3D-consistent manner. In\ncontrast to traditional approaches that rely on text-based prompting or\nexplicit spatial masks, EditP23 enables intuitive edits by conditioning on a\npair of images: an original view and its user-edited counterpart. These image\nprompts are used to guide an edit-aware flow in the latent space of a\npre-trained multi-view diffusion model, allowing the edit to be coherently\npropagated across views. Our method operates in a feed-forward manner, without\noptimization, and preserves the identity of the original object, in both\nstructure and appearance. We demonstrate its effectiveness across a range of\nobject categories and editing scenarios, achieving high fidelity to the source\nwhile requiring no manual masks.",
      "url": "http://arxiv.org/abs/2506.20652v1",
      "published_time_eastern_timestamp": 1750873820.0
    },
    {
      "title": "Towards Community-Driven Agents for Machine Learning Engineering",
      "summary": "Large language model-based machine learning (ML) agents have shown great\npromise in automating ML research. However, existing agents typically operate\nin isolation on a given research problem, without engaging with the broader\nresearch community, where human researchers often gain insights and contribute\nby sharing knowledge. To bridge this gap, we introduce MLE-Live, a live\nevaluation framework designed to assess an agent's ability to communicate with\nand leverage collective knowledge from a simulated Kaggle research community.\nBuilding on this framework, we propose CoMind, a novel agent that excels at\nexchanging insights and developing novel solutions within a community context.\nCoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%\nhuman competitors on average across four ongoing Kaggle competitions. Our code\nis released at https://github.com/comind-ml/CoMind.",
      "url": "http://arxiv.org/abs/2506.20640v1",
      "published_time_eastern_timestamp": 1750872962.0
    },
    {
      "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
      "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, \\textbf{DiffuCoder}, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose \\textbf{coupled-GRPO}, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
      "url": "http://arxiv.org/abs/2506.20639v1",
      "published_time_eastern_timestamp": 1750872947.0
    },
    {
      "title": "AI Assistants to Enhance and Exploit the PETSc Knowledge Base",
      "summary": "Generative AI, especially through large language models (LLMs), is\ntransforming how technical knowledge can be accessed, reused, and extended.\nPETSc, a widely used numerical library for high-performance scientific\ncomputing, has accumulated a rich but fragmented knowledge base over its three\ndecades of development, spanning source code, documentation, mailing lists,\nGitLab issues, Discord conversations, technical papers, and more. Much of this\nknowledge remains informal and inaccessible to users and new developers. To\nactivate and utilize this knowledge base more effectively, the PETSc team has\nbegun building an LLM-powered system that combines PETSc content with custom\nLLM tools -- including retrieval-augmented generation (RAG), reranking\nalgorithms, and chatbots -- to assist users, support developers, and propose\nupdates to formal documentation. This paper presents initial experiences\ndesigning and evaluating these tools, focusing on system architecture, using\nRAG and reranking for PETSc-specific information, evaluation methodologies for\nvarious LLMs and embedding models, and user interface design. Leveraging the\nArgonne Leadership Computing Facility resources, we analyze how LLM responses\ncan enhance the development and use of numerical software, with an initial\nfocus on scalable Krylov solvers. Our goal is to establish an extensible\nframework for knowledge-centered AI in scientific software, enabling scalable\nsupport, enriched documentation, and enhanced workflows for research and\ndevelopment. We conclude by outlining directions for expanding this system into\na robust, evolving platform that advances software ecosystems to accelerate\nscientific discovery.",
      "url": "http://arxiv.org/abs/2506.20608v1",
      "published_time_eastern_timestamp": 1750870805.0
    },
    {
      "title": "Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior\n  Toward Beneficence or Harm",
      "summary": "Agents based on Large Language Models (LLMs) have demonstrated strong\ncapabilities across a wide range of tasks. However, deploying LLM-based agents\nin high-stakes domains comes with significant safety and ethical risks.\nUnethical behavior by these agents can directly result in serious real-world\nconsequences, including physical harm and financial loss. To efficiently steer\nthe ethical behavior of agents, we frame agent behavior steering as a model\nediting task, which we term Behavior Editing. Model editing is an emerging area\nof research that enables precise and efficient modifications to LLMs while\npreserving their overall capabilities. To systematically study and evaluate\nthis approach, we introduce BehaviorBench, a multi-tier benchmark grounded in\npsychological moral theories. This benchmark supports both the evaluation and\nediting of agent behaviors across a variety of scenarios, with each tier\nintroducing more complex and ambiguous scenarios. We first demonstrate that\nBehavior Editing can dynamically steer agents toward the target behavior within\nspecific scenarios. Moreover, Behavior Editing enables not only\nscenario-specific local adjustments but also more extensive shifts in an\nagent's global moral alignment. We demonstrate that Behavior Editing can be\nused to promote ethical and benevolent behavior or, conversely, to induce\nharmful or malicious behavior. Through comprehensive evaluations on agents\nbased on frontier LLMs, BehaviorBench shows the effectiveness of Behavior\nEditing across different models and scenarios. Our findings offer key insights\ninto a new paradigm for steering agent behavior, highlighting both the promise\nand perils of Behavior Editing.",
      "url": "http://arxiv.org/abs/2506.20606v1",
      "published_time_eastern_timestamp": 1750870311.0
    },
    {
      "title": "IKEBANA: A Neural-Network approach for the K-shell ionization by\n  electron impact",
      "summary": "A fully connected neural network was trained to model the K-shell ionization\ncross sections based on two input features: the atomic number and the incoming\nelectron overvoltage. The training utilized a recent, updated compilation of\nexperimental data, covering elements from H to U, and incident electron\nenergies ranging from the threshold to relativistic values. The neural network\ndemonstrated excellent predictive performance, compared with the experimental\ndata, when available, and with full theoretical predictions. The developed\nmodel is provided in the ikebana code, which is openly available and requires\nonly the user-selected atomic number and electron energy range as inputs.",
      "url": "http://arxiv.org/abs/2506.20604v1",
      "published_time_eastern_timestamp": 1750870086.0
    },
    {
      "title": "Video Perception Models for 3D Scene Synthesis",
      "summary": "Traditionally, 3D scene synthesis requires expert knowledge and significant\nmanual effort. Automating this process could greatly benefit fields such as\narchitectural design, robotics simulation, virtual reality, and gaming. Recent\napproaches to 3D scene synthesis often rely on the commonsense reasoning of\nlarge language models (LLMs) or strong visual priors of modern image generation\nmodels. However, current LLMs demonstrate limited 3D spatial reasoning ability,\nwhich restricts their ability to generate realistic and coherent 3D scenes.\nMeanwhile, image generation-based methods often suffer from constraints in\nviewpoint selection and multi-view inconsistencies. In this work, we present\nVideo Perception models for 3D Scene synthesis (VIPScene), a novel framework\nthat exploits the encoded commonsense knowledge of the 3D physical world in\nvideo generation models to ensure coherent scene layouts and consistent object\nplacements across views. VIPScene accepts both text and image prompts and\nseamlessly integrates video generation, feedforward 3D reconstruction, and\nopen-vocabulary perception models to semantically and geometrically analyze\neach object in a scene. This enables flexible scene synthesis with high realism\nand structural consistency. For more precise analysis, we further introduce\nFirst-Person View Score (FPVScore) for coherence and plausibility evaluation,\nutilizing continuous first-person perspective to capitalize on the reasoning\nability of multimodal large language models. Extensive experiments show that\nVIPScene significantly outperforms existing methods and generalizes well across\ndiverse scenarios. The code will be released.",
      "url": "http://arxiv.org/abs/2506.20601v1",
      "published_time_eastern_timestamp": 1750869617.0
    },
    {
      "title": "SFNet: Fusion of Spatial and Frequency-Domain Features for Remote\n  Sensing Image Forgery Detection",
      "summary": "The rapid advancement of generative artificial intelligence is producing fake\nremote sensing imagery (RSI) that is increasingly difficult to detect,\npotentially leading to erroneous intelligence, fake news, and even conspiracy\ntheories. Existing forgery detection methods typically rely on single visual\nfeatures to capture predefined artifacts, such as spatial-domain cues to detect\nforged objects like roads or buildings in RSI, or frequency-domain features to\nidentify artifacts from up-sampling operations in adversarial generative\nnetworks (GANs). However, the nature of artifacts can significantly differ\ndepending on geographic terrain, land cover types, or specific features within\nthe RSI. Moreover, these complex artifacts evolve as generative models become\nmore sophisticated. In short, over-reliance on a single visual cue makes\nexisting forgery detectors struggle to generalize across diverse remote sensing\ndata. This paper proposed a novel forgery detection framework called SFNet,\ndesigned to identify fake images in diverse remote sensing data by leveraging\nspatial and frequency domain features. Specifically, to obtain rich and\ncomprehensive visual information, SFNet employs two independent feature\nextractors to capture spatial and frequency domain features from input RSIs. To\nfully utilize the complementary domain features, the domain feature mapping\nmodule and the hybrid domain feature refinement module(CBAM attention) of SFNet\nare designed to successively align and fuse the multi-domain features while\nsuppressing redundant information. Experiments on three datasets show that\nSFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art\nRS forgery detection methods and exhibits robust generalization capabilities.\nThe code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.",
      "url": "http://arxiv.org/abs/2506.20599v1",
      "published_time_eastern_timestamp": 1750869517.0
    },
    {
      "title": "WonderFree: Enhancing Novel View Quality and Cross-View Consistency for\n  3D Scene Exploration",
      "summary": "Interactive 3D scene generation from a single image has gained significant\nattention due to its potential to create immersive virtual worlds. However, a\nkey challenge in current 3D generation methods is the limited explorability,\nwhich cannot render high-quality images during larger maneuvers beyond the\noriginal viewpoint, particularly when attempting to move forward into unseen\nareas. To address this challenge, we propose WonderFree, the first model that\nenables users to interactively generate 3D worlds with the freedom to explore\nfrom arbitrary angles and directions. Specifically, we decouple this challenge\ninto two key subproblems: novel view quality, which addresses visual artifacts\nand floating issues in novel views, and cross-view consistency, which ensures\nspatial consistency across different viewpoints. To enhance rendering quality\nin novel views, we introduce WorldRestorer, a data-driven video restoration\nmodel designed to eliminate floaters and artifacts. In addition, a data\ncollection pipeline is presented to automatically gather training data for\nWorldRestorer, ensuring it can handle scenes with varying styles needed for 3D\nscene generation. Furthermore, to improve cross-view consistency, we propose\nConsistView, a multi-view joint restoration mechanism that simultaneously\nrestores multiple perspectives while maintaining spatiotemporal coherence.\nExperimental results demonstrate that WonderFree not only enhances rendering\nquality across diverse viewpoints but also significantly improves global\ncoherence and consistency. These improvements are confirmed by CLIP-based\nmetrics and a user study showing a 77.20% preference for WonderFree over\nWonderWorld enabling a seamless and immersive 3D exploration experience. The\ncode, model, and data will be publicly available.",
      "url": "http://arxiv.org/abs/2506.20590v1",
      "published_time_eastern_timestamp": 1750868920.0
    },
    {
      "title": "Communicating Smartly in Molecular Communication Environments: Neural\n  Networks in the Internet of Bio-Nano Things",
      "summary": "Recent developments in the Internet of Bio-Nano Things (IoBNT) are laying the\ngroundwork for innovative applications across the healthcare sector.\nNanodevices designed to operate within the body, managed remotely via the\ninternet, are envisioned to promptly detect and actuate on potential diseases.\nIn this vision, an inherent challenge arises due to the limited capabilities of\nindividual nanosensors; specifically, nanosensors must communicate with one\nanother to collaborate as a cluster. Aiming to research the boundaries of the\nclustering capabilities, this survey emphasizes data-driven communication\nstrategies in molecular communication (MC) channels as a means of linking\nnanosensors. Relying on the flexibility and robustness of machine learning (ML)\nmethods to tackle the dynamic nature of MC channels, the MC research community\nfrequently refers to neural network (NN) architectures. This interdisciplinary\nresearch field encompasses various aspects, including the use of NNs to\nfacilitate communication in MC environments, their implementation at the\nnanoscale, explainable approaches for NNs, and dataset generation for training.\nWithin this survey, we provide a comprehensive analysis of fundamental\nperspectives on recent trends in NN architectures for MC, the feasibility of\ntheir implementation at the nanoscale, applied explainable artificial\nintelligence (XAI) techniques, and the accessibility of datasets along with\nbest practices for their generation. Additionally, we offer open-source code\nrepositories that illustrate NN-based methods to support reproducible research\nfor key MC scenarios. Finally, we identify emerging research challenges, such\nas robust NN architectures, biologically integrated NN modules, and scalable\ntraining strategies.",
      "url": "http://arxiv.org/abs/2506.20589v1",
      "published_time_eastern_timestamp": 1750868910.0
    },
    {
      "title": "AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical\n  Image Segmentation",
      "summary": "Vision Transformer has recently gained tremendous popularity in medical image\nsegmentation task due to its superior capability in capturing long-range\ndependencies. However, transformer requires a large amount of labeled data to\nbe effective, which hinders its applicability in annotation scarce\nsemi-supervised learning scenario where only limited labeled data is available.\nState-of-the-art semi-supervised learning methods propose combinatorial\nCNN-Transformer learning to cross teach a transformer with a convolutional\nneural network, which achieves promising results. However, it remains a\nchallenging task to effectively train the transformer with limited labeled\ndata. In this paper, we propose an adversarial masked image modeling method to\nfully unleash the potential of transformer for semi-supervised medical image\nsegmentation. The key challenge in semi-supervised learning with transformer\nlies in the lack of sufficient supervision signal. To this end, we propose to\nconstruct an auxiliary masked domain from original domain with masked image\nmodeling and train the transformer to predict the entire segmentation mask with\nmasked inputs to increase supervision signal. We leverage the original labels\nfrom labeled data and pseudo-labels from unlabeled data to learn the masked\ndomain. To further benefit the original domain from masked domain, we provide a\ntheoretical analysis of our method from a multi-domain learning perspective and\ndevise a novel adversarial training loss to reduce the domain gap between the\noriginal and masked domain, which boosts semi-supervised learning performance.\nWe also extend adversarial masked image modeling to CNN network. Extensive\nexperiments on three public medical image segmentation datasets demonstrate the\neffectiveness of our method, where our method outperforms existing methods\nsignificantly. Our code is publicly available at\nhttps://github.com/zlheui/AdvMIM.",
      "url": "http://arxiv.org/abs/2506.20563v1",
      "published_time_eastern_timestamp": 1750867218.0
    },
    {
      "title": "CCISolver: End-to-End Detection and Repair of Method-Level Code-Comment\n  Inconsistency",
      "summary": "Comments within code serve as a crucial foundation for software\ndocumentation, facilitating developers to communicate and understand the code\neffectively. However, code-comment inconsistency (CCI) can negatively affect\nsoftware development, testing, and maintenance. Recent efforts to mitigate this\nissue have emerged, but existing studies often suffer from inaccurate datasets\nand inadequate solutions, weakening their practical effectiveness. In this\nstudy, we first conduct a quantitative analysis of existing datasets, revealing\na substantial portion of sampled data are mislabeled. To address these data\nlimitations, we introduce CCIBench, a refined dataset comprising high-quality\ndata, to support the training and evaluation of method-level CCI methods.\nFurthermore, we present an innovative end-to-end LLM-based framework,\nCCISolver, designed to improve code quality by identifying and rectifying CCIs.\nComprehensive evaluations demonstrate CCISolver's superior performance. For\ndetection, it establishes a new state-of-the-art with an F1-score of 89.54%. In\nfixing task, it achieves a remarkable 18.84% relative improvement in GLEU score\nover the strongest baseline. This superiority is confirmed by human evaluation,\nwhere CCISolver's fixing success rate of 0.6533 significantly surpasses\nexisting methods. Critically, in a practical end-to-end setting, CCISolver's\ninnovative architecture is approximately 36% faster for inference than the\nbaseline model, underscoring its scalability and real-world applicability.",
      "url": "http://arxiv.org/abs/2506.20558v1",
      "published_time_eastern_timestamp": 1750866967.0
    },
    {
      "title": "Large Language Model-Driven Code Compliance Checking in Building\n  Information Modeling",
      "summary": "This research addresses the time-consuming and error-prone nature of manual\ncode compliance checking in Building Information Modeling (BIM) by introducing\na Large Language Model (LLM)-driven approach to semi-automate this critical\nprocess. The developed system integrates LLMs such as GPT, Claude, Gemini, and\nLlama, with Revit software to interpret building codes, generate Python\nscripts, and perform semi-automated compliance checks within the BIM\nenvironment. Case studies on a single-family residential project and an office\nbuilding project demonstrated the system's ability to reduce the time and\neffort required for compliance checks while improving accuracy. It streamlined\nthe identification of violations, such as non-compliant room dimensions,\nmaterial usage, and object placements, by automatically assessing relationships\nand generating actionable reports. Compared to manual methods, the system\neliminated repetitive tasks, simplified complex regulations, and ensured\nreliable adherence to standards. By offering a comprehensive, adaptable, and\ncost-effective solution, this proposed approach offers a promising advancement\nin BIM-based compliance checking, with potential applications across diverse\nregulatory documents in construction projects.",
      "url": "http://arxiv.org/abs/2506.20551v1",
      "published_time_eastern_timestamp": 1750866634.0
    },
    {
      "title": "Causal Inference for Latent Outcomes Learned with Factor Models",
      "summary": "In many fields$\\unicode{x2013}$including genomics, epidemiology, natural\nlanguage processing, social and behavioral sciences, and\neconomics$\\unicode{x2013}$it is increasingly important to address causal\nquestions in the context of factor models or representation learning. In this\nwork, we investigate causal effects on $\\textit{latent outcomes}$ derived from\nhigh-dimensional observed data using nonnegative matrix factorization. To the\nbest of our knowledge, this is the first study to formally address causal\ninference in this setting. A central challenge is that estimating a latent\nfactor model can cause an individual's learned latent outcome to depend on\nother individuals' treatments, thereby violating the standard causal inference\nassumption of no interference. We formalize this issue as\n$\\textit{learning-induced interference}$ and distinguish it from interference\npresent in a data-generating process. To address this, we propose a novel,\nintuitive, and theoretically grounded algorithm to estimate causal effects on\nlatent outcomes while mitigating learning-induced interference and improving\nestimation efficiency. We establish theoretical guarantees for the consistency\nof our estimator and demonstrate its practical utility through simulation\nstudies and an application to cancer mutational signature analysis. All\nbaseline and proposed methods are available in our open-source R package, ${\\tt\ncausalLFO}$.",
      "url": "http://arxiv.org/abs/2506.20549v1",
      "published_time_eastern_timestamp": 1750866443.0
    },
    {
      "title": "Pay Less Attention to Deceptive Artifacts: Robust Detection of\n  Compressed Deepfakes on Online Social Networks",
      "summary": "With the rapid advancement of deep learning, particularly through generative\nadversarial networks (GANs) and diffusion models (DMs), AI-generated images, or\n``deepfakes\", have become nearly indistinguishable from real ones. These images\nare widely shared across Online Social Networks (OSNs), raising concerns about\ntheir misuse. Existing deepfake detection methods overlook the ``block effects\"\nintroduced by compression in OSNs, which obscure deepfake artifacts, and\nprimarily focus on raw images, rarely encountered in real-world scenarios. To\naddress these challenges, we propose PLADA (Pay Less Attention to Deceptive\nArtifacts), a novel framework designed to tackle the lack of paired data and\nthe ineffective use of compressed images. PLADA consists of two core modules:\nBlock Effect Eraser (B2E), which uses a dual-stage attention mechanism to\nhandle block effects, and Open Data Aggregation (ODA), which processes both\npaired and unpaired data to improve detection. Extensive experiments across 26\ndatasets demonstrate that PLADA achieves a remarkable balance in deepfake\ndetection, outperforming SoTA methods in detecting deepfakes on OSNs, even with\nlimited paired data and compression. More importantly, this work introduces the\n``block effect\" as a critical factor in deepfake detection, providing a robust\nsolution for open-world scenarios. Our code is available at\nhttps://github.com/ManyiLee/PLADA.",
      "url": "http://arxiv.org/abs/2506.20548v1",
      "published_time_eastern_timestamp": 1750866401.0
    },
    {
      "title": "When Life Gives You Samples: The Benefits of Scaling up Inference\n  Compute for Multilingual LLMs",
      "summary": "Recent advancements in large language models (LLMs) have shifted focus toward\nscaling inference-time compute, improving performance without retraining the\nmodel. A common approach is to sample multiple outputs in parallel, and select\none of these as the final output. However, work to date has focused on English\nand a handful of domains such as math and code. In contrast, we are most\ninterested in techniques that generalize across open-ended tasks, formally\nverifiable tasks, and across languages. In this work, we study how to robustly\nscale inference-time compute for open-ended generative tasks in a multilingual,\nmulti-task setting.\n  Our findings show that both sampling strategy based on temperature variation\nand selection strategy must be adapted to account for diverse domains and\nvaried language settings. We evaluate existing selection methods, revealing\nthat strategies effective in English often fail to generalize across languages.\nWe propose novel sampling and selection strategies specifically adapted for\nmultilingual and multi-task inference scenarios, and show they yield notable\ngains across languages and tasks. In particular, our combined sampling and\nselection methods lead to an average +6.8 jump in win-rates for our 8B models\non m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At\nlarger scale, Command-A (111B model) equipped with our methods, shows +9.0\nimprovement in win-rates on the same benchmark with just five samples against\nsingle-sample decoding, a substantial increase at minimal cost. Our results\nunderscore the need for language- and task-aware approaches to inference-time\ncompute, aiming to democratize performance improvements in underrepresented\nlanguages.",
      "url": "http://arxiv.org/abs/2506.20544v1",
      "published_time_eastern_timestamp": 1750865873.0
    },
    {
      "title": "Resolvent4py: a parallel Python package for analysis, model reduction\n  and control of large-scale linear systems",
      "summary": "In this paper, we present resolvent4py, a parallel Python package for the\nanalysis, model reduction and control of large-scale linear systems with\nmillions or billions of degrees of freedom. This package provides the user with\na friendly Python-like experience (akin to that of well-established libraries\nsuch as numpy and scipy), while enabling MPI-based parallelism through mpi4py,\npetsc4py and slepc4py. In turn, this allows for the development of streamlined\nand efficient Python code that can be used to solve several problems in fluid\nmechanics, solid mechanics, graph theory, molecular dynamics and several other\nfields.",
      "url": "http://arxiv.org/abs/2506.20539v1",
      "published_time_eastern_timestamp": 1750865556.0
    },
    {
      "title": "WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon\n  Footprint of AI Workloads",
      "summary": "The rapid advancement of AI, particularly large language models (LLMs), has\nraised significant concerns about the energy use and carbon emissions\nassociated with model training and inference. However, existing tools for\nmeasuring and reporting such impacts are often fragmented, lacking systematic\nmetric integration and offering limited support for correlation analysis among\nthem. This paper presents WattsOnAI, a comprehensive software toolkit for the\nmeasurement, analysis, and visualization of energy use, power draw, hardware\nperformance, and carbon emissions across AI workloads. By seamlessly\nintegrating with existing AI frameworks, WattsOnAI offers standardized reports\nand exports fine-grained time-series data to support benchmarking and\nreproducibility in a lightweight manner. It further enables in-depth\ncorrelation analysis between hardware metrics and model performance and thus\nfacilitates bottleneck identification and performance enhancement. By\naddressing critical limitations in existing tools, WattsOnAI encourages the\nresearch community to weigh environmental impact alongside raw performance of\nAI workloads and advances the shift toward more sustainable \"Green AI\"\npractices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.",
      "url": "http://arxiv.org/abs/2506.20535v1",
      "published_time_eastern_timestamp": 1750865085.0
    }
  ]
}
{
  "last_updated": "2025-11-05T13:17:36.104032-05:00",
  "papers": [
    {
      "title": "GeoCrossBench: Cross-Band Generalization for Remote Sensing",
      "summary": "The number and diversity of remote sensing satellites grows over time, while\nthe vast majority of labeled data comes from older satellites. As the\nfoundation models for Earth observation scale up, the cost of (re-)training to\nsupport new satellites grows too, so the generalization capabilities of the\nmodels towards new satellites become increasingly important. In this work we\nintroduce GeoCrossBench, an extension of the popular GeoBench benchmark with a\nnew evaluation protocol: it tests the in-distribution performance;\ngeneralization to satellites with no band overlap; and generalization to\nsatellites with additional bands with respect to the training set. We also\ndevelop a self-supervised extension of ChannelViT, ChiViT, to improve its\ncross-satellite performance. First, we show that even the best foundation\nmodels for remote sensing (DOFA, TerraFM) do not outperform general purpose\nmodels like DINOv3 in the in-distribution setting. Second, when generalizing to\nnew satellites with no band overlap, all models suffer 2-4x drop in\nperformance, and ChiViT significantly outperforms the runner-up DINOv3. Third,\nthe performance of all tested models drops on average by 5-25\\% when given\nadditional bands during test time. Finally, we show that fine-tuning just the\nlast linear layer of these models using oracle labels from all bands can get\nrelatively consistent performance across all satellites, highlighting that the\nbenchmark is far from being saturated. We publicly release the code and the\ndatasets to encourage the development of more future-proof remote sensing\nmodels with stronger cross-satellite generalization.",
      "url": "http://arxiv.org/abs/2511.02831v1",
      "published_time_eastern_timestamp": 1762282700.0
    },
    {
      "title": "Densemarks: Learning Canonical Embeddings for Human Heads Images via\n  Point Tracks",
      "summary": "We propose DenseMarks - a new learned representation for human heads,\nenabling high-quality dense correspondences of human head images. For a 2D\nimage of a human head, a Vision Transformer network predicts a 3D embedding for\neach pixel, which corresponds to a location in a 3D canonical unit cube. In\norder to train our network, we collect a dataset of pairwise point matches,\nestimated by a state-of-the-art point tracker over a collection of diverse\nin-the-wild talking heads videos, and guide the mapping via a contrastive loss,\nencouraging matched points to have close embeddings. We further employ\nmulti-task learning with face landmarks and segmentation constraints, as well\nas imposing spatial continuity of embeddings through latent cube features,\nwhich results in an interpretable and queryable canonical space. The\nrepresentation can be used for finding common semantic parts, face/head\ntracking, and stereo reconstruction. Due to the strong supervision, our method\nis robust to pose variations and covers the entire head, including hair.\nAdditionally, the canonical space bottleneck makes sure the obtained\nrepresentations are consistent across diverse poses and individuals. We\ndemonstrate state-of-the-art results in geometry-aware point matching and\nmonocular head tracking with 3D Morphable Models. The code and the model\ncheckpoint will be made available to the public.",
      "url": "http://arxiv.org/abs/2511.02830v1",
      "published_time_eastern_timestamp": 1762282683.0
    },
    {
      "title": "From Code Changes to Quality Gains: An Empirical Study in Python ML\n  Systems with PyQu",
      "summary": "In an era shaped by Generative Artificial Intelligence for code generation\nand the rising adoption of Python-based Machine Learning systems (MLS),\nsoftware quality has emerged as a major concern. As these systems grow in\ncomplexity and importance, a key obstacle lies in understanding exactly how\nspecific code changes affect overall quality-a shortfall aggravated by the lack\nof quality assessment tools and a clear mapping between ML systems code changes\nand their quality effects. Although prior work has explored code changes in\nMLS, it mostly stops at what the changes are, leaving a gap in our knowledge of\nthe relationship between code changes and the MLS quality. To address this gap,\nwe conducted a large-scale empirical study of 3,340 open-source Python ML\nprojects, encompassing more than 3.7 million commits and 2.7 trillion lines of\ncode. We introduce PyQu, a novel tool that leverages low level software metrics\nto identify quality-enhancing commits with an average accuracy, precision, and\nrecall of 0.84 and 0.85 of average F1 score. Using PyQu and a thematic\nanalysis, we identified 61 code changes, each demonstrating a direct impact on\nenhancing software quality, and we classified them into 13 categories based on\ncontextual characteristics. 41% of the changes are newly discovered by our\nstudy and have not been identified by state-of-the-art Python changes detection\ntools. Our work offers a vital foundation for researchers, practitioners,\neducators, and tool developers, advancing the quest for automated quality\nassessment and best practices in Python-based ML software.",
      "url": "http://arxiv.org/abs/2511.02827v1",
      "published_time_eastern_timestamp": 1762282519.0
    },
    {
      "title": "Kosmos: An AI Scientist for Autonomous Discovery",
      "summary": "Data-driven scientific discovery requires iterative cycles of literature\nsearch, hypothesis generation, and data analysis. Substantial progress has been\nmade towards AI agents that can automate scientific research, but all such\nagents remain limited in the number of actions they can take before losing\ncoherence, thus limiting the depth of their findings. Here we present Kosmos,\nan AI scientist that automates data-driven discovery. Given an open-ended\nobjective and a dataset, Kosmos runs for up to 12 hours performing cycles of\nparallel data analysis, literature search, and hypothesis generation before\nsynthesizing discoveries into scientific reports. Unlike prior systems, Kosmos\nuses a structured world model to share information between a data analysis\nagent and a literature search agent. The world model enables Kosmos to\ncoherently pursue the specified objective over 200 agent rollouts, collectively\nexecuting an average of 42,000 lines of code and reading 1,500 papers per run.\nKosmos cites all statements in its reports with code or primary literature,\nensuring its reasoning is traceable. Independent scientists found 79.4% of\nstatements in Kosmos reports to be accurate, and collaborators reported that a\nsingle 20-cycle Kosmos run performed the equivalent of 6 months of their own\nresearch time on average. Furthermore, collaborators reported that the number\nof valuable scientific findings generated scales linearly with Kosmos cycles\n(tested up to 20 cycles). We highlight seven discoveries made by Kosmos that\nspan metabolomics, materials science, neuroscience, and statistical genetics.\nThree discoveries independently reproduce findings from preprinted or\nunpublished manuscripts that were not accessed by Kosmos at runtime, while four\nmake novel contributions to the scientific literature.",
      "url": "http://arxiv.org/abs/2511.02824v1",
      "published_time_eastern_timestamp": 1762282252.0
    },
    {
      "title": "A Construction of Infinite Families of Self-Orthogonal Quasi-Cyclic\n  Codes Using Constituent Codes.pdf",
      "summary": "Quasi-cyclic codes have been recently employed in the constructions of\nquantum error-correcting codes. In this paper, we propose a construction of\ninfinite families of quasi-cyclic codes which are self-orthogonal with respect\nto the Euclidean and Hermitian inner products. In particular, their dimension\nand a lower bound for their minimum distance are computed using their\nconstituent codes defined over field extensions of $\\mathbb{F}_q$. We also show\nthat the lower bound for the minimum distance satisfies the square-root-like\nlower bound and also show how self-dual quasi-cyclic codes can arise from our\nconstruction. Using the CSS construction, we show the existence of quantum\nerror-correcting codes with good parameters.",
      "url": "http://arxiv.org/abs/2511.02813v1",
      "published_time_eastern_timestamp": 1762281427.0
    },
    {
      "title": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via\n  End-to-End Reinforcement Learning",
      "summary": "Typical search agents concatenate the entire interaction history into the LLM\ncontext, preserving information integrity but producing long, noisy contexts,\nresulting in high computation and memory costs. In contrast, using only the\ncurrent turn avoids this overhead but discards essential information. This\ntrade-off limits the scalability of search agents. To address this challenge,\nwe propose MemSearcher, an agent workflow that iteratively maintains a compact\nmemory and combines the current turn with it. At each turn, MemSearcher fuses\nthe user's question with the memory to generate reasoning traces, perform\nsearch actions, and update memory to retain only information essential for\nsolving the task. This design stabilizes context length across multi-turn\ninteractions, improving efficiency without sacrificing accuracy. To optimize\nthis workflow, we introduce multi-context GRPO, an end-to-end RL framework that\njointly optimize reasoning, search strategies, and memory management of\nMemSearcher Agents. Specifically, multi-context GRPO samples groups of\ntrajectories under different contexts and propagates trajectory-level\nadvantages across all conversations within them. Trained on the same dataset as\nSearch-R1, MemSearcher achieves significant improvements over strong baselines\non seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on\nQwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher\neven outperforms 7B-based baselines, demonstrating that striking a balance\nbetween information integrity and efficiency yields both higher accuracy and\nlower computational overhead. The code and models will be publicly available at\nhttps://github.com/icip-cas/MemSearcher",
      "url": "http://arxiv.org/abs/2511.02805v1",
      "published_time_eastern_timestamp": 1762280859.0
    },
    {
      "title": "Optimal Source Coding of Markov Chains for Real-Time Remote Estimation",
      "summary": "We revisit the source coding problem for a Markov chain under the assumption\nthat the transmission times and how fast the Markov chain transitions its state\nhappen at the same time-scale. Specifically, we assume that the transmission of\neach bit takes a single time slot, and the Markov chain updates its state in\nthe same time slot. Thus, the length of the codeword assigned to a symbol\ndetermines the number of non-transmitted symbols, as well as, the probability\nof the realization of the next symbol to be transmitted. We aim to minimize the\naverage transmission duration over an infinite horizon by proposing an optimal\nsource coding policy based on the last transmitted symbol and its transmission\nduration. To find the optimal policy, we formulate the problem with a Markov\ndecision process (MDP) by augmenting the symbols alongside the transmission\nduration of the symbols. Finally, we analyze two Huffman-based benchmark\npolicies and compare their performances with the proposed optimal policy. We\nobserve that, in randomly generated processes, our proposed optimal policy\ndecreases the average transmission duration compared to benchmark policies. The\nperformance gain varies based on the parameters of the Markov process.",
      "url": "http://arxiv.org/abs/2511.02803v1",
      "published_time_eastern_timestamp": 1762280823.0
    },
    {
      "title": "Cesam2k20: A code for a new generation of stellar evolution models. I.\n  Description of the code",
      "summary": "We present Cesam2k20, the latest version of the hydrostatic stellar evolution\ncode CESAM originally developed by P. Morel and collaborators. Over the last\nthree decades, it has undergone many improvements and has been extensively\ntested against other stellar evolution codes before being selected to compute\nthe first-generation grid of stellar models for the PLATO mission. Among all\nthe developments made thus far, Cesam2k20 now implements state-of-the-art\nmodels for the transport of chemical elements and angular momentum. It was\nrecently made publicly available with an ecosystem of other codes interfaced\nwith it: 1D and 2D oscillation codes ADIPLS and ACOR, optimisation program OSM,\nand Python utility package pycesam. This paper recalls the numerical\npeculiarities of Cesam2k20, namely, the use of a collocation method where the\nstructure variables are decomposed as piecewise polynomials projected on a\nB-spline basis. Here, we review the options available for modelling the\ndifferent physical processes. In particular, we illustrate the improvements\nmade in the transport of chemical elements and angular momentum with a series\nof standard and non-standard solar models.",
      "url": "http://arxiv.org/abs/2511.02801v1",
      "published_time_eastern_timestamp": 1762280702.0
    },
    {
      "title": "1 PoCo: Agentic Proof-of-Concept Exploit Generation for Smart Contracts",
      "summary": "Smart contracts operate in a highly adversarial environment, where\nvulnerabilities can lead to substantial financial losses. Thus, smart contracts\nare subject to security audits. In auditing, proof-of-concept (PoC) exploits\nplay a critical role by demonstrating to the stakeholders that the reported\nvulnerabilities are genuine, reproducible, and actionable. However, manually\ncreating PoCs is time-consuming, error-prone, and often constrained by tight\naudit schedules. We introduce POCO, an agentic framework that automatically\ngenerates executable PoC exploits from natural-language vulnerability\ndescriptions written by auditors. POCO autonomously generates PoC exploits in\nan agentic manner by interacting with a set of code-execution tools in a\nReason-Act-Observe loop. It produces fully executable exploits compatible with\nthe Foundry testing framework, ready for integration into audit reports and\nother security tools. We evaluate POCO on a dataset of 23 real-world\nvulnerability reports. POCO consistently outperforms the prompting and workflow\nbaselines, generating well-formed and logically correct PoCs. Our results\ndemonstrate that agentic frameworks can significantly reduce the effort\nrequired for high-quality PoCs in smart contract audits. Our contribution\nprovides readily actionable knowledge for the smart contract security\ncommunity.",
      "url": "http://arxiv.org/abs/2511.02780v1",
      "published_time_eastern_timestamp": 1762279392.0
    },
    {
      "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual\n  Representation",
      "summary": "Code has emerged as a precise and executable medium for reasoning and action\nin the agent era. Yet, progress has largely focused on language-centric tasks\nsuch as program synthesis and debugging, leaving visual-centric coding\nunderexplored. Inspired by how humans reason over sketches, we advocate SVG\ncode as a compact, interpretable, and executable visual representation. We\nintroduce VCode, a benchmark that reframes multimodal understanding as code\ngeneration: given an image, a model must produce SVG that preserves symbolic\nmeaning for downstream reasoning. VCode covers three domains - general\ncommonsense (MM-Vet), professional disciplines (MMMU), and visual-centric\nperception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel\nevaluation protocol in which a policy model answers questions over rendered\nSVGs; correct answers indicate faithful symbolic preservation. Empirically,\nfrontier VLMs struggle to generate faithful SVGs, revealing a persistent gap\nbetween language-centric and visual-centric coding. To close this gap, we\nintroduce VCoder, an agentic framework that augments VLMs along two axes: (i)\nThinking with Revision, which iteratively analyzes discrepancies and refines\nSVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply\nstructured cues such as objects, shapes, and text beyond the model's intrinsic\ncapacity. Across benchmarks, frontier VLMs with strong reasoning capabilities\nscore well overall yet remain limited in professional knowledge and 3D\nreasoning. VCoder delivers a 12.3-point overall gain over the top-performing\nClaude-4-Opus. Human studies show that both humans and VLMs perform worse on\nrendered SVGs, their consistency reveals the promise of symbolic visual\nrepresentation. The benchmark and code are available at\nhttps://github.com/CSU-JPG/VCode.",
      "url": "http://arxiv.org/abs/2511.02778v1",
      "published_time_eastern_timestamp": 1762279218.0
    },
    {
      "title": "XR-1: Towards Versatile Vision-Language-Action Models via Learning\n  Unified Vision-Motion Representations",
      "summary": "Recent progress in large-scale robotic datasets and vision-language models\n(VLMs) has advanced research on vision-language-action (VLA) models. However,\nexisting VLA models still face two fundamental challenges: (i) producing\nprecise low-level actions from high-dimensional observations, (ii) bridging\ndomain gaps across heterogeneous data sources, including diverse robot\nembodiments and human demonstrations. Existing methods often encode latent\nvariables from either visual dynamics or robotic actions to guide policy\nlearning, but they fail to fully exploit the complementary multi-modal\nknowledge present in large-scale, heterogeneous datasets. In this work, we\npresent X Robotic Model 1 (XR-1), a novel framework for versatile and scalable\nVLA learning across diverse robots, tasks, and environments. XR-1 introduces\nthe \\emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representation\nlearned via a dual-branch VQ-VAE that jointly encodes visual dynamics and\nrobotic motion. UVMC addresses these challenges by (i) serving as an\nintermediate representation between the observations and actions, and (ii)\naligning multimodal dynamic information from heterogeneous data sources to\ncapture complementary knowledge. To effectively exploit UVMC, we propose a\nthree-stage training paradigm: (i) self-supervised UVMC learning, (ii)\nUVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and\n(iii) task-specific post-training. We validate XR-1 through extensive\nreal-world experiments with more than 14,000 rollouts on six different robot\nembodiments, spanning over 120 diverse manipulation tasks. XR-1 consistently\noutperforms state-of-the-art baselines such as $\\pi_{0.5}$, $\\pi_0$, RDT,\nUniVLA, and GR00T-N1.5 while demonstrating strong generalization to novel\nobjects, background variations, distractors, and illumination changes. Our\nproject is at https://xr-1-vla.github.io/.",
      "url": "http://arxiv.org/abs/2511.02776v1",
      "published_time_eastern_timestamp": 1762279152.0
    },
    {
      "title": "From Solo to Symphony: Orchestrating Multi-Agent Collaboration with\n  Single-Agent Demos",
      "summary": "Training a team of agents from scratch in multi-agent reinforcement learning\n(MARL) is highly inefficient, much like asking beginners to play a symphony\ntogether without first practicing solo. Existing methods, such as offline or\ntransferable MARL, can ease this burden, but they still rely on costly\nmulti-agent data, which often becomes the bottleneck. In contrast, solo\nexperiences are far easier to obtain in many important scenarios, e.g.,\ncollaborative coding, household cooperation, and search-and-rescue. To unlock\ntheir potential, we propose Solo-to-Collaborative RL (SoCo), a framework that\ntransfers solo knowledge into cooperative learning. SoCo first pretrains a\nshared solo policy from solo demonstrations, then adapts it for cooperation\nduring multi-agent training through a policy fusion mechanism that combines an\nMoE-like gating selector and an action editor. Experiments across diverse\ncooperative tasks show that SoCo significantly boosts the training efficiency\nand performance of backbone algorithms. These results demonstrate that solo\ndemonstrations provide a scalable and effective complement to multi-agent data,\nmaking cooperative learning more practical and broadly applicable.",
      "url": "http://arxiv.org/abs/2511.02762v1",
      "published_time_eastern_timestamp": 1762278251.0
    },
    {
      "title": "LLM-Supported Formal Knowledge Representation for Enhancing Control\n  Engineering Content with an Interactive Semantic Layer",
      "summary": "The rapid growth of research output in control engineering calls for new\napproaches to structure and formalize domain knowledge. This paper briefly\ndescribes an LLM-supported method for semi-automated generation of formal\nknowledge representations that combine human readability with machine\ninterpretability and increased expressiveness. Based on the Imperative\nRepresentation of Knowledge (PyIRK) framework, we demonstrate how language\nmodels can assist in transforming natural-language descriptions and\nmathematical definitions (available as LaTeX source code) into a formalized\nknowledge graph. As a first application we present the generation of an\n``interactive semantic layer'' to enhance the source documents in order to\nfacilitate knowledge transfer. From our perspective this contributes to the\nvision of easily accessible, collaborative, and verifiable knowledge bases for\nthe control engineering domain.",
      "url": "http://arxiv.org/abs/2511.02759v1",
      "published_time_eastern_timestamp": 1762277817.0
    },
    {
      "title": "Investigating the Experience of Autistic Individuals in Software\n  Engineering",
      "summary": "Context: Autism spectrum disorder (ASD) leads to various issues in the\neveryday life of autistic individuals, often resulting in unemployment and\nmental health problems. To improve the inclusion of autistic adults, existing\nstudies have highlighted the strengths these individuals possess in comparison\nto non-autistic individuals, e.g., high attention to detail or excellent\nlogical reasoning skills. If fostered, these strengths could be valuable in\nsoftware engineering activities, such for identifying specific kinds of bugs in\ncode. However, existing work in SE has primarily studied the challenges of\nautistic individuals and possible accommodations, with little attention their\nstrengths. Objective: Our goal is to analyse the experiences of autistic\nindividuals in software engineering activities, such as code reviews, with a\nparticular emphasis on strengths. Methods: This study combines Social-Technical\nGrounded Theory through semi-structured interviews with 16 autistic software\nengineers and a survey with 49 respondents, including 5 autistic participants.\nWe compare the emerging themes with the theory by Gama et al. on the Effect of\nNeurodivergent Cognitive Dysfunctions in Software Engineering Performance.\nResults: Our results suggest that autistic software engineers are often skilled\nin logical thinking, attention to detail, and hyperfocus in programming; and\nthey enjoy learning new programming languages and programming-related\ntechnologies. Confirming previous work, they tend to prefer written\ncommunication and remote work. Finally, we report a high comfort level in\ninteracting with AI-based systems. Conclusions: Our findings extend existing\nwork by providing further evidence on the strengths of autistic software\nengineers.",
      "url": "http://arxiv.org/abs/2511.02736v1",
      "published_time_eastern_timestamp": 1762275571.0
    },
    {
      "title": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated\n  Release Note Generation",
      "summary": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs.",
      "url": "http://arxiv.org/abs/2511.02713v1",
      "published_time_eastern_timestamp": 1762273904.0
    },
    {
      "title": "Making PLUMED fly: a tutorial on optimizing performance",
      "summary": "PLUMED is an open-source software package that is widely used for analyzing\nand enhancing molecular dynamics simulations that works in conjunction with\nmost available molecular dynamics softwares. While the computational cost of\nPLUMED calculations is typically negligible compared to the molecular dynamics\ncode's force evaluation, the software is increasingly being employed for more\ncomputationally demanding tasks where performance optimization becomes\ncritical. In this tutorial, we describe a recently implemented tool that can be\nused to reliably measure code performance. We then use this tool to generate\ndetailed performance benchmarks that show how calculations of large-numbers of\ndistances, angles or torsions can be optimized by using vector-based commands\nrather than individual scalar operations. We then present benchmarks that\nillustrate how to optimize calculations of atomic order parameters and\nsecondary structure variables. Throughout the tutorial and in our\nimplementations we endeavor to explain the algorithmic tricks that are being\nused to optimize the calculations so others can make use of these prescriptions\nboth when they are using PLUMED and when they are writing their own codes.",
      "url": "http://arxiv.org/abs/2511.02699v1",
      "published_time_eastern_timestamp": 1762273301.0
    },
    {
      "title": "Implementing Multi-GPU Scientific Computing Miniapps Across Performance\n  Portable Frameworks",
      "summary": "Scientific computing in the exascale era demands increased computational\npower to solve complex problems across various domains. With the rise of\nheterogeneous computing architectures the need for vendor-agnostic, performance\nportability frameworks has been highlighted. Libraries like Kokkos have become\nessential for enabling high-performance computing applications to execute\nefficiently across different hardware platforms with minimal code changes. In\nthis direction, this paper presents preliminary time-to-solution results for\ntwo representative scientific computing applications: an N-body simulation and\na structured grid simulation. Both applications used a distributed memory\napproach and hardware acceleration through four performance portability\nframeworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single\nnode of the Polaris supercomputer using four NVIDIA A100 GPUs revealed\nsignificant performance variability among frameworks. OCCA demonstrated faster\nexecution times for small-scale validation problems, likely due to JIT\ncompilation, however its lack of optimized reduction algorithms may limit\nscalability for larger simulations while using its out of the box API. OpenMP\nperformed poorly in the structured grid simulation most likely due to\ninefficiencies in inter-node data synchronization and communication. These\nfindings highlight the need for further optimization to maximize each\nframework's capabilities. Future work will focus on enhancing reduction\nalgorithms, data communication, memory management, as wells as performing\nscalability studies, and a comprehensive statistical analysis to evaluate and\ncompare framework performance.",
      "url": "http://arxiv.org/abs/2511.02655v1",
      "published_time_eastern_timestamp": 1762270018.0
    },
    {
      "title": "Relaxed vs. Full Local Decodability with Few Queries: Equivalence and\n  Separations for Linear Codes",
      "summary": "A locally decodable code (LDC) $C \\colon \\{0,1\\}^k \\to \\{0,1\\}^n$ is an\nerror-correcting code that allows one to recover any bit of the original\nmessage with good probability while only reading a small number of bits from a\ncorrupted codeword. A relaxed locally decodable code (RLDC) is a weaker notion\nwhere the decoder is additionally allowed to abort and output a special symbol\n$\\bot$ if it detects an error. For a large constant number of queries $q$,\nthere is a large gap between the blocklength $n$ of the best $q$-query LDC and\nthe best $q$-query RLDC. Existing constructions of RLDCs achieve polynomial\nlength $n = k^{1 + O(1/q)}$, while the best-known $q$-LDCs only achieve\nsubexponential length $n = 2^{k^{o(1)}}$. On the other hand, for $q = 2$, it is\nknown that RLDCs and LDCs are equivalent. We thus ask the question: what is the\nsmallest $q$ such that there exists a $q$-RLDC that is not a $q$-LDC?\n  In this work, we show that any linear $3$-query RLDC is in fact a $3$-LDC,\ni.e., linear RLDCs and LDCs are equivalent at $3$ queries. More generally, we\nshow for any constant $q$, there is a soundness error threshold $s(q)$ such\nthat any linear $q$-RLDC with soundness error below this threshold must be a\n$q$-LDC. This implies that linear RLDCs cannot have \"strong soundness\" -- a\nstricter condition satisfied by linear LDCs that says the soundness error is\nproportional to the fraction of errors in the corrupted codeword -- unless they\nare simply LDCs.\n  In addition, we give simple constructions of linear $15$-query RLDCs that are\nnot $q$-LDCs for any constant $q$, showing that for $q = 15$, linear RLDCs and\nLDCs are not equivalent.\n  We also prove nearly identical results for locally correctable codes and\ntheir corresponding relaxed counterpart.",
      "url": "http://arxiv.org/abs/2511.02633v1",
      "published_time_eastern_timestamp": 1762268479.0
    },
    {
      "title": "Neural Network Interoperability Across Platforms",
      "summary": "The development of smart systems (i.e., systems enhanced with AI components)\nhas thrived thanks to the rapid advancements in neural networks (NNs). A wide\nrange of libraries and frameworks have consequently emerged to support NN\ndesign and implementation. The choice depends on factors such as available\nfunctionalities, ease of use, documentation and community support. After\nadopting a given NN framework, organizations might later choose to switch to\nanother if performance declines, requirements evolve, or new features are\nintroduced. Unfortunately, migrating NN implementations across libraries is\nchallenging due to the lack of migration approaches specifically tailored for\nNNs. This leads to increased time and effort to modernize NNs, as manual\nupdates are necessary to avoid relying on outdated implementations and ensure\ncompatibility with new features. In this paper, we propose an approach to\nautomatically migrate neural network code across deep learning frameworks. Our\nmethod makes use of a pivot NN model to create an abstraction of the NN prior\nto migration. We validate our approach using two popular NN frameworks, namely\nPyTorch and TensorFlow. We also discuss the challenges of migrating code\nbetween the two frameworks and how they were approached in our method.\nExperimental evaluation on five NNs shows that our approach successfully\nmigrates their code and produces NNs that are functionally equivalent to the\noriginals. Artefacts from our work are available online.",
      "url": "http://arxiv.org/abs/2511.02610v1",
      "published_time_eastern_timestamp": 1762266904.0
    },
    {
      "title": "UniChange: Unifying Change Detection with Multimodal Large Language\n  Model",
      "summary": "Change detection (CD) is a fundamental task for monitoring and analyzing land\ncover dynamics. While recent high performance models and high quality datasets\nhave significantly advanced the field, a critical limitation persists. Current\nmodels typically acquire limited knowledge from single-type annotated data and\ncannot concurrently leverage diverse binary change detection (BCD) and semantic\nchange detection (SCD) datasets. This constraint leads to poor generalization\nand limited versatility. The recent advancements in Multimodal Large Language\nModels (MLLMs) introduce new possibilities for a unified CD framework. We\nleverage the language priors and unification capabilities of MLLMs to develop\nUniChange, the first MLLM-based unified change detection model. UniChange\nintegrates generative language abilities with specialized CD functionalities.\nOur model successfully unifies both BCD and SCD tasks through the introduction\nof three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange\nutilizes text prompts to guide the identification of change categories,\neliminating the reliance on predefined classification heads. This design allows\nUniChange to effectively acquire knowledge from multi-source datasets, even\nwhen their class definitions conflict. Experiments on four public benchmarks\n(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,\nachieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,\nsurpassing all previous methods. The code is available at\nhttps://github.com/Erxucomeon/UniChange.",
      "url": "http://arxiv.org/abs/2511.02607v1",
      "published_time_eastern_timestamp": 1762266666.0
    }
  ]
}
{
  "last_updated": "2025-08-06T11:15:11.294667-04:00",
  "papers": [
    {
      "title": "LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences",
      "summary": "Generative world models have become essential data engines for autonomous\ndriving, yet most existing efforts focus on videos or occupancy grids,\noverlooking the unique LiDAR properties. Extending LiDAR generation to dynamic\n4D world modeling presents challenges in controllability, temporal coherence,\nand evaluation standardization. To this end, we present LiDARCrafter, a unified\nframework for 4D LiDAR generation and editing. Given free-form natural language\ninputs, we parse instructions into ego-centric scene graphs, which condition a\ntri-branch diffusion network to generate object structures, motion\ntrajectories, and geometry. These structured conditions enable diverse and\nfine-grained scene editing. Additionally, an autoregressive module generates\ntemporally coherent 4D LiDAR sequences with smooth transitions. To support\nstandardized evaluation, we establish a comprehensive benchmark with diverse\nmetrics spanning scene-, object-, and sequence-level aspects. Experiments on\nthe nuScenes dataset using this benchmark demonstrate that LiDARCrafter\nachieves state-of-the-art performance in fidelity, controllability, and\ntemporal consistency across all levels, paving the way for data augmentation\nand simulation. The code and benchmark are released to the community.",
      "url": "http://arxiv.org/abs/2508.03692v1",
      "published_time_eastern_timestamp": 1754416796.0
    },
    {
      "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
      "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.",
      "url": "http://arxiv.org/abs/2508.03686v1",
      "published_time_eastern_timestamp": 1754416524.0
    },
    {
      "title": "Self-Questioning Language Models",
      "summary": "Can large language models improve without external data -- by generating\ntheir own questions and answers? We hypothesize that a pre-trained language\nmodel can improve its reasoning skills given only a single prompt specifying\nthe topic (e.g., algebra word problems) and asking the model to generate its\nown questions. To do this, we propose Self-Questioning Language Models (SQLM):\nan asymmetric self-play framework where a proposer is given the topic and\ngenerates a question for a solver, who tries to answer it. Both the proposer\nand solver are trained via reinforcement learning. The proposer receives a\nreward if the problem is not too easy or too difficult, and the solver receives\na reward based on majority voting, a proxy for correctness in the absence of\nground-truth answers. For coding, the proposer can instead generate unit tests\nwhich are used for verification. We study this asymmetric self-play framework\non three benchmarks: three-digit multiplication, algebra problems from the\nOMEGA benchmark, and programming problems from Codeforces. By continually\ngenerating more interesting problems and attempting to solve them, language\nmodels can improve on downstream benchmarks without access to any curated\ntraining datasets.",
      "url": "http://arxiv.org/abs/2508.03682v1",
      "published_time_eastern_timestamp": 1754416293.0
    },
    {
      "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
      "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
      "url": "http://arxiv.org/abs/2508.03680v1",
      "published_time_eastern_timestamp": 1754416213.0
    },
    {
      "title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code\n  Generation",
      "summary": "State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement.",
      "url": "http://arxiv.org/abs/2508.03678v1",
      "published_time_eastern_timestamp": 1754416188.0
    },
    {
      "title": "Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland\n  Waterways",
      "summary": "Accurate geospatial information is crucial for safe, autonomous Inland\nWaterway Transport (IWT), as existing charts (IENC) lack real-time detail and\nconventional LiDAR SLAM fails in waterway environments. These challenges lead\nto vertical drift and non-semantic maps, hindering autonomous navigation.\n  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It\nuses an improved feature extraction and a water surface planar constraint to\nmitigate vertical drift. A novel pipeline transforms 3D point clouds into\nstructured 2D semantic maps using voxel-based geometric analysis, enabling\nreal-time computation of navigational parameters like bridge clearances. An\nautomated module extracts shorelines and exports them into a lightweight,\nIENC-compatible format.\n  Evaluations on a real-world dataset show Inland-LOAM achieves superior\nlocalization accuracy over state-of-the-art methods. The generated semantic\nmaps and shorelines align with real-world conditions, providing reliable data\nfor enhanced situational awareness. The code and dataset will be publicly\navailable",
      "url": "http://arxiv.org/abs/2508.03672v1",
      "published_time_eastern_timestamp": 1754415463.0
    },
    {
      "title": "Can Large Vision-Language Models Understand Multimodal Sarcasm?",
      "summary": "Sarcasm is a complex linguistic phenomenon that involves a disparity between\nliteral and intended meanings, making it challenging for sentiment analysis and\nother emotion-sensitive tasks. While traditional sarcasm detection methods\nprimarily focus on text, recent approaches have incorporated multimodal\ninformation. However, the application of Large Visual Language Models (LVLMs)\nin Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we\nevaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm\nDetection and Multimodal Sarcasm Explanation. Through comprehensive\nexperiments, we identify key limitations, such as insufficient visual\nunderstanding and a lack of conceptual knowledge. To address these issues, we\npropose a training-free framework that integrates in-depth object extraction\nand external conceptual knowledge to improve the model's ability to interpret\nand explain sarcasm in multimodal contexts. The experimental results on\nmultiple models show the effectiveness of our proposed framework. The code is\navailable at https://github.com/cp-cp/LVLM-MSA.",
      "url": "http://arxiv.org/abs/2508.03654v1",
      "published_time_eastern_timestamp": 1754413511.0
    },
    {
      "title": "DiWA: Diffusion Policy Adaptation with World Models",
      "summary": "Fine-tuning diffusion policies with reinforcement learning (RL) presents\nsignificant challenges. The long denoising sequence for each action prediction\nimpedes effective reward propagation. Moreover, standard RL methods require\nmillions of real-world interactions, posing a major bottleneck for practical\nfine-tuning. Although prior work frames the denoising process in diffusion\npolicies as a Markov Decision Process to enable RL-based updates, its strong\ndependence on environment interaction remains highly inefficient. To bridge\nthis gap, we introduce DiWA, a novel framework that leverages a world model for\nfine-tuning diffusion-based robotic skills entirely offline with reinforcement\nlearning. Unlike model-free approaches that require millions of environment\ninteractions to fine-tune a repertoire of robot skills, DiWA achieves effective\nadaptation using a world model trained once on a few hundred thousand offline\nplay interactions. This results in dramatically improved sample efficiency,\nmaking the approach significantly more practical and safer for real-world robot\nlearning. On the challenging CALVIN benchmark, DiWA improves performance across\neight tasks using only offline adaptation, while requiring orders of magnitude\nfewer physical interactions than model-free baselines. To our knowledge, this\nis the first demonstration of fine-tuning diffusion policies for real-world\nrobotic skills using an offline world model. We make the code publicly\navailable at https://diwa.cs.uni-freiburg.de.",
      "url": "http://arxiv.org/abs/2508.03645v1",
      "published_time_eastern_timestamp": 1754412950.0
    },
    {
      "title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via\n  Generalizable Gaussian Splatting from Unposed Multi-View Images",
      "summary": "Reconstructing and semantically interpreting 3D scenes from sparse 2D views\nremains a fundamental challenge in computer vision. Conventional methods often\ndecouple semantic understanding from reconstruction or necessitate costly\nper-scene optimization, thereby restricting their scalability and\ngeneralizability. In this paper, we introduce Uni3R, a novel feed-forward\nframework that jointly reconstructs a unified 3D scene representation enriched\nwith open-vocabulary semantics, directly from unposed multi-view images. Our\napproach leverages a Cross-View Transformer to robustly integrate information\nacross arbitrary multi-view inputs, which then regresses a set of 3D Gaussian\nprimitives endowed with semantic feature fields. This unified representation\nfacilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic\nsegmentation, and depth prediction, all within a single, feed-forward pass.\nExtensive experiments demonstrate that Uni3R establishes a new state-of-the-art\nacross multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on\nScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D\nscene reconstruction and understanding. The code is available at\nhttps://github.com/HorizonRobotics/Uni3R.",
      "url": "http://arxiv.org/abs/2508.03643v1",
      "published_time_eastern_timestamp": 1754412895.0
    },
    {
      "title": "Intent Preserving Generation of Diverse and Idiomatic (Code-)Artifacts",
      "summary": "When automatically generating programming exercise tasks one often also needs\nto automatically generate programs. At the very least when providing sample\nsolutions is part of automated feedback. But programs can also be used as part\nof the exercise task description to communicate a task's requirements.\n  Writing good program generators that produce varied yet idiomatic code while\nbeing easily adaptable for new tasks is challenging. The challenges are\nintensified if task generation requires additional artifacts, like a more\ngeneral behavior specification for testing or additional textual descriptions.\nManually writing generators for multiple different but strongly related\nartifacts gets complicated quickly.\n  We present an approach where instead of writing monolithic generators for\nmultiple connected artifacts one specifies a small set of abstract building\nblocks and for each such building block defines sets of concrete realizations\nfor various kinds of artifacts. Then the intended structure of the resulting\nartifacts is specified as a composition of the small abstract building blocks.\nThis abstract description then serves as the common source from which related\nartifacts can be derived automatically. The approach is generic in the kind of\nartifacts it can produce and is therefore adaptable to a wide range of\ncontexts.",
      "url": "http://arxiv.org/abs/2508.03642v1",
      "published_time_eastern_timestamp": 1754412855.0
    },
    {
      "title": "Refining Critical Thinking in LLM Code Generation: A Faulty\n  Premise-based Evaluation Framework",
      "summary": "With the advancement of code generation capabilities in large language models\n(LLMs), their reliance on input premises has intensified. When users provide\ninputs containing faulty premises, the probability of code generation\nhallucinations rises significantly, exposing deficiencies in their\nself-scrutiny capabilities. This paper proposes Faulty Premises Bench\n(FPBench), the first code generation evaluation framework targeting faulty\npremises. By systematically constructing three categories of faulty premises\nand integrating multi-dimensional evaluation metrics, it conducts in-depth\nassessments of 15 representative LLMs. The key findings are as follows: (1)\nMost models exhibit poor reasoning abilities and suboptimal code generation\nperformance under faulty premises, heavily relying on explicit prompts for\nerror detection, with limited self-scrutiny capabilities; (2) Faulty premises\ntrigger a point of diminishing returns in resource investment, leading to\nblindly increasing length fails to enhance quality; (3) The three types of\nfaulty premises respectively activate distinct defect patterns in models,\nrevealing a triple dissociation in the cognitive mechanisms of code generation\nmodels. This study not only highlights the urgent need for LLMs to proactively\nverify premises in code generation but also, through the proposed FPBench\nframework and multi-dimensional evaluation system, provides a theoretical\nfoundation and practical pathway for developing reliable, human-centric code\ngeneration models.",
      "url": "http://arxiv.org/abs/2508.03622v1",
      "published_time_eastern_timestamp": 1754411979.0
    },
    {
      "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction",
      "summary": "We introduce Goedel-Prover-V2, a series of open-source language models that\nset a new state-of-the-art in automated theorem proving. Built on the standard\nexpert iteration and reinforcement learning pipeline, our approach incorporates\nthree key innovations: (1) Scaffolded data synthesis: We generate synthetic\ntasks of increasing difficulty to train the model to master increasingly\ncomplex theorems; (2) Verifier-guided self-correction: We enable the model to\niteratively revise its proofs by leveraging feedback from the Lean compiler;\n(3) Model averaging: We merge model checkpoints to mitigate the decrease in\nmodel output diversity in later stages of training. Our small model,\nGoedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms\nDeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our\nflagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in\nstandard mode and 90.4% in self-correction mode, outperforming prior SOTA by a\nlarge margin. Additionally, our flagship model solves 86 problems on\nPutnamBench at pass@184, securing the first place among open-source models on\nthe leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47\nproblems by pass@1024 with a significantly smaller model size and compute\nbudget. At the time of its release (July-August 2025), Goedel-Prover-V2\nachieves the strongest overall performance among all open-source theorem\nprovers. It also ranks among the top-performing models--including closed-source\nsystems with publicly reported performance--under a constrained test-time\ncompute budget. Our models, code, and data are released at\nhttps://github.com/Goedel-LM/Goedel-Prover-V2.",
      "url": "http://arxiv.org/abs/2508.03613v1",
      "published_time_eastern_timestamp": 1754411302.0
    },
    {
      "title": "Block: Balancing Load in LLM Serving with Context, Knowledge and\n  Predictive Scheduling",
      "summary": "This paper presents Block, a distributed scheduling framework designed to\noptimize load balancing and auto-provisioning across instances in large\nlanguage model serving frameworks by leveraging contextual information from\nincoming requests. Unlike popular model serving systems that rely on monolithic\nand heuristic task schedulers, Block operates as a fully distributed,\nstateless, and predictive scheduling system to achieve low overhead,\nreliability, and scalability. It leverages the deterministic and predictable\ncharacteristics of LLM inferences, such as host configurations, response\nlengths, and hardware performance, to make scheduling decisions based on\naccurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block\nsignificantly outperforms heuristic schedulers, boosting serving capacity by up\nto 16.7\\% and reducing P99 tail latency by up to 49.5\\%. These performance\ngains remain consistent across diverse models, workloads and configurations.\nCode and data are open-sourced.",
      "url": "http://arxiv.org/abs/2508.03611v1",
      "published_time_eastern_timestamp": 1754411230.0
    },
    {
      "title": "ReFuzzer: Feedback-Driven Approach to Enhance Validity of LLM-Generated\n  Test Programs",
      "summary": "Existing LLM-based compiler fuzzers often produce syntactically or\nsemantically invalid test programs, limiting their effectiveness in exercising\ncompiler optimizations and backend components. We introduce ReFuzzer, a\nframework for refining LLM-generated test programs by systematically detecting\nand correcting compilation and runtime violations (e.g. division by zero or\narray out-of-bounds accesses). ReFuzzer employs a feedback loop with a local\nLLM to validate and filter erroneous programs before execution, improving\nfuzzing effectiveness beyond crash detection and enabling the generation of\ndiverse yet valid test programs.\n  We evaluated ReFuzzer's effectiveness across black-, grey- and white-box\nfuzzing approaches targeting LLVM/Clang. ReFuzzer improved test programs'\nvalidity from 47.0-49.4% to 96.6-97.3%, with an average processing time of\n2.9-3.5 s per test program on a dual-GPU machine. Further, refuzzing\nsignificantly increased code coverage in critical optimization and IR\ngeneration components. For example, vectorization coverage had an absolute\nimprovement of 9.2%, 2.3%, and 7.1% in black-, grey-, and white-box fuzzing,\nenhancing testing effectiveness.",
      "url": "http://arxiv.org/abs/2508.03603v1",
      "published_time_eastern_timestamp": 1754410622.0
    },
    {
      "title": "Optimal Quantum $(r,δ)$-Locally Repairable Codes From\n  Matrix-Product Codes",
      "summary": "This paper studies optimal quantum $(r,\\delta)$-LRCs from matrix-product (MP)\ncodes. We establish a necessary and sufficient condition for an MP code to be\nan optimal $(r,\\delta)$-LRC. Based on this, we present a characterization for\noptimal quantum $(r,\\delta)$-LRCs from MP codes with nested constituent codes,\nand also study optimal quantum $(r,\\delta)$-LRCs constructed from MP codes with\nnon-nested constituent codes. Through Hermitian dual-containing and Euclidean\ndual-containing MP codes, we present five infinite families of optimal quantum\n$(r,\\delta)$-LRCs with flexible parameters.",
      "url": "http://arxiv.org/abs/2508.03597v1",
      "published_time_eastern_timestamp": 1754409914.0
    },
    {
      "title": "On the (In)Significance of Feature Selection in High-Dimensional\n  Datasets",
      "summary": "Extensive research has been done on feature selection (FS) algorithms for\nhigh-dimensional datasets aiming to improve model performance, reduce\ncomputational cost and identify features of interest. We test the null\nhypothesis of using randomly selected features to compare against features\nselected by FS algorithms to validate the performance of the latter. Our\nresults show that FS on high-dimensional datasets (in particular gene\nexpression) in classification tasks is not useful. We find that (1) models\ntrained on small subsets (0.02%-1% of all features) of randomly selected\nfeatures almost always perform comparably to those trained on all features, and\n(2) a \"typical\"- sized random subset provides comparable or superior\nperformance to that of top-k features selected in various published studies.\nThus, our work challenges many feature selection results on high dimensional\ndatasets, particularly in computational genomics. It raises serious concerns\nabout studies that propose drug design or targeted interventions based on\ncomputationally selected genes, without further validation in a wet lab.",
      "url": "http://arxiv.org/abs/2508.03593v1",
      "published_time_eastern_timestamp": 1754409511.0
    },
    {
      "title": "Detectability of compact intermediate-mass black hole binaries as\n  low-frequency gravitational wave sources: the influence of dynamical friction\n  of dark matter",
      "summary": "The black hole (BH) spin could significantly change the density of dark\nmatter (DM) in its vicinity, creating a mini-spike of the density of DM. The\ndynamical friction (DF) between DM and the companion star of a BH can provide\nan efficient loss of angular momentum, driving the BH-main sequence (MS) star\nbinary to evolve toward a compact orbit system. We investigate the influence of\nthe DF of DM on the detectability of intermediate-mass black hole (IMBH)-MS\nbinaries as low-frequency gravitational wave (GW) sources. Taking into account\nthe DF of DM, we employ the detailed binary evolution code MESA to model the\nevolution of a large number of IMBH-MS binaries. Our simulation shows that the\nDF of DM can drive those IMBH-MS binaries to evolve toward low-frequency GW\nsources for a low donor-star mass, a high spike index, or a short initial\norbital period. When the spike index $\\gamma=1.60$, those IMBH-MS binaries with\ndonor-star masses of $1.0-3.4~ M_{\\odot}$ and initial orbital periods of\n$0.65-16.82~ \\rm days$ could potentially evolve into visible LISA sources\nwithin a distance of $10~\\rm kpc$. The DF of DM can enlarge the initial\nparameter space and prolong the bifurcation periods. In the low-frequency GW\nsource stage, the X-ray luminosities of those IMBH X-ray binaries are $\\sim\n10^{35}-10^{36}~\\rm erg\\,s^{-1}$, hence they are ideal multimessenger objects.",
      "url": "http://arxiv.org/abs/2508.03582v1",
      "published_time_eastern_timestamp": 1754408974.0
    },
    {
      "title": "In-Memory Non-Binary LDPC Decoding",
      "summary": "Low-density parity-check (LDPC) codes are an important feature of several\ncommunication and storage applications, offering a flexible and effective\nmethod for error correction. These codes are computationally complex and\nrequire the exploitation of parallel processing to meet real-time constraints.\nAs advancements in arithmetic and logic unit technology allowed for higher\nperformance of computing systems, memory technology has not kept the same pace\nof development, creating a data movement bottleneck and affecting parallel\nprocessing systems more dramatically. To alleviate the severity of this\nbottleneck, several solutions have been proposed, namely the processing\nin-memory (PiM) paradigm that involves the design of compute units to where (or\nnear) the data is stored, utilizing thousands of low-complexity processing\nunits to perform out bit-wise and simple arithmetic operations. This paper\npresents a novel efficient solution for near-memory non-binary LDPC decoders in\nthe UPMEM system, for the best of our knowledge the first real hardware\nPiM-based non-binary LDPC decoder that is benchmarked against low-power GPU\nparallel solutions highly optimized for throughput performance. PiM-based\nnon-binary LDPC decoders can achieve 76 Mbit/s of decoding throughput, which is\neven competitive when compared against implementations running in edge GPUs.",
      "url": "http://arxiv.org/abs/2508.03567v1",
      "published_time_eastern_timestamp": 1754408218.0
    },
    {
      "title": "SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation\n  Models to Downstream Segmentation Tasks",
      "summary": "Recent studies have highlighted the potential of adapting the Segment\nAnything Model (SAM) for various downstream tasks. However, constructing a more\npowerful and generalizable encoder to further enhance performance remains an\nopen challenge. In this work, we propose SAM2-UNeXT, an advanced framework that\nbuilds upon the core principles of SAM2-UNet while extending the\nrepresentational capacity of SAM2 through the integration of an auxiliary\nDINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue\nlayer, our approach enables more accurate segmentation with a simple\narchitecture, relaxing the need for complex decoder designs. Extensive\nexperiments conducted on four benchmarks, including dichotomous image\nsegmentation, camouflaged object detection, marine animal segmentation, and\nremote sensing saliency detection, demonstrate the superior performance of our\nproposed method. The code is available at\nhttps://github.com/WZH0120/SAM2-UNeXT.",
      "url": "http://arxiv.org/abs/2508.03566v1",
      "published_time_eastern_timestamp": 1754408173.0
    },
    {
      "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought",
      "summary": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.",
      "url": "http://arxiv.org/abs/2508.03560v1",
      "published_time_eastern_timestamp": 1754407728.0
    }
  ]
}
{
  "last_updated": "2025-09-12T05:11:56.557586-04:00",
  "papers": [
    {
      "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
      "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .",
      "url": "http://arxiv.org/abs/2509.09680v1",
      "published_time_eastern_timestamp": 1757613599.0
    },
    {
      "title": "Cosmology inference with perturbative forward modeling at the field\n  level: a comparison with joint power spectrum and bispectrum analyses",
      "summary": "We extend field-level inference to jointly constrain the cosmological\nparameters $\\{A,\\omega_{\\rm cdm},H_0\\}$, in both real and redshift space. Our\nanalyses are based on mock data generated using a perturbative forward model,\nwith noise drawn from a Gaussian distribution with a constant power spectrum.\nThis idealized setting, where the field-level likelihood is exactly Gaussian,\nallows us to precisely quantify the information content in the nonlinear field\non large scales. We find that field-level inference accurately recovers all\ncosmological parameters in both real and redshift space, with uncertainties\nconsistent with perturbation theory expectations. We show that these error bars\nare comparable to those obtained from a joint power spectrum and bispectrum\nanalysis using the same perturbative model. Finally, we perform several tests\nusing the Gaussian field-level likelihood to fit the mock data where the true\nnoise model is non-Gaussian, and find significant biases in the inferred\ncosmological parameters. These results highlight that the success of\nfield-level inference critically depends on using the correct likelihood, which\nmay be the primary challenge for applying this method to smaller scales even in\nthe perturbative regime.",
      "url": "http://arxiv.org/abs/2509.09673v1",
      "published_time_eastern_timestamp": 1757613552.0
    },
    {
      "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
      "summary": "Hallucinations in multimodal large language models (MLLMs) -- where the model\ngenerates content inconsistent with the input image -- pose significant risks\nin real-world applications, from misinformation in visual question answering to\nunsafe errors in decision-making. Existing benchmarks primarily test\nrecognition accuracy, i.e., evaluating whether models can select the correct\nanswer among distractors. This overlooks an equally critical capability for\ntrustworthy AI: recognizing when none of the provided options are correct, a\nbehavior reflecting epistemic humility. We present HumbleBench, a new\nhallucination benchmark designed to evaluate MLLMs' ability to reject plausible\nbut incorrect answers across three hallucination types: object, relation, and\nattribute. Built from a panoptic scene graph dataset, we leverage fine-grained\nscene graph annotations to extract ground-truth entities and relations, and\nprompt GPT-4-Turbo to generate multiple-choice questions, followed by a\nrigorous manual filtering process. Each question includes a \"None of the above\"\noption, requiring models not only to recognize correct visual information but\nalso to identify when no provided answer is valid. We evaluate a variety of\nstate-of-the-art MLLMs -- including both general-purpose and specialized\nreasoning models -- on HumbleBench and share valuable findings and insights\nwith the community. By incorporating explicit false-option rejection,\nHumbleBench fills a key gap in current evaluation suites, providing a more\nrealistic measure of MLLM reliability in safety-critical settings. Our code and\ndataset are released publicly and can be accessed at\nhttps://github.com/maifoundations/HumbleBench.",
      "url": "http://arxiv.org/abs/2509.09658v1",
      "published_time_eastern_timestamp": 1757613240.0
    },
    {
      "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio\n  Regulations",
      "summary": "We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG.",
      "url": "http://arxiv.org/abs/2509.09651v1",
      "published_time_eastern_timestamp": 1757612622.0
    },
    {
      "title": "Orthogonal Latin Squares of Order Ten with Two Relations: A SAT\n  Investigation",
      "summary": "A $k$-net($n$) is a combinatorial design equivalent to $k-2$ mutually\northogonal Latin squares of order $n$. A relation in a net is a linear\ndependency over $\\mathbb{F}_2$ in the incidence matrix of the net. A\ncomputational enumeration of all orthogonal pairs of Latin squares of order 10\nwhose corresponding nets have at least two nontrivial relations was achieved by\nDelisle in 2010 and verified by an independent search of Myrvold. In this\npaper, we confirm the correctness of their exhaustive enumerations with a\nsatisfiability (SAT) solver approach instead of using custom-written\nbacktracking code. Performing the enumeration using a SAT solver has at least\nthree advantages. First, it reduces the amount of trust necessary, as SAT\nsolvers produce independently-verifiable certificates that their enumerations\nare complete. These certificates can be checked by formal proof verifiers that\nare relatively simple pieces of software, and therefore easier to trust.\nSecond, it is typically more straightforward and less error-prone to use a SAT\nsolver over writing search code. Third, it can be more efficient to use a\nSAT-based approach, as SAT solvers are highly optimized pieces of software\nincorporating backtracking-with-learning for improving the efficiency of the\nbacktracking search. For example, the SAT solver completely enumerates all\northogonal pairs of Latin squares of order ten with two nontrivial relations in\nunder 2 hours on a desktop machine, while Delisle's 2010 search used 11,700 CPU\nhours. Although computer hardware was slower in 2010, this alone cannot explain\nthe improvement in the efficiency of our SAT-based search.",
      "url": "http://arxiv.org/abs/2509.09633v1",
      "published_time_eastern_timestamp": 1757611112.0
    },
    {
      "title": "I Know Who Clones Your Code: Interpretable Smart Contract Similarity\n  Detection",
      "summary": "Widespread reuse of open-source code in smart contract development boosts\nprogramming efficiency but significantly amplifies bug propagation across\ncontracts, while dedicated methods for detecting similar smart contract\nfunctions remain very limited. Conventional abstract-syntax-tree (AST) based\nmethods for smart contract similarity detection face challenges in handling\nintricate tree structures, which impedes detailed semantic comparison of code.\nRecent deep-learning based approaches tend to overlook code syntax and\ndetection interpretability, resulting in suboptimal performance.\n  To fill this research gap, we introduce SmartDetector, a novel approach for\ncomputing similarity between smart contract functions, explainable at the\nfine-grained statement level. Technically, SmartDetector decomposes the AST of\na smart contract function into a series of smaller statement trees, each\nreflecting a structural element of the source code. Then, SmartDetector uses a\nclassifier to compute the similarity score of two functions by comparing each\npair of their statement trees. To address the infinite hyperparameter space of\nthe classifier, we mathematically derive a cosine-wise diffusion process to\nefficiently search optimal hyperparameters. Extensive experiments conducted on\nthree large real-world datasets demonstrate that SmartDetector outperforms\ncurrent state-of-the-art methods by an average improvement of 14.01% in\nF1-score, achieving an overall average F1-score of 95.88%.",
      "url": "http://arxiv.org/abs/2509.09630v1",
      "published_time_eastern_timestamp": 1757610951.0
    },
    {
      "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
      "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
      "url": "http://arxiv.org/abs/2509.09614v1",
      "published_time_eastern_timestamp": 1757609704.0
    }
  ]
}
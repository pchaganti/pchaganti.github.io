{
  "last_updated": "2025-05-27T17:11:17.781208-04:00",
  "papers": [
    {
      "title": "Multimodal Federated Learning With Missing Modalities through Feature\n  Imputation Network",
      "summary": "Multimodal federated learning holds immense potential for collaboratively\ntraining models from multiple sources without sharing raw data, addressing both\ndata scarcity and privacy concerns, two key challenges in healthcare. A major\nchallenge in training multimodal federated models in healthcare is the presence\nof missing modalities due to multiple reasons, including variations in clinical\npractice, cost and accessibility constraints, retrospective data collection,\nprivacy concerns, and occasional technical or human errors. Previous methods\ntypically rely on publicly available real datasets or synthetic data to\ncompensate for missing modalities. However, obtaining real datasets for every\ndisease is impractical, and training generative models to synthesize missing\nmodalities is computationally expensive and prone to errors due to the high\ndimensionality of medical data. In this paper, we propose a novel, lightweight,\nlow-dimensional feature translator to reconstruct bottleneck features of the\nmissing modalities. Our experiments on three different datasets (MIMIC-CXR, NIH\nOpen-I, and CheXpert), in both homogeneous and heterogeneous settings\nconsistently improve the performance of competitive baselines. The code and\nimplementation details are available at:\nhttps://github.com/bhattarailab/FedFeatGen",
      "url": "http://arxiv.org/abs/2505.20232v1",
      "published_time_eastern_timestamp": 1748279463.0
    },
    {
      "title": "Towards the Automated Extraction and Refactoring of NoSQL Schemas from\n  Application Code",
      "summary": "In this paper, we present a static code analysis strategy to extract logical\nschemas from NoSQL applications. Our solution is based on a model-driven\nreverse engineering process composed of a chain of platform-independent model\ntransformations. The extracted schema conforms to the \\uschema{} unified\nmetamodel, which can represent both NoSQL and relational schemas. To support\nthis process, we define a metamodel capable of representing the core elements\nof object-oriented languages. Application code is first injected into a code\nmodel, from which a control flow model is derived. This, in turn, enables the\ngeneration of a model representing both data access operations and the\nstructure of stored data. From these models, the \\uschema{} logical schema is\ninferred. Additionally, the extracted information can be used to identify\nrefactoring opportunities. We illustrate this capability through the detection\nof join-like query patterns and the automated application of field duplication\nstrategies to eliminate expensive joins. All stages of the process are\ndescribed in detail, and the approach is validated through a round-trip\nexperiment in which a application using a MongoDB store is automatically\ngenerated from a predefined schema. The inferred schema is then compared to the\noriginal to assess the accuracy of the extraction process.",
      "url": "http://arxiv.org/abs/2505.20230v1",
      "published_time_eastern_timestamp": 1748279283.0
    },
    {
      "title": "From What to How: Attributing CLIP's Latent Components Reveals\n  Unexpected Semantic Reliance",
      "summary": "Transformer-based CLIP models are widely used for text-image probing and\nfeature extraction, making it relevant to understand the internal mechanisms\nbehind their predictions. While recent works show that Sparse Autoencoders\n(SAEs) yield interpretable latent components, they focus on what these encode\nand miss how they drive predictions. We introduce a scalable framework that\nreveals what latent components activate for, how they align with expected\nsemantics, and how important they are to predictions. To achieve this, we adapt\nattribution patching for instance-wise component attributions in CLIP and\nhighlight key faithfulness limitations of the widely used Logit Lens technique.\nBy combining attributions with semantic alignment scores, we can automatically\nuncover reliance on components that encode semantically unexpected or spurious\nconcepts. Applied across multiple CLIP variants, our method uncovers hundreds\nof surprising components linked to polysemous words, compound nouns, visual\ntypography and dataset artifacts. While text embeddings remain prone to\nsemantic ambiguity, they are more robust to spurious correlations compared to\nlinear classifiers trained on image embeddings. A case study on skin lesion\ndetection highlights how such classifiers can amplify hidden shortcuts,\nunderscoring the need for holistic, mechanistic interpretability. We provide\ncode at https://github.com/maxdreyer/attributing-clip.",
      "url": "http://arxiv.org/abs/2505.20229v1",
      "published_time_eastern_timestamp": 1748279282.0
    },
    {
      "title": "FLAME-MoE: A Transparent End-to-End Research Platform for\n  Mixture-of-Experts Language Models",
      "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE.",
      "url": "http://arxiv.org/abs/2505.20225v1",
      "published_time_eastern_timestamp": 1748279185.0
    },
    {
      "title": "Fine-grained List-wise Alignment for Generative Medication\n  Recommendation",
      "summary": "Accurate and safe medication recommendations are critical for effective\nclinical decision-making, especially in multimorbidity cases. However, existing\nsystems rely on point-wise prediction paradigms that overlook synergistic drug\neffects and potential adverse drug-drug interactions (DDIs). We propose FLAME,\na fine-grained list-wise alignment framework for large language models (LLMs),\nenabling drug-by-drug generation of drug lists. FLAME formulates recommendation\nas a sequential decision process, where each step adds or removes a single\ndrug. To provide fine-grained learning signals, we devise step-wise Group\nRelative Policy Optimization (GRPO) with potential-based reward shaping, which\nexplicitly models DDIs and optimizes the contribution of each drug to the\noverall prescription. Furthermore, FLAME enhances patient modeling by\nintegrating structured clinical knowledge and collaborative information into\nthe representation space of LLMs. Experiments on benchmark datasets demonstrate\nthat FLAME achieves state-of-the-art performance, delivering superior accuracy,\ncontrollable safety-accuracy trade-offs, and strong generalization across\ndiverse clinical scenarios. Our code is available at\nhttps://github.com/cxfann/Flame.",
      "url": "http://arxiv.org/abs/2505.20218v1",
      "published_time_eastern_timestamp": 1748278763.0
    },
    {
      "title": "Dependency Parsing is More Parameter-Efficient with Normalization",
      "summary": "Dependency parsing is the task of inferring natural language structure, often\napproached by modeling word interactions via attention through biaffine\nscoring. This mechanism works like self-attention in Transformers, where scores\nare calculated for every pair of words in a sentence. However, unlike\nTransformer attention, biaffine scoring does not use normalization prior to\ntaking the softmax of the scores. In this paper, we provide theoretical\nevidence and empirical results revealing that a lack of normalization\nnecessarily results in overparameterized parser models, where the extra\nparameters compensate for the sharp softmax outputs produced by high variance\ninputs to the biaffine scoring function. We argue that biaffine scoring can be\nmade substantially more efficient by performing score normalization. We conduct\nexperiments on six datasets for semantic and syntactic dependency parsing using\na one-hop parser. We train N-layer stacked BiLSTMs and evaluate the parser's\nperformance with and without normalizing biaffine scores. Normalizing allows us\nto beat the state of the art on two datasets, with fewer samples and trainable\nparameters. Code: https://anonymous.4open.science/r/EfficientSDP-70C1",
      "url": "http://arxiv.org/abs/2505.20215v1",
      "published_time_eastern_timestamp": 1748278567.0
    },
    {
      "title": "Tools for Characterizing the Numerical Error of Stellar Oscillation\n  Codes",
      "summary": "Stellar oscillation codes are software instruments that evaluate the\nnormal-mode frequencies of an input stellar model. While inter-code comparisons\nare often used to confirm the correctness of calculations, they are not\nsuitable for characterizing the numerical error of an individual code. To\naddress this issue, we introduce a set of tools -- 'error measures' -- that\nfacilitate this characterization. We explore the behavior of these error\nmeasures as calculation parameters, such as the number of radial grid points\nused to discretize the oscillation equations, are varied; and we summarize this\nbehavior via an idealized error model. While our analysis focuses on the GYRE\ncode, it remains broadly applicable to other oscillation codes.",
      "url": "http://arxiv.org/abs/2505.20212v1",
      "published_time_eastern_timestamp": 1748278384.0
    },
    {
      "title": "Evaluating Large Language Models for Code Review",
      "summary": "Context: Code reviews are crucial for software quality. Recent AI advances\nhave allowed large language models (LLMs) to review and fix code; now, there\nare tools that perform these reviews. However, their reliability and accuracy\nhave not yet been systematically evaluated. Objective: This study compares\ndifferent LLMs' performance in detecting code correctness and suggesting\nimprovements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated\ncode blocks of varying correctness, along with 164 canonical code blocks from\nthe HumanEval benchmark. To simulate the code review task objectively, we\nexpected LLMs to assess code correctness and improve the code if needed. We ran\nexperiments with different configurations and reported on the results. Results:\nWith problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code\ncorrectness 68.50% and 63.89% of the time, respectively, and corrected the code\n67.83% and 54.26% of the time for the 492 code blocks of varying correctness.\nWithout problem descriptions, performance declined. The results for the 164\ncanonical code blocks differed, suggesting that performance depends on the type\nof code. Conclusion: LLM code reviews can help suggest improvements and assess\ncorrectness, but there is a risk of faulty outputs. We propose a process that\ninvolves humans, called the \"Human in the loop LLM Code Review\" to promote\nknowledge sharing while mitigating the risk of faulty outputs.",
      "url": "http://arxiv.org/abs/2505.20206v1",
      "published_time_eastern_timestamp": 1748278049.0
    },
    {
      "title": "FunReason: Enhancing Large Language Models' Function Calling via\n  Self-Refinement Multiscale Loss and Automated Data Refinement",
      "summary": "The integration of large language models (LLMs) with function calling has\nemerged as a crucial capability for enhancing their practical utility in\nreal-world applications. However, effectively combining reasoning processes\nwith accurate function execution remains a significant challenge. Traditional\ntraining approaches often struggle to balance the detailed reasoning steps with\nthe precision of function calls, leading to suboptimal performance. To address\nthese limitations, we introduce FunReason, a novel framework that enhances\nLLMs' function calling capabilities through an automated data refinement\nstrategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason\nleverages LLMs' natural reasoning abilities to generate high-quality training\nexamples, focusing on query parseability, reasoning coherence, and function\ncall precision. The SRML approach dynamically balances the contribution of\nreasoning processes and function call accuracy during training, addressing the\ninherent trade-off between these two critical aspects. FunReason achieves\nperformance comparable to GPT-4o while effectively mitigating catastrophic\nforgetting during fine-tuning. FunReason provides a comprehensive solution for\nenhancing LLMs' function calling capabilities by introducing a balanced\ntraining methodology and a data refinement pipeline. For code and dataset,\nplease refer to our repository at GitHub\nhttps://github.com/BingguangHao/FunReason",
      "url": "http://arxiv.org/abs/2505.20192v1",
      "published_time_eastern_timestamp": 1748277486.0
    },
    {
      "title": "Research on feature fusion and multimodal patent text based on graph\n  attention network",
      "summary": "Aiming at the problems of cross-modal feature fusion, low efficiency of long\ntext modeling and lack of hierarchical semantic coherence in patent text\nsemantic mining, this study proposes HGM-Net, a deep learning framework that\nintegrates Hierarchical Comparative Learning (HCL), Multi-modal Graph Attention\nNetwork (M-GAT) and Multi-Granularity Sparse Attention (MSA), which builds a\ndynamic mask, contrast and cross-structural similarity constraints on the word,\nsentence and paragraph hierarchies through HCL. Contrast and cross-structural\nsimilarity constraints are constructed at the word and paragraph levels by HCL\nto strengthen the local semantic and global thematic consistency of patent\ntext; M-GAT models patent classification codes, citation relations and text\nsemantics as heterogeneous graph structures, and achieves dynamic fusion of\nmulti-source features by cross-modal gated attention; MSA adopts a hierarchical\nsparsity strategy to optimize the computational efficiency of long text\nmodeling at word, phrase, sentence and paragraph granularity. Experiments show\nthat the framework demonstrates significant advantages over existing deep\nlearning methods in tasks such as patent classification and similarity\nmatching, and provides a solution with both theoretical innovation and\npractical value for solving the problems of patent examination efficiency\nimprovement and technology relevance mining.",
      "url": "http://arxiv.org/abs/2505.20188v1",
      "published_time_eastern_timestamp": 1748277163.0
    },
    {
      "title": "Eradicating the Unseen: Detecting, Exploiting, and Remediating a Path\n  Traversal Vulnerability across GitHub",
      "summary": "Vulnerabilities in open-source software can cause cascading effects in the\nmodern digital ecosystem. It is especially worrying if these vulnerabilities\nrepeat across many projects, as once the adversaries find one of them, they can\nscale up the attack very easily. Unfortunately, since developers frequently\nreuse code from their own or external code resources, some nearly identical\nvulnerabilities exist across many open-source projects.\n  We conducted a study to examine the prevalence of a particular vulnerable\ncode pattern that enables path traversal attacks (CWE-22) across open-source\nGitHub projects. To handle this study at the GitHub scale, we developed an\nautomated pipeline that scans GitHub for the targeted vulnerable pattern,\nconfirms the vulnerability by first running a static analysis and then\nexploiting the vulnerability in the context of the studied project, assesses\nits impact by calculating the CVSS score, generates a patch using GPT-4, and\nreports the vulnerability to the maintainers.\n  Using our pipeline, we identified 1,756 vulnerable open-source projects, some\nof which are very influential. For many of the affected projects, the\nvulnerability is critical (CVSS score higher than 9.0), as it can be exploited\nremotely without any privileges and critically impact the confidentiality and\navailability of the system. We have responsibly disclosed the vulnerability to\nthe maintainers, and 14\\% of the reported vulnerabilities have been remediated.\n  We also investigated the root causes of the vulnerable code pattern and\nassessed the side effects of the large number of copies of this vulnerable\npattern that seem to have poisoned several popular LLMs. Our study highlights\nthe urgent need to help secure the open-source ecosystem by leveraging scalable\nautomated vulnerability management solutions and raising awareness among\ndevelopers.",
      "url": "http://arxiv.org/abs/2505.20186v1",
      "published_time_eastern_timestamp": 1748276961.0
    },
    {
      "title": "THiNK: Can Large Language Models Think-aloud?",
      "summary": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository.",
      "url": "http://arxiv.org/abs/2505.20184v1",
      "published_time_eastern_timestamp": 1748276822.0
    },
    {
      "title": "An Empirical Study on Strong-Weak Model Collaboration for Repo-level\n  Code Generation",
      "summary": "We study cost-efficient collaboration between strong and weak language models\nfor repository-level code generation, where the weak model handles simpler\ntasks at lower cost, and the most challenging tasks are delegated to the strong\nmodel. While many works propose architectures for this task, few analyze\nperformance relative to cost. We evaluate a broad spectrum of collaboration\nstrategies: context-based, pipeline-based, and dynamic, on GitHub issue\nresolution. Our most effective collaborative strategy achieves equivalent\nperformance to the strong model while reducing the cost by 40%. Based on our\nfindings, we offer actionable guidelines for choosing collaboration strategies\nunder varying budget and performance constraints. Our results show that\nstrong-weak collaboration substantially boosts the weak model's performance at\na fraction of the cost, pipeline and context-based methods being most\nefficient. We release the code for our work at\nhttps://github.com/shubhamrgandhi/codegen-strong-weak-collab.",
      "url": "http://arxiv.org/abs/2505.20182v1",
      "published_time_eastern_timestamp": 1748276738.0
    },
    {
      "title": "Program of Equations Thoughts to Solve Algebra Word Problems",
      "summary": "Solving algebraic word problems (AWPs) has recently emerged as an important\nnatural language processing task. Recently, large language models (LLMs) have\ndemonstrated powerful mathematical capabilities, and the Chain-of-Thought\ntechnique, which guides LLMs through step-by-step reasoning, has yielded\nimpressive results. However, this reasoning ability is limited by the\ncomputational weaknesses of LLMs themselves, where calculation errors can\naccumulate, leading to incorrect final answers. To address this, we propose\nProgram of Equations Thoughts (POET), which transforms the task of generating\nstep-by-step reasoning answers into a two-stage task of predicting equations\nand generating code, offloading complex computations to a Python interpreter to\navoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which\nutilizes a manually designed template to enable LLMs to directly generate\nPython code for one-step solving. Our method achieves accuracies of 95.3% and\n98.0% on the PEN and ALG514 datasets, respectively, setting a new\nstate-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%\non the DRAW-1K dataset.",
      "url": "http://arxiv.org/abs/2505.20170v1",
      "published_time_eastern_timestamp": 1748275924.0
    },
    {
      "title": "Evaluating Software Plagiarism Detection in the Age of AI: Automated\n  Obfuscation and Lessons for Academic Integrity",
      "summary": "Plagiarism in programming assignments is a persistent issue in computer\nscience education, increasingly complicated by the emergence of automated\nobfuscation attacks. While software plagiarism detectors are widely used to\nidentify suspicious similarities at scale and are resilient to simple\nobfuscation techniques, they are vulnerable to advanced obfuscation based on\nstructural modification of program code that preserves the original program\nbehavior. While different defense mechanisms have been proposed to increase\nresilience against these attacks, their current evaluation is limited to the\nscope of attacks used and lacks a comprehensive investigation regarding\nAI-based obfuscation. In this paper, we investigate the resilience of these\ndefense mechanisms against a broad range of automated obfuscation attacks,\nincluding both algorithmic and AI-generated methods, and for a wide variety of\nreal-world datasets. We evaluate the improvements of two defense mechanisms\nover the plagiarism detector JPlag across over four million pairwise program\ncomparisons. Our results show significant improvements in detecting obfuscated\nplagiarism instances, and we observe an improved detection of AI-generated\nprograms, even though the defense mechanisms are not designed for this use\ncase. Based on our findings, we provide an in-depth discussion of their broader\nimplications for academic integrity and the role of AI in education.",
      "url": "http://arxiv.org/abs/2505.20158v1",
      "published_time_eastern_timestamp": 1748275141.0
    },
    {
      "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models",
      "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.",
      "url": "http://arxiv.org/abs/2505.20152v1",
      "published_time_eastern_timestamp": 1748274928.0
    },
    {
      "title": "Error Optimization: Overcoming Exponential Signal Decay in Deep\n  Predictive Coding Networks",
      "summary": "Predictive Coding (PC) offers a biologically plausible alternative to\nbackpropagation for neural network training, yet struggles with deeper\narchitectures. This paper identifies the root cause: an inherent signal decay\nproblem where gradients attenuate exponentially with depth, becoming\ncomputationally negligible due to numerical precision constraints. To address\nthis fundamental limitation, we introduce Error Optimization (EO), a novel\nreparameterization that preserves PC's theoretical properties while eliminating\nsignal decay. By optimizing over prediction errors rather than states, EO\nenables signals to reach all layers simultaneously and without attenuation,\nconverging orders of magnitude faster than standard PC. Experiments across\nmultiple architectures and datasets demonstrate that EO matches\nbackpropagation's performance even for deeper models where conventional PC\nstruggles. Besides practical improvements, our work provides theoretical\ninsight into PC dynamics and establishes a foundation for scaling\nbiologically-inspired learning to deeper architectures on digital hardware and\nbeyond.",
      "url": "http://arxiv.org/abs/2505.20137v1",
      "published_time_eastern_timestamp": 1748273956.0
    },
    {
      "title": "Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based\n  Continual Learning",
      "summary": "Replay-based continual learning (CL) methods assume that models trained on a\nsmall subset can also effectively minimize the empirical risk of the complete\ndataset. These methods maintain a memory buffer that stores a sampled subset of\ndata from previous tasks to consolidate past knowledge. However, this\nassumption is not guaranteed in practice due to the limited capacity of the\nmemory buffer and the heuristic criteria used for buffer data selection. To\naddress this issue, we propose a new dataset distillation framework tailored\nfor CL, which maintains a learnable memory buffer to distill the global\ninformation from the current task data and accumulated knowledge preserved in\nthe previous memory buffer. Moreover, to avoid the computational overhead and\noverfitting risks associated with parameterizing the entire buffer during\ndistillation, we introduce a lightweight distillation module that can achieve\nglobal information distillation solely by generating learnable soft labels for\nthe memory buffer data. Extensive experiments show that, our method can achieve\ncompetitive results and effectively mitigates forgetting across various\ndatasets. The source code will be publicly available.",
      "url": "http://arxiv.org/abs/2505.20135v1",
      "published_time_eastern_timestamp": 1748273830.0
    },
    {
      "title": "TUNA: Comprehensive Fine-grained Temporal Understanding Evaluation on\n  Dense Dynamic Videos",
      "summary": "Videos are unique in their integration of temporal elements, including\ncamera, scene, action, and attribute, along with their dynamic relationships\nover time. However, existing benchmarks for video understanding often treat\nthese properties separately or narrowly focus on specific aspects, overlooking\nthe holistic nature of video content. To address this, we introduce TUNA, a\ntemporal-oriented benchmark for fine-grained understanding on dense dynamic\nvideos, with two complementary tasks: captioning and QA. Our TUNA features\ndiverse video scenarios and dynamics, assisted by interpretable and robust\nevaluation criteria. We evaluate several leading models on our benchmark,\nproviding fine-grained performance assessments across various dimensions. This\nevaluation reveals key challenges in video temporal understanding, such as\nlimited action description, inadequate multi-subject understanding, and\ninsensitivity to camera motion, offering valuable insights for improving video\nunderstanding models. The data and code are available at\nhttps://friedrichor.github.io/projects/TUNA.",
      "url": "http://arxiv.org/abs/2505.20124v1",
      "published_time_eastern_timestamp": 1748273046.0
    },
    {
      "title": "AdaTP: Attention-Debiased Token Pruning for Video Large Language Models",
      "summary": "Video Large Language Models (Video LLMs) have achieved remarkable results in\nvideo understanding tasks. However, they often suffer from heavy computational\noverhead due to the large number of visual tokens generated from multiple video\nframes. Existing visual token compression methods often rely on attention\nscores from language models as guidance. However, these scores exhibit inherent\nbiases: global bias reflects a tendency to focus on the two ends of the visual\ntoken sequence, while local bias leads to an over-concentration on the same\nspatial positions across different frames. To address the issue of attention\nbias, we propose $\\textbf{A}$ttention-$\\textbf{D}$ebi$\\textbf{a}$sed\n$\\textbf{T}$oken $\\textbf{P}$runing for Video Large Language Models\n($\\textbf{AdaTP}$), a novel token pruning pipeline for Video LLMs. AdaTP\nintegrates two dedicated debiasing modules into the pipeline, targeting global\nattention bias and local attention bias, respectively. Without the need for\nadditional training, our method significantly reduces the computational\noverhead of Video LLMs while retaining the performance of vanilla models.\nExtensive evaluation shows that AdaTP achieves state-of-the-art performance in\nvarious commonly used video understanding benchmarks. In particular, on\nLLaVA-OneVision-7B, AdaTP maintains performance without degradation while using\nonly up to $27.3\\%$ FLOPs compared to the vanilla model. Our code will be\nreleased soon.",
      "url": "http://arxiv.org/abs/2505.20100v1",
      "published_time_eastern_timestamp": 1748272117.0
    }
  ]
}
{
  "last_updated": "2026-01-13T10:16:44.772824-05:00",
  "papers": [
    {
      "title": "Tuning-free Visual Effect Transfer across Videos",
      "summary": "We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video's existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input's motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website $\\href{https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{at\\ this\\ URL}$.",
      "url": "http://arxiv.org/abs/2601.07833v1",
      "published_time_eastern_timestamp": 1768244372.0
    },
    {
      "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
      "summary": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\\% improvement on ImageNet classification, a 6.3\\% gain on NLP, a 12.6\\% improvement on image generation, and a 41\\% enhancement on video generation under the same time complexity.",
      "url": "http://arxiv.org/abs/2601.07832v1",
      "published_time_eastern_timestamp": 1768244358.0
    },
    {
      "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation",
      "summary": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.",
      "url": "http://arxiv.org/abs/2601.07821v1",
      "published_time_eastern_timestamp": 1768243991.0
    },
    {
      "title": "More Images, More Problems? A Controlled Analysis of VLM Failure Modes",
      "summary": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.",
      "url": "http://arxiv.org/abs/2601.07812v1",
      "published_time_eastern_timestamp": 1768243513.0
    },
    {
      "title": "Exchange Is All You Need for Remote Sensing Change Detection",
      "summary": "Remote sensing change detection fundamentally relies on the effective fusion and discrimination of bi-temporal features. Prevailing paradigms typically utilize Siamese encoders bridged by explicit difference computation modules, such as subtraction or concatenation, to identify changes. In this work, we challenge this complexity with SEED (Siamese Encoder-Exchange-Decoder), a streamlined paradigm that replaces explicit differencing with parameter-free feature exchange. By sharing weights across both Siamese encoders and decoders, SEED effectively operates as a single parameter set model. Theoretically, we formalize feature exchange as an orthogonal permutation operator and prove that, under pixel consistency, this mechanism preserves mutual information and Bayes optimal risk, whereas common arithmetic fusion methods often introduce information loss. Extensive experiments across five benchmarks, including SYSU-CD, LEVIR-CD, PX-CLCD, WaterCD, and CDD, and three backbones, namely SwinT, EfficientNet, and ResNet, demonstrate that SEED matches or surpasses state of the art methods despite its simplicity. Furthermore, we reveal that standard semantic segmentation models can be transformed into competitive change detectors solely by inserting this exchange mechanism, referred to as SEG2CD. The proposed paradigm offers a robust, unified, and interpretable framework for change detection, demonstrating that simple feature exchange is sufficient for high performance information fusion. Code and full training and evaluation protocols will be released at https://github.com/dyzy41/open-rscd.",
      "url": "http://arxiv.org/abs/2601.07805v1",
      "published_time_eastern_timestamp": 1768243011.0
    },
    {
      "title": "Foundations of local iterated function systems",
      "summary": "In this paper we present a systematic study of continuous local iterated function systems. We prove local iterated function systems admit compact attractors and, under a contractivity assumption, construct their code space and present an extended shift that describes admissible compositions. In particular, the possible combinatorial structure of a local iterated function system is in bijection with the space of invariant subsets of the full shift. Nevertheless, these objects reveal a degree of unexpectedness relative to the classical framework, as we build examples of local iterated function systems which are not modeled by subshifts of finite type and give rise to non self-similar attractors. We also prove that all attractors of graph-directed IFSs are obtained from local IFSs on an enriched compact metric space.\n  We provide several classes of examples illustrating the scope of our results, emphasizing both their contrasts and connections with the classical theory of iterated function systems.",
      "url": "http://arxiv.org/abs/2601.07804v1",
      "published_time_eastern_timestamp": 1768242985.0
    },
    {
      "title": "Lossy Source Coding with Broadcast Side Information",
      "summary": "This paper considers the source coding problem with broadcast side information. The side information is sent to two receivers through a noisy broadcast channel. We provide an outer bound of the rate--distortion--bandwidth (RDB) quadruples and achievable RDB quadruples when the helper uses a separation-based scheme. Some special cases with full characterization are also provided. We then compare the separation-based scheme with the uncoded scheme in the quadratic Gaussian case.",
      "url": "http://arxiv.org/abs/2601.07797v1",
      "published_time_eastern_timestamp": 1768241544.0
    },
    {
      "title": "\"TODO: Fix the Mess Gemini Created\": Towards Understanding GenAI-Induced Self-Admitted Technical Debt",
      "summary": "As large language models (LLMs) such as ChatGPT, Copilot, Claude, and Gemini become integrated into software development workflows, developers increasingly leave traces of AI involvement in their code comments. Among these, some comments explicitly acknowledge both the use of generative AI and the presence of technical shortcomings. Analyzing 6,540 LLM-referencing code comments from public Python and JavaScript-based GitHub repositories (November 2022-July 2025), we identified 81 that also self-admit technical debt(SATD). Developers most often describe postponed testing, incomplete adaptation, and limited understanding of AI-generated code, suggesting that AI assistance affects both when and why technical debt emerges. We term GenAI-Induced Self-admitted Technical debt (GIST) as a proposed conceptual lens to describe recurring cases where developers incorporate AI-generated code while explicitly expressing uncertainty about its behavior or correctness.",
      "url": "http://arxiv.org/abs/2601.07786v1",
      "published_time_eastern_timestamp": 1768240774.0
    },
    {
      "title": "DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference",
      "summary": "We introduce DT-ICU, a multimodal digital twin framework for continuous risk estimation in intensive care. DT-ICU integrates variable-length clinical time series with static patient information in a unified multitask architecture, enabling predictions to be updated as new observations accumulate over the ICU stay. We evaluate DT-ICU on the large, publicly available MIMIC-IV dataset, where it consistently outperforms established baseline models under different evaluation settings. Our test-length analysis shows that meaningful discrimination is achieved shortly after admission, while longer observation windows further improve the ranking of high-risk patients in highly imbalanced cohorts. To examine how the model leverages heterogeneous data sources, we perform systematic modality ablations, revealing that the model learnt a reasonable structured reliance on interventions, physiological response observations, and contextual information. These analyses provide interpretable insights into how multimodal signals are combined and how trade-offs between sensitivity and precision emerge. Together, these results demonstrate that DT-ICU delivers accurate, temporally robust, and interpretable predictions, supporting its potential as a practical digital twin framework for continuous patient monitoring in critical care. The source code and trained model weights for DT-ICU are publicly available at https://github.com/GUO-W/DT-ICU-release.",
      "url": "http://arxiv.org/abs/2601.07778v1",
      "published_time_eastern_timestamp": 1768240459.0
    },
    {
      "title": "Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training",
      "summary": "Recent works such as REPA have shown that guiding diffusion models with external semantic features (e.g., DINO) can significantly accelerate the training of diffusion transformers (DiTs). However, this requires the use of pretrained external networks, introducing additional dependencies and reducing flexibility. In this work, we argue that DiTs actually have the power to guide the training of themselves, and propose \\textbf{Self-Transcendence}, a simple yet effective method that achieves fast convergence using internal feature supervision only. It is found that the slow convergence in DiT training primarily stems from the difficulty of representation learning in shallow layers. To address this, we initially train the DiT model by aligning its shallow features with the latent representations from the pretrained VAE for a short phase (e.g., 40 epochs), then apply classifier-free guidance to the intermediate features, enhancing their discriminative capability and semantic expressiveness. These enriched internal features, learned entirely within the model, are used as supervision signals to guide a new DiT training. Compared to existing self-contained methods, our approach brings a significant performance boost. It can even surpass REPA in terms of generation quality and convergence speed, but without the need for any external pretrained models. Our method is not only more flexible for different backbones but also has the potential to be adopted for a wider range of diffusion-based generative tasks. The source code of our method can be found at https://github.com/csslc/Self-Transcendence.",
      "url": "http://arxiv.org/abs/2601.07773v1",
      "published_time_eastern_timestamp": 1768240331.0
    },
    {
      "title": "Tensor Decompositions for Online Grid-Based Terrain-Aided Navigation",
      "summary": "This paper presents a practical and scalable grid-based state estimation method for high-dimensional models with invertible linear dynamics and with highly non-linear measurements, such as the nearly constant velocity model with measurements of e.g. altitude, bearing, and/or range. Unlike previous tensor decomposition-based approaches, which have largely remained at the proof-of-concept stage, the proposed method delivers an efficient and practical solution by exploiting decomposable model structure-specifically, block-diagonal dynamics and sparsely coupled measurement dimensions. The algorithm integrates a Lagrangian formulation for the time update and leverages low-rank tensor decompositions to compactly represent and effectively propagate state densities. This enables real-time estimation for models with large state dimension, significantly extending the practical reach of grid-based filters beyond their traditional low-dimensional use. Although demonstrated in the context of terrain-aided navigation, the method is applicable to a wide range of models with decomposable structure. The computational complexity and estimation accuracy depend on the specific structure of the model. All experiments are fully reproducible, with source code provided alongside this paper (GitHub link: https://github.com/pesslovany/Matlab-LagrangianPMF).",
      "url": "http://arxiv.org/abs/2601.07728v1",
      "published_time_eastern_timestamp": 1768237447.0
    },
    {
      "title": "Weak Composition Lattices and Ring-Linear Anticodes",
      "summary": "Lattices and partially ordered sets have played an increasingly important role in coding theory, providing combinatorial frameworks for studying structural and algebraic properties of error-correcting codes. Motivated by recent works connecting lattice theory, anticodes, and coding-theoretic invariants, we investigate ring-linear codes endowed with the Lee metric. We introduce and characterize optimal Lee-metric anticodes over the ring $\\mathbb{Z}/p^s\\mathbb{Z}$. We show that the family of such anticodes admits a natural partition into subtypes and forms a lattice under inclusion. We establish a bijection between this lattice and a lattice of weak compositions ordered by dominance. As an application, we use this correspondence to introduce new invariants for Lee-metric codes via an anticode approach.",
      "url": "http://arxiv.org/abs/2601.07725v1",
      "published_time_eastern_timestamp": 1768237070.0
    },
    {
      "title": "FMAC: a Fair Fiducial Marker Accuracy Comparison Software",
      "summary": "This paper presents a method for carrying fair comparisons of the accuracy of pose estimation using fiducial markers. These comparisons rely on large sets of high-fidelity synthetic images enabling deep exploration of the 6 degrees of freedom. A low-discrepancy sampling of the space allows to check the correlations between each degree of freedom and the pose errors by plotting the 36 pairs of combinations. The images are rendered using a physically based ray tracing code that has been specifically developed to use the standard calibration coefficients of any camera directly. The software reproduces image distortions, defocus and diffraction blur. Furthermore, sub-pixel sampling is applied to sharp edges to enhance the fidelity of the rendered image. After introducing the rendering algorithm and its experimental validation, the paper proposes a method for evaluating the pose accuracy. This method is applied to well-known markers, revealing their strengths and weaknesses for pose estimation. The code is open source and available on GitHub.",
      "url": "http://arxiv.org/abs/2601.07723v1",
      "published_time_eastern_timestamp": 1768236926.0
    },
    {
      "title": "Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids",
      "summary": "Achieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalability and training complexity; specifically, some previous works using virtual obstacles are implemented case-by-case. In this work, we present \\textit{Hiking in the Wild}, a scalable, end-to-end parkour perceptive framework designed for robust humanoid hiking. To ensure safety and training stability, we introduce two key mechanisms: a foothold safety mechanism combining scalable \\textit{Terrain Edge Detection} with \\textit{Foot Volume Points} to prevent catastrophic slippage on edges, and a \\textit{Flat Patch Sampling} strategy that mitigates reward hacking by generating feasible navigation targets. Our approach utilizes a single-stage reinforcement learning scheme, mapping raw depth inputs and proprioception directly to joint actions, without relying on external state estimation. Extensive field experiments on a full-size humanoid demonstrate that our policy enables robust traversal of complex terrains at speeds up to 2.5 m/s. The training and deployment code is open-sourced to facilitate reproducible research and deployment on real robots with minimal hardware modifications.",
      "url": "http://arxiv.org/abs/2601.07718v1",
      "published_time_eastern_timestamp": 1768236650.0
    },
    {
      "title": "Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation",
      "summary": "LiDAR scene synthesis is an emerging solution to scarcity in 3D data for robotic tasks such as autonomous driving. Recent approaches employ diffusion or flow matching models to generate realistic scenes, but 3D data remains limited compared to RGB datasets with millions of samples. We introduce R3DPA, the first LiDAR scene generation method to unlock image-pretrained priors for LiDAR point clouds, and leverage self-supervised 3D representations for state-of-the-art results. Specifically, we (i) align intermediate features of our generative model with self-supervised 3D features, which substantially improves generation quality; (ii) transfer knowledge from large-scale image-pretrained generative models to LiDAR generation, mitigating limited LiDAR datasets; and (iii) enable point cloud control at inference for object inpainting and scene mixing with solely an unconditional model. On the KITTI-360 benchmark R3DPA achieves state of the art performance. Code and pretrained models are available at https://github.com/valeoai/R3DPA.",
      "url": "http://arxiv.org/abs/2601.07692v1",
      "published_time_eastern_timestamp": 1768234820.0
    },
    {
      "title": "AptaFind: A lightweight local interface for automated aptamer curation from scientific literature",
      "summary": "Aptamer researchers face a literature landscape scattered across publications, supplements, and databases, with each search consuming hours that could be spent at the bench. AptaFind transforms this navigation problem through a three-tier intelligence architecture that recognizes research mining is a spectrum, not a binary success or failure. The system delivers direct sequence extraction when possible, curated research leads when extraction fails, and exhaustive literature discovery for additional confidence. By combining local language models for semantic understanding with deterministic algorithms for reliability, AptaFind operates without cloud dependencies or subscription barriers. Validation across 300 University of Texas Aptamer Database targets demonstrates 84 % with some literature found, 84 % with curated research leads, and 79 % with a direct sequence extraction, at a laptop-compute rate of over 900 targets an hour. The platform proves that even when direct sequence extraction fails, automation can still deliver the actionable intelligence researchers need by rapidly narrowing the search to high quality references.",
      "url": "http://arxiv.org/abs/2601.07684v1",
      "published_time_eastern_timestamp": 1768234585.0
    },
    {
      "title": "New $X$-Secure $T$-Private Information Retrieval Schemes via Rational Curves and Hermitian Curves",
      "summary": "$X$-secure and $T$-private information retrieval (XSTPIR) is a variant of private information retrieval where data security is guaranteed against collusion among up to $X$ servers and the user's retrieval privacy is guaranteed against collusion among up to $T$ servers. Recently, researchers have constructed XSTPIR schemes through the theory of algebraic geometry codes and algebraic curves, with the aim of obtaining XSTPIR schemes that have higher maximum PIR rates for fixed field size and $X,T$ (the number of servers $N$ is not restricted). The mainstream approach is to employ curves of higher genus that have more rational points, evolving from rational curves to elliptic curves to hyperelliptic curves and, most recently, to Hermitian curves.\n  In this paper, we propose a different perspective: with the shared goal of constructing XSTPIR schemes with higher maximum PIR rates, we move beyond the mainstream approach of seeking curves with higher genus and more rational points. Instead, we aim to achieve this goal by enhancing the utilization efficiency of rational points on curves that have already been considered in previous work. By introducing a family of bases for the polynomial space $\\text{span}_{\\mathbb{F}_q}\\{1,x,\\dots,x^{k-1}\\}$ as an alternative to the Lagrange interpolation basis, we develop two new families of XSTPIR schemes based on rational curves and Hermitian curves, respectively. Parameter comparisons demonstrate that our schemes achieve superior performance. Specifically, our Hermitian-curve-based XSTPIR scheme provides the largest known maximum PIR rates when the field size $q^2\\geq 14^2$ and $X+T\\geq 4q$. Moreover, for any field size $q^2\\geq 28^2$ and $X+T\\geq 4$, our two XSTPIR schemes collectively provide the largest known maximum PIR rates.",
      "url": "http://arxiv.org/abs/2601.07676v1",
      "published_time_eastern_timestamp": 1768233757.0
    },
    {
      "title": "Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference",
      "summary": "Due to the prevalence of large language models (LLMs), key-value (KV) cache reduction for LLM inference has received remarkable attention. Among numerous works that have been proposed in recent years, layer-wise token pruning approaches, which select a subset of tokens at particular layers to retain in KV cache and prune others, are one of the most popular schemes. They primarily adopt a set of pre-defined layers, at which tokens are selected. Such design is inflexible in the sense that the accuracy significantly varies across tasks and deteriorates in harder tasks such as KV retrieval. In this paper, we propose ASL, a training-free method that adaptively chooses the selection layer for KV cache reduction, exploiting the variance of token ranks ordered by attention score. The proposed method balances the performance across different tasks while meeting the user-specified KV budget requirement. ASL operates during the prefilling stage and can be jointly used with existing KV cache reduction methods such as SnapKV to optimize the decoding stage. By evaluations on the InfiniteBench, RULER, and NIAH benchmarks, we show that equipped with one-shot token selection, where tokens are selected at a layer and propagated to deeper layers, ASL outperforms state-of-the-art layer-wise token selection methods in accuracy while maintaining decoding speed and KV cache reduction.",
      "url": "http://arxiv.org/abs/2601.07667v1",
      "published_time_eastern_timestamp": 1768232855.0
    },
    {
      "title": "Role of Shafranov shift, zonal structures on the behavior of TAEs, AAEs and microinstabilities in the presence of energetic particles",
      "summary": "In future nuclear fusion reactors, even a small fraction of fusion-born energetic particles (EP) about 100 times hotter than the thermal bulk species, contributes substantially to the kinetic pressure and therefore affect the MHD equilibrium, mainly via the Shafranov shift. In this work, we perform first-principles numerical simulations using the gyrokinetic, electromagnetic, global code ORB5 to study the effect of a self-consistent finite $β$ equilibrium on the arising Alfvén Eigenmodes (destabilized by EPs), Ion Temperature Gradient (ITG), and Kinetic Ballooning Modes (KBM) microturbulence (destabilized by thermal species). Linearly, we explore the complex interplay between EP fraction, bulk gradients and a self-consistent Shafranov shift on the plasma stability. We choose single toroidal mode numbers to represent the system's instabilities and study the characteristic nonlinear evolutions of TAEs, KBMs and ITGs separately and including the axisymmetric field response to each mode separately. This study focuses on the impact of Shafranov shift equilibrium consistency, as well as the self-generated zonal ${E \\times B}$ flows, the saturation levels and resulting heat and particle fluxes. In the ITG cases including the $n=0$ perturbations reduces turbulent fluxes, as expected, however, for the TAE cases including the $n=0$ perturbations is shown to enhance the fluxes. We show for the first time that Axisymmetric Alfvén Eigenmodes (AAEs) play a role in this mechanism.",
      "url": "http://arxiv.org/abs/2601.07652v1",
      "published_time_eastern_timestamp": 1768231993.0
    },
    {
      "title": "Multi-pathline flow visualization using PIV images",
      "summary": "One of the oldest flow visualization techniques is through multiple pathlines generated by the movement of seeding particles spatially distributed in the flow. In the computerized era, particle images are used in quantitative measurements, such as particle image and particle tracking velocimetry (PIV and PTV). Here, we present several methods for post-processing raw particle images to generate enhanced flow visualization without a need for conducting additional experiments. Three post-processing methods will be shown: 1) controlling the exposure time, 2) color-coding temporal information, and 3) changing the frame of reference. We showcase how employing these three methods can highlight different flow features in three canonical flow cases: vortex ring, leading edge vortex, and turbulent boundary layer. In addition to the quantitative flow field, the multi-pathline visualization is expected to augment our ability to observe fluid flow from many different perspectives.",
      "url": "http://arxiv.org/abs/2601.07643v1",
      "published_time_eastern_timestamp": 1768231598.0
    }
  ]
}
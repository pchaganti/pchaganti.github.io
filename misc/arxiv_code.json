{
  "last_updated": "2025-05-14T17:09:55.925391-04:00",
  "papers": [
    {
      "title": "CodePDE: An Inference Framework for LLM-driven PDE Solver Generation",
      "summary": "Partial differential equations (PDEs) are fundamental to modeling physical\nsystems, yet solving them remains a complex challenge. Traditional numerical\nsolvers rely on expert knowledge to implement and are computationally\nexpensive, while neural-network-based solvers require large training datasets\nand often lack interpretability. In this work, we frame PDE solving as a code\ngeneration task and introduce CodePDE, the first inference framework for\ngenerating PDE solvers using large language models (LLMs). Leveraging advanced\ninference-time algorithms and scaling strategies, CodePDE unlocks critical\ncapacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and\ntest-time scaling -- all without task-specific tuning. CodePDE achieves\nsuperhuman performance across a range of representative PDE problems. We also\npresent a systematic empirical analysis of LLM generated solvers, analyzing\ntheir accuracy, efficiency, and numerical scheme choices. Our findings\nhighlight the promise and the current limitations of LLMs in PDE solving,\noffering a new perspective on solver design and opportunities for future model\ndevelopment. Our code is available at https://github.com/LithiumDA/CodePDE.",
      "url": "http://arxiv.org/abs/2505.08783v1",
      "published_time_eastern_timestamp": 1747159088.0
    },
    {
      "title": "HealthBench: Evaluating Large Language Models Towards Improved Human\n  Health",
      "summary": "We present HealthBench, an open-source benchmark measuring the performance\nand safety of large language models in healthcare. HealthBench consists of\n5,000 multi-turn conversations between a model and an individual user or\nhealthcare professional. Responses are evaluated using conversation-specific\nrubrics created by 262 physicians. Unlike previous multiple-choice or\nshort-answer benchmarks, HealthBench enables realistic, open-ended evaluation\nthrough 48,562 unique rubric criteria spanning several health contexts (e.g.,\nemergencies, transforming clinical data, global health) and behavioral\ndimensions (e.g., accuracy, instruction following, communication). HealthBench\nperformance over the last two years reflects steady initial progress (compare\nGPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3\nscores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms\nGPT-4o and is 25 times cheaper. We additionally release two HealthBench\nvariations: HealthBench Consensus, which includes 34 particularly important\ndimensions of model behavior validated via physician consensus, and HealthBench\nHard, where the current top score is 32%. We hope that HealthBench grounds\nprogress towards model development and applications that benefit human health.",
      "url": "http://arxiv.org/abs/2505.08775v1",
      "published_time_eastern_timestamp": 1747158839.0
    },
    {
      "title": "Kudzu: Fast and Simple High-Throughput BFT",
      "summary": "We present Kudzu, a high-throughput atomic broadcast protocol with an\nintegrated fast path. Our contribution is based on the combination of two lines\nof work. Firstly, our protocol achieves finality in just two rounds of\ncommunication if all but $p$ out of $n = 3f + 2p + 1$ participating replicas\nbehave correctly, where $f$ is the number of Byzantine faults that are\ntolerated. Due to the seamless integration of the fast path, even in the\npresence of more than $p$ faults, our protocol maintains state-of-the-art\ncharacteristics. Secondly, our protocol utilizes the bandwidth of participating\nreplicas in a balanced way, alleviating the bottleneck at the leader, and thus\nenabling high throughput. This is achieved by disseminating blocks using\nerasure codes. Despite combining a novel set of advantages, Kudzu is remarkably\nsimple: intricacies such as progress certificates, complex view changes, and\nspeculative execution are avoided.",
      "url": "http://arxiv.org/abs/2505.08771v1",
      "published_time_eastern_timestamp": 1747158605.0
    },
    {
      "title": "SPAT: Sensitivity-based Multihead-attention Pruning on Time Series\n  Forecasting Models",
      "summary": "Attention-based architectures have achieved superior performance in\nmultivariate time series forecasting but are computationally expensive.\nTechniques such as patching and adaptive masking have been developed to reduce\ntheir sizes and latencies. In this work, we propose a structured pruning\nmethod, SPAT ($\\textbf{S}$ensitivity $\\textbf{P}$runer for\n$\\textbf{At}$tention), which selectively removes redundant attention mechanisms\nand yields highly effective models. Different from previous approaches, SPAT\naims to remove the entire attention module, which reduces the risk of\noverfitting and enables speed-up without demanding specialized hardware. We\npropose a dynamic sensitivity metric, $\\textbf{S}$ensitivity\n$\\textbf{E}$nhanced $\\textbf{N}$ormalized $\\textbf{D}$ispersion (SEND) that\nmeasures the importance of each attention module during the pre-training phase.\nExperiments on multivariate datasets demonstrate that SPAT-pruned models\nachieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs.\nFurthermore, SPAT-pruned models outperform existing lightweight, Mamba-based\nand LLM-based SOTA methods in both standard and zero-shot inference,\nhighlighting the importance of retaining only the most effective attention\nmechanisms. We have made our code publicly available\nhttps://anonymous.4open.science/r/SPAT-6042.",
      "url": "http://arxiv.org/abs/2505.08768v1",
      "published_time_eastern_timestamp": 1747157971.0
    },
    {
      "title": "Towards Autonomous UAV Visual Object Search in City Space: Benchmark and\n  Agentic Methodology",
      "summary": "Aerial Visual Object Search (AVOS) tasks in urban environments require\nUnmanned Aerial Vehicles (UAVs) to autonomously search for and identify target\nobjects using visual and textual cues without external guidance. Existing\napproaches struggle in complex urban environments due to redundant semantic\nprocessing, similar object distinction, and the exploration-exploitation\ndilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS,\nthe first benchmark dataset for autonomous search of common urban objects. This\ndataset comprises 2,420 tasks across six object categories with varying\ndifficulty levels, enabling comprehensive evaluation of UAV agents' search\ncapabilities. To solve the AVOS tasks, we also propose PRPSearcher\n(Perception-Reasoning-Planning Searcher), a novel agentic method powered by\nmulti-modal large language models (MLLMs) that mimics human three-tier\ncognition. Specifically, PRPSearcher constructs three specialized maps: an\nobject-centric dynamic semantic map enhancing spatial perception, a 3D\ncognitive map based on semantic attraction values for target reasoning, and a\n3D uncertainty map for balanced exploration-exploitation search. Also, our\napproach incorporates a denoising mechanism to mitigate interference from\nsimilar objects and utilizes an Inspiration Promote Thought (IPT) prompting\nmechanism for adaptive action planning. Experimental results on CityAVOS\ndemonstrate that PRPSearcher surpasses existing baselines in both success rate\nand search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and\n-46.40% NE). While promising, the performance gap compared to humans highlights\nthe need for better semantic reasoning and spatial exploration capabilities in\nAVOS tasks. This work establishes a foundation for future advances in embodied\ntarget search. Dataset and source code are available at\nhttps://anonymous.4open.science/r/CityAVOS-3DF8.",
      "url": "http://arxiv.org/abs/2505.08765v1",
      "published_time_eastern_timestamp": 1747157694.0
    },
    {
      "title": "On the Radiation Effects of Strontium Ions on Satellite Solar Cells in\n  Low Earth Orbits",
      "summary": "This study focuses on the radiation effects of Sr+ ions--generated from\nhigh-altitude nuclear explosions (HANE)--on satellite solar cells in low-Earth\norbits (LEO). Along four selected satellite orbits, ion fluences are sampled\ninside the evolving Sr+ ion distributions for days, determined from our newly\ndeveloped HANE environment model. These fluences, along with the help of\nradiation transport codes including the MULASSIS and SRIM models, enable us to\nquantify the radiation damages by determining the values of total ionizing\ndoses and the equivalent 1 MeV electron fluences for displacement damages.\nComparing the dose values to existing experimental data, we conclude that\nHANE-generated Sr+ ions have limited darkening effects to quartz solar cell\ncoverglasses in LEO with apogees of 100s to 1000 km. In addition, with the\nextremely high equivalent fluences, we also conclude that these Sr ions may\ncause severe or even fatal displacement damage to exposed solar photovoltaic\n(PV) cells on satellites in LEO. The radiation effects of Sr+ ions are much\nless significant for the orbits with high apogees beyond ten thousand km. We\nalso conducted model parameter sensitivity studies on the charge exchange\ncross-sections, neutral atmosphere density profiles and explosion local time\npositions, and the above conclusions stay unchanged. The methodology developed\nin this study can be extended to other HANE-generated heavy ion species in the\nfuture.",
      "url": "http://arxiv.org/abs/2505.08749v1",
      "published_time_eastern_timestamp": 1747155720.0
    },
    {
      "title": "Implet: A Post-hoc Subsequence Explainer for Time Series Models",
      "summary": "Explainability in time series models is crucial for fostering trust,\nfacilitating debugging, and ensuring interpretability in real-world\napplications. In this work, we introduce Implet, a novel post-hoc explainer\nthat generates accurate and concise subsequence-level explanations for time\nseries models. Our approach identifies critical temporal segments that\nsignificantly contribute to the model's predictions, providing enhanced\ninterpretability beyond traditional feature-attribution methods. Based on it,\nwe propose a cohort-based (group-level) explanation framework designed to\nfurther improve the conciseness and interpretability of our explanations. We\nevaluate Implet on several standard time-series classification benchmarks,\ndemonstrating its effectiveness in improving interpretability. The code is\navailable at https://github.com/LbzSteven/implet",
      "url": "http://arxiv.org/abs/2505.08748v1",
      "published_time_eastern_timestamp": 1747155683.0
    },
    {
      "title": "Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse\n  Problems in Parametric Differential Equations",
      "summary": "Parametric differential equations of the form du/dt = f(u, x, t, p) are\nfundamental in science and engineering. While deep learning frameworks such as\nthe Fourier Neural Operator (FNO) can efficiently approximate solutions, they\nstruggle with inverse problems, sensitivity estimation (du/dp), and concept\ndrift. We address these limitations by introducing a sensitivity-based\nregularization strategy, called Sensitivity-Constrained Fourier Neural\nOperators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths\nand consistently outperforms standard FNO and FNO with physics-informed\nregularization. It improves performance in parameter inversion tasks, scales to\nhigh-dimensional parameter spaces (tested with up to 82 parameters), and\nreduces both data and training requirements. These gains are achieved with a\nmodest increase in training time (30% to 130% per epoch) and generalize across\nvarious types of differential equations and neural operators. Code and selected\nexperiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators",
      "url": "http://arxiv.org/abs/2505.08740v1",
      "published_time_eastern_timestamp": 1747155250.0
    },
    {
      "title": "NurValues: Real-World Nursing Values Evaluation for Large Language\n  Models in Clinical Context",
      "summary": "This work introduces the first benchmark for nursing value alignment,\nconsisting of five core value dimensions distilled from international nursing\ncodes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The\nbenchmark comprises 1,100 real-world nursing behavior instances collected\nthrough a five-month longitudinal field study across three hospitals of varying\ntiers. These instances are annotated by five clinical nurses and then augmented\nwith LLM-generated counterfactuals with reversed ethic polarity. Each original\ncase is paired with a value-aligned and a value-violating version, resulting in\n2,200 labeled instances that constitute the Easy-Level dataset. To increase\nadversarial complexity, each instance is further transformed into a\ndialogue-based format that embeds contextual cues and subtle misleading\nsignals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA)\nLLMs on their alignment with nursing values. Our findings reveal three key\ninsights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level\ndataset (94.55), where Claude 3.5 Sonnet outperforms other models on the\nHard-Level dataset (89.43), significantly surpassing the medical LLMs; (2)\nJustice is consistently the most difficult nursing value dimension to evaluate;\nand (3) in-context learning significantly improves alignment. This work aims to\nprovide a foundation for value-sensitive LLMs development in clinical settings.\nThe dataset and the code are available at\nhttps://huggingface.co/datasets/Ben012345/NurValues.",
      "url": "http://arxiv.org/abs/2505.08734v1",
      "published_time_eastern_timestamp": 1747154785.0
    },
    {
      "title": "Extending Large Vision-Language Model for Diverse Interactive Tasks in\n  Autonomous Driving",
      "summary": "The Large Visual-Language Models (LVLMs) have significantly advanced image\nunderstanding. Their comprehension and reasoning capabilities enable promising\napplications in autonomous driving scenarios. However, existing research\ntypically focuses on front-view perspectives and partial objects within scenes,\nstruggling to achieve comprehensive scene understanding. Meanwhile, existing\nLVLMs suffer from the lack of mapping relationship between 2D and 3D and\ninsufficient integration of 3D object localization and instruction\nunderstanding. To tackle these limitations, we first introduce NuInteract, a\nlarge-scale dataset with over 1.5M multi-view image language pairs spanning\ndense scene captions and diverse interactive tasks. Furthermore, we propose\nDriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs\nwith a spatial processor using a series of learnable queries. The spatial\nprocessor, designed as a plug-and-play component, can be initialized with\npre-trained 3D detectors to improve 3D perception. Our experiments show that\nDriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable\nimprovement on the 3D visual grounding task. The dataset and code will be\nreleased at https://github.com/zc-zhao/DriveMonkey.",
      "url": "http://arxiv.org/abs/2505.08725v1",
      "published_time_eastern_timestamp": 1747154211.0
    },
    {
      "title": "TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series",
      "summary": "Satellite image time series (SITS) provide continuous observations of the\nEarth's surface, making them essential for applications such as environmental\nmanagement and disaster assessment. However, existing spatiotemporal foundation\nmodels rely on plain vision transformers, which encode entire temporal\nsequences without explicitly capturing multiscale spatiotemporal relationships\nbetween land objects. This limitation hinders their effectiveness in downstream\ntasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision\ntransformer foundation model tailored for SITS analysis. At its core, we\nintroduce a spatiotemporal gyroscope attention mechanism that dynamically\ncaptures evolving multiscale patterns across both time and space. For\npre-training, we curate MillionST, a large-scale dataset of one million images\nfrom 100,000 geographic locations, each captured across 10 temporal phases over\nfive years, encompassing diverse geospatial changes and seasonal variations.\nLeveraging this dataset, we adapt masked image modeling to pre-train TiMo,\nenabling it to effectively learn and encode generalizable spatiotemporal\nrepresentations.Extensive experiments across multiple spatiotemporal\ntasks-including deforestation monitoring, land cover segmentation, crop type\nclassification, and flood detection-demonstrate TiMo's superiority over\nstate-of-the-art methods. Code, model, and dataset will be released at\nhttps://github.com/MiliLab/TiMo.",
      "url": "http://arxiv.org/abs/2505.08723v1",
      "published_time_eastern_timestamp": 1747154111.0
    },
    {
      "title": "AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based\n  Physics-Informed Kolmogorov-Arnold Networks",
      "summary": "Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving\npartial differential equations (PDEs). Yet their original formulation is\ncomputationally and memory intensive, motivating the introduction of Chebyshev\nType-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed\nthe vanilla KANs architecture, our rigorous theoretical analysis reveals that\nthey still suffer from rank collapse, ultimately limiting their expressive\ncapacity. To overcome these limitations, we enhance Chebyshev1KANs by\nintegrating wavelet-activated MLPs with learnable parameters and an internal\nattention mechanism. We prove that this design preserves a full-rank Jacobian\nand is capable of approximating solutions to PDEs of arbitrary order.\nFurthermore, to alleviate the loss instability and imbalance introduced by the\nChebyshev polynomial basis, we externally incorporate a Residual Gradient\nAttention (RGA) mechanism that dynamically re-weights individual loss terms\naccording to their gradient norms and residual magnitudes. By jointly\nleveraging internal and external attention, we present AC-PKAN, a novel\narchitecture that constitutes an enhancement to weakly supervised\nPhysics-Informed Neural Networks (PINNs) and extends the expressive power of\nKANs. Experimental results from nine benchmark tasks across three domains show\nthat AC-PKAN consistently outperforms or matches state-of-the-art models such\nas PINNsFormer, establishing it as a highly effective tool for solving complex\nreal-world engineering problems in zero-data or data-sparse regimes. The code\nwill be made publicly available upon acceptance.",
      "url": "http://arxiv.org/abs/2505.08687v1",
      "published_time_eastern_timestamp": 1747151170.0
    },
    {
      "title": "CAD-Coder:Text-Guided CAD Files Code Generation",
      "summary": "Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D\nmodels of real-world products. Traditional CAD typically relies on hand-drawing\nby experts or modifications of existing library files, which doesn't allow for\nrapid personalization. With the emergence of generative artificial\nintelligence, convenient and efficient personalized CAD generation has become\npossible. However, existing generative methods typically produce outputs that\nlack interactive editability and geometric annotations, limiting their\npractical applications in manufacturing. To enable interactive generative CAD,\nwe propose CAD-Coder, a framework that transforms natural language instructions\ninto CAD script codes, which can be executed in Python environments to generate\nhuman-editable CAD files (.Dxf). To facilitate the generation of editable CAD\nsketches with annotation information, we construct a comprehensive dataset\ncomprising 29,130 Dxf files with their corresponding script codes, where each\nsketch preserves both editability and geometric annotations. We evaluate\nCAD-Coder on various 2D/3D CAD generation tasks against existing methods,\ndemonstrating superior interactive capabilities while uniquely providing\neditable sketches with geometric annotations.",
      "url": "http://arxiv.org/abs/2505.08686v1",
      "published_time_eastern_timestamp": 1747151146.0
    },
    {
      "title": "Performance of rotation-symmetric bosonic codes in the presence of\n  random telegraph noise",
      "summary": "Decoherence in quantum devices, such as qubits and resonators, is often\ncaused by bistable fluctuators modeled as random telegraph noise (RTN), leading\nto significant dephasing. We analyze the impact of individual and multiple\nfluctuators on a bosonic mode in continuous variable systems, identifying\nnon-Markovian behavior governed by two timescales: the fluctuator switching\nrate ($\\xi$) and coupling strength ($\\nu$). Using the Breuer-Piilo-Laine (BLP)\nmeasure, we show that for Gaussian states, squeezing and thermal fluctuations\ndo not enhance non-Markovianity. In contrast, for non-Gaussian states, the\nmeasure becomes unbounded. For rotation-symmetric bosonic (RSB) codes, known\nfor their error correction advantages, non-Markovianity grows linearly with\ncode symmetry. We evaluate the performance of RSB codes under simultaneous loss\nand RTN dephasing. For a teleportation-based Knill error-correction circuit,\nthe codes perform robustly in the Markovian limit. In the non-Markovian regime,\nthe performance depends on the time the error correction is performed for a\ngiven codeword. The average gate fidelity of the error-corrected state in this\ncase exhibits oscillations as a function of time due to the oscillatory nature\nof the dephasing function of the RTN noise; however, for most of the parameter\nranges, the values stay above the break-even point. Extending to multiple\nfluctuators that produce $1/f$ noise, we observe that non-Markovianity decays\nwith increasing fluctuator count, while the performance of RSB codes remains\neffective with increasing number of fluctuators.",
      "url": "http://arxiv.org/abs/2505.08670v1",
      "published_time_eastern_timestamp": 1747150338.0
    },
    {
      "title": "Claycode: Stylable and Deformable 2D Scannable Codes",
      "summary": "This paper introduces Claycode, a novel 2D scannable code designed for\nextensive stylization and deformation. Unlike traditional matrix-based codes\n(e.g., QR codes), Claycodes encode their message in a tree structure. During\nthe encoding process, bits are mapped into a topology tree, which is then\ndepicted as a nesting of color regions drawn within the boundaries of a target\npolygon shape. When decoding, Claycodes are extracted and interpreted in\nreal-time from a camera stream. We detail the end-to-end pipeline and show that\nClaycodes allow for extensive stylization without compromising their\nfunctionality. We then empirically demonstrate Claycode's high tolerance to\nheavy deformations, outperforming traditional 2D scannable codes in scenarios\nwhere they typically fail.",
      "url": "http://arxiv.org/abs/2505.08666v1",
      "published_time_eastern_timestamp": 1747150086.0
    },
    {
      "title": "TRAIL: Trace Reasoning and Agentic Issue Localization",
      "summary": "The increasing adoption of agentic workflows across diverse domains brings a\ncritical need to scalably and systematically evaluate the complex traces these\nsystems generate. Current evaluation methods depend on manual, domain-specific\nhuman analysis of lengthy workflow traces - an approach that does not scale\nwith the growing complexity and volume of agentic outputs. Error analysis in\nthese settings is further complicated by the interplay of external tool outputs\nand language model reasoning, making it more challenging than traditional\nsoftware debugging. In this work, we (1) articulate the need for robust and\ndynamic evaluation methods for agentic workflow traces, (2) introduce a formal\ntaxonomy of error types encountered in agentic systems, and (3) present a set\nof 148 large human-annotated traces (TRAIL) constructed using this taxonomy and\ngrounded in established agentic benchmarks. To ensure ecological validity, we\ncurate traces from both single and multi-agent systems, focusing on real-world\napplications such as software engineering and open-world information retrieval.\nOur evaluations reveal that modern long context LLMs perform poorly at trace\ndebugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our\ndataset and code are made publicly available to support and accelerate future\nresearch in scalable evaluation for agentic workflows.",
      "url": "http://arxiv.org/abs/2505.08638v1",
      "published_time_eastern_timestamp": 1747148131.0
    },
    {
      "title": "WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree\n  Complex Wavelet and Graph Neural Networks",
      "summary": "Deepfake technology poses increasing risks such as privacy invasion and\nidentity theft. To address these threats, we propose WaveGuard, a proactive\nwatermarking framework that enhances robustness and imperceptibility via\nfrequency-domain embedding and graph-based structural consistency.\nSpecifically, we embed watermarks into high-frequency sub-bands using Dual-Tree\nComplex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph\nNeural Network (SC-GNN) to preserve visual quality. We also design an attention\nmodule to refine embedding precision. Experimental results on face swap and\nreenactment tasks demonstrate that WaveGuard outperforms state-of-the-art\nmethods in both robustness and visual quality. Code is available at\nhttps://github.com/vpsg-research/WaveGuard.",
      "url": "http://arxiv.org/abs/2505.08614v1",
      "published_time_eastern_timestamp": 1747146702.0
    },
    {
      "title": "Demonstration of logical quantum phase estimation for X-ray absorption\n  spectra",
      "summary": "In this study, we employed Fourier-based quantum phase estimation (QPE) to\ncalculate X-ray absorption spectroscopy (XAS) spectra. The primary focus of\nthis study is the calculation of the XAS spectra of transition metal\n$L_{2,3}$-edges, which are dominated by strong correlation effects. First, the\nFe $L_{2,3}$-edge X-ray absorption near-edge structure of FePO$_4$ is\ncalculated using a noiseless simulator. The present computation involves a\ncomparison of three types of input states: a uniform superposition state,\noptimal entangled input state, and Slater function state. Subsequently, we\ninvestigated the resolution error of the QPE and statistical error attributed\nto the measurements. It was revealed that post-processing to introduce\nLorentzian broadening reduces the statistical error, which becomes a\nsignificant problem for a large number of qubits. Subsequently, we implemented\nQPE on a trapped-ion quantum computer, encompassing three orbitals within the\nactive space. To this end, we implemented QPE using dynamic circuits to reduce\nancilla qubits and [[k+2, k, 2]] quantum error detection code to mitigate the\nquantum noise inherent in current quantum computers. As a result, it was\ndemonstrated that hardware noise was reduced, and spectra close to the\nnoiseless ones were obtained.",
      "url": "http://arxiv.org/abs/2505.08612v1",
      "published_time_eastern_timestamp": 1747146638.0
    },
    {
      "title": "PrePrompt: Predictive prompting for class incremental learning",
      "summary": "Class Incremental Learning (CIL) based on pre-trained models offers a\npromising direction for open-world continual learning. Existing methods\ntypically rely on correlation-based strategies, where an image's classification\nfeature is used as a query to retrieve the most related key prompts and select\nthe corresponding value prompts for training. However, these approaches face an\ninherent limitation: fitting the entire feature space of all tasks with only a\nfew trainable prompts is fundamentally challenging. We propose Predictive\nPrompting (PrePrompt), a novel CIL framework that circumvents correlation-based\nlimitations by leveraging pre-trained models' natural classification ability to\npredict task-specific prompts. Specifically, PrePrompt decomposes CIL into a\ntwo-stage prediction framework: task-specific prompt prediction followed by\nlabel prediction. While theoretically appealing, this framework risks bias\ntoward recent classes due to missing historical data for older classifier\ncalibration. PrePrompt then mitigates this by incorporating feature\ntranslation, dynamically balancing stability and plasticity. Experiments across\nmultiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art\nprompt-based CIL methods. The code will be released upon acceptance.",
      "url": "http://arxiv.org/abs/2505.08586v1",
      "published_time_eastern_timestamp": 1747144676.0
    },
    {
      "title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible\n  Long-term Tracking",
      "summary": "Surgical scene segmentation is critical in computer-assisted surgery and is\nvital for enhancing surgical quality and patient outcomes. Recently, referring\nsurgical segmentation is emerging, given its advantage of providing surgeons\nwith an interactive experience to segment the target object. However, existing\nmethods are limited by low efficiency and short-term tracking, hindering their\napplicability in complex real-world surgical scenarios. In this paper, we\nintroduce ReSurgSAM2, a two-stage surgical referring segmentation framework\nthat leverages Segment Anything Model 2 to perform text-referred target\ndetection, followed by tracking with reliable initial frame identification and\ndiversity-driven long-term memory. For the detection stage, we propose a\ncross-modal spatial-temporal Mamba to generate precise detection and\nsegmentation results. Based on these results, our credible initial frame\nselection strategy identifies the reliable frame for the subsequent tracking.\nUpon selecting the initial frame, our method transitions to the tracking stage,\nwhere it incorporates a diversity-driven memory mechanism that maintains a\ncredible and diverse memory bank, ensuring consistent long-term tracking.\nExtensive experiments demonstrate that ReSurgSAM2 achieves substantial\nimprovements in accuracy and efficiency compared to existing methods, operating\nin real-time at 61.2 FPS. Our code and datasets will be available at\nhttps://github.com/jinlab-imvr/ReSurgSAM2.",
      "url": "http://arxiv.org/abs/2505.08581v1",
      "published_time_eastern_timestamp": 1747144570.0
    }
  ]
}
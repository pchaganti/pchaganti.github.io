{
  "last_updated": "2025-05-21T08:22:59.629800-04:00",
  "papers": [
    {
      "title": "Language Models use Lookbacks to Track Beliefs",
      "summary": "How do language models (LMs) represent characters' beliefs, especially when\nthose beliefs may differ from reality? This question lies at the heart of\nunderstanding the Theory of Mind (ToM) capabilities of LMs. We analyze\nLlama-3-70B-Instruct's ability to reason about characters' beliefs using causal\nmediation and abstraction. We construct a dataset that consists of simple\nstories where two characters each separately change the state of two objects,\npotentially unaware of each other's actions. Our investigation uncovered a\npervasive algorithmic pattern that we call a lookback mechanism, which enables\nthe LM to recall important information when it becomes necessary. The LM binds\neach character-object-state triple together by co-locating reference\ninformation about them, represented as their Ordering IDs (OIs) in low rank\nsubspaces of the state token's residual stream. When asked about a character's\nbeliefs regarding the state of an object, the binding lookback retrieves the\ncorresponding state OI and then an answer lookback retrieves the state token.\nWhen we introduce text specifying that one character is (not) visible to the\nother, we find that the LM first generates a visibility ID encoding the\nrelation between the observing and the observed character OIs. In a visibility\nlookback, this ID is used to retrieve information about the observed character\nand update the observing character's beliefs. Our work provides insights into\nthe LM's belief tracking mechanisms, taking a step toward reverse-engineering\nToM reasoning in LMs.",
      "url": "http://arxiv.org/abs/2505.14685v1",
      "published_time_eastern_timestamp": 1747763985.0
    },
    {
      "title": "Emerging Properties in Unified Multimodal Pretraining",
      "summary": "Unifying multimodal understanding and generation has shown impressive\ncapabilities in cutting-edge proprietary systems. In this work, we introduce\nBAGEL, an open0source foundational model that natively supports multimodal\nunderstanding and generation. BAGEL is a unified, decoder0only model pretrained\non trillions of tokens curated from large0scale interleaved text, image, video,\nand web data. When scaled with such diverse multimodal interleaved data, BAGEL\nexhibits emerging capabilities in complex multimodal reasoning. As a result, it\nsignificantly outperforms open-source unified models in both multimodal\ngeneration and understanding across standard benchmarks, while exhibiting\nadvanced multimodal reasoning abilities such as free-form image manipulation,\nfuture frame prediction, 3D manipulation, and world navigation. In the hope of\nfacilitating further opportunities for multimodal research, we share the key\nfindings, pretraining details, data creation protocal, and release our code and\ncheckpoints to the community. The project page is at https://bagel-ai.org/",
      "url": "http://arxiv.org/abs/2505.14683v1",
      "published_time_eastern_timestamp": 1747763970.0
    },
    {
      "title": "UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in\n  Large Language Models",
      "summary": "Lifelong learning enables large language models (LLMs) to adapt to evolving\ninformation by continually updating their internal knowledge. An ideal system\nshould support efficient, wide-ranging updates while preserving existing\ncapabilities and ensuring reliable deployment. Model editing stands out as a\npromising solution for this goal, offering a focused and efficient way to\nrevise a model's internal knowledge. Although recent paradigms have made\nnotable progress, they often struggle to meet the demands of practical lifelong\nadaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally\nnew editing solution that is training-, subject- and memory-free, making it\nparticularly well-suited for ultra-scalable, real-world lifelong model editing.\nULTRAEDIT performs editing through a self-contained process that relies solely\non lightweight linear algebra operations to compute parameter shifts, enabling\nfast and consistent parameter modifications with minimal overhead. To improve\nscalability in lifelong settings, ULTRAEDIT employs a lifelong normalization\nstrategy that continuously updates feature statistics across turns, allowing it\nto adapt to distributional shifts and maintain consistency over time. ULTRAEDIT\nachieves editing speeds over 7x faster than the previous state-of-the-art\nmethod-which was also the fastest known approach-while consuming less than 1/3\nthe VRAM, making it the only method currently capable of editing a 7B LLM on a\n24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest\ndataset in the field to date, with over 2M editing pairs-and demonstrate that\nour method supports up to 1M edits while maintaining high accuracy.\nComprehensive experiments on four datasets and six models show that ULTRAEDIT\nconsistently achieves superior performance across diverse model editing\nscenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.",
      "url": "http://arxiv.org/abs/2505.14679v1",
      "published_time_eastern_timestamp": 1747763944.0
    },
    {
      "title": "UniCTokens: Boosting Personalized Understanding and Generation via\n  Unified Concept Tokens",
      "summary": "Personalized models have demonstrated remarkable success in understanding and\ngenerating concepts provided by users. However, existing methods use separate\nconcept tokens for understanding and generation, treating these tasks in\nisolation. This may result in limitations for generating images with complex\nprompts. For example, given the concept $\\langle bo\\rangle$, generating\n\"$\\langle bo\\rangle$ wearing its hat\" without additional textual descriptions\nof its hat. We call this kind of generation personalized knowledge-driven\ngeneration. To address the limitation, we present UniCTokens, a novel framework\nthat effectively integrates personalized information into a unified vision\nlanguage model (VLM) for understanding and generation. UniCTokens trains a set\nof unified concept tokens to leverage complementary semantics, boosting two\npersonalized tasks. Moreover, we propose a progressive training strategy with\nthree stages: understanding warm-up, bootstrapping generation from\nunderstanding, and deepening understanding from generation to enhance mutual\nbenefits between both tasks. To quantitatively evaluate the unified VLM\npersonalization, we present UnifyBench, the first benchmark for assessing\nconcept understanding, concept generation, and knowledge-driven generation.\nExperimental results on UnifyBench indicate that UniCTokens shows competitive\nperformance compared to leading methods in concept understanding, concept\ngeneration, and achieving state-of-the-art results in personalized\nknowledge-driven generation. Our research demonstrates that enhanced\nunderstanding improves generation, and the generation process can yield\nvaluable insights into understanding. Our code and dataset will be released at:\n\\href{https://github.com/arctanxarc/UniCTokens}{https://github.com/arctanxarc/UniCTokens}.",
      "url": "http://arxiv.org/abs/2505.14671v1",
      "published_time_eastern_timestamp": 1747763761.0
    },
    {
      "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
      "summary": "The rapid advancement of large language models (LLMs) has been paralleled by\nunprecedented increases in computational demands, with training costs for\nstate-of-the-art models doubling every few months. Training models directly in\nlow-precision arithmetic offers a solution, by improving both computational\nthroughput and energy efficiency. Specifically, NVIDIA's recent Blackwell\narchitecture facilitates extremely low-precision operations, specifically FP4\nvariants, promising substantial efficiency gains. Yet, current algorithms for\ntraining LLMs in FP4 precision face significant accuracy degradation and often\nrely on mixed-precision fallbacks. In this paper, we systematically investigate\nhardware-supported FP4 training and introduce Quartet, a new approach enabling\naccurate, end-to-end FP4 training with all the major computations (in e.g.\nlinear layers) being performed in low precision. Through extensive evaluations\non Llama-type models, we reveal a new low-precision scaling law that quantifies\nperformance trade-offs across varying bit-widths and allows us to identify a\n\"near-optimal\" low-precision training technique in terms of\naccuracy-vs-computation, called Quartet. We implement Quartet using optimized\nCUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve\nstate-of-the-art accuracy for FP4 precision, successfully training\nbillion-scale models. Our method demonstrates that fully FP4-based training is\na competitive alternative to standard-precision and FP8 training. Our code is\navailable at https://github.com/IST-DASLab/Quartet.",
      "url": "http://arxiv.org/abs/2505.14669v1",
      "published_time_eastern_timestamp": 1747763750.0
    },
    {
      "title": "AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of\n  Cross-Modal Embeddings",
      "summary": "Cross-modal embeddings form the foundation for multi-modal models. However,\nvisualization methods for interpreting cross-modal embeddings have been\nprimarily confined to traditional dimensionality reduction (DR) techniques like\nPCA and t-SNE. These DR methods primarily focus on feature distributions within\na single modality, whilst failing to incorporate metrics (e.g., CLIPScore)\nacross multiple modalities.This paper introduces AKRMap, a new DR technique\ndesigned to visualize cross-modal embeddings metric with enhanced accuracy by\nlearning kernel regression of the metric landscape in the projection space.\nSpecifically, AKRMap constructs a supervised projection network guided by a\npost-projection kernel regression loss, and employs adaptive generalized\nkernels that can be jointly optimized with the projection. This approach\nenables AKRMap to efficiently generate visualizations that capture complex\nmetric distributions, while also supporting interactive features such as zoom\nand overlay for deeper exploration. Quantitative experiments demonstrate that\nAKRMap outperforms existing DR methods in generating more accurate and\ntrustworthy visualizations. We further showcase the effectiveness of AKRMap in\nvisualizing and comparing cross-modal embeddings for text-to-image models. Code\nand demo are available at https://github.com/yilinye/AKRMap.",
      "url": "http://arxiv.org/abs/2505.14664v1",
      "published_time_eastern_timestamp": 1747763523.0
    },
    {
      "title": "CRYPTONITE: Scalable Accelerator Design for Cryptographic Primitives and\n  Algorithms",
      "summary": "Cryptographic primitives, consisting of repetitive operations with different\ninputs, are typically implemented using straight-line C code due to traditional\nexecution on CPUs. Computing these primitives is necessary for secure\ncommunication; thus, dedicated hardware accelerators are required in resource\nand latency-constrained environments. High-Level Synthesis (HLS) generates\nhardware from high-level implementations in languages like C, enabling the\nrapid prototyping and evaluation of designs, leading to its prominent use in\ndeveloping dedicated hardware accelerators. However, directly synthesizing the\nstraight-line C implementations of cryptographic primitives can lead to large\nhardware designs with excessive resource usage or suboptimal performance.\n  We introduce Cryptonite, a tool that automatically generates efficient,\nsynthesizable, and correct-by-design hardware accelerators for cryptographic\nprimitives directly from straight-line C code. Cryptonite first identifies\nhigh-level hardware constructs through verified rewriting, emphasizing resource\nreuse. The second stage automatically explores latency-oriented implementations\nof the compact design. This enables the flexible scaling of a particular\naccelerator to meet the hardware requirements. We demonstrate Cryptonite's\neffectiveness using implementations from the Fiat Cryptography project, a\nlibrary of verified and auto-generated cryptographic primitives for\nelliptic-curve cryptography. Our results show that Cryptonite achieves scalable\ndesigns with up to 88.88\\% reduced resource usage and a 54.31\\% improvement in\nlatency compared to naively synthesized designs.",
      "url": "http://arxiv.org/abs/2505.14657v1",
      "published_time_eastern_timestamp": 1747763025.0
    },
    {
      "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
      "summary": "Reinforcement learning (RL) has recently demonstrated strong potential in\nenhancing the reasoning capabilities of large language models (LLMs).\nParticularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero,\nenables direct RL training of base LLMs without relying on an intermediate\nsupervised fine-tuning stage. Despite these advancements, current works for LLM\nreasoning mainly focus on mathematical and coding domains, largely due to data\nabundance and the ease of answer verification. This limits the applicability\nand generalization of such models to broader domains, where questions often\nhave diverse answer representations, and data is more scarce. In this paper, we\npropose General-Reasoner, a novel training paradigm designed to enhance LLM\nreasoning capabilities across diverse domains. Our key contributions include:\n(1) constructing a large-scale, high-quality dataset of questions with\nverifiable answers curated by web crawling, covering a wide range of\ndisciplines; and (2) developing a generative model-based answer verifier, which\nreplaces traditional rule-based verification with the capability of\nchain-of-thought and context-awareness. We train a series of models and\nevaluate them on a wide range of datasets covering wide domains like physics,\nchemistry, finance, electronics etc. Our comprehensive evaluation across these\n12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC)\ndemonstrates that General-Reasoner outperforms existing baseline methods,\nachieving robust and generalizable reasoning performance while maintaining\nsuperior effectiveness in mathematical reasoning tasks.",
      "url": "http://arxiv.org/abs/2505.14652v1",
      "published_time_eastern_timestamp": 1747762893.0
    },
    {
      "title": "A model of magnetised and rotating convection for stellar and planetary\n  interiors",
      "summary": "Convection is a fundamental mechanism for energy transport in stars and\nplanets, playing a pivotal role in shaping their structures and evolution. The\nMixing-Length Theory, a monomodal approach to convection, is widely adopted and\nimplemented in 1D stellar structure and evolution codes. However, it overlooks\nthe combined effects of rotation and magnetic fields, which are ubiquitous\nacross a wide range of stars and planets. To address this limitation, we extend\nthe Mixing-Length Theory including both rotation and magnetic fields within a\nCartesian set-up. Building on the work by Stevenson 1979, we use a heat-flux\nmaximisation principle, which amounts to selecting the convective mode that\ncarries the most heat. Our findings show that both rotation and magnetic fields\nindividually tend to suppress convection. However, when combined, they can\nenhance convection strength under certain conditions. We derive expressions for\nthe root-mean-square (rms) velocity, characteristic length scale, and degree of\nsuperadiabaticity as functions of the rotation rate and magnetic field\nstrength. These results offer new insights for more accurately modeling\nconvection and its impact on stellar and planetary structures in\none-dimensional and forthcoming multi-dimensional evolution models.",
      "url": "http://arxiv.org/abs/2505.14650v1",
      "published_time_eastern_timestamp": 1747762813.0
    },
    {
      "title": "CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided\n  Design Code Generation",
      "summary": "Efficient creation of accurate and editable 3D CAD models is critical in\nengineering design, significantly impacting cost and time-to-market in product\ninnovation. Current manual workflows remain highly time-consuming and demand\nextensive user expertise. While recent developments in AI-driven CAD generation\nshow promise, existing models are limited by incomplete representations of CAD\noperations, inability to generalize to real-world images, and low output\naccuracy. This paper introduces CAD-Coder, an open-source Vision-Language Model\n(VLM) explicitly fine-tuned to generate editable CAD code (CadQuery Python)\ndirectly from visual input. Leveraging a novel dataset that we\ncreated--GenCAD-Code, consisting of over 163k CAD-model image and code\npairs--CAD-Coder outperforms state-of-the-art VLM baselines such as GPT-4.5 and\nQwen2.5-VL-72B, achieving a 100% valid syntax rate and the highest accuracy in\n3D solid similarity. Notably, our VLM demonstrates some signs of\ngeneralizability, successfully generating CAD code from real-world images and\nexecuting CAD operations unseen during fine-tuning. The performance and\nadaptability of CAD-Coder highlights the potential of VLMs fine-tuned on code\nto streamline CAD workflows for engineers and designers. CAD-Coder is publicly\navailable at: https://github.com/anniedoris/CAD-Coder.",
      "url": "http://arxiv.org/abs/2505.14646v1",
      "published_time_eastern_timestamp": 1747762484.0
    },
    {
      "title": "Minimizing Contaminant Leakage in Internal Linear Combination Maps Using\n  a Data-Driven Approach",
      "summary": "The thermal Sunyaev-Zel'dovich (tSZ) effect, the inverse-Compton scattering\nof cosmic microwave background (CMB) photons off high-energy electrons, is a\npowerful probe of hot, ionized gas in the Universe. It is often measured via\ncross-correlations of CMB data with large-scale structure (LSS) tracers to\nconstrain gas physics and improve cosmological constraints. The largest source\nof bias to these measurements is the leakage of poorly understood thermal dust\nemission from star-forming galaxies -- the cosmic infrared background (CIB) --\ninto the tSZ maps. This CIB contamination is difficult to clean via\nmultifrequency component separation methods, such as internal linear\ncombination (ILC), due to uncertainty in its spectral energy distribution\n(SED), which exhibits spatial and line-of-sight variation and decorrelation.\nThus, improved ILC-based techniques have been developed to null (\"deproject\")\nboth the CIB and its first moment with respect to the emissivity index $\\beta$\nin order to robustly remove the CIB despite the lack of first-principles\nknowledge of its SED. While decreasing the bias, such procedures can\nsignificantly increase the noise in the resulting ILC maps. In this paper, we\ndevelop a data-driven algorithm for determining the optimal CIB SED to\ndeproject when measuring a tSZ-LSS cross-correlation, obviating the need to\ndeproject the first moment in the ILC map used for such a measurement. Our\nmethod gives an unbiased cross-correlation with increased signal-to-noise. We\ndemonstrate its efficacy on simulations, finding a 60% improvement in the\nsignal-to-noise ratio for an example tSZ cross-correlation with a halo sample\nat redshifts $0.8 < z < 1.8$, as compared to moment deprojection approaches.\nThough used here for CIB removal in tSZ cross-correlations, our method is\nbroadly applicable to minimizing contaminant leakage in ILC maps. Our code is\navailable in CIB-deproj.",
      "url": "http://arxiv.org/abs/2505.14644v1",
      "published_time_eastern_timestamp": 1747762273.0
    },
    {
      "title": "Bridging Predictive Coding and MDL: A Two-Part Code Framework for Deep\n  Learning",
      "summary": "We present the first theoretical framework that connects predictive coding\n(PC), a biologically inspired local learning rule, with the minimum description\nlength (MDL) principle in deep networks. We prove that layerwise PC performs\nblock-coordinate descent on the MDL two-part code objective, thereby jointly\nminimizing empirical risk and model complexity. Using Hoeffding's inequality\nand a prefix-code prior, we derive a novel generalization bound of the form\n$R(\\theta) \\le \\^{R}(\\theta) + \\frac{L(\\theta)}{N}$, capturing the tradeoff\nbetween fit and compression. We further prove that each PC sweep monotonically\ndecreases the empirical two-part codelength, yielding tighter high-probability\nrisk bounds than unconstrained gradient descent. Finally, we show that repeated\nPC updates converge to a block-coordinate stationary point, providing an\napproximate MDL-optimal solution. To our knowledge, this is the first result\noffering formal generalization and convergence guarantees for PC-trained deep\nmodels, positioning PC as a theoretically grounded and biologically plausible\nalternative to backpropagation.",
      "url": "http://arxiv.org/abs/2505.14635v1",
      "published_time_eastern_timestamp": 1747761916.0
    },
    {
      "title": "Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values\n  Prioritization with AIRiskDilemmas",
      "summary": "Detecting AI risks becomes more challenging as stronger models emerge and\nfind novel methods such as Alignment Faking to circumvent these detection\nattempts. Inspired by how risky behaviors in humans (i.e., illegal activities\nthat may hurt others) are sometimes guided by strongly-held values, we believe\nthat identifying values within AI models can be an early warning system for\nAI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal\nAI models' priorities on a range of AI value classes. Then, we collect\nAIRiskDilemmas, a diverse collection of dilemmas that pit values against one\nanother in scenarios relevant to AI safety risks such as Power Seeking. By\nmeasuring an AI model's value prioritization using its aggregate choices, we\nobtain a self-consistent set of predicted value priorities that uncover\npotential risks. We show that values in LitmusValues (including seemingly\ninnocuous ones like Care) can predict for both seen risky behaviors in\nAIRiskDilemmas and unseen risky behaviors in HarmBench.",
      "url": "http://arxiv.org/abs/2505.14633v1",
      "published_time_eastern_timestamp": 1747761849.0
    },
    {
      "title": "KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large\n  Language Models",
      "summary": "Recent advances in large language models (LLMs) and the abundance of food\ndata have resulted in studies to improve food understanding using LLMs. Despite\nseveral recommendation systems utilizing LLMs and Knowledge Graphs (KGs), there\nhas been limited research on integrating food related KGs with LLMs. We\nintroduce KERL, a unified system that leverages food KGs and LLMs to provide\npersonalized food recommendations and generates recipes with associated\nmicro-nutritional information. Given a natural language question, KERL extracts\nentities, retrieves subgraphs from the KG, which are then fed into the LLM as\ncontext to select the recipes that satisfy the constraints. Next, our system\ngenerates the cooking steps and nutritional information for each recipe. To\nevaluate our approach, we also develop a benchmark dataset by curating recipe\nrelated questions, combined with constraints and personal preferences. Through\nextensive experiments, we show that our proposed KG-augmented LLM significantly\noutperforms existing approaches, offering a complete and coherent solution for\nfood recommendation, recipe generation, and nutritional analysis. Our code and\nbenchmark datasets are publicly available at\nhttps://github.com/mohbattharani/KERL.",
      "url": "http://arxiv.org/abs/2505.14629v1",
      "published_time_eastern_timestamp": 1747761597.0
    },
    {
      "title": "TinyV: Reducing False Negatives in Verification Improves RL for LLM\n  Reasoning",
      "summary": "Reinforcement Learning (RL) has become a powerful tool for enhancing the\nreasoning abilities of large language models (LLMs) by optimizing their\npolicies with reward signals. Yet, RL's success relies on the reliability of\nrewards, which are provided by verifiers. In this paper, we expose and analyze\na widespread problem--false negatives--where verifiers wrongly reject correct\nmodel outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals\nthat over 38% of model-generated responses suffer from false negatives, where\nthe verifier fails to recognize correct answers. We show, both empirically and\ntheoretically, that these false negatives severely impair RL training by\ndepriving the model of informative gradient signals and slowing convergence. To\nmitigate this, we propose tinyV, a lightweight LLM-based verifier that augments\nexisting rule-based methods, which dynamically identifies potential false\nnegatives and recovers valid responses to produce more accurate reward\nestimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts\npass rates by up to 10% and accelerates convergence relative to the baseline.\nOur findings highlight the critical importance of addressing verifier false\nnegatives and offer a practical approach to improve RL-based fine-tuning of\nLLMs. Our code is available at https://github.com/uw-nsl/TinyV.",
      "url": "http://arxiv.org/abs/2505.14625v1",
      "published_time_eastern_timestamp": 1747761404.0
    },
    {
      "title": "sudoLLM : On Multi-role Alignment of Language Models",
      "summary": "User authorization-based access privileges are a key feature in many\nsafety-critical systems, but have thus far been absent from the large language\nmodel (LLM) realm. In this work, drawing inspiration from such access control\nsystems, we introduce sudoLLM, a novel framework that results in multi-role\naligned LLMs, i.e., LLMs that account for, and behave in accordance with, user\naccess rights. sudoLLM injects subtle user-based biases into queries and trains\nan LLM to utilize this bias signal in order to produce sensitive information if\nand only if the user is authorized. We present empirical results demonstrating\nthat this approach shows substantially improved alignment, generalization, and\nresistance to prompt-based jailbreaking attacks. The persistent tension between\nthe language modeling objective and safety alignment, which is often exploited\nto jailbreak LLMs, is somewhat resolved with the aid of the injected bias\nsignal. Our framework is meant as an additional security layer, and complements\nexisting guardrail mechanisms for enhanced end-to-end safety with LLMs.",
      "url": "http://arxiv.org/abs/2505.14607v1",
      "published_time_eastern_timestamp": 1747760074.0
    },
    {
      "title": "Electrostatics from Laplacian Eigenbasis for Neural Network Interatomic\n  Potentials",
      "summary": "Recent advances in neural network interatomic potentials have emerged as a\npromising research direction. However, popular deep learning models often lack\nauxiliary constraints grounded in physical laws, which could accelerate\ntraining and improve fidelity through physics-based regularization. In this\nwork, we introduce $\\Phi$-Module, a universal plugin module that enforces\nPoisson's equation within the message-passing framework to learn electrostatic\ninteractions in a self-supervised manner. Specifically, each atom-wise\nrepresentation is encouraged to satisfy a discretized Poisson's equation,\nmaking it possible to acquire a potential $\\boldsymbol{\\phi}$ and a\ncorresponding charge density $\\boldsymbol{\\rho}$ linked to the learnable\nLaplacian eigenbasis coefficients of a given molecular graph. We then derive an\nelectrostatic energy term, crucial for improved total energy predictions. This\napproach integrates seamlessly into any existing neural potential with\ninsignificant computational overhead. Experiments on the OE62 and MD22\nbenchmarks confirm that models combined with $\\Phi$-Module achieve robust\nimprovements over baseline counterparts. For OE62 error reduction ranges from\n4.5\\% to 17.8\\%, and for MD22, baseline equipped with $\\Phi$-Module achieves\nbest results on 5 out of 14 cases. Our results underscore how embedding a\nfirst-principles constraint in neural interatomic potentials can significantly\nimprove performance while remaining hyperparameter-friendly, memory-efficient\nand lightweight in training. Code will be available at\n\\href{https://github.com/dunnolab/phi-module}{dunnolab/phi-module}.",
      "url": "http://arxiv.org/abs/2505.14606v1",
      "published_time_eastern_timestamp": 1747760065.0
    },
    {
      "title": "AdaKWS: Towards Robust Keyword Spotting with Test-Time Adaptation",
      "summary": "Spoken keyword spotting (KWS) aims to identify keywords in audio for wide\napplications, especially on edge devices. Current small-footprint KWS systems\nfocus on efficient model designs. However, their inference performance can\ndecline in unseen environments or noisy backgrounds. Test-time adaptation (TTA)\nhelps models adapt to test samples without needing the original training data.\nIn this study, we present AdaKWS, the first TTA method for robust KWS to the\nbest of our knowledge. Specifically, 1) We initially optimize the model's\nconfidence by selecting reliable samples based on prediction entropy\nminimization and adjusting the normalization statistics in each batch. 2) We\nintroduce pseudo-keyword consistency (PKC) to identify critical, reliable\nfeatures without overfitting to noise. Our experiments show that AdaKWS\noutperforms other methods across various conditions, including Gaussian noise\nand real-scenario noises. The code will be released in due course.",
      "url": "http://arxiv.org/abs/2505.14600v1",
      "published_time_eastern_timestamp": 1747759821.0
    },
    {
      "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models",
      "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.",
      "url": "http://arxiv.org/abs/2505.14599v1",
      "published_time_eastern_timestamp": 1747759780.0
    },
    {
      "title": "Success is in the Details: Evaluate and Enhance Details Sensitivity of\n  Code LLMs through Counterfactuals",
      "summary": "Code Sensitivity refers to the ability of Code LLMs to recognize and respond\nto details changes in problem descriptions. While current code benchmarks and\ninstruction data focus on difficulty and diversity, sensitivity is overlooked.\nWe first introduce the CTF-Code benchmark, constructed using counterfactual\nperturbations, minimizing input changes while maximizing output changes. The\nevaluation shows that many LLMs have a more than 10\\% performance drop compared\nto the original problems. To fully utilize sensitivity, CTF-Instruct, an\nincremental instruction fine-tuning framework, extends on existing data and\nuses a selection mechanism to meet the three dimensions of difficulty,\ndiversity, and sensitivity. Experiments show that LLMs fine-tuned with\nCTF-Instruct data achieve over a 2\\% improvement on CTF-Code, and more than a\n10\\% performance boost on LiveCodeBench, validating the feasibility of\nenhancing LLMs' sensitivity to improve performance.",
      "url": "http://arxiv.org/abs/2505.14597v1",
      "published_time_eastern_timestamp": 1747759737.0
    }
  ]
}
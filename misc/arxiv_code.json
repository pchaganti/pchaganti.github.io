{
  "last_updated": "2025-06-09T23:43:31.017028-04:00",
  "papers": [
    {
      "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning\n  from Partially Annotated Synthetic Datasets",
      "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.",
      "url": "http://arxiv.org/abs/2506.08013v1",
      "published_time_eastern_timestamp": 1749491999.0
    },
    {
      "title": "Why M-dwarf flares have limited impact on sub-Neptunes' atmospheric\n  evaporation",
      "summary": "M-type stars are prime targets for exoplanet searches within their habitable\nzones (HZs). M-type stars also exhibit significant magnetic flaring activity,\nespecially during the first billion years, which can potentially accelerate the\nevaporation of the hydrogen-helium envelopes of close-in planets. We use the\ntime-dependent photo-ionization hydrodynamics code ATES to investigate the\nimpact of flares on atmospheric escape, focusing on a sub-Neptune-sized planet\norbiting an early M-type star at distances of 0.01, 0.1, and 0.18-0.36 AU--the\ninner and outer edges of the HZ. Stellar flaring is modeled as a 1 Gyr-long\nhigh-activity phase followed by a 4 Gyr-long low-activity phase, each\ncharacterized by a suitable flare frequency distribution. We find that flares\nhave a modest impact (i.e., less than a factor of two) on increasing the\ncumulative atmospheric mass loss, with the greatest enhancement occurring when\nthe planet is at its closest separation. However, the relative enhancement in\nmass loss between flaring and non-flaring cases is greater at larger\nseparations. This trend arises because, as stellar irradiation fluctuates\nbetween quiescent levels and peak flares, the proportion of time the planet\nspends in energy-limited versus recombination-limited mass loss regimes depends\non its orbital separation. Additionally, we demonstrate the existence of a\ncharacteristic flare energy, intermediate between the minimum and maximum\nvalues, that maximizes the fractional contribution to flare-driven mass loss.\nOur results indicate that the flaring activity of M-dwarfs does not\nsignificantly affect the atmospheric retention of close-in Neptune-sized\nplanets, including within the HZ. The potential occurrence of rare\nsuper-flares, which observational campaigns might be biased against, does not\nalter our conclusions.",
      "url": "http://arxiv.org/abs/2506.08014v1",
      "published_time_eastern_timestamp": 1749491999.0
    },
    {
      "title": "Vision Transformers Don't Need Trained Registers",
      "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.",
      "url": "http://arxiv.org/abs/2506.08010v1",
      "published_time_eastern_timestamp": 1749491997.0
    },
    {
      "title": "Dreamland: Controllable World Creation with Simulator and Generative\n  Models",
      "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.",
      "url": "http://arxiv.org/abs/2506.08006v1",
      "published_time_eastern_timestamp": 1749491992.0
    },
    {
      "title": "Generative Modeling of Weights: Generalization or Memorization?",
      "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.",
      "url": "http://arxiv.org/abs/2506.07998v1",
      "published_time_eastern_timestamp": 1749491916.0
    },
    {
      "title": "PairEdit: Learning Semantic Variations for Exemplar-based Image Editing",
      "summary": "Recent advancements in text-guided image editing have achieved notable\nsuccess by leveraging natural language prompts for fine-grained semantic\ncontrol. However, certain editing semantics are challenging to specify\nprecisely using textual descriptions alone. A practical alternative involves\nlearning editing semantics from paired source-target examples. Existing\nexemplar-based editing methods still rely on text prompts describing the change\nwithin paired examples or learning implicit text-based editing instructions. In\nthis paper, we introduce PairEdit, a novel visual editing method designed to\neffectively learn complex editing semantics from a limited number of image\npairs or even a single image pair, without using any textual guidance. We\npropose a target noise prediction that explicitly models semantic variations\nwithin paired images through a guidance direction term. Moreover, we introduce\na content-preserving noise schedule to facilitate more effective semantic\nlearning. We also propose optimizing distinct LoRAs to disentangle the learning\nof semantic variations from content. Extensive qualitative and quantitative\nevaluations demonstrate that PairEdit successfully learns intricate semantics\nwhile significantly improving content consistency compared to baseline methods.\nCode will be available at https://github.com/xudonmao/PairEdit.",
      "url": "http://arxiv.org/abs/2506.07992v1",
      "published_time_eastern_timestamp": 1749491835.0
    },
    {
      "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers",
      "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}",
      "url": "http://arxiv.org/abs/2506.07986v1",
      "published_time_eastern_timestamp": 1749491644.0
    },
    {
      "title": "Exposing Hidden Backdoors in NFT Smart Contracts: A Static Security\n  Analysis of Rug Pull Patterns",
      "summary": "The explosive growth of Non-Fungible Tokens (NFTs) has revolutionized digital\nownership by enabling the creation, exchange, and monetization of unique assets\non blockchain networks. However, this surge in popularity has also given rise\nto a disturbing trend: the emergence of rug pulls - fraudulent schemes where\ndevelopers exploit trust and smart contract privileges to drain user funds or\ninvalidate asset ownership. Central to many of these scams are hidden backdoors\nembedded within NFT smart contracts. Unlike unintentional bugs, these backdoors\nare deliberately coded and often obfuscated to bypass traditional audits and\nexploit investor confidence. In this paper, we present a large-scale static\nanalysis of 49,940 verified NFT smart contracts using Slither, a static\nanalysis framework, to uncover latent vulnerabilities commonly linked to rug\npulls. We introduce a custom risk scoring model that classifies contracts into\nhigh, medium, or low risk tiers based on the presence and severity of rug pull\nindicators. Our dataset was derived from verified contracts on the Ethereum\nmainnet, and we generate multiple visualizations to highlight red flag\nclusters, issue prevalence, and co-occurrence of critical vulnerabilities.\nWhile we do not perform live exploits, our results reveal how malicious\npatterns often missed by simple reviews can be surfaced through static analysis\nat scale. We conclude by offering mitigation strategies for developers,\nmarketplaces, and auditors to enhance smart contract security. By exposing how\nhidden backdoors manifest in real-world smart contracts, this work contributes\na practical foundation for detecting and mitigating NFT rug pulls through\nscalable automated analysis.",
      "url": "http://arxiv.org/abs/2506.07974v1",
      "published_time_eastern_timestamp": 1749491344.0
    },
    {
      "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in\n  Combinatorial Optimization",
      "summary": "While Large Language Models (LLMs) have demonstrated significant advancements\nin reasoning and agent-based problem-solving, current evaluation methodologies\nfail to adequately assess their capabilities: existing benchmarks either rely\non closed-ended questions prone to saturation and memorization, or subjective\ncomparisons that lack consistency and rigor. In this work, we introduce\nHeuriGym, an agentic framework designed for evaluating heuristic algorithms\ngenerated by LLMs for combinatorial optimization problems, characterized by\nclearly defined objectives and expansive solution spaces. HeuriGym empowers\nLLMs to propose heuristics, receive evaluative feedback via code execution, and\niteratively refine their solutions. We evaluate nine state-of-the-art models on\nnine problems across domains such as computer systems, logistics, and biology,\nexposing persistent limitations in tool use, planning, and adaptive reasoning.\nTo quantify performance, we propose the Quality-Yield Index (QYI), a metric\nthat captures both solution pass rate and quality. Even top models like\nGPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below\nthe expert baseline of 1. Our open-source benchmark aims to guide the\ndevelopment of LLMs toward more effective and realistic problem-solving in\nscientific and engineering domains.",
      "url": "http://arxiv.org/abs/2506.07972v1",
      "published_time_eastern_timestamp": 1749491207.0
    },
    {
      "title": "CyberV: Cybernetics for Test-time Scaling in Video Understanding",
      "summary": "Current Multimodal Large Language Models (MLLMs) may struggle with\nunderstanding long or complex videos due to computational demands at test time,\nlack of robustness, and limited accuracy, primarily stemming from their\nfeed-forward processing nature. These limitations could be more severe for\nmodels with fewer parameters. To address these limitations, we propose a novel\nframework inspired by cybernetic principles, redesigning video MLLMs as\nadaptive systems capable of self-monitoring, self-correction, and dynamic\nresource allocation during inference. Our approach, CyberV, introduces a\ncybernetic loop consisting of an MLLM Inference System, a Sensor, and a\nController. Specifically, the sensor monitors forward processes of the MLLM and\ncollects intermediate interpretations, such as attention drift, then the\ncontroller determines when and how to trigger self-correction and generate\nfeedback to guide the next round. This test-time adaptive scaling framework\nenhances frozen MLLMs without requiring retraining or additional components.\nExperiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B\nby 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive\nproprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%\nimprovement, achieving performance even comparable to human experts.\nFurthermore, our method demonstrates consistent gains on general-purpose\nbenchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and\ngeneralization capabilities in making MLLMs more robust and accurate for\ndynamic video understanding. The code is released at\nhttps://github.com/marinero4972/CyberV.",
      "url": "http://arxiv.org/abs/2506.07971v1",
      "published_time_eastern_timestamp": 1749491118.0
    },
    {
      "title": "A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in\n  High-Speed Flow Modeling",
      "summary": "We consider the problem of modeling high-speed flows using machine learning\nmethods. While most prior studies focus on low-speed fluid flows in which\nuniform time-stepping is practical, flows approaching and exceeding the speed\nof sound exhibit sudden changes such as shock waves. In such cases, it is\nessential to use adaptive time-stepping methods to allow a temporal resolution\nsufficient to resolve these phenomena while simultaneously balancing\ncomputational costs. Here, we propose a two-phase machine learning method,\nknown as ShockCast, to model high-speed flows with adaptive time-stepping. In\nthe first phase, we propose to employ a machine learning model to predict the\ntimestep size. In the second phase, the predicted timestep is used as an input\nalong with the current fluid fields to advance the system state by the\npredicted timestep. We explore several physically-motivated components for\ntimestep prediction and introduce timestep conditioning strategies inspired by\nneural ODE and Mixture of Experts. As ShockCast is the first framework for\nlearning high-speed flows, we evaluate our methods by generating two supersonic\nflow datasets, available at https://huggingface.co/datasets/divelab. Our code\nis publicly available as part of the AIRS library\n(https://github.com/divelab/AIRS).",
      "url": "http://arxiv.org/abs/2506.07969v1",
      "published_time_eastern_timestamp": 1749491060.0
    },
    {
      "title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models\n  in Compositional Spatial Intelligence",
      "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nvarious multimodal tasks. To pursue higher intelligence in space, MLLMs require\nintegrating multiple atomic spatial capabilities to handle complex and dynamic\ntasks. However, existing benchmarks struggle to comprehensively evaluate the\nspatial intelligence of common MLLMs from the atomic level to the compositional\nlevel. To fill this gap, we present SpaCE-10, a comprehensive benchmark for\ncompositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial\ncapabilities, which are combined to form 8 compositional capabilities. Based on\nthese definitions, we propose a novel hierarchical annotation pipeline to\ngenerate high-quality and diverse question-answer (QA) pairs. With over 150+\nhours of human expert effort, we obtain over 5k QA pairs for 811 real indoor\nscenes in SpaCE-10, which covers various evaluation settings like point cloud\ninput and multi-choice QA. We conduct an extensive evaluation of common MLLMs\non SpaCE-10 and find that even the most advanced MLLM still lags behind humans\nby large margins. Through our careful study, we also draw several significant\nfindings that benefit the MLLM community. For example, we reveal that the\nshortcoming of counting capability greatly limits the compositional spatial\ncapabilities of existing MLLMs. The evaluation code and benchmark datasets are\navailable at https://github.com/Cuzyoung/SpaCE-10.",
      "url": "http://arxiv.org/abs/2506.07966v1",
      "published_time_eastern_timestamp": 1749490896.0
    },
    {
      "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from\n  Design",
      "summary": "Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.",
      "url": "http://arxiv.org/abs/2506.07964v1",
      "published_time_eastern_timestamp": 1749490788.0
    },
    {
      "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication\n  Protocols",
      "summary": "Recent advances in Large Language Models (LLMs) have shown promising\ncapabilities in generating code for general-purpose programming languages. In\ncontrast, their applicability for hardware description languages, particularly\nfor generating synthesizable and functionally correct designs, remains\nsignificantly underexplored. HDLs such as SystemVerilog are logic-oriented and\ndemand strict adherence to timing semantics, concurrency, and synthesizability\nconstraints. Moreover, HDL-based design flows encompass a broad set of tasks\nbeyond structural code generation, including testbench development,\nassertion-based verification, timing closure, and protocol-level integration\nfor on-chip communication. The objective of our paper is to analyze the\ncapabilities of state-of-the-art LLMs in generating SystemVerilog\nimplementations of standard communication protocols, a core component of\nembedded and System-on-Chip (SoC) architectures. This paper introduces the\nfirst benchmark suite targeting four widely used protocols: SPI, I2C, UART, and\nAXI. We define code generation tasks that capture varying levels of design\nabstraction and prompt specificity. The generated designs are assessed for\nsyntactic correctness, synthesizability, and functional fidelity via waveform\nsimulation and test benches.",
      "url": "http://arxiv.org/abs/2506.07945v1",
      "published_time_eastern_timestamp": 1749489047.0
    },
    {
      "title": "Adversarial Attack Classification and Robustness Testing for Large\n  Language Models for Code",
      "summary": "Large Language Models (LLMs) have become vital tools in software development\ntasks such as code generation, completion, and analysis. As their integration\ninto workflows deepens, ensuring robustness against vulnerabilities especially\nthose triggered by diverse or adversarial inputs becomes increasingly\nimportant. Such vulnerabilities may lead to incorrect or insecure code\ngeneration when models encounter perturbed task descriptions, code, or\ncomments. Prior research often overlooks the role of natural language in\nguiding code tasks. This study investigates how adversarial perturbations in\nnatural language inputs including prompts, comments, and descriptions affect\nLLMs for Code (LLM4Code). It examines the effects of perturbations at the\ncharacter, word, and sentence levels to identify the most impactful\nvulnerabilities. We analyzed multiple projects (e.g., ReCode, OpenAttack) and\ndatasets (e.g., HumanEval, MBPP), establishing a taxonomy of adversarial\nattacks. The first dimension classifies the input type code, prompts, or\ncomments while the second dimension focuses on granularity: character, word, or\nsentence-level changes. We adopted a mixed-methods approach, combining\nquantitative performance metrics with qualitative vulnerability analysis.\nLLM4Code models show varying robustness across perturbation types.\nSentence-level attacks were least effective, suggesting models are resilient to\nbroader contextual changes. In contrast, word-level perturbations posed serious\nchallenges, exposing semantic vulnerabilities. Character-level effects varied,\nshowing model sensitivity to subtle syntactic deviations.Our study offers a\nstructured framework for testing LLM4Code robustness and emphasizes the\ncritical role of natural language in adversarial evaluation. Improving model\nresilience to semantic-level disruptions is essential for secure and reliable\ncode-generation systems.",
      "url": "http://arxiv.org/abs/2506.07942v1",
      "published_time_eastern_timestamp": 1749488549.0
    },
    {
      "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
      "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.",
      "url": "http://arxiv.org/abs/2506.07932v1",
      "published_time_eastern_timestamp": 1749487930.0
    },
    {
      "title": "Solving Inequality Proofs with Large Language Models",
      "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
      "url": "http://arxiv.org/abs/2506.07927v1",
      "published_time_eastern_timestamp": 1749487418.0
    },
    {
      "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State\n  Spaces",
      "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.",
      "url": "http://arxiv.org/abs/2506.07903v1",
      "published_time_eastern_timestamp": 1749486020.0
    },
    {
      "title": "FunDiff: Diffusion Models over Function Spaces for Physics-Informed\n  Generative Modeling",
      "summary": "Recent advances in generative modeling -- particularly diffusion models and\nflow matching -- have achieved remarkable success in synthesizing discrete data\nsuch as images and videos. However, adapting these models to physical\napplications remains challenging, as the quantities of interest are continuous\nfunctions governed by complex physical laws. Here, we introduce\n$\\textbf{FunDiff}$, a novel framework for generative modeling in function\nspaces. FunDiff combines a latent diffusion process with a function autoencoder\narchitecture to handle input functions with varying discretizations, generate\ncontinuous functions evaluable at arbitrary locations, and seamlessly\nincorporate physical priors. These priors are enforced through architectural\nconstraints or physics-informed loss functions, ensuring that generated samples\nsatisfy fundamental physical laws. We theoretically establish minimax\noptimality guarantees for density estimation in function spaces, showing that\ndiffusion-based estimators achieve optimal convergence rates under suitable\nregularity conditions. We demonstrate the practical effectiveness of FunDiff\nacross diverse applications in fluid dynamics and solid mechanics. Empirical\nresults show that our method generates physically consistent samples with high\nfidelity to the target distribution and exhibits robustness to noisy and\nlow-resolution data. Code and datasets are publicly available at\nhttps://github.com/sifanexisted/fundiff.",
      "url": "http://arxiv.org/abs/2506.07902v1",
      "published_time_eastern_timestamp": 1749485999.0
    },
    {
      "title": "Stone Soup: ADS-B-based Multi-Target Tracking with Stochastic\n  Integration Filter",
      "summary": "This paper focuses on the multi-target tracking using the Stone Soup\nframework. In particular, we aim at evaluation of two multi-target tracking\nscenarios based on the simulated class-B dataset and ADS-B class-A dataset\nprovided by OpenSky Network. The scenarios are evaluated w.r.t. selection of a\nlocal state estimator using a range of the Stone Soup metrics. Source code with\nscenario definitions and Stone Soup set-up are provided along with the paper.",
      "url": "http://arxiv.org/abs/2506.07889v1",
      "published_time_eastern_timestamp": 1749484907.0
    }
  ]
}
{
  "last_updated": "2025-08-07T08:26:55.498932-04:00",
  "papers": [
    {
      "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience",
      "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
      "url": "http://arxiv.org/abs/2508.04700v1",
      "published_time_eastern_timestamp": 1754503126.0
    },
    {
      "title": "Finite 2-group gauge theory and its 3+1D lattice realization",
      "summary": "In this work, we employ the Tannaka-Krein reconstruction to compute the\nquantum double $\\mathcal D(\\mathcal G)$ of a finite 2-group $\\mathcal G$ as a\nHopf monoidal category. We also construct a 3+1D lattice model from the\nDijkgraaf-Witten TQFT functor for the 2-group $\\mathcal G$, generalizing\nKitaev's 2+1D quantum double model. Notably, the string-like local operators in\nthis lattice model are shown to form $\\mathcal D(\\mathcal G)$. Specializing to\n$\\mathcal G = \\mathbb{Z}_2$, we demonstrate that the topological defects in the\n3+1D toric code model are modules over $\\mathcal D(\\mathbb{Z}_2)$.",
      "url": "http://arxiv.org/abs/2508.04693v1",
      "published_time_eastern_timestamp": 1754502976.0
    },
    {
      "title": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in\n  Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario",
      "summary": "Multi-agent robotic systems (MARS) build upon multi-agent systems by\nintegrating physical and task-related constraints, increasing the complexity of\naction execution and agent coordination. However, despite the availability of\nadvanced multi-agent frameworks, their real-world deployment on robots remains\nlimited, hindering the advancement of MARS research in practice. To bridge this\ngap, we conducted two studies to investigate performance trade-offs of\nhierarchical multi-agent frameworks in a simulated real-world multi-robot\nhealthcare scenario. In Study 1, using CrewAI, we iteratively refine the\nsystem's knowledge base, to systematically identify and categorize coordination\nfailures (e.g., tool access violations, lack of timely handling of failure\nreports) not resolvable by providing contextual knowledge alone. In Study 2,\nusing AutoGen, we evaluate a redesigned bidirectional communication structure\nand further measure the trade-offs between reasoning and non-reasoning models\noperating within the same robotic team setting. Drawing from our empirical\nfindings, we emphasize the tension between autonomy and stability and the\nimportance of edge-case testing to improve system reliability and safety for\nfuture real-world deployment. Supplementary materials, including codes, task\nagent setup, trace outputs, and annotated examples of coordination failures and\nreasoning behaviors, are available at:\nhttps://byc-sophie.github.io/mas-to-mars/.",
      "url": "http://arxiv.org/abs/2508.04691v1",
      "published_time_eastern_timestamp": 1754502850.0
    },
    {
      "title": "Time-dependent response of protoplanetary disk temperature to an FU\n  Ori-type luminosity outburst",
      "summary": "Context. The most prominent cases of young star variability are accretion\noutbursts in FU Ori-type systems. The high power of such outbursts causes\ndramatic changes in the physical and chemical structure of a surrounding\nprotoplanetary disk. As characteristic thermal timescales in the disk are\ncomparable to the duration of the outburst, the response of its thermal\nstructure is inherently time dependent.\n  Aims. We analyzed how the disk thermal structure evolves under the\nsubstantial-yet transient-eating of the outburst. To cover different possible\nphysical mechanisms driving the outburst, we examined two scenarios: one in\nwhich the increased accretion rate is confined to a compact sub-au inner region\nand the other where it affects the entire disk.\n  Methods. To model the disk temperature response to the outburst we performed\ntime-dependent radiation transfer using the HURAKAN code. The disk structure\nand the luminosity profile roughly correspond to those of the FU Ori system\nitself, which went into outburst about 90 years ago and reached a luminosity of\n450 L_Sun.\n  Results. We find that optically thick disk regions require several years to\nbecome fully heated during the outburst and a decade to cool after it. The\nupper layers and outer parts of the disk, which are optically thin to thermal\nradiation, are heated and cooled almost instantaneously. This creates an\nunusual radial temperature profile during the early heating phase with minima\nat several au both for the fully active and compact active disk scenarios. At\nthe cooling phase, upper layers being colder than the midplane for both\nscenarios. Near- and mid-infrared SEDs demonstrate a significant and almost\ninstantaneous rise by 1 - 2 orders of magnitude during the outburst, while the\nmillimeter flux shows a change of only a factor of a few, and is slightly\ndelayed with respect to the central region luminosity profile.",
      "url": "http://arxiv.org/abs/2508.04686v1",
      "published_time_eastern_timestamp": 1754502589.0
    },
    {
      "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay",
      "summary": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.",
      "url": "http://arxiv.org/abs/2508.04676v1",
      "published_time_eastern_timestamp": 1754502142.0
    },
    {
      "title": "How are CS students using resources and AI tools for coding tasks?",
      "summary": "A survey of 26 CS students reveals that AI coding assistants are mainly used\nfor writing code (second to online searches) while AI chatbots are the top\nresource for debugging. Participants with different coding experience prefer\nonline help over direct human help from peers and instructors.",
      "url": "http://arxiv.org/abs/2508.04667v1",
      "published_time_eastern_timestamp": 1754501755.0
    },
    {
      "title": "PixCuboid: Room Layout Estimation from Multi-view Featuremetric\n  Alignment",
      "summary": "Coarse room layout estimation provides important geometric cues for many\ndownstream tasks. Current state-of-the-art methods are predominantly based on\nsingle views and often assume panoramic images. We introduce PixCuboid, an\noptimization-based approach for cuboid-shaped room layout estimation, which is\nbased on multi-view alignment of dense deep features. By training with the\noptimization end-to-end, we learn feature maps that yield large convergence\nbasins and smooth loss landscapes in the alignment. This allows us to\ninitialize the room layout using simple heuristics.\n  For the evaluation we propose two new benchmarks based on ScanNet++ and\n2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough\nexperiments we validate our approach and significantly outperform the\ncompetition. Finally, while our network is trained with single cuboids, the\nflexibility of the optimization-based approach allow us to easily extend to\nmulti-room estimation, e.g. larger apartments or offices. Code and model\nweights are available at https://github.com/ghanning/PixCuboid.",
      "url": "http://arxiv.org/abs/2508.04659v1",
      "published_time_eastern_timestamp": 1754501270.0
    },
    {
      "title": "X-SAM: From Segment Anything to Any Segmentation",
      "summary": "Large Language Models (LLMs) demonstrate strong capabilities in broad\nknowledge representation, yet they are inherently deficient in pixel-level\nperceptual understanding. Although the Segment Anything Model (SAM) represents\na significant advancement in visual-prompt-driven image segmentation, it\nexhibits notable limitations in multi-mask prediction and category-specific\nsegmentation tasks, and it cannot integrate all segmentation tasks within a\nunified model architecture. To address these limitations, we present X-SAM, a\nstreamlined Multimodal Large Language Model (MLLM) framework that extends the\nsegmentation paradigm from \\textit{segment anything} to \\textit{any\nsegmentation}. Specifically, we introduce a novel unified framework that\nenables more advanced pixel-level perceptual comprehension for MLLMs.\nFurthermore, we propose a new segmentation task, termed Visual GrounDed (VGD)\nsegmentation, which segments all instance objects with interactive visual\nprompts and empowers MLLMs with visual grounded, pixel-wise interpretative\ncapabilities. To enable effective training on diverse data sources, we present\na unified training strategy that supports co-training across multiple datasets.\nExperimental results demonstrate that X-SAM achieves state-of-the-art\nperformance on a wide range of image segmentation benchmarks, highlighting its\nefficiency for multimodal, pixel-level visual understanding. Code is available\nat https://github.com/wanghao9610/X-SAM.",
      "url": "http://arxiv.org/abs/2508.04655v1",
      "published_time_eastern_timestamp": 1754500750.0
    },
    {
      "title": "LLM Collaboration With Multi-Agent Reinforcement Learning",
      "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.",
      "url": "http://arxiv.org/abs/2508.04652v1",
      "published_time_eastern_timestamp": 1754500705.0
    },
    {
      "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards",
      "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.",
      "url": "http://arxiv.org/abs/2508.04632v1",
      "published_time_eastern_timestamp": 1754499654.0
    }
  ]
}
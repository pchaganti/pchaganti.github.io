{
  "last_updated": "2025-08-05T11:16:11.442700-04:00",
  "papers": [
    {
      "title": "Raw Data Matters: Enhancing Prompt Tuning by Internal Augmentation on\n  Vision-Language Models",
      "summary": "For CLIP-based prompt tuning, introducing more data as additional knowledge\nfor enhancing fine-tuning process is proved to be an effective approach.\nExisting data amplification strategies for prompt tuning typically rely on\nexternal knowledge (e.g., large language models or pre-structured knowledge\nbases), resulting in higher costs for data collection and processing, while\ngenerally ignoring further utilization of features in image modality. To\naddress this, we propose Augmentation-driven Prompt Tuning (AugPT), a\nself-contained distillation-based prompt tuning approach using only internal\naugmentation on raw dataset to better exploit known features. Specifically,\nAugPT employs self-supervised augmentation on unlabeled images in the training\nset, and introduces a novel gating mechanism based on consensus test, reusing\nthe pre-trained prompt tuning backbone model to spontaneously filter noisy\nsamples, further enhancing the quality of augmented views. Extensive\nexperiments validate that AugPT simultaneously enhances model performance and\ngeneralization capability without using appended external knowledge. The code\nof AugPT is available at: https://github.com/JREion/AugPT .",
      "url": "http://arxiv.org/abs/2508.02671v1",
      "published_time_eastern_timestamp": 1754330396.0
    },
    {
      "title": "atommovr: An open-source simulation framework for rearrangement in\n  atomic arrays",
      "summary": "The task of atom rearrangement has emerged in the last decade as a\nfundamental building block for the development of neutral atom-based quantum\nprocessors. However, despite many recent efforts to develop algorithms with\nfavorable asymptotic scaling, no time-optimal algorithm has been developed for\nany rearrangement task. Moreover, no open-source code exists to reproduce or\nbenchmark existing algorithms, and to assist the development of new\nrearrangement protocols. To address this deficiency, we develop an open-source\nsimulation framework for developing, comparing, and benchmarking algorithms\nunder realistic and customizable noise models. Using this framework, we\n\\textbf{a)} numerically extract lower bounds for the scaling of a time-optimal\nrearrangement algorithm and compare it to existing heuristic algorithms\n\\textbf{b)} develop a naive dual-species algorithm able to prepare arbitrary\ntargets with near-unity success rate. With this framework, we hope to develop a\ncommon tool for the community to study rearrangement, lower the barrier to\nentry for new experimental groups, and stimulate progress in developing\nalgorithms which approach time-optimal scaling.",
      "url": "http://arxiv.org/abs/2508.02670v1",
      "published_time_eastern_timestamp": 1754330387.0
    },
    {
      "title": "MedVLThinker: Simple Baselines for Multimodal Medical Reasoning",
      "summary": "Large Reasoning Models (LRMs) have introduced a new paradigm in AI by\nenabling models to ``think before responding\" via chain-of-thought reasoning.\nHowever, the absence of open and reproducible recipes for building\nreasoning-centric medical LMMs hinders community-wide research, analysis, and\ncomparison. In this paper, we present MedVLThinker, a suite of simple yet\nstrong baselines. Our fully open recipe consists of: (1) systematic data\ncuration for both text-only and image-text medical data, filtered according to\nvarying levels of reasoning difficulty, and (2) two training paradigms:\nSupervised Fine-Tuning (SFT) on distilled reasoning traces and Reinforcement\nLearning with Verifiable Rewards (RLVR) based on final answer correctness.\nAcross extensive experiments on the Qwen2.5-VL model family (3B, 7B) and six\nmedical QA benchmarks, we find that RLVR consistently and significantly\noutperforms SFT. Additionally, under the RLVR framework, a key,\ncounter-intuitive finding is that training on our curated text-only reasoning\ndata provides a more substantial performance boost than training on multimodal\nimage-text data. Our best open 7B model, trained using the RLVR recipe on\ntext-only data, establishes a new state-of-the-art on existing public VQA\nbenchmarks, surpassing all previous open-source medical LMMs. Furthermore,\nscaling our model to 32B achieves performance on par with the proprietary\nGPT-4o. We release all curated data, models, and code to provide the community\nwith a strong, open foundation for future research in multimodal medical\nreasoning.",
      "url": "http://arxiv.org/abs/2508.02669v1",
      "published_time_eastern_timestamp": 1754330378.0
    },
    {
      "title": "LOST: Low-rank and Sparse Pre-training for Large Language Models",
      "summary": "While large language models (LLMs) have achieved remarkable performance\nacross a wide range of tasks, their massive scale incurs prohibitive\ncomputational and memory costs for pre-training from scratch. Recent studies\nhave investigated the use of low-rank parameterization as a means of reducing\nmodel size and training cost. In this context, sparsity is often employed as a\ncomplementary technique to recover important information lost in low-rank\ncompression by capturing salient features in the residual space. However,\nexisting approaches typically combine low-rank and sparse components in a\nsimplistic or ad hoc manner, often resulting in undesirable performance\ndegradation compared to full-rank training. In this paper, we propose\n\\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for\nLLMs, a novel method that ingeniously integrates low-rank and sparse structures\nto enable effective training of LLMs from scratch under strict efficiency\nconstraints. LOST applies singular value decomposition to weight matrices,\npreserving the dominant low-rank components, while allocating the remaining\nsingular values to construct channel-wise sparse components to complement the\nexpressiveness of low-rank training. We evaluate LOST on LLM pretraining\nranging from 60M to 7B parameters. Our experiments show that LOST achieves\ncompetitive or superior performance compared to full-rank models, while\nsignificantly reducing both memory and compute overhead. Moreover, Code is\navailable at\n\\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST\nRepo}",
      "url": "http://arxiv.org/abs/2508.02668v1",
      "published_time_eastern_timestamp": 1754330302.0
    },
    {
      "title": "Testing Dark Matter with Generative Models for Extragalactic Stellar\n  Streams",
      "summary": "Upcoming ground and space-based surveys are poised to illuminate low surface\nbrightness tidal features, providing a new observable connection to dark matter\nphysics. From imaging of tidal debris, the morphology of stellar streams can be\nused to infer the geometry of dark matter halos. In this paper, we develop a\ngenerative approach, X-Stream, which translates stream imaging into constraints\non the radial density profile of dark matter halos--from the inner region out\nto the virial radius. Using the GPU-accelerated code streamsculptor, we\ngenerate thousands of stream realizations in trial gravitational potentials and\napply nested sampling with a custom objective function to explore viable\nregions of parameter space. We find that multiple stellar streams can be used\nto constrain the entire radial density profile of a halo, including both its\ninner and outer density slopes. These constraints provide a test for\nalternatives to cold dark matter, such as self-interacting dark matter, which\npredicts cored density profiles. From cosmological simulations, the outer\ndensity slope is expected to correlate with merger histories though remains\nunderexplored observationally. With ongoing and upcoming missions such as\nEuclid, the Rubin Observatory, ARRAKIHS, and the Nancy Grace Roman Space\nTelescope, X-Stream will enable detailed mapping of dark matter for thousands\nof galaxies across a wide range of redshifts and halo masses.",
      "url": "http://arxiv.org/abs/2508.02666v1",
      "published_time_eastern_timestamp": 1754330149.0
    },
    {
      "title": "CAK: Emergent Audio Effects from Minimal Deep Learning",
      "summary": "We demonstrate that a single 3x3 convolutional kernel can produce emergent\naudio effects when trained on 200 samples from a personalized corpus. We\nachieve this through two key techniques: (1) Conditioning Aware Kernels (CAK),\nwhere output = input + (learned_pattern x control), with a soft-gate mechanism\nsupporting identity preservation at zero control; and (2) AuGAN (Audit GAN),\nwhich reframes adversarial training from \"is this real?\" to \"did you apply the\nrequested value?\" Rather than learning to generate or detect forgeries, our\nnetworks cooperate to verify control application, discovering unique\ntransformations. The learned kernel exhibits a diagonal structure creating\nfrequency-dependent temporal shifts that are capable of producing musical\neffects based on input characteristics. Our results show the potential of\nadversarial training to discover audio transformations from minimal data,\nenabling new approaches to effect design.",
      "url": "http://arxiv.org/abs/2508.02643v1",
      "published_time_eastern_timestamp": 1754328816.0
    },
    {
      "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents",
      "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
      "url": "http://arxiv.org/abs/2508.02629v1",
      "published_time_eastern_timestamp": 1754327894.0
    },
    {
      "title": "Boosting the Efficiency of the Differential Algebra-based Fast Multipole\n  Method Using Symbolic Differential Algebra",
      "summary": "The Fast Multipole Method (FMM) computes pairwise interactions between\nparticles with an efficiency that scales linearly with the number of particles.\nThe method works by grouping particles based on their spatial distribution and\napproximating interactions with distant regions through series expansions.\nDifferential Algebra (DA), also known as Truncated Power Series Algebra (TPSA),\ncomputes the Taylor expansion of a function at a given point and allows users\nto manipulate Taylor expansions as easily as numerical values in computation.\nThis makes it a convenient and powerful tool for constructing expansions in\nFMM. However, DA-based FMM operators typically suffer from lower efficiency\ncompared to implementations based on other mathematical frameworks, such as\nCartesian tensors or spherical harmonics. To address this, we developed a C++\nlibrary for symbolic DA computation, enabling the derivation of explicit\nexpressions for DA-based FMM operators. These symbolic expressions are then\nused to generate highly optimized code that eliminates the redundant\ncomputations inherent in numerical DA packages. For individual FMM operators,\nthis approach achieves a speedup of 20- to 50-fold. We further evaluate the\nnumerical performance of the enhanced DA-FMM and benchmark it against two\nstate-of-the-art FMM implementations, pyfmmlib and the traceless Cartesian\ntensor-based FMM, for the Coulomb potential. For relative errors on the order\nof $10^{-7}$ or higher, the enhanced DA-FMM consistently outperforms both\nalternatives.",
      "url": "http://arxiv.org/abs/2508.02626v1",
      "published_time_eastern_timestamp": 1754327757.0
    },
    {
      "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous\n  Healthcare Research",
      "summary": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.",
      "url": "http://arxiv.org/abs/2508.02621v1",
      "published_time_eastern_timestamp": 1754327327.0
    },
    {
      "title": "Meta-RAG on Large Codebases Using Code Summarization",
      "summary": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.",
      "url": "http://arxiv.org/abs/2508.02611v1",
      "published_time_eastern_timestamp": 1754326870.0
    },
    {
      "title": "ReMoMask: Retrieval-Augmented Masked Motion Generation",
      "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic and semantically\naligned human motion sequences from natural language descriptions. However,\ncurrent approaches face dual challenges: Generative models (e.g., diffusion\nmodels) suffer from limited diversity, error accumulation, and physical\nimplausibility, while Retrieval-Augmented Generation (RAG) methods exhibit\ndiffusion inertia, partial-mode collapse, and asynchronous artifacts. To\naddress these limitations, we propose ReMoMask, a unified framework integrating\nthree key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples\nnegative sample scale from batch size via momentum queues, substantially\nimproving cross-modal retrieval precision; 2) A Semantic Spatio-temporal\nAttention mechanism enforces biomechanical constraints during part-level fusion\nto eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates\nminor unconditional generation to enhance generalization. Built upon MoMask's\nRVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal\nsteps. Extensive experiments on standard benchmarks demonstrate the\nstate-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%\nimprovement in FID scores on HumanML3D and KIT-ML, respectively, compared to\nthe previous SOTA method RAG-T2M. Code:\nhttps://github.com/AIGeeksGroup/ReMoMask. Website:\nhttps://aigeeksgroup.github.io/ReMoMask.",
      "url": "http://arxiv.org/abs/2508.02605v1",
      "published_time_eastern_timestamp": 1754326595.0
    },
    {
      "title": "The Length of Functional Batch and PIR Codes",
      "summary": "We consider the problem of computing the minimum length of functional batch\nand PIR codes of fixed dimension and for a fixed list size, over an arbitrary\nfinite field. We recover, generalize, and refine several results that were\npreviously obtained for binary codes. We present new upper and lower bounds for\nthe minimum length, and discuss the asymptotic behaviour of this parameter. We\nalso compute its value for several parameter sets. The paper also offers\ninsights into the \"correct\" list size to consider for the Functional Batch\nConjecture over non-binary finite fields, and establishes various supporting\nresults.",
      "url": "http://arxiv.org/abs/2508.02586v1",
      "published_time_eastern_timestamp": 1754325789.0
    },
    {
      "title": "Factorizable embeddings and the period of an irreducible sofic shift",
      "summary": "Generalizing a result of MacDonald we give necessary and sufficient\nconditions for an arbitrary subshift to embed into an irreducible sofic shift\nfactoring through a given cover by an irreducible subshift of finite type\n(SFT). We obtain also necessary and sufficient conditions for an arbitrary\nsubshift to embed into an irreducible sofic shift factoring through \\emph{some}\nsliding block code out of an irreducible SFT. We do that when the code is\nrequired to be surjective, and hence a factor code, and when it is required to\nbe injective or is allowed to be arbitrary. These results require concepts of\nthe period of an irreducible sofic shift as well as a concept of a $p$-periodic\nsubshift. Several equivalent formulations of the period are developed.",
      "url": "http://arxiv.org/abs/2508.02554v1",
      "published_time_eastern_timestamp": 1754323515.0
    },
    {
      "title": "Stakeholder Perspectives on Humanistic Implementation of Computer\n  Perception in Healthcare: A Qualitative Study",
      "summary": "Computer perception (CP) technologies (digital phenotyping, affective\ncomputing and related passive sensing approaches) offer unprecedented\nopportunities to personalize healthcare, but provoke concerns about privacy,\nbias and the erosion of empathic, relationship-centered practice. A\ncomprehensive understanding of perceived risks, benefits, and implementation\nchallenges from those who design, deploy and experience these tools in\nreal-world settings remains elusive. This study provides the first\nevidence-based account of key stakeholder perspectives on the relational,\ntechnical, and governance challenges raised by the integration of CP\ntechnologies into patient care. We conducted in-depth, semi-structured\ninterviews with 102 stakeholders: adolescent patients and their caregivers,\nfrontline clinicians, technology developers, and ethics, legal, policy or\nphilosophy scholars. Transcripts underwent thematic analysis by a\nmultidisciplinary team; reliability was enhanced through double coding and\nconsensus adjudication. Stakeholders articulated seven interlocking concern\ndomains: (1) trustworthiness and data integrity; (2) patient-specific\nrelevance; (3) utility and workflow integration; (4) regulation and governance;\n(5) privacy and data protection; (6) direct and indirect patient harms; and (7)\nphilosophical critiques of reductionism. To operationalize humanistic\nsafeguards, we propose \"personalized roadmaps\": co-designed plans that\npredetermine which metrics will be monitored, how and when feedback is shared,\nthresholds for clinical action, and procedures for reconciling discrepancies\nbetween algorithmic inferences and lived experience. By translating these\ninsights into personalized roadmaps, we offer a practical framework for\ndevelopers, clinicians and policymakers seeking to harness continuous\nbehavioral data while preserving the humanistic core of care.",
      "url": "http://arxiv.org/abs/2508.02550v1",
      "published_time_eastern_timestamp": 1754323316.0
    },
    {
      "title": "Automatic Identification of Machine Learning-Specific Code Smells",
      "summary": "Machine learning (ML) has rapidly grown in popularity, becoming vital to many\nindustries. Currently, the research on code smells in ML applications lacks\ntools and studies that address the identification and validity of ML-specific\ncode smells. This work investigates suitable methods and tools to design and\ndevelop a static code analysis tool (MLpylint) based on code smell criteria.\nThis research employed the Design Science Methodology. In the problem\nidentification phase, a literature review was conducted to identify ML-specific\ncode smells. In solution design, a secondary literature review and\nconsultations with experts were performed to select methods and tools for\nimplementing the tool. We evaluated the tool on data from 160 open-source ML\napplications sourced from GitHub. We also conducted a static validation through\nan expert survey involving 15 ML professionals. The results indicate the\neffectiveness and usefulness of the MLpylint. We aim to extend our current\napproach by investigating ways to introduce MLpylint seamlessly into\ndevelopment workflows, fostering a more productive and innovative developer\nenvironment.",
      "url": "http://arxiv.org/abs/2508.02541v1",
      "published_time_eastern_timestamp": 1754322675.0
    },
    {
      "title": "Hubness Reduction with Dual Bank Sinkhorn Normalization for Cross-Modal\n  Retrieval",
      "summary": "The past decade has witnessed rapid advancements in cross-modal retrieval,\nwith significant progress made in accurately measuring the similarity between\ncross-modal pairs. However, the persistent hubness problem, a phenomenon where\na small number of targets frequently appear as nearest neighbors to numerous\nqueries, continues to hinder the precision of similarity measurements. Despite\nseveral proposed methods to reduce hubness, their underlying mechanisms remain\npoorly understood. To bridge this gap, we analyze the widely-adopted Inverted\nSoftmax approach and demonstrate its effectiveness in balancing target\nprobabilities during retrieval. Building on these insights, we propose a\nprobability-balancing framework for more effective hubness reduction. We\ncontend that balancing target probabilities alone is inadequate and, therefore,\nextend the framework to balance both query and target probabilities by\nintroducing Sinkhorn Normalization (SN). Notably, we extend SN to scenarios\nwhere the true query distribution is unknown, showing that current methods,\nwhich rely solely on a query bank to estimate target hubness, produce\nsuboptimal results due to a significant distributional gap between the query\nbank and targets. To mitigate this issue, we introduce Dual Bank Sinkhorn\nNormalization (DBSN), incorporating a corresponding target bank alongside the\nquery bank to narrow this distributional gap. Our comprehensive evaluation\nacross various cross-modal retrieval tasks, including image-text retrieval,\nvideo-text retrieval, and audio-text retrieval, demonstrates consistent\nperformance improvements, validating the effectiveness of both SN and DBSN. All\ncodes are publicly available at https://github.com/ppanzx/DBSN.",
      "url": "http://arxiv.org/abs/2508.02538v1",
      "published_time_eastern_timestamp": 1754322348.0
    },
    {
      "title": "Towards Reliable Audio Deepfake Attribution and Model Recognition: A\n  Multi-Level Autoencoder-Based Framework",
      "summary": "The proliferation of audio deepfakes poses a growing threat to trust in\ndigital communications. While detection methods have advanced, attributing\naudio deepfakes to their source models remains an underexplored yet crucial\nchallenge. In this paper we introduce LAVA (Layered Architecture for Voice\nAttribution), a hierarchical framework for audio deepfake detection and model\nrecognition that leverages attention-enhanced latent representations extracted\nby a convolutional autoencoder trained solely on fake audio. Two specialized\nclassifiers operate on these features: Audio Deepfake Attribution (ADA), which\nidentifies the generation technology, and Audio Deepfake Model Recognition\n(ADMR), which recognize the specific generative model instance. To improve\nrobustness under open-set conditions, we incorporate confidence-based rejection\nthresholds. Experiments on ASVspoof2021, FakeOrReal, and CodecFake show strong\nperformance: the ADA classifier achieves F1-scores over 95% across all\ndatasets, and the ADMR module reaches 96.31% macro F1 across six classes.\nAdditional tests on unseen attacks from ASVpoof2019 LA and error propagation\nanalysis confirm LAVA's robustness and reliability. The framework advances the\nfield by introducing a supervised approach to deepfake attribution and model\nrecognition under open-set conditions, validated on public benchmarks and\naccompanied by publicly released models and code. Models and code are available\nat https://www.github.com/adipiz99/lava-framework.",
      "url": "http://arxiv.org/abs/2508.02521v1",
      "published_time_eastern_timestamp": 1754321473.0
    },
    {
      "title": "Engagement Prediction of Short Videos with Large Multimodal Models",
      "summary": "The rapid proliferation of user-generated content (UGC) on short-form video\nplatforms has made video engagement prediction increasingly important for\noptimizing recommendation systems and guiding content creation. However, this\ntask remains challenging due to the complex interplay of factors such as\nsemantic content, visual quality, audio characteristics, and user background.\nPrior studies have leveraged various types of features from different\nmodalities, such as visual quality, semantic content, background sound, etc.,\nbut often struggle to effectively model their cross-feature and cross-modality\ninteractions. In this work, we empirically investigate the potential of large\nmultimodal models (LMMs) for video engagement prediction. We adopt two\nrepresentative LMMs: VideoLLaMA2, which integrates audio, visual, and language\nmodalities, and Qwen2.5-VL, which models only visual and language modalities.\nSpecifically, VideoLLaMA2 jointly processes key video frames, text-based\nmetadata, and background sound, while Qwen2.5-VL utilizes only key video frames\nand text-based metadata. Trained on the SnapUGC dataset, both models\ndemonstrate competitive performance against state-of-the-art baselines,\nshowcasing the effectiveness of LMMs in engagement prediction. Notably,\nVideoLLaMA2 consistently outperforms Qwen2.5-VL, highlighting the importance of\naudio features in engagement prediction. By ensembling two types of models, our\nmethod achieves first place in the ICCV VQualA 2025 EVQA-SnapUGC Challenge on\nshort-form video engagement prediction. The code is available at\nhttps://github.com/sunwei925/LMM-EVQA.git.",
      "url": "http://arxiv.org/abs/2508.02516v1",
      "published_time_eastern_timestamp": 1754320889.0
    },
    {
      "title": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots",
      "summary": "Panoramic cameras, capturing comprehensive 360-degree environmental data, are\nsuitable for quadruped robots in surrounding perception and interaction with\ncomplex environments. However, the scarcity of high-quality panoramic training\ndata-caused by inherent kinematic constraints and complex sensor calibration\nchallenges-fundamentally limits the development of robust perception systems\ntailored to these embodied platforms. To address this issue, we propose\nQuaDreamer-the first panoramic data generation engine specifically designed for\nquadruped robots. QuaDreamer focuses on mimicking the motion paradigm of\nquadruped robots to generate highly controllable, realistic panoramic videos,\nproviding a data source for downstream tasks. Specifically, to effectively\ncapture the unique vertical vibration characteristics exhibited during\nquadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts\ncontrollable vertical signals through frequency-domain feature filtering and\nprovides high-quality prompts. To facilitate high-quality panoramic video\ngeneration under jitter signal control, we propose a Scene-Object Controller\n(SOC) that effectively manages object motion and boosts background jitter\ncontrol through the attention mechanism. To address panoramic distortions in\nwide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream\narchitecture that synergizes frequency-texture refinement for local detail\nenhancement with spatial-structure correction for global geometric consistency.\nWe further demonstrate that the generated video sequences can serve as training\ndata for the quadruped robot's panoramic visual perception model, enhancing the\nperformance of multi-object tracking in 360-degree scenes. The source code and\nmodel weights will be publicly available at\nhttps://github.com/losehu/QuaDreamer.",
      "url": "http://arxiv.org/abs/2508.02512v1",
      "published_time_eastern_timestamp": 1754320681.0
    },
    {
      "title": "Rethinking Transparent Object Grasping: Depth Completion with Monocular\n  Depth Estimation and Instance Mask",
      "summary": "Due to the optical properties, transparent objects often lead depth cameras\nto generate incomplete or invalid depth data, which in turn reduces the\naccuracy and reliability of robotic grasping. Existing approaches typically\ninput the RGB-D image directly into the network to output the complete depth,\nexpecting the model to implicitly infer the reliability of depth values.\nHowever, while effective in training datasets, such methods often fail to\ngeneralize to real-world scenarios, where complex light interactions lead to\nhighly variable distributions of valid and invalid depth data. To address this,\nwe propose ReMake, a novel depth completion framework guided by an instance\nmask and monocular depth estimation. By explicitly distinguishing transparent\nregions from non-transparent ones, the mask enables the model to concentrate on\nlearning accurate depth estimation in these areas from RGB-D input during\ntraining. This targeted supervision reduces reliance on implicit reasoning and\nimproves generalization to real-world scenarios. Additionally, monocular depth\nestimation provides depth context between the transparent object and its\nsurroundings, enhancing depth prediction accuracy. Extensive experiments show\nthat our method outperforms existing approaches on both benchmark datasets and\nreal-world scenarios, demonstrating superior accuracy and generalization\ncapability. Code and videos are available at\nhttps://chengyaofeng.github.io/ReMake.github.io/.",
      "url": "http://arxiv.org/abs/2508.02507v1",
      "published_time_eastern_timestamp": 1754320487.0
    }
  ]
}
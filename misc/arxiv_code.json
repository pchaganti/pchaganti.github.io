{
  "last_updated": "2025-05-26T17:10:23.116569-04:00",
  "papers": [
    {
      "title": "REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders",
      "summary": "We introduce the Region Encoder Network (REN), a fast and effective model for\ngenerating region-based image representations using point prompts. Recent\nmethods combine class-agnostic segmenters (e.g., SAM) with patch-based image\nencoders (e.g., DINO) to produce compact and effective region representations,\nbut they suffer from high computational cost due to the segmentation step. REN\nbypasses this bottleneck using a lightweight module that directly generates\nregion tokens, enabling 60x faster token generation with 35x less memory, while\nalso improving token quality. It uses a few cross-attention blocks that take\npoint prompts as queries and features from a patch-based image encoder as keys\nand values to produce region tokens that correspond to the prompted objects. We\ntrain REN with three popular encoders-DINO, DINOv2, and OpenCLIP-and show that\nit can be extended to other encoders without dedicated training. We evaluate\nREN on semantic segmentation and retrieval tasks, where it consistently\noutperforms the original encoders in both performance and compactness, and\nmatches or exceeds SAM-based region methods while being significantly faster.\nNotably, REN achieves state-of-the-art results on the challenging Ego4D VQ2D\nbenchmark and outperforms proprietary LMMs on Visual Haystacks' single-needle\nchallenge. Code and models are available at: https://github.com/savya08/REN.",
      "url": "http://arxiv.org/abs/2505.18153v1",
      "published_time_eastern_timestamp": 1748023173.0
    },
    {
      "title": "Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry\n  Understanding in LLMs",
      "summary": "Arabic poetry stands as one of the most sophisticated and culturally embedded\nforms of expression in the Arabic language, known for its layered meanings,\nstylistic diversity, and deep historical continuity. Although large language\nmodels (LLMs) have demonstrated strong performance across languages and tasks,\ntheir ability to understand Arabic poetry remains largely unexplored. In this\nwork, we introduce `Fann or Flop`, the first benchmark designed to assess the\ncomprehension of Arabic poetry by LLMs in twelve historical eras, covering 21\ncore poetic genres and a variety of metrical forms, from classical structures\nto contemporary free verse. The benchmark comprises a curated corpus of poems\nwith explanations that assess semantic understanding, metaphor interpretation,\nprosodic awareness, and cultural context. We argue that poetic comprehension\noffers a strong indicator for testing how good the LLM is in understanding\nclassical Arabic through the Arabic poetry. Unlike surface-level tasks, this\ndomain demands deeper interpretive reasoning and cultural sensitivity. Our\nevaluation of state-of-the-art LLMs shows that most models struggle with poetic\nunderstanding despite strong results on standard Arabic benchmarks. We release\n`Fann or Flop` along with the evaluation suite as an open-source resource to\nenable rigorous evaluation and advancement for Arabic language models. Code is\navailable at: https://github.com/mbzuai-oryx/FannOrFlop.",
      "url": "http://arxiv.org/abs/2505.18152v1",
      "published_time_eastern_timestamp": 1748023169.0
    },
    {
      "title": "WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions",
      "summary": "WonderPlay is a novel framework integrating physics simulation with video\ngeneration for generating action-conditioned dynamic 3D scenes from a single\nimage. While prior works are restricted to rigid body or simple elastic\ndynamics, WonderPlay features a hybrid generative simulator to synthesize a\nwide range of 3D dynamics. The hybrid generative simulator first uses a physics\nsolver to simulate coarse 3D dynamics, which subsequently conditions a video\ngenerator to produce a video with finer, more realistic motion. The generated\nvideo is then used to update the simulated dynamic 3D scene, closing the loop\nbetween the physics solver and the video generator. This approach enables\nintuitive user control to be combined with the accurate dynamics of\nphysics-based simulators and the expressivity of diffusion-based video\ngenerators. Experimental results demonstrate that WonderPlay enables users to\ninteract with various scenes of diverse content, including cloth, sand, snow,\nliquid, smoke, elastic, and rigid bodies -- all using a single image input.\nCode will be made public. Project website:\nhttps://kyleleey.github.io/WonderPlay/",
      "url": "http://arxiv.org/abs/2505.18151v1",
      "published_time_eastern_timestamp": 1748023164.0
    },
    {
      "title": "Stochastic agent-based Monte Carlo simulations for reaction-diffusion\n  models, population dynamics, and epidemic spreading",
      "summary": "We provide a succinct overview of the implementation of Monte Carlo\nalgorithms based on Markovian stochastic dynamics to study interacting and\nreacting many-particle systems away from thermal equilibrium. Such agent-based\ncomputer simulations constitute an effective tool to introduce undergraduate\nand beginning graduate students to current frontier research without requiring\nmuch prior knowledge or experience: Starting from direct visualization of\nsimulation data, students may gain immediate insight into emerging macroscopic\nfeatures of a complex model system and subsequently apply more sophisticated\ndata analysis to quantitatively characterize its often rich dynamical\nproperties, both in stationary and transient regimes. We utilize numerical\ninvestigations of paradigmatic reaction-diffusion systems, as well as\nstochastic models for population dynamics and epidemic spreading, to exemplify\nhow interdisciplinary computational research can be effectively utilized in\nbottom-up undergraduate and graduate education through learning by doing. In\naddition, we give helpful hints for the practical setup of Monte Carlo\nsimulation algorithms, provide sample codes, explain some typical data analysis\ntools, and describe various potential error sources and pitfalls, with tips for\navoiding them.",
      "url": "http://arxiv.org/abs/2505.18145v1",
      "published_time_eastern_timestamp": 1748022952.0
    },
    {
      "title": "Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism\n  Detection",
      "summary": "We introduce a next-generation vandalism detection system for Wikidata, one\nof the largest open-source structured knowledge bases on the Web. Wikidata is\nhighly complex: its items incorporate an ever-expanding universe of factual\ntriples and multilingual texts. While edits can alter both structured and\ntextual content, our approach converts all edits into a single space using a\nmethod we call Graph2Text. This allows for evaluating all content changes for\npotential vandalism using a single multilingual language model. This unified\napproach improves coverage and simplifies maintenance. Experiments demonstrate\nthat our solution outperforms the current production system. Additionally, we\nare releasing the code under an open license along with a large dataset of\nvarious human-generated knowledge alterations, enabling further research.",
      "url": "http://arxiv.org/abs/2505.18136v1",
      "published_time_eastern_timestamp": 1748022246.0
    },
    {
      "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
      "summary": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.",
      "url": "http://arxiv.org/abs/2505.18134v1",
      "published_time_eastern_timestamp": 1748022207.0
    },
    {
      "title": "Joint Encryption and Error Correction for Secure Quantum Communication",
      "summary": "Secure quantum networks are a bedrock requirement for developing a future\nquantum internet. However, quantum channels are susceptible to channel noise\nthat introduce errors in the transmitted data. The traditional approach to\nproviding error correction typically encapsulates the message in an error\ncorrection code after encryption. Such separate processes incur overhead that\nmust be avoided when possible. We, consequently, provide a single integrated\nprocess that allows for encryption as well as error correction. This is a first\nattempt to do so for secure quantum communication and combines the\nCalderbank-Shor-Steane (CSS) code with the three-stage secure quantum\ncommunication protocol. Lastly, it allows for arbitrary qubits to be\ntransmitted from sender to receiver making the proposed protocol general\npurpose.",
      "url": "http://arxiv.org/abs/2505.18133v1",
      "published_time_eastern_timestamp": 1748022123.0
    },
    {
      "title": "BiggerGait: Unlocking Gait Recognition with Layer-wise Representations\n  from Large Vision Models",
      "summary": "Large vision models (LVM) based gait recognition has achieved impressive\nperformance. However, existing LVM-based approaches may overemphasize gait\npriors while neglecting the intrinsic value of LVM itself, particularly the\nrich, distinct representations across its multi-layers. To adequately unlock\nLVM's potential, this work investigates the impact of layer-wise\nrepresentations on downstream recognition tasks. Our analysis reveals that\nLVM's intermediate layers offer complementary properties across tasks,\nintegrating them yields an impressive improvement even without rich\nwell-designed gait priors. Building on this insight, we propose a simple and\nuniversal baseline for LVM-based gait recognition, termed BiggerGait.\nComprehensive evaluations on CCPG, CAISA-B*, SUSTech1K, and CCGR\\_MINI validate\nthe superiority of BiggerGait across both within- and cross-domain tasks,\nestablishing it as a simple yet practical baseline for gait representation\nlearning. All the models and code will be publicly available.",
      "url": "http://arxiv.org/abs/2505.18132v1",
      "published_time_eastern_timestamp": 1748022114.0
    },
    {
      "title": "ProgRM: Build Better GUI Agents with Progress Rewards",
      "summary": "LLM-based (Large Language Model) GUI (Graphical User Interface) agents can\npotentially reshape our daily lives significantly. However, current LLM-based\nGUI agents suffer from the scarcity of high-quality training data owing to the\ndifficulties of trajectory collection and reward annotation. Existing works\nhave been exploring LLMs to collect trajectories for imitation learning or to\noffer reward signals for online RL training. However, the Outcome Reward Model\n(ORM) used in existing works cannot provide finegrained feedback and can\nover-penalize the valuable steps in finally failed trajectories. To this end,\nwe propose Progress Reward Model (ProgRM) to provide dense informative\nintermediate rewards by predicting a task completion progress for each step in\nonline training. To handle the challenge of progress reward label annotation,\nwe further design an efficient LCS-based (Longest Common Subsequence)\nself-annotation algorithm to discover the key steps in trajectories and assign\nprogress labels accordingly. ProgRM is evaluated with extensive experiments and\nanalyses. Actors trained with ProgRM outperform leading proprietary LLMs and\nORM-trained actors, illustrating the effectiveness of ProgRM. The codes for\nexperiments will be made publicly available upon acceptance.",
      "url": "http://arxiv.org/abs/2505.18121v1",
      "published_time_eastern_timestamp": 1748020991.0
    },
    {
      "title": "Instructify: Demystifying Metadata to Visual Instruction Tuning Data\n  Conversion",
      "summary": "Visual Instruction Tuning (VisIT) data, commonly available as human-assistant\nconversations with images interleaved in the human turns, are currently the\nmost widespread vehicle for aligning strong LLMs to understand visual inputs,\nconverting them to strong LMMs. While many VisIT datasets are available, most\nare constructed using ad-hoc techniques developed independently by different\ngroups. They are often poorly documented, lack reproducible code, and rely on\npaid, closed-source model APIs such as GPT-4, Gemini, or Claude to convert\nimage metadata (labels) into VisIT instructions. This leads to high costs and\nmakes it challenging to scale, enhance quality, or generate VisIT data for new\ndatasets. In this work, we address these challenges and propose an open and\nunified recipe and approach,~\\textbf{\\method}, for converting available\nmetadata to VisIT instructions using open LLMs. Our multi-stage \\method\nfeatures an efficient framework for metadata grouping, quality control, data\nand prompt organization, and conversation sampling. We show that our approach\ncan reproduce or enhance the data quality of available VisIT datasets when\napplied to the same image data and metadata sources, improving GPT-4 generated\nVisIT instructions by ~3\\% on average and up to 12\\% on individual benchmarks\nusing open models, such as Gemma 2 27B and LLaMa 3.1 70B. Additionally, our\napproach enables effective performance scaling - both in quantity and quality -\nby enhancing the resulting LMM performance across a wide range of benchmarks.\nWe also analyze the impact of various factors, including conversation format,\nbase model selection, and resampling strategies. Our code, which supports the\nreproduction of equal or higher-quality VisIT datasets and facilities future\nmetadata-to-VisIT data conversion for niche domains, is released at\nhttps://github.com/jacob-hansen/Instructify.",
      "url": "http://arxiv.org/abs/2505.18115v1",
      "published_time_eastern_timestamp": 1748020452.0
    },
    {
      "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with\n  Multimodal LLM",
      "summary": "Humans naturally understand moments in a video by integrating visual and\nauditory cues. For example, localizing a scene in the video like \"A scientist\npassionately speaks on wildlife conservation as dramatic orchestral music\nplays, with the audience nodding and applauding\" requires simultaneous\nprocessing of visual, audio, and speech signals. However, existing models often\nstruggle to effectively fuse and interpret audio information, limiting their\ncapacity for comprehensive video temporal understanding. To address this, we\npresent TriSense, a triple-modality large language model designed for holistic\nvideo temporal understanding through the integration of visual, audio, and\nspeech modalities. Central to TriSense is a Query-Based Connector that\nadaptively reweights modality contributions based on the input query, enabling\nrobust performance under modality dropout and allowing flexible combinations of\navailable inputs. To support TriSense's multimodal capabilities, we introduce\nTriSense-2M, a high-quality dataset of over 2 million curated samples generated\nvia an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes\nlong-form videos and diverse modality combinations, facilitating broad\ngeneralization. Extensive experiments across multiple benchmarks demonstrate\nthe effectiveness of TriSense and its potential to advance multimodal video\nanalysis. Code and dataset will be publicly released.",
      "url": "http://arxiv.org/abs/2505.18110v1",
      "published_time_eastern_timestamp": 1748019867.0
    },
    {
      "title": "ManuSearch: Democratizing Deep Search in Large Language Models with a\n  Transparent and Open Multi-Agent Framework",
      "summary": "Recent advances in web-augmented large language models (LLMs) have exhibited\nstrong performance in complex reasoning tasks, yet these capabilities are\nmostly locked in proprietary systems with opaque architectures. In this work,\nwe propose \\textbf{ManuSearch}, a transparent and modular multi-agent framework\ndesigned to democratize deep search for LLMs. ManuSearch decomposes the search\nand reasoning process into three collaborative agents: (1) a solution planning\nagent that iteratively formulates sub-queries, (2) an Internet search agent\nthat retrieves relevant documents via real-time web search, and (3) a\nstructured webpage reading agent that extracts key evidence from raw web\ncontent. To rigorously evaluate deep reasoning abilities, we introduce\n\\textbf{ORION}, a challenging benchmark focused on open-web reasoning over\nlong-tail entities, covering both English and Chinese. Experimental results\nshow that ManuSearch substantially outperforms prior open-source baselines and\neven surpasses leading closed-source systems. Our work paves the way for\nreproducible, extensible research in open deep search systems. We release the\ndata and code in https://github.com/RUCAIBox/ManuSearch",
      "url": "http://arxiv.org/abs/2505.18105v1",
      "published_time_eastern_timestamp": 1748019722.0
    },
    {
      "title": "Early-Exit Graph Neural Networks",
      "summary": "Early-exit mechanisms allow deep neural networks to halt inference as soon as\nclassification confidence is high enough, adaptively trading depth for\nconfidence, and thereby cutting latency and energy on easy inputs while\nretaining full-depth accuracy for harder ones. Similarly, adding early exit\nmechanisms to Graph Neural Networks (GNNs), the go-to models for\ngraph-structured data, allows for dynamic trading depth for confidence on\nsimple graphs while maintaining full-depth accuracy on harder and more complex\ngraphs to capture intricate relationships. Although early exits have proven\neffective across various deep learning domains, their potential within GNNs in\nscenarios that require deep architectures while resisting over-smoothing and\nover-squashing remains largely unexplored. We unlock that potential by first\nintroducing Symmetric-Anti-Symmetric Graph Neural Networks (SAS-GNN), whose\nsymmetry-based inductive biases mitigate these issues and yield stable\nintermediate representations that can be useful to allow early exiting in GNNs.\nBuilding on this backbone, we present Early-Exit Graph Neural Networks\n(EEGNNs), which append confidence-aware exit heads that allow on-the-fly\ntermination of propagation based on each node or the entire graph. Experiments\nshow that EEGNNs preserve robust performance as depth grows and deliver\ncompetitive accuracy on heterophilic and long-range benchmarks, matching\nattention-based and asynchronous message-passing models while substantially\nreducing computation and latency. We plan to release the code to reproduce our\nexperiments.",
      "url": "http://arxiv.org/abs/2505.18088v1",
      "published_time_eastern_timestamp": 1748018714.0
    },
    {
      "title": "CXReasonBench: A Benchmark for Evaluating Structured Diagnostic\n  Reasoning in Chest X-rays",
      "summary": "Recent progress in Large Vision-Language Models (LVLMs) has enabled promising\napplications in medical tasks, such as report generation and visual question\nanswering. However, existing benchmarks focus mainly on the final diagnostic\nanswer, offering limited insight into whether models engage in clinically\nmeaningful reasoning. To address this, we present CheXStruct and CXReasonBench,\na structured pipeline and benchmark built on the publicly available\nMIMIC-CXR-JPG dataset. CheXStruct automatically derives a sequence of\nintermediate reasoning steps directly from chest X-rays, such as segmenting\nanatomical regions, deriving anatomical landmarks and diagnostic measurements,\ncomputing diagnostic indices, and applying clinical thresholds. CXReasonBench\nleverages this pipeline to evaluate whether models can perform clinically valid\nreasoning steps and to what extent they can learn from structured guidance,\nenabling fine-grained and transparent assessment of diagnostic reasoning. The\nbenchmark comprises 18,988 QA pairs across 12 diagnostic tasks and 1,200 cases,\neach paired with up to 4 visual inputs, and supports multi-path, multi-stage\nevaluation including visual grounding via anatomical region selection and\ndiagnostic measurements. Even the strongest of 10 evaluated LVLMs struggle with\nstructured reasoning and generalization, often failing to link abstract\nknowledge with anatomically grounded visual interpretation. The code is\navailable at https://github.com/ttumyche/CXReasonBench",
      "url": "http://arxiv.org/abs/2505.18087v1",
      "published_time_eastern_timestamp": 1748018661.0
    },
    {
      "title": "An Iterative Framework for Generative Backmapping of Coarse Grained\n  Proteins",
      "summary": "The techniques of data-driven backmapping from coarse-grained (CG) to\nfine-grained (FG) representation often struggle with accuracy, unstable\ntraining, and physical realism, especially when applied to complex systems such\nas proteins. In this work, we introduce a novel iterative framework by using\nconditional Variational Autoencoders and graph-based neural networks,\nspecifically designed to tackle the challenges associated with such large-scale\nbiomolecules. Our method enables stepwise refinement from CG beads to full\natomistic details. We outline the theory of iterative generative backmapping\nand demonstrate via numerical experiments the advantages of multistep schemes\nby applying them to proteins of vastly different structures with very coarse\nrepresentations. This multistep approach not only improves the accuracy of\nreconstructions but also makes the training process more computationally\nefficient for proteins with ultra-CG representations.",
      "url": "http://arxiv.org/abs/2505.18082v1",
      "published_time_eastern_timestamp": 1748018425.0
    },
    {
      "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
      "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.",
      "url": "http://arxiv.org/abs/2505.18079v1",
      "published_time_eastern_timestamp": 1748018256.0
    },
    {
      "title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video\n  Generation",
      "summary": "Controllable video generation (CVG) has advanced rapidly, yet current systems\nfalter when more than one actor must move, interact, and exchange positions\nunder noisy control signals. We address this gap with DanceTogether, the first\nend-to-end diffusion framework that turns a single reference image plus\nindependent pose-mask streams into long, photorealistic videos while strictly\npreserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at\nevery denoising step by fusing robust tracking masks with semantically rich-but\nnoisy-pose heat-maps, eliminating the identity drift and appearance bleeding\nthat plague frame-wise pipelines. To train and evaluate at scale, we introduce\n(i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii)\nHumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain\ntransfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the\nDanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure\nskating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a\nsignificant margin. Moreover, we show that a one-hour fine-tune yields\nconvincing human-robot videos, underscoring broad generalization to embodied-AI\nand HRI tasks. Extensive ablations confirm that persistent identity-action\nbinding is critical to these gains. Together, our model, datasets, and\nbenchmark lift CVG from single-subject choreography to compositionally\ncontrollable, multi-actor interaction, opening new avenues for digital\nproduction, simulation, and embodied intelligence. Our video demos and code are\navailable at https://DanceTog.github.io/.",
      "url": "http://arxiv.org/abs/2505.18078v1",
      "published_time_eastern_timestamp": 1748018234.0
    },
    {
      "title": "Extended Inductive Reasoning for Personalized Preference Inference from\n  Behavioral Signals",
      "summary": "Large language models (LLMs) have demonstrated significant success in complex\nreasoning tasks such as math and coding. In contrast to these tasks where\ndeductive reasoning predominates, inductive reasoning\\textemdash the ability to\nderive general rules from incomplete evidence, remains underexplored. This\npaper investigates extended inductive reasoning in LLMs through the lens of\npersonalized preference inference, a critical challenge in LLM alignment where\ncurrent approaches struggle to capture diverse user preferences. The task\ndemands strong inductive reasoning capabilities as user preferences are\ntypically embedded implicitly across various interaction forms, requiring\nmodels to synthesize consistent preference patterns from scattered signals. We\npropose \\textsc{AlignXplore}, a model that leverages extended reasoning chains\nto enable systematic preference inference from behavioral signals in users'\ninteraction histories. We develop \\textsc{AlignXplore} by combining cold-start\ntraining based on synthetic data with subsequent online reinforcement learning.\nThrough extensive experiments, we demonstrate that \\textsc{AlignXplore}\nachieves substantial improvements over the backbone model by an average of\n11.05\\% on in-domain and out-of-domain benchmarks, while maintaining strong\ngeneralization ability across different input formats and downstream models.\nFurther analyses establish best practices for preference inference learning\nthrough systematic comparison of reward modeling strategies, while revealing\nthe emergence of human-like inductive reasoning patterns during training.",
      "url": "http://arxiv.org/abs/2505.18071v1",
      "published_time_eastern_timestamp": 1748017006.0
    },
    {
      "title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline",
      "summary": "Establishing semantic correspondence is a challenging task in computer\nvision, aiming to match keypoints with the same semantic information across\ndifferent images. Benefiting from the rapid development of deep learning,\nremarkable progress has been made over the past decade. However, a\ncomprehensive review and analysis of this task remains absent. In this paper,\nwe present the first extensive survey of semantic correspondence methods. We\nfirst propose a taxonomy to classify existing methods based on the type of\ntheir method designs. These methods are then categorized accordingly, and we\nprovide a detailed analysis of each approach. Furthermore, we aggregate and\nsummarize the results of methods in literature across various benchmarks into a\nunified comparative table, with detailed configurations to highlight\nperformance variations. Additionally, to provide a detailed understanding on\nexisting methods for semantic matching, we thoroughly conduct controlled\nexperiments to analyse the effectiveness of the components of different\nmethods. Finally, we propose a simple yet effective baseline that achieves\nstate-of-the-art performance on multiple benchmarks, providing a solid\nfoundation for future research in this field. We hope this survey serves as a\ncomprehensive reference and consolidated baseline for future development. Code\nis publicly available at: https://github.com/Visual-AI/Semantic-Correspondence.",
      "url": "http://arxiv.org/abs/2505.18060v1",
      "published_time_eastern_timestamp": 1748016436.0
    },
    {
      "title": "Modelling multiwavelength afterglows of the VHE-GRB population",
      "summary": "The recent detection of very high energy (VHE, $>$ 100 GeV) $\\gamma$-ray\nemission from gamma-ray bursts (GRBs) has provided new insights into afterglow\nphysics. Understanding the temporal and spectral evolution of VHE GRBs requires\ndetailed modelling of multiwavelength observations spanning radio to VHE\n$\\gamma$ rays. Previous studies on afterglow emission of VHE GRBs were\ninterpreted using a range of frameworks, including single- and multi-zone jet\nconfigurations, synchrotron radiation from forward and reverse shocks,\nsynchrotron self-Compton (SSC) processes, as well as hadronic emission\nprocesses. We conducted a detailed multiwavelength modelling of five\nlong-duration VHE GRBs - GRB 180720B, GRB 190114C, GRB 190829A, GRB 201216C and\nGRB 221009A; using the NAIMA code and modifications to it. The code deals with\nsingle-zone synchrotron radiation, SSC and external Compton (EC) radiation with\ncomplete Klein Nishina cross-section, extragalactic background light\ncorrection, and the Markov chain Monte Carlo techniques. Our analysis\nconstrains key parameters governing the emission and surrounding environments\nof these GRBs. The results indicate that SSC is the dominant VHE emission\nmechanism, with negligible contribution from EC. Most VHE GRBs are well\ndescribed by the forward shock model in a spherical jet configuration, where\nconstant density interstellar medium is preferred over wind medium.\nAdditionally, we find that VHE GRBs tend to occur in environments with lower\nmagnetic fields and higher ambient medium densities. Interestingly, VHE GRBs\nlie at the edge of the 3-$\\sigma$ region of the $E_{\\rm k,iso}$ - $\\epsilon_B$\ncorrelation observed in other energetic GRBs. Our model slightly over predicts\nthe radio fluxes, indicating that a more complicated modelling might be\nrequired in some cases. These findings provide crucial constraints on VHE GRB\nemission sites and mechanisms (Abridged).",
      "url": "http://arxiv.org/abs/2505.18041v1",
      "published_time_eastern_timestamp": 1748015161.0
    }
  ]
}
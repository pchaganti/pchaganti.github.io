{
  "last_updated": "2026-02-09T23:57:32.661530-05:00",
  "papers": [
    {
      "title": "An Exploration of the Equation of State Dependence of Core-Collapse Supernova Explosion Outcomes and Signatures",
      "summary": "We explore, using a state-of-the-art simulation code in 3D and to late enough times to witness final observables, the dependence of core-collapse supernova explosions on the nuclear equation of state. Going beyond questions of explodability, we compare final explosion energies, nucleosynthetic yields, recoil kicks, and gravitational-wave and neutrino signatures using the SFHo and DD2 nuclear equations of state (EOS) for a 9-$M_{\\odot}$/solar-metallicity progenitor star. The DD2 EOS is stiffer and has a lower effective nucleon mass. The result is a more extended protoneutron star (PNS) and lower central densities. As a consequence, the mean neutrino energies, final explosion energy, and recoil kick speed are lower. Moreover, the evolution of PNS convection differs between the two EOS models in significant ways. This translates in part into interestingly altered neutrino ``light\" curves and noticeably altered gravitational-wave signal strengths and frequency characteristics that may be diagnostic. The faster exploding model (SFHo) yields slightly more neutron-rich ejecta and more species with atomic weights between 60 and 90 and a weak r-process. However, this is merely a preliminary study. The next step is a more comprehensive and multi-progenitor set of 3D supernova simulations for various EOSes to late times when the observables have asymptoted. Such a future investigation will have a direct bearing on the neutron star and black hole birth mass functions and the quest towards a fully quantitative theory of supernova observables.",
      "url": "http://arxiv.org/abs/2602.09025v1",
      "published_time_eastern_timestamp": 1770663599.0
    },
    {
      "title": "$χ_{0}$: Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies",
      "summary": "High-reliability long-horizon robotic manipulation has traditionally relied on large-scale data and compute to understand complex real-world dynamics. However, we identify that the primary bottleneck to real-world robustness is not resource scale alone, but the distributional shift among the human demonstration distribution, the inductive bias learned by the policy, and the test-time execution distribution -- a systematic inconsistency that causes compounding errors in multi-stage tasks. To mitigate these inconsistencies, we propose $χ_{0}$, a resource-efficient framework with effective modules designated to achieve production-level robustness in robotic manipulation. Our approach builds off three technical pillars: (i) Model Arithmetic, a weight-space merging strategy that efficiently soaks up diverse distributions of different demonstrations, varying from object appearance to state variations; (ii) Stage Advantage, a stage-aware advantage estimator that provides stable, dense progress signals, overcoming the numerical instability of prior non-stage approaches; and (iii) Train-Deploy Alignment, which bridges the distribution gap via spatio-temporal augmentation, heuristic DAgger corrections, and temporal chunk-wise smoothing. $χ_{0}$ enables two sets of dual-arm robots to collaboratively orchestrate long-horizon garment manipulation, spanning tasks from flattening, folding, to hanging different clothes. Our method exhibits high-reliability autonomy; we are able to run the system from arbitrary initial state for consecutive 24 hours non-stop. Experiments validate that $χ_{0}$ surpasses the state-of-the-art $π_{0.5}$ in success rate by nearly 250%, with only 20-hour data and 8 A100 GPUs. Code, data and models will be released to facilitate the community.",
      "url": "http://arxiv.org/abs/2602.09021v1",
      "published_time_eastern_timestamp": 1770663585.0
    },
    {
      "title": "Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction",
      "summary": "Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.",
      "url": "http://arxiv.org/abs/2602.09016v1",
      "published_time_eastern_timestamp": 1770663526.0
    },
    {
      "title": "CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection",
      "summary": "Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they can embed harmful content such as malware or malicious URLs inside standard document formats. Although phishing email defenses have improved a lot, attackers continue to abuse attachments, enabling malicious content to bypass security measures. Moreover, another challenge that researches face in training advance models, is lack of an unified and comprehensive dataset that covers the most prevalent data types. To address this gap, we generated CIC-Trap4Phish, a multi-format dataset containing both malicious and benign samples across five categories commonly used in phishing campaigns: Microsoft Word documents, Excel spreadsheets, PDF files, HTML pages, and QR code images. For the first four file types, a set of execution-free static feature pipeline was proposed, designed to capture structural, lexical, and metadata-based indicators without the need to open or execute files. Feature selection was performed using a combination of SHAP analysis and feature importance, yielding compact, discriminative feature subsets for each file type. The selected features were evaluated by using lightweight machine learning models, including Random Forest, XGBoost, and Decision Tree. All models demonstrate high detection accuracy across formats. For QR code-based phishing (quishing), two complementary methods were implemented: image-based detection by employing Convolutional Neural Networks (CNNs) and lexical analysis of decoded URLs using recent lightweight language models.",
      "url": "http://arxiv.org/abs/2602.09015v1",
      "published_time_eastern_timestamp": 1770663420.0
    },
    {
      "title": "Complete discrete Schoenberg-Delsarte theory for homogeneous spaces",
      "summary": "We develop a theory of partially defined complete positivity preservers, extending Schoenberg's classical characterization to functions defined only on discrete subsets or constrained domains. We frame the extension problem through the theory of completely positive maps on operator systems -- we characterize general partially defined completely positive definite functions on general homogeneous spaces. We apply our interpolation to constrained packing problems and Delsarte theory, where one uses positive definite functions on homogeneous spaces to obtain bounds on various packing problems. We prove the specific positive definite function witnesses that a code is sharp for constrained angle codes must be from polynomials.",
      "url": "http://arxiv.org/abs/2602.09010v1",
      "published_time_eastern_timestamp": 1770663305.0
    },
    {
      "title": "ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification",
      "summary": "Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discriminative motifs such as shapelets. In this work, we propose ShapeCond, a novel and efficient condensation framework for time series classification that leverages shapelet-based dataset knowledge via a shapelet-guided optimization strategy. Our shapelet-assisted synthesis cost is independent of sequence length: longer series yield larger speedups in synthesis (e.g., 29$\\times$ faster over prior state-of-the-art method CondTSC for time-series condensation, and up to 10,000$\\times$ over naively using shapelets on the Sleep dataset with 3,000 timesteps). By explicitly preserving critical local patterns, ShapeCond improves downstream accuracy and consistently outperforms all prior state-of-the-art time series dataset condensation methods across extensive experiments. Code is available at https://github.com/lunaaa95/ShapeCond.",
      "url": "http://arxiv.org/abs/2602.09008v1",
      "published_time_eastern_timestamp": 1770663188.0
    },
    {
      "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
      "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
      "url": "http://arxiv.org/abs/2602.09007v1",
      "published_time_eastern_timestamp": 1770663122.0
    },
    {
      "title": "CLUE: Crossmodal disambiguation via Language-vision Understanding with attEntion",
      "summary": "With the increasing integration of robots into daily life, human-robot interaction has become more complex and multifaceted. A critical component of this interaction is Interactive Visual Grounding (IVG), through which robots must interpret human intentions and resolve ambiguity. Existing IVG models generally lack a mechanism to determine when to ask clarification questions, as they implicitly rely on their learned representations. CLUE addresses this gap by converting the VLM's cross-modal attention into an explicit, spatially grounded signal for deciding when to ask. We extract text to image attention maps and pass them to a lightweight CNN to detect referential ambiguity, while a LoRA fine-tuned decoder conducts the dialog and emits grounding location tokens. We train on a real-world interactive dataset for IVG, and a mixed ambiguity set for the detector. With InViG-only supervision, our model surpasses a state-of-the-art method while using parameter-efficient fine-tuning. Similarly, the ambiguity detector outperforms prior baselines. Overall, CLUE turns the internal cross-modal attention of a VLM into an explicit, spatially grounded signal for deciding when to ask. The data and code are publicly available at: mouadabrini.github.io/clue",
      "url": "http://arxiv.org/abs/2602.08999v1",
      "published_time_eastern_timestamp": 1770662675.0
    },
    {
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "summary": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
      "url": "http://arxiv.org/abs/2602.08990v1",
      "published_time_eastern_timestamp": 1770662166.0
    },
    {
      "title": "W-SLDA Toolkit: A simulation platform for ultracold Fermi gases",
      "summary": "We present the W-SLDA Toolkit, a general-purpose software package for simulating ultracold Fermi gases within the framework of density functional theory and its time-dependent extensions. The toolkit enables fully microscopic studies of interacting superfluid systems across the BCS-BEC crossover, including spin-imbalanced configurations and arbitrary external geometries. It provides both static and time-dependent solvers capable of describing a broad range of phenomena in one-, two-, and three-dimensional settings. In addition, the toolkit incorporates functionality for solving the standard Bogoliubov-de Gennes equations for fermions, extending its applicability to other physical systems such as superconductors. The code is implemented in C with GPU acceleration and is optimized for hybrid CPU/GPU execution on modern high-performance computing platforms. It ensures scalability on leadership-class supercomputers, enabling fully three-dimensional simulations with large atomic numbers, and allows for direct benchmarks of ultracold-atom experimental setups. Its modular architecture facilitates straightforward extensions, user customization, and seamless interoperability with other scientific software frameworks. Furthermore, an extensive collection of practical usage examples is provided through the integrated reproducibility packs functionality, ensuring transparency and reproducibility of computational results.",
      "url": "http://arxiv.org/abs/2602.08982v1",
      "published_time_eastern_timestamp": 1770661730.0
    },
    {
      "title": "Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting",
      "summary": "Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D",
      "url": "http://arxiv.org/abs/2602.08962v1",
      "published_time_eastern_timestamp": 1770659933.0
    },
    {
      "title": "Clique-Based Deletion-Correcting Codes via Penalty-Guided Clique Search",
      "summary": "We study the construction of $d$-deletion-correcting binary codes by formulating the problem as a Maximum Clique Problem (MCP). In this formulation, vertices represent candidate codewords and edges connect pairs whose longest common subsequence (LCS) distance guarantees correction of up to $d$ deletions. A valid codebook corresponds to a clique in the resulting graph, and finding the largest codebook is equivalent to identifying a maximum clique. While MCP-based formulations for deletion-correcting codes have previously been explored, we demonstrate that applying Penalty-Guided Clique Search (PGCS), a lightweight stochastic clique-search heuristic inspired by Dynamic Local Search (DLS), consistently yields larger codebooks than existing graph-based heuristics, including minimum-degree and coloring methods, for block lengths $n = 8,9,\\dots,14$ and deletion parameters $d = 1,2,3$. In several finite-length regimes, the resulting codebooks match known optimal sizes and outperform classical constructions such as Helberg codes. For decoding under segmented reception, where codeword boundaries are known, we propose an optimized LCS-based decoder that exploits symbol-count filtering and early termination to substantially reduce the number of LCS evaluations while preserving exact decoding guarantees. These optimizations lead to significantly lower average-case decoding complexity than the baseline $O(|C| n^2)$ approach.",
      "url": "http://arxiv.org/abs/2602.08952v1",
      "published_time_eastern_timestamp": 1770659231.0
    },
    {
      "title": "StealthRL: Reinforcement Learning Paraphrase Attacks for Multi-Detector Evasion of AI-Text Detectors",
      "summary": "AI-text detectors face a critical robustness challenge: adversarial paraphrasing attacks that preserve semantics while evading detection. We introduce StealthRL, a reinforcement learning framework that stress-tests detector robustness under realistic adversarial conditions. StealthRL trains a paraphrase policy against a multi-detector ensemble using Group Relative Policy Optimization (GRPO) with LoRA adapters on Qwen3-4B, optimizing a composite reward that balances detector evasion with semantic preservation. We evaluate six attack settings (M0-M5) against three detector families (RoBERTa, FastDetectGPT, and Binoculars) at the security-relevant 1% false positive rate operating point. StealthRL achieves near-zero detection (0.001 mean TPR@1%FPR), reduces mean AUROC from 0.74 to 0.27, and attains a 99.9% attack success rate. Critically, attacks transfer to a held-out detector family not seen during training, revealing shared architectural vulnerabilities rather than detector-specific brittleness. We additionally conduct LLM-based quality evaluation via Likert scoring, analyze detector score distributions to explain why evasion succeeds, and provide per-detector AUROC with bootstrap confidence intervals. Our results expose significant robustness gaps in current AI-text detection and establish StealthRL as a principled adversarial evaluation protocol. Code and evaluation pipeline are publicly available at https://github.com/suraj-ranganath/StealthRL.",
      "url": "http://arxiv.org/abs/2602.08934v1",
      "published_time_eastern_timestamp": 1770658426.0
    },
    {
      "title": "Comparing AI Coding Agents: A Task-Stratified Analysis of Pull Request Acceptance",
      "summary": "The rapid adoption of AI-powered coding assistants is transforming software development practices, yet systematic comparisons of their effectiveness across different task types and over time remain limited. This paper presents an empirical study comparing five popular agents (OpenAI Codex, GitHub Copilot, Devin, Cursor, and Claude Code), analyzing 7,156 pull requests (PRs) from the AIDev dataset. Temporal trend analysis reveals heterogeneous evolution patterns: Devin exhibits the only consistent positive trend in acceptance rate (+0.77% per week over 32 weeks), whereas other agents remain largely stable. Our analysis suggests that the PR task type is a dominant factor influencing acceptance rates: documentation tasks achieve 82.1% acceptance compared to 66.1% for new features - a 16 percentage point gap that exceeds typical inter-agent variance for most tasks. OpenAI Codex achieves consistently high acceptance rates across all nine task categories (59.6%-88.6%), with stratified Chi-square tests confirming statistically significant advantages over other agents in several task categories. However, no single agent performs best across all task types: Claude Code leads in documentation (92.3%) and features (72.6%), while Cursor excels in fix tasks (80.4%).",
      "url": "http://arxiv.org/abs/2602.08915v1",
      "published_time_eastern_timestamp": 1770657286.0
    },
    {
      "title": "Efficient and Stable Reinforcement Learning for Diffusion Language Models",
      "summary": "Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \\textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \\textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.",
      "url": "http://arxiv.org/abs/2602.08905v1",
      "published_time_eastern_timestamp": 1770656663.0
    },
    {
      "title": "DeepQuali: Initial results of a study on the use of large language models for assessing the quality of user stories",
      "summary": "Generative artificial intelligence (GAI), specifically large language models (LLMs), are increasingly used in software engineering, mainly for coding tasks. However, requirements engineering - particularly requirements validation - has seen limited application of GAI. The current focus of using GAI for requirements is on eliciting, transforming, and classifying requirements, not on quality assessment. We propose and evaluate the LLM-based (GPT-4o) approach \"DeepQuali\", for assessing and improving requirements quality in agile software development. We applied it to projects in two small companies, where we compared LLM-based quality assessments with expert judgments. Experts also participated in walkthroughs of the solution, provided feedback, and rated their acceptance of the approach. Experts largely agreed with the LLM's quality assessments, especially regarding overall ratings and explanations. However, they did not always agree with the other experts on detailed ratings, suggesting that expertise and experience may influence judgments. Experts recognized the usefulness of the approach but criticized the lack of integration into their workflow. LLMs show potential in supporting software engineers with the quality assessment and improvement of requirements. The explicit use of quality models and explanatory feedback increases acceptance.",
      "url": "http://arxiv.org/abs/2602.08887v1",
      "published_time_eastern_timestamp": 1770655794.0
    },
    {
      "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression",
      "summary": "Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.",
      "url": "http://arxiv.org/abs/2602.08885v1",
      "published_time_eastern_timestamp": 1770655620.0
    },
    {
      "title": "Weak and reversed magnetic shear effects on internal kink and fishbone modes",
      "summary": "Advanced tokamak scenarios often feature weak or reversed magnetic shear configurations. In this study, the hybrid kinetic-MHD model implemented in the NIMROD code is used to investigate the effects of reversed magnetic shear on internal kink and fishbone mode in a circular shaped limiter tokamak. In the absence of energetic particles (EPs), the mode growth rate initially increases and then decreases as the magnetic shear changes from positive to negative, indicating stabilizing effects of the reversed magnetic shear on the internal kink mode. In the presence of EPs, when the reversed magnetic shear region is sufficiently narrow, the transition from internal kink/fishbone modes to double kink/fishbone modes takes place, and the stabilizing effects of the reversed magnetic shear can significantly dominate the destabilization of EPs. For non-resonant modes, the EP beta fraction $β_f$ for excitation increases with $q_{min}$, concurrent with progressively lower growth rates in non-resonant fishbone modes. When the equilibrium profile has an internal transport barrier (ITB), broader ITB widths suppress internal kink modes more effectively, whereas steeper temperature gradients strengthen EP stabilization.",
      "url": "http://arxiv.org/abs/2602.08884v1",
      "published_time_eastern_timestamp": 1770655546.0
    },
    {
      "title": "Whose Name Comes Up? Benchmarking and Intervention-Based Auditing of LLM-Based Scholar Recommendation",
      "summary": "Large language models (LLMs) are increasingly used for academic expert recommendation. Existing audits typically evaluate model outputs in isolation, largely ignoring end-user inference-time interventions. As a result, it remains unclear whether failures such as refusals, hallucinations, and uneven coverage stem from model choice or deployment decisions. We introduce LLMScholarBench, a benchmark for auditing LLM-based scholar recommendation that jointly evaluates model infrastructure and end-user interventions across multiple tasks. LLMScholarBench measures both technical quality and social representation using nine metrics. We instantiate the benchmark in physics expert recommendation and audit 22 LLMs under temperature variation, representation-constrained prompting, and retrieval-augmented generation (RAG) via web search. Our results show that end-user interventions do not yield uniform improvements but instead redistribute error across dimensions. Higher temperature degrades validity, consistency, and factuality. Representation-constrained prompting improves diversity at the expense of factuality, while RAG primarily improves technical quality while reducing diversity and parity. Overall, end-user interventions reshape trade-offs rather than providing a general fix. We release code and data that can be adapted to other disciplines by replacing domain-specific ground truth and metrics.",
      "url": "http://arxiv.org/abs/2602.08873v1",
      "published_time_eastern_timestamp": 1770654897.0
    },
    {
      "title": "ArkEval: Benchmarking and Evaluating Automated CodeRepair for ArkTS",
      "summary": "Large language models have transformed code generation, enabling unprecedented automation in software development. As mobile ecosystems evolve, HarmonyOS has emerged as a critical platform requiring robust development tools. Software development for the HarmonyOS ecosystem relies heavily on ArkTS, a statically typed extension of TypeScript. Despite its growing importance, the ecosystem lacks robust tools for automated code repair, primarily due to the absence of a high-quality benchmark for evaluation. To address this gap, we present ArkEval, a unified framework for ArkTS automated repair workflow evaluation and benchmark construction. It provides the first comprehensive benchmark specifically designed for ArkTS automated program repair. We constructed this benchmark by mining issues from a large-scale official Huawei repository containing over 400 independent ArkTS applications. Through a rigorous multi-stage filtering process, we curated 502 reproducible issues. To ensure testability, we employed a novel LLM-based test generation and voting mechanism involving Claude and other models. Furthermore, we standardized problem statements to facilitate fair evaluation. Finally, we evaluated four state-of-the-art Large Language Models (LLMs) on our benchmark using a retrieval-augmented repair workflow. Our results highlight the current capabilities and limitations of LLMs in repairing ArkTS code, paving the way for future research in this low-resource language domain.",
      "url": "http://arxiv.org/abs/2602.08866v1",
      "published_time_eastern_timestamp": 1770654509.0
    }
  ]
}
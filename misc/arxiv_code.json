{
  "last_updated": "2025-09-15T08:22:59.861484-04:00",
  "papers": [
    {
      "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and\n  Adaptability Across Alzheimer's Prediction Tasks and Datasets",
      "summary": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes\nmemory loss and cognitive decline. While there has been extensive research in\napplying deep learning models to Alzheimer's prediction tasks, these models\nremain limited by lack of available labeled data, poor generalization across\ndatasets, and inflexibility to varying numbers of input scans and time\nintervals between scans. In this study, we adapt three state-of-the-art\ntemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,\nand add novel extensions designed to handle variable-length inputs and learn\nrobust spatial features. We aggregate four publicly available datasets\ncomprising 3,161 patients for pre-training, and show the performance of our\nmodel across multiple Alzheimer's prediction tasks including diagnosis\nclassification, conversion detection, and future conversion prediction.\nImportantly, our SSL model implemented with temporal order prediction and\ncontrastive learning outperforms supervised learning on six out of seven\ndownstream tasks. It demonstrates adaptability and generalizability across\ntasks and number of input images with varying time intervals, highlighting its\ncapacity for robust performance across clinical applications. We release our\ncode and model publicly at https://github.com/emilykaczmarek/SSL-AD.",
      "url": "http://arxiv.org/abs/2509.10453v1",
      "published_time_eastern_timestamp": 1757699972.0
    },
    {
      "title": "The CHARA Array Polarization Model and Prospects for Spectropolarimetry",
      "summary": "Polarimetric data provide key insights into infrared emission mechanisms in\nthe inner disks of YSOs and the details of dust formation around AGB stars.\nWhile polarization measurements are well-established in radio interferometry,\nthey remain challenging at visible and near-infrared due to the significant\ntime-variable birefringence introduced by the complex optical beamtrain. In\nthis study, we characterize instrumental polarization effects within the\noptical path of the CHARA Array, focusing on the H-band MIRC-X and K-band\nMYSTIC beam combiners. Using Jones matrix formalism, we developed a\ncomprehensive model describing diattenuation and retardance across the array.\nBy applying this model to an unpolarized calibrator, we derived the\ninstrumental parameters for both MIRC-X and MYSTIC. Our results show\ndifferential diattenuation consistent with >= 97% reflectivity per\naluminum-coated surface at 45 deg incidence. The differential retardance\nexhibits small wavelength-dependent variations, in some cases larger than we\nexpected. Notably, telescope W2 exhibits a significantly larger phase shift in\nthe Coude path, attributable to a fixed aluminum mirror (M4) used in place of\ndeformable mirrors present on the other telescopes during the observing run. We\nalso identify misalignments in the LiNbO_3 birefringent compensator plates on\nS1 (MIRC-X) and W2 (MYSTIC). After correcting for night-to-night offsets, we\nachieve calibration accuracies of $\\pm$ 3.4% in visibility ratio and $\\pm$ 1.4\ndeg in differential phase for MIRC-X, and $\\pm$ 5.9% and $\\pm$ 2.4 deg,\nrespectively, for MYSTIC. Given that the differential intrinsic polarization of\nspatially resolved sources, such as AGB stars and YSOs, typically greater than\nthese instrumental uncertainties, our results demonstrate that CHARA is now\ncapable of achieving high-accuracy measurements of intrinsic polarization in\nastrophysical targets.",
      "url": "http://arxiv.org/abs/2509.10451v1",
      "published_time_eastern_timestamp": 1757699902.0
    },
    {
      "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and\n  Multi-Turn RL",
      "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive.",
      "url": "http://arxiv.org/abs/2509.10446v1",
      "published_time_eastern_timestamp": 1757699555.0
    },
    {
      "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question\n  Solutions in Cloud and Edge Deployment",
      "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture.",
      "url": "http://arxiv.org/abs/2509.10436v1",
      "published_time_eastern_timestamp": 1757699062.0
    },
    {
      "title": "DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with\n  Disentangled Context-Aware Pre-Training",
      "summary": "Trajectory prediction is a critical component of autonomous driving,\nessential for ensuring both safety and efficiency on the road. However,\ntraditional approaches often struggle with the scarcity of labeled data and\nexhibit suboptimal performance in multi-agent prediction scenarios. To address\nthese challenges, we introduce a disentangled context-aware pre-training\nframework for multi-agent motion prediction, named DECAMP. Unlike existing\nmethods that entangle representation learning with pretext tasks, our framework\ndecouples behavior pattern learning from latent feature reconstruction,\nprioritizing interpretable dynamics and thereby enhancing scene representation\nfor downstream prediction. Additionally, our framework incorporates\ncontext-aware representation learning alongside collaborative spatial-motion\npretext tasks, which enables joint optimization of structural and intentional\nreasoning while capturing the underlying dynamic intentions. Our experiments on\nthe Argoverse 2 benchmark showcase the superior performance of our method, and\nthe results attained underscore its effectiveness in multi-agent motion\nforecasting. To the best of our knowledge, this is the first context\nautoencoder framework for multi-agent motion forecasting in autonomous driving.\nThe code and models will be made publicly available.",
      "url": "http://arxiv.org/abs/2509.10426v1",
      "published_time_eastern_timestamp": 1757698142.0
    },
    {
      "title": "TASC: Task-Aware Shared Control for Teleoperated Manipulation",
      "summary": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc.",
      "url": "http://arxiv.org/abs/2509.10416v1",
      "published_time_eastern_timestamp": 1757697198.0
    },
    {
      "title": "Multimodal SAM-adapter for Semantic Segmentation",
      "summary": "Semantic segmentation, a key task in computer vision with broad applications\nin autonomous driving, medical imaging, and robotics, has advanced\nsubstantially with deep learning. Nevertheless, current approaches remain\nvulnerable to challenging conditions such as poor lighting, occlusions, and\nadverse weather. To address these limitations, multimodal methods that\nintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,\nproviding complementary information that enhances robustness. In this work, we\npresent MM SAM-adapter, a novel framework that extends the capabilities of the\nSegment Anything Model (SAM) for multimodal semantic segmentation. The proposed\nmethod employs an adapter network that injects fused multimodal features into\nSAM's rich RGB features. This design enables the model to retain the strong\ngeneralization ability of RGB features while selectively incorporating\nauxiliary modalities only when they contribute additional cues. As a result, MM\nSAM-adapter achieves a balanced and efficient use of multimodal information. We\nevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,\nwhere MM SAM-adapter delivers state-of-the-art performance. To further analyze\nmodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard\nsubsets. Results consistently demonstrate that our framework outperforms\ncompeting methods in both favorable and adverse conditions, highlighting the\neffectiveness of multimodal adaptation for robust scene understanding. The code\nis available at the following link:\nhttps://github.com/iacopo97/Multimodal-SAM-Adapter.",
      "url": "http://arxiv.org/abs/2509.10408v1",
      "published_time_eastern_timestamp": 1757696331.0
    },
    {
      "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over\n  Standards",
      "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.",
      "url": "http://arxiv.org/abs/2509.10407v1",
      "published_time_eastern_timestamp": 1757696300.0
    },
    {
      "title": "Developer-LLM Conversations: An Empirical Study of Interactions and\n  Generated Code Quality",
      "summary": "Large Language Models (LLMs) are becoming integral to modern software\ndevelopment workflows, assisting developers with code generation, API\nexplanation, and iterative problem-solving through natural language\nconversations. Despite widespread adoption, there is limited understanding of\nhow developers interact with LLMs in practice and how these conversational\ndynamics influence task outcomes, code quality, and software engineering\nworkflows. To address this, we leverage CodeChat, a large dataset comprising\n82,845 real-world developer-LLM conversations, containing 368,506 code snippets\ngenerated across over 20 programming languages, derived from the WildChat\ndataset. We find that LLM responses are substantially longer than developer\nprompts, with a median token-length ratio of 14:1. Multi-turn conversations\naccount for 68% of the dataset and often evolve due to shifting requirements,\nincomplete prompts, or clarification requests. Topic analysis identifies web\ndesign (9.6% of conversations) and neural network training (8.7% of\nconversations) as the most frequent LLM-assisted tasks. Evaluation across five\nlanguages (i.e., Python, JavaScript, C++, Java, and C#) reveals prevalent and\nlanguage-specific issues in LLM-generated code: generated Python and JavaScript\ncode often include undefined variables (83.4% and 75.3% of code snippets,\nrespectively); Java code lacks required comments (75.9%); C++ code frequently\nomits headers (41.1%) and C# code shows unresolved namespaces (49.2%). During a\nconversation, syntax and import errors persist across turns; however,\ndocumentation quality in Java improves by up to 14.7%, and import handling in\nPython improves by 3.7% over 5 turns. Prompts that point out mistakes in code\ngenerated in prior turns and explicitly request a fix are most effective for\nresolving errors.",
      "url": "http://arxiv.org/abs/2509.10402v1",
      "published_time_eastern_timestamp": 1757695969.0
    },
    {
      "title": "Diversified recommendations of cultural activities with personalized\n  determinantal point processes",
      "summary": "While optimizing recommendation systems for user engagement is a\nwell-established practice, effectively diversifying recommendations without\nnegatively impacting core business metrics remains a significant industry\nchallenge. In line with our initiative to broaden our audience's cultural\npractices, this study investigates using personalized Determinantal Point\nProcesses (DPPs) to sample diverse and relevant recommendations. We rely on a\nwell-known quality-diversity decomposition of the similarity kernel to give\nmore weight to user preferences. In this paper, we present our implementations\nof the personalized DPP sampling, evaluate the trade-offs between relevance and\ndiversity through both offline and online metrics, and give insights for\npractitioners on their use in a production environment. For the sake of\nreproducibility, we release the full code for our platform and experiments on\nGitHub.",
      "url": "http://arxiv.org/abs/2509.10392v1",
      "published_time_eastern_timestamp": 1757694847.0
    },
    {
      "title": "A Fibonacci-Based Gödel Numbering: $Δ_0$ Semantics Without\n  Exponentiation",
      "summary": "We introduce a more efficient G\\\"odel encoding scheme based on Zeckendorf\nrepresentations of natural numbers, avoiding the use of unbounded\nexponentiation and without appealing to the fundamental theorem of arithmetic.\nThe resulting encoding is injective, primitive recursive, and\n$\\Delta_0$-definable in weak arithmetical theories such as $I\\Delta_0 +\n\\Omega_1$. This allows for the construction of fixed points and the\nformalization of incompleteness theorems entirely within bounded arithmetic.\nCompared to traditional prime-exponent $\\Pi$ codings, the Zeckendorf approach\nyields longer codes but supports simpler substitution mechanisms and more\ntransparent definability properties.",
      "url": "http://arxiv.org/abs/2509.10382v1",
      "published_time_eastern_timestamp": 1757693817.0
    },
    {
      "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
      "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
      "url": "http://arxiv.org/abs/2509.10372v1",
      "published_time_eastern_timestamp": 1757693127.0
    },
    {
      "title": "Characterizing the Efficiency of Distributed Training: A Power,\n  Performance, and Thermal Perspective",
      "summary": "The rapid scaling of Large Language Models (LLMs) has pushed training\nworkloads far beyond the limits of single-node analysis, demanding a deeper\nunderstanding of how these models behave across large-scale, multi-GPU systems.\nIn this paper, we present a comprehensive characterization of LLM training\nacross diverse real-world workloads and hardware platforms, including NVIDIA\nH100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various\nparallelism strategies -- tensor, pipeline, data, and expert -- and evaluate\ntheir effects on hardware utilization, power consumption, and thermal behavior.\nWe further evaluate the effectiveness of optimizations such as activation\nrecomputation and compute-communication overlap. Our findings show that\nperformance is not determined solely by scaling hardware capacity. Scale-up\nsystems with fewer, higher-memory GPUs can outperform scale-out systems in\ncommunication-bound regimes, but only under carefully tuned configurations; in\nother cases, scale-out deployments achieve superior throughput. We also show\nthat certain parallelism combinations, such as tensor with pipeline, lead to\nbandwidth underutilization due to inefficient data chunking, while increasing\nmicrobatch sizes beyond a certain point induces bursty execution and peak power\nexcursions that worsen thermal throttling. These insights reveal how training\nperformance is shaped by complex interactions between hardware, system\ntopology, and model execution. We conclude by offering recommendations for\nsystem and hardware design to improve the scalability and reliability of future\nLLM systems and workloads. The source code of this project is available at\nhttps://github.com/sitar-lab/CharLLM-PPT.",
      "url": "http://arxiv.org/abs/2509.10371v1",
      "published_time_eastern_timestamp": 1757693107.0
    },
    {
      "title": "Efficient Learned Image Compression Through Knowledge Distillation",
      "summary": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .",
      "url": "http://arxiv.org/abs/2509.10366v1",
      "published_time_eastern_timestamp": 1757692795.0
    },
    {
      "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
      "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
      "url": "http://arxiv.org/abs/2509.10312v1",
      "published_time_eastern_timestamp": 1757688825.0
    },
    {
      "title": "GraphCSVAE: Graph Categorical Structured Variational Autoencoder for\n  Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable\n  Post-Disaster Risk Reduction",
      "summary": "In the aftermath of disasters, many institutions worldwide face challenges in\ncontinually monitoring changes in disaster risk, limiting the ability of key\ndecision-makers to assess progress towards the UN Sendai Framework for Disaster\nRisk Reduction 2015-2030. While numerous efforts have substantially advanced\nthe large-scale modeling of hazard and exposure through Earth observation and\ndata-driven methods, progress remains limited in modeling another equally\nimportant yet challenging element of the risk equation: physical vulnerability.\nTo address this gap, we introduce Graph Categorical Structured Variational\nAutoencoder (GraphCSVAE), a novel probabilistic data-driven framework for\nmodeling physical vulnerability by integrating deep learning, graph\nrepresentation, and categorical probabilistic inference, using time-series\nsatellite-derived datasets and prior expert belief systems. We introduce a\nweakly supervised first-order transition matrix that reflects the changes in\nthe spatiotemporal distribution of physical vulnerability in two\ndisaster-stricken and socioeconomically disadvantaged areas: (1) the\ncyclone-impacted coastal Khurushkul community in Bangladesh and (2) the\nmudslide-affected city of Freetown in Sierra Leone. Our work reveals\npost-disaster regional dynamics in physical vulnerability, offering valuable\ninsights into localized spatiotemporal auditing and sustainable strategies for\npost-disaster risk reduction.",
      "url": "http://arxiv.org/abs/2509.10308v1",
      "published_time_eastern_timestamp": 1757688656.0
    },
    {
      "title": "Near-Optimal Recovery Performance of PhaseLift for Phase Retrieval from\n  Coded Diffraction Patterns",
      "summary": "The PhaseLift algorithm is an effective convex method for solving the phase\nretrieval problem from Fourier measurements with coded diffraction patterns\n(CDP). While exact reconstruction guarantees are well-established in the\nnoiseless case, the stability of recovery under noise remains less well\nunderstood. In particular, when the measurements are corrupted by an additive\nnoise vector $\\mathbf{w} \\in \\mathbb{R}^m$, existing recovery bounds scale on\nthe order of $\\|\\mathbf{w}\\|_2$, which is conjectured to be suboptimal. More\nrecently, Soltanolkotabi conjectured that the optimal PhaseLift recovery bound\nshould scale with the average noise magnitude, that is, on the order of\n$\\|\\mathbf{w}\\|_2/\\sqrt m$. However, establishing this theoretically is\nconsiderably more challenging and has remained an open problem. In this paper,\nwe focus on this conjecture and provide a nearly optimal recovery bound for it.\nWe prove that under adversarial noise, the recovery error of PhaseLift is\nbounded by $O(\\log n \\cdot \\|\\mathbf{w}\\|_2/\\sqrt m)$, and further show that\nthere exists a noise vector for which the error lower bound exceeds\n$O\\bigl(\\frac{1}{\\sqrt{\\log n}} \\cdot \\frac{\\|\\mathbf{w}\\|_2}{\\sqrt m}\\bigr)$.\nHere, $n$ is the dimension of the signals we aim to recover. Moreover, for\nmean-zero sub-Gaussian noise vector $\\mathbf{w} \\in \\mathbb R^m$ with\nsub-Gaussian norm $\\sigma$, we establish a bound of order $O\\bigl(\\sigma\n\\sqrt{\\frac{n \\log^4 n}{m}}\\bigr)$, and also provide a corresponding minimax\nlower bound. Our results affirm Soltanolkotabi's conjecture up to logarithmic\nfactors, providing a new insight into the stability of PhaseLift under noisy\nCDP measurements.",
      "url": "http://arxiv.org/abs/2509.10300v1",
      "published_time_eastern_timestamp": 1757687978.0
    },
    {
      "title": "Targeted Test Selection Approach in Continuous Integration",
      "summary": "In modern software development change-based testing plays a crucial role.\nHowever, as codebases expand and test suites grow, efficiently managing the\ntesting process becomes increasingly challenging, especially given the high\nfrequency of daily code commits. We propose Targeted Test Selection (T-TS), a\nmachine learning approach for industrial test selection. Our key innovation is\na data representation that represent commits as Bags-of-Words of changed files,\nincorporates cross-file and additional predictive features, and notably avoids\nthe use of coverage maps. Deployed in production, T-TS was comprehensively\nevaluated against industry standards and recent methods using both internal and\npublic datasets, measuring time efficiency and fault detection. On live\nindustrial data, T-TS selects only 15% of tests, reduces execution time by\n$5.9\\times$, accelerates the pipeline by $5.6\\times$, and detects over 95% of\ntest failures. The implementation is publicly available to support further\nresearch and practical adoption.",
      "url": "http://arxiv.org/abs/2509.10279v1",
      "published_time_eastern_timestamp": 1757686851.0
    },
    {
      "title": "Experimental study of turbulent mixing in a T-shaped mixer",
      "summary": "One of the most widespread canonical devices for fluid mixing is the T-shaped\nmixer, in which two opposing miscible liquid streams meet at a junction and\nthen mix along a main channel. Laminar steady and time-periodic flows in\nT-shaped mixers have been thoroughly studied, but turbulent flows have received\nmuch less scrutiny despite their prevalence in applications. We here introduce\na novel experimental setup with a hydraulic diameter of four centimetres that\nenables the optical study of turbulent mixing at small scales. Using this\nsetup, we perform two-dimensional particle image velocimetry and planar\nlaser-induced fluorescence measurements. First, we successfully replicate\ncharacteristic flow regimes observed in micro-scale T-shaped mixers at low\nReynolds numbers. We then focus on the turbulent regime and characterize the\nturbulent kinetic energy and dissipation along the mixing channel. Further, we\nmeasure the scalar concentration variance and its corresponding probability\ndensity function and spectra. The latter exhibits an incipient Batchelor\nscaling. We estimate the mechanical-to-scalar timescale ratio and examine the\nlink between the turbulent velocity and scalar fields. The measurement data are\ncompared with model predictions and correlations used in engineering practice,\nand with our own direct numerical simulations performed with a spectral-element\ncode.",
      "url": "http://arxiv.org/abs/2509.10264v1",
      "published_time_eastern_timestamp": 1757686015.0
    },
    {
      "title": "The 21-cm signature of X-ray heated halos around galaxies during cosmic\n  dawn",
      "summary": "X-rays emitted by high mass X-ray binaries (HMXBs) and supernovae-driven\nwinds in the first galaxies during Cosmic Dawn are expected to warm the\nintergalactic medium prior to its reionization. While most of the heating will\nbe uniform on measurable scales, exceptionally bright sources will produce a\nwarm ring around them with a distinctive 21-cm signature. The detection of such\nsystems would confirm X-rays are a source of IGM heating during Cosmic Dawn and\nprovide a test of models predicting higher X-ray luminosities per star\nformation rate compared with present-day galaxies. We illustrate the effect for\na star-forming galaxy in a $10^{11}\\, M_\\odot$ halo at $z=12$, treating the\nphotoionizing radiation and X-rays using a novel fully time-dependent 3D\nray-tracing radiative transfer code. We consider a range in possible spectra\nfor the HMXBs and star formation efficiencies, as well as the possible effect\nof an extended halo around the galaxy. We find detection of the signal would\nrequire integration times of a few thousand hours using SKA1-Low except for a\nbright galaxy like a starburst, but only a thousand hours for the expected\nnoise levels of SKA2-Low. Depending on the surrounding gas density profile, the\n21-cm signature of X-ray heating may still require an exceptionally high star\nformation rate, either intrinsic to the source or provided by other systems\nclustered near it, to avoid dominance of the signal by absorption from the\nsurrounding gas.",
      "url": "http://arxiv.org/abs/2509.10255v1",
      "published_time_eastern_timestamp": 1757685504.0
    }
  ]
}
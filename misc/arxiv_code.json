{
  "last_updated": "2025-11-30T20:10:11.179203-05:00",
  "papers": [
    {
      "title": "G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning",
      "summary": "Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.",
      "url": "http://arxiv.org/abs/2511.21688v1",
      "published_time_eastern_timestamp": 1764183579.0
    },
    {
      "title": "Holographically Emergent Gauge Theory in Symmetric Quantum Circuits",
      "summary": "We develop a novel holographic framework for mixed-state phases in random quantum circuits, both unitary and non-unitary, with a global symmetry $G$. Viewing the circuit as a tensor network, we decompose it into two parts: a symmetric layer, which defines an emergent gauge wavefunction in one higher dimension, and a random non-symmetric layer, which consists of random multiplicity tensors. For unitarity circuits, the bulk gauge state is deconfined, but under a generic non-unitary circuit (e.g. channels), the bulk gauge theory can undergo a decoherence-induced phase transition: for $G\\,{=}\\,\\mathbb{Z}_N$ with local symmetric noise, the circuit can act as a quantum error-correcting code with a distinguished logical subspace inheriting the $\\mathbb{Z}_N$-surface code's topological protection. We then identify that the charge sharpening transition from the measurement side is complementary to a decodability transition in the bulk: noise of the bulk can be interpreted as measurement from the environment. For $N\\,{\\leq}\\,4$, weak measurements drive a single transition from a charge-fuzzy phase with sharpening time $t_{\\#}\\sim e^{L}$ to a charge-sharp phase with $t_{\\#}\\sim \\mathcal{O}(1)$, corresponding to confinement that destroys logical information. For $N>4$, measurements generically generate an intermediate quasi-long-range ordered Coulomb phase with gapless photons and purification time $t_{\\#}\\sim \\mathcal{O}(L)$.",
      "url": "http://arxiv.org/abs/2511.21685v1",
      "published_time_eastern_timestamp": 1764183491.0
    },
    {
      "title": "Mean-field Modelling of Moiré Materials: A User's Guide with Selected Applications to Twisted Bilayer Graphene",
      "summary": "We review the theoretical modelling of moiré materials, focusing on various aspects of magic-angle twisted bilayer graphene (MA-TBG) viewed through the lens of Hartree-Fock mean-field theory. We first provide an elementary introduction to the continuum modelling of moiré bandstructures, and explain how interactions are incorporated to study correlated states. We then discuss how to implement mean-field simulations of ground state structure and collective excitations in this setting. With this background established, we rationalize the power of mean-field approximations in MA-TBG, by discussing the idealized \"chiral-flat\" strong-coupling limit, in which ground states at electron densities commensurate with the moiré superlattice are exactly captured by mean-field ansätze. We then illustrate the phenomenological shortcomings of this limit, leading us naturally into a discussion of the intermediate-coupling incommensurate Kekulé spiral (IKS) order and its origins in ever-present heterostrain. IKS and its placement within an expanded Hartree-Fock manifold form our first \"case study\". Our second case study involves time-dependence, and focuses on the collective modes of various broken-symmetry insulators in MA-TBG. As a third and final case study, we return to the strong-coupling picture, which can be stabilized by aligning MA-TBG to an hBN substrate. In this limit, we show how mean field theory can be adapted to the translationally non-invariant setting in order to quantitatively study the energetics of domain walls in orbital Chern insulating states. We close with a discussion of extensions and further applications. Used either as a standalone reference or alongside the accompanying open-source code, this review should enable readers with a basic knowledge of band theory and many-body physics to systematically build and analyze detailed models of generic moiré systems.",
      "url": "http://arxiv.org/abs/2511.21683v1",
      "published_time_eastern_timestamp": 1764183442.0
    },
    {
      "title": "Uncertainty Quantification for Visual Object Pose Estimation",
      "summary": "Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.",
      "url": "http://arxiv.org/abs/2511.21666v1",
      "published_time_eastern_timestamp": 1764182384.0
    },
    {
      "title": "FPGA-tailored algorithms for real-time decoding of quantum LDPC codes",
      "summary": "Real-time decoding is crucial for fault-tolerant quantum computing but likely requires specialized hardware such as field-programmable gate arrays (FPGAs), whose parallelism can alter relative algorithmic performance. We analyze FPGA-tailored versions of three decoder classes for quantum low-density parity-check (qLDPC) codes: message passing, ordered statistics, and clustering. For message passing, we analyze the recently introduced Relay decoder and its FPGA implementation; for ordered statistics decoding (OSD), we introduce a filtered variant that concentrates computation on high-likelihood fault locations; and for clustering, we design an FPGA-adapted generalized union-find decoder. We design a systolic algorithm for Gaussian elimination on rank-deficient systems that runs in linear parallel time, enabling fast validity checks and local corrections in clustering and eliminating costly full-rank inversion in filtered-OSD. Despite these improvements, both remain far slower and less accurate than Relay, suggesting message passing is the most viable route to real-time qLDPC decoding.",
      "url": "http://arxiv.org/abs/2511.21660v1",
      "published_time_eastern_timestamp": 1764182027.0
    },
    {
      "title": "Nearly Tight Lower Bounds for Relaxed Locally Decodable Codes via Robust Daisies",
      "summary": "We show a nearly optimal lower bound on the length of linear relaxed locally decodable codes (RLDCs). Specifically, we prove that any $q$-query linear RLDC $C\\colon \\{0,1\\}^k \\to \\{0,1\\}^n$ must satisfy $n = k^{1+Ω(1/q)}$. This bound closely matches the known upper bound of $n = k^{1+O(1/q)}$ by Ben-Sasson, Goldreich, Harsha, Sudan, and Vadhan (STOC 2004).\n  Our proof introduces the notion of robust daisies, which are relaxed sunflowers with pseudorandom structure, and leverages a new spread lemma to extract dense robust daisies from arbitrary distributions.",
      "url": "http://arxiv.org/abs/2511.21659v1",
      "published_time_eastern_timestamp": 1764181964.0
    },
    {
      "title": "EvilGenie: A Reward Hacking Benchmark",
      "summary": "We introduce EvilGenie, a benchmark for reward hacking in programming settings. We source problems from LiveCodeBench and create an environment in which agents can easily reward hack, such as by hardcoding test cases or editing the testing files. We measure reward hacking in three ways: held out unit tests, LLM judges, and test file edit detection. We verify these methods against human review and each other. We find the LLM judge to be highly effective at detecting reward hacking in unambiguous cases, and observe only minimal improvement from the use of held out test cases. In addition to testing many models using Inspect's basic_agent scaffold, we also measure reward hacking rates for three popular proprietary coding agents: OpenAI's Codex, Anthropic's Claude Code, and Google's Gemini CLI Using GPT-5, Claude Sonnet 4, and Gemini 2.5 Pro, respectively. We observe explicit reward hacking by both Codex and Claude Code, and misaligned behavior by all three agents. Our codebase can be found at https://github.com/JonathanGabor/EvilGenie.",
      "url": "http://arxiv.org/abs/2511.21654v1",
      "published_time_eastern_timestamp": 1764181637.0
    },
    {
      "title": "CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow",
      "summary": "Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow",
      "url": "http://arxiv.org/abs/2511.21653v1",
      "published_time_eastern_timestamp": 1764181541.0
    },
    {
      "title": "Mechanisms of Non-Monotonic Scaling in Vision Transformers",
      "summary": "Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.",
      "url": "http://arxiv.org/abs/2511.21635v1",
      "published_time_eastern_timestamp": 1764180434.0
    },
    {
      "title": "Qwen3-VL Technical Report",
      "summary": "We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.",
      "url": "http://arxiv.org/abs/2511.21631v1",
      "published_time_eastern_timestamp": 1764179948.0
    },
    {
      "title": "TAGFN: A Text-Attributed Graph Dataset for Fake News Detection in the Age of LLMs",
      "summary": "Large Language Models (LLMs) have recently revolutionized machine learning on text-attributed graphs, but the application of LLMs to graph outlier detection, particularly in the context of fake news detection, remains significantly underexplored. One of the key challenges is the scarcity of large-scale, realistic, and well-annotated datasets that can serve as reliable benchmarks for outlier detection. To bridge this gap, we introduce TAGFN, a large-scale, real-world text-attributed graph dataset for outlier detection, specifically fake news detection. TAGFN enables rigorous evaluation of both traditional and LLM-based graph outlier detection methods. Furthermore, it facilitates the development of misinformation detection capabilities in LLMs through fine-tuning. We anticipate that TAGFN will be a valuable resource for the community, fostering progress in robust graph-based outlier detection and trustworthy AI. The dataset is publicly available at https://huggingface.co/datasets/kayzliu/TAGFN and our code is available at https://github.com/kayzliu/tagfn.",
      "url": "http://arxiv.org/abs/2511.21624v1",
      "published_time_eastern_timestamp": 1764179380.0
    },
    {
      "title": "Entropy Coding for Non-Rectangular Transform Blocks using Partitioned DCT Dictionaries for AV1",
      "summary": "Recent video codecs such as VVC and AV1 apply a Non-rectangular (NR) partitioning to combine prediction signals using a smooth blending around the boundary, followed by a rectangular transform on the whole block. The NR signal transformation is not yet supported. A transformation technique that applies the same partitioning to the 2D Discrete Cosine Transform (DCT) bases and finds a sparse representation of the NR signal in such a dictionary showed promising gains in an experimental setup outside the reference software. This method uses the regular inverse transformation at the decoder to reconstruct a rectangular signal and discards the signal outside the region of interest. This design is appealing due to the minimal changes required at the decoder. However, current entropy coding schemes are not well-suited for optimally encoding these coefficients because they are primarily designed for DCT coefficients. This work introduces an entropy coding method that efficiently codes these transform coefficients by effectively modeling their properties. The design offers significant theoretical rate savings, estimated using conditional entropy, particularly for scenarios that are more dissimilar to DCT in an experimental setup.",
      "url": "http://arxiv.org/abs/2511.21609v1",
      "published_time_eastern_timestamp": 1764178204.0
    },
    {
      "title": "Visualizing LLM Latent Space Geometry Through Dimensionality Reduction",
      "summary": "Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.",
      "url": "http://arxiv.org/abs/2511.21594v1",
      "published_time_eastern_timestamp": 1764177099.0
    },
    {
      "title": "On the Limits of Innate Planning in Large Language Models",
      "summary": "Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.",
      "url": "http://arxiv.org/abs/2511.21591v1",
      "published_time_eastern_timestamp": 1764176893.0
    },
    {
      "title": "Formation and nature of \"Huntsman\" binary pulsars",
      "summary": "Spider systems are a class of close binaries in which a neutron star first accretes from a normal companion, and later ablates it in some cases. New observations have expanded this category, with the addition of a Huntsman group, tentatively linked to a short donor phase along the red bump in the secondary evolutionary track. We present explicit evolutionary tracks that support the Huntsman nature recently suggested, and discuss how the whole class of spiders emerges from the full consideration of irradiation and ablating winds. We address the irradiation feedback (IFB) effects and the hydrogen-shell burning detachment (HSBD) simultaneously, and show that they act independently and do not interfere with each other, supporting a physical picture of the Huntsman group. We employ our binary evolution code to compute a suite of binary systems formed by a donor star and a neutron star for different initial orbital periods, assuming solar composition and Z=0.01. Although many models do not consider IFB, we also present the evolution with IFB for one system as an example. We found that the recently suggested association of Huntsman pulsar with the evolutionary stage where (as a consequence of the dynamics of HSBD) the system remains detached for a few million years is plausible. However, this feature alone is unable to account for the occurrence of Redback spider pulsars. Meanwhile, models including IFB, with pulsed mass transfer, display detachment episodes that can be naturally associated with the Redback stage. Irradiation feedback does not preclude or modify HSBD and in fact, Huntsman systems were already present as an implicit prediction in our earlier calculations. We conclude that Huntsman is an expected stage of the spider systems under quite general conditions. This is another step towards a unified picture of spider pulsars as a group.",
      "url": "http://arxiv.org/abs/2511.21589v1",
      "published_time_eastern_timestamp": 1764176873.0
    },
    {
      "title": "Approximate Bayesian Computation Made Easy: A Practical Guide to ABC-SMC for Dynamical Systems with \\texttt{pymc}",
      "summary": "Mechanistic models are essential tools across ecology, epidemiology, and the life sciences, but parameter inference remains challenging when likelihood functions are intractable. Approximate Bayesian Computation with Sequential Monte Carlo (ABC-SMC) offers a powerful likelihood-free alternative that requires only the ability to simulate data from mechanistic models. Despite its potential, many researchers remain hesitant to adopt these methods due to perceived complexity. This tutorial bridges that gap by providing a practical, example-driven introduction to ABC-SMC using Python. From predator-prey dynamics to hierarchical epidemic models, we illustrate by example how to implement, diagnose, and interpret ABC-SMC analyses. Each example builds intuition about when and why ABC-SMC works, how partial observability affects parameter identifiability, and how hierarchical structures naturally emerge in Bayesian frameworks. All code leverages PyMC's modern probabilistic programming interface, ensuring reproducibility and easy adaptation to new problems. The code its fully available for download at \\href{https://github.com/mariocastro73/ABCSMC_pymc_by_example}{mariocastro73/ABCSMC\\_pymc\\_by\\_example}",
      "url": "http://arxiv.org/abs/2511.21587v1",
      "published_time_eastern_timestamp": 1764176727.0
    },
    {
      "title": "Learning When to Stop: Adaptive Latent Reasoning via Reinforcement Learning",
      "summary": "Latent reasoning represents a new development in Transformer language models that has shown potential in compressing reasoning lengths compared to chain-of-thought reasoning. By directly passing the information-rich previous final latent state into the next sequence, latent reasoning removes the restriction to human language tokens as the medium for reasoning. We develop adaptive-length latent reasoning models and introduce a post-SFT reinforcement-learning methodology to optimize latent reasoning length by minimizing reasoning length while maintaining accuracy. This, in turn, further reduces compute usage and raises the bar on the compressive capabilities of latent reasoning models. Experiments on the Llama 3.2 1B model and the GSM8K-Aug dataset show a $52\\%$ drop in total reasoning length with no penalty to accuracy. In future work, we plan to extend to additional models and datasets, analyze relationships between training coefficients, experiment with architecture variations, and continue our knowledge distillation for latent reasoning SFT efforts. We make our code and pretrained weights available at https://github.com/apning/adaptive-latent-reasoning.",
      "url": "http://arxiv.org/abs/2511.21581v1",
      "published_time_eastern_timestamp": 1764176046.0
    },
    {
      "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation",
      "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.",
      "url": "http://arxiv.org/abs/2511.21535v1",
      "published_time_eastern_timestamp": 1764172892.0
    },
    {
      "title": "Simulations of high-energy neutrino emissions from blazars with the LeHa-Paris code",
      "summary": "The identification of astrophysical sources responsible for high-energy cosmic neutrinos has long been a challenge. A significant milestone was achieved with the blazar TXS 0506+056, which was found to be in a flaring state of high gamma-ray emission and associated at the 3$σ$ level with a 290 TeV neutrino detected by IceCube in September 2017. This discovery motivated deeper exploration of the theoretical link between photon and neutrino emissions. In this context, simulations of proton-photon interactions in blazars and radiative processes are conducted using advanced numerical codes to predict neutrino spectra. The LeHa-Paris code, previously applied to TXS 0506+056, enables the computation of both leptonic and hadronic components of blazar spectral energy distributions, facilitating exploration of a broad parameter space. In this work, starting from the case of PKS 2155-304, one of the brightest and most studied High-frequency-peaked BL Lacs (HBLs), known for its extreme variability and subject of multi-wavelength observational campaigns, a methodology has been developed to extend neutrino flux templates, optimized via LeHa-Paris, to the full class of HBLs. Afterwards, neutrino emission models for a subset of HBLs from the 3HSP catalogue are derived.",
      "url": "http://arxiv.org/abs/2511.21532v1",
      "published_time_eastern_timestamp": 1764172814.0
    },
    {
      "title": "Self-Paced Learning for Images of Antinuclear Antibodies",
      "summary": "Antinuclear antibody (ANA) testing is a crucial method for diagnosing autoimmune disorders, including lupus, Sjögren's syndrome, and scleroderma. Despite its importance, manual ANA detection is slow, labor-intensive, and demands years of training. ANA detection is complicated by over 100 coexisting antibody types, resulting in vast fluorescent pattern combinations. Although machine learning and deep learning have enabled automation, ANA detection in real-world clinical settings presents unique challenges as it involves multi-instance, multi-label (MIML) learning. In this paper, a novel framework for ANA detection is proposed that handles the complexities of MIML tasks using unaltered microscope images without manual preprocessing. Inspired by human labeling logic, it identifies consistent ANA sub-regions and assigns aggregated labels accordingly. These steps are implemented using three task-specific components: an instance sampler, a probabilistic pseudo-label dispatcher, and self-paced weight learning rate coefficients. The instance sampler suppresses low-confidence instances by modeling pattern confidence, while the dispatcher adaptively assigns labels based on instance distinguishability. Self-paced learning adjusts training according to empirical label observations. Our framework overcomes limitations of traditional MIML methods and supports end-to-end optimization. Extensive experiments on one ANA dataset and three public medical MIML benchmarks demonstrate the superiority of our framework. On the ANA dataset, our model achieves up to +7.0% F1-Macro and +12.6% mAP gains over the best prior method, setting new state-of-the-art results. It also ranks top-2 across all key metrics on public datasets, reducing Hamming loss and one-error by up to 18.2% and 26.9%, respectively. The source code can be accessed at https://github.com/fletcherjiang/ANA-SelfPacedLearning.",
      "url": "http://arxiv.org/abs/2511.21519v1",
      "published_time_eastern_timestamp": 1764172203.0
    }
  ]
}
{
  "last_updated": "2025-07-23T14:18:25.651115-04:00",
  "papers": [
    {
      "title": "MegaScience: Pushing the Frontiers of Post-Training Datasets for Science\n  Reasoning",
      "summary": "Scientific reasoning is critical for developing AI scientists and supporting\nhuman researchers in advancing the frontiers of natural science discovery.\nHowever, the open-source community has primarily focused on mathematics and\ncoding while neglecting the scientific domain, largely due to the absence of\nopen, large-scale, high-quality, verifiable scientific reasoning datasets. To\nbridge this gap, we first present TextbookReasoning, an open dataset featuring\ntruthful reference answers extracted from 12k university-level scientific\ntextbooks, comprising 650k reasoning questions spanning 7 scientific\ndisciplines. We further introduce MegaScience, a large-scale mixture of\nhigh-quality open-source datasets totaling 1.25 million instances, developed\nthrough systematic ablation studies that evaluate various data selection\nmethodologies to identify the optimal subset for each publicly available\nscientific dataset. Meanwhile, we build a comprehensive evaluation system\ncovering diverse subjects and question types across 15 benchmarks,\nincorporating comprehensive answer extraction strategies to ensure accurate\nevaluation metrics. Our experiments demonstrate that our datasets achieve\nsuperior performance and training efficiency with more concise response lengths\ncompared to existing open-source scientific datasets. Furthermore, we train\nLlama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which\nsignificantly outperform the corresponding official instruct models in average\nperformance. In addition, MegaScience exhibits greater effectiveness for larger\nand stronger models, suggesting a scaling benefit for scientific tuning. We\nrelease our data curation pipeline, evaluation system, datasets, and seven\ntrained models to the community to advance scientific reasoning research.",
      "url": "http://arxiv.org/abs/2507.16812v1",
      "published_time_eastern_timestamp": 1753207143.0
    },
    {
      "title": "Rethinking LLM-Based RTL Code Optimization Via Timing Logic\n  Metamorphosis",
      "summary": "Register Transfer Level(RTL) code optimization is crucial for achieving high\nperformance and low power consumption in digital circuit design. However,\ntraditional optimization methods often rely on manual tuning and heuristics,\nwhich can be time-consuming and error-prone. Recent studies proposed to\nleverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs\ncan generate optimized code snippets based on natural language descriptions,\npotentially speeding up the optimization process. However, existing approaches\nhave not thoroughly evaluated the effectiveness of LLM-Based code optimization\nmethods for RTL code with complex timing logic. To address this gap, we\nconducted a comprehensive empirical investigation to assess the capability of\nLLM-Based RTL code optimization methods in handling RTL code with complex\ntiming logic. In this study, we first propose a new benchmark for RTL\noptimization evaluation. It comprises four subsets, each corresponding to a\nspecific area of RTL code optimization. Then we introduce a method based on\nmetamorphosis to systematically evaluate the effectiveness of LLM-Based RTL\ncode optimization methods.Our key insight is that the optimization\neffectiveness should remain consistent for semantically equivalent but more\ncomplex code. After intensive experiments, we revealed several key findings.\n(1) LLM-Based RTL optimization methods can effectively optimize logic\noperations and outperform existing compiler-based methods. (2) LLM-Based RTL\noptimization methods do not perform better than existing compiler-based methods\non RTL code with complex timing logic, particularly in timing control flow\noptimization and clock domain optimization. This is primarily attributed to the\nchallenges LLMs face in understanding timing logic in RTL code. Based on these\nfindings, we provide insights for further research in leveraging LLMs for RTL\ncode optimization.",
      "url": "http://arxiv.org/abs/2507.16808v1",
      "published_time_eastern_timestamp": 1753207022.0
    },
    {
      "title": "No-go theorems for logical gates on product quantum codes",
      "summary": "Quantum error-correcting codes are essential to the implementation of\nfault-tolerant quantum computation. Homological products of classical codes\noffer a versatile framework for constructing quantum error-correcting codes\nwith desirable properties, especially quantum low-density parity check (qLDPC)\ncodes. Based on extensions of the Bravyi--K\\\"{o}nig theorem that encompass\ncodes without geometric locality, we establish a series of general no-go\ntheorems for fault-tolerant logical gates supported by hypergraph product\ncodes. Specifically, we show that non-Clifford logical gates cannot be\nimplemented transversally on hypergraph product codes of all product\ndimensions, and that the dimensions impose various limitations on the\naccessible level of the Clifford hierarchy gates by constant-depth local\ncircuits. We also discuss examples both with and without geometric locality\nwhich attain the Clifford hierarchy bounds. Our results reveal fundamental\nrestrictions on logical gates originating from highly general algebraic\nstructures, extending beyond existing knowledge only in geometrically local,\nfinite logical qubits, transversal, or 2-dimensional product cases, and may\nguide the vital study of fault-tolerant quantum computation with qLDPC codes.",
      "url": "http://arxiv.org/abs/2507.16797v1",
      "published_time_eastern_timestamp": 1753206405.0
    },
    {
      "title": "Task-Specific Zero-shot Quantization-Aware Training for Object Detection",
      "summary": "Quantization is a key technique to reduce network size and computational\ncomplexity by representing the network parameters with a lower precision.\nTraditional quantization methods rely on access to original training data,\nwhich is often restricted due to privacy concerns or security challenges.\nZero-shot Quantization (ZSQ) addresses this by using synthetic data generated\nfrom pre-trained models, eliminating the need for real training data. Recently,\nZSQ has been extended to object detection. However, existing methods use\nunlabeled task-agnostic synthetic images that lack the specific information\nrequired for object detection, leading to suboptimal performance. In this\npaper, we propose a novel task-specific ZSQ framework for object detection\nnetworks, which consists of two main stages. First, we introduce a bounding box\nand category sampling strategy to synthesize a task-specific calibration set\nfrom the pre-trained network, reconstructing object locations, sizes, and\ncategory distributions without any prior knowledge. Second, we integrate\ntask-specific training into the knowledge distillation process to restore the\nperformance of quantized detection networks. Extensive experiments conducted on\nthe MS-COCO and Pascal VOC datasets demonstrate the efficiency and\nstate-of-the-art performance of our method. Our code is publicly available at:\nhttps://github.com/DFQ-Dojo/dfq-toolkit .",
      "url": "http://arxiv.org/abs/2507.16782v1",
      "published_time_eastern_timestamp": 1753205309.0
    },
    {
      "title": "When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning\n  LLMs",
      "summary": "Large Language Models (LLMs) have become integral to automated code analysis,\nenabling tasks such as vulnerability detection and code comprehension. However,\ntheir integration introduces novel attack surfaces. In this paper, we identify\nand investigate a new class of prompt-based attacks, termed Copy-Guided Attacks\n(CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs.\nBy injecting carefully crafted triggers into external code snippets,\nadversaries can induce the model to replicate malicious content during\ninference. This behavior enables two classes of vulnerabilities: inference\nlength manipulation, where the model generates abnormally short or excessively\nlong reasoning traces; and inference result manipulation, where the model\nproduces misleading or incorrect conclusions. We formalize CGA as an\noptimization problem and propose a gradient-based approach to synthesize\neffective triggers. Empirical evaluation on state-of-the-art reasoning LLMs\nshows that CGA reliably induces infinite loops, premature termination, false\nrefusals, and semantic distortions in code analysis tasks. While highly\neffective in targeted settings, we observe challenges in generalizing CGA\nacross diverse prompts due to computational constraints, posing an open\nquestion for future research. Our findings expose a critical yet underexplored\nvulnerability in LLM-powered development pipelines and call for urgent advances\nin prompt-level defense mechanisms.",
      "url": "http://arxiv.org/abs/2507.16773v1",
      "published_time_eastern_timestamp": 1753204896.0
    },
    {
      "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
      "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
      "url": "http://arxiv.org/abs/2507.16768v1",
      "published_time_eastern_timestamp": 1753204427.0
    },
    {
      "title": "Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos\n  Networks",
      "summary": "Faithfulness and interpretability are essential for deploying deep neural\nnetworks (DNNs) in safety-critical domains such as medical imaging. B-cos\nnetworks offer a promising solution by replacing standard linear layers with a\nweight-input alignment mechanism, producing inherently interpretable,\nclass-specific explanations without post-hoc methods. While maintaining\ndiagnostic performance competitive with state-of-the-art DNNs, standard B-cos\nmodels suffer from severe aliasing artifacts in their explanation maps, making\nthem unsuitable for clinical use where clarity is essential. Additionally, the\noriginal B-cos formulation is limited to multi-class settings, whereas chest\nX-ray analysis often requires multi-label classification due to co-occurring\nabnormalities. In this work, we address both limitations: (1) we introduce\nanti-aliasing strategies using FLCPooling (FLC) and BlurPool (BP) to\nsignificantly improve explanation quality, and (2) we extend B-cos networks to\nsupport multi-label classification. Our experiments on chest X-ray datasets\ndemonstrate that the modified $\\text{B-cos}_\\text{FLC}$ and\n$\\text{B-cos}_\\text{BP}$ preserve strong predictive performance while providing\nfaithful and artifact-free explanations suitable for clinical application in\nmulti-label settings. Code available at:\n$\\href{https://github.com/mkleinma/B-cos-medical-paper}{GitHub repository}$.",
      "url": "http://arxiv.org/abs/2507.16761v1",
      "published_time_eastern_timestamp": 1753203362.0
    },
    {
      "title": "Never Come Up Empty: Adaptive HyDE Retrieval for Improving LLM Developer\n  Support",
      "summary": "Large Language Models (LLMs) have shown promise in assisting developers with\ncode-related questions; however, LLMs carry the risk of generating unreliable\nanswers. To address this, Retrieval-Augmented Generation (RAG) has been\nproposed to reduce the unreliability (i.e., hallucinations) of LLMs. However,\ndesigning effective pipelines remains challenging due to numerous design\nchoices. In this paper, we construct a retrieval corpus of over 3 million Java\nand Python related Stack Overflow posts with accepted answers, and explore\nvarious RAG pipeline designs to answer developer questions, evaluating their\neffectiveness in generating accurate and reliable responses. More specifically,\nwe (1) design and evaluate 7 different RAG pipelines and 63 pipeline variants\nto answer questions that have historically similar matches, and (2) address new\nquestions without any close prior matches by automatically lowering the\nsimilarity threshold during retrieval, thereby increasing the chance of finding\npartially relevant context and improving coverage for unseen cases. We find\nthat implementing a RAG pipeline combining hypothetical-documentation-embedding\n(HyDE) with the full-answer context performs best in retrieving and answering\nsimilarcontent for Stack Overflow questions. Finally, we apply our optimal RAG\npipeline to 4 open-source LLMs and compare the results to their zero-shot\nperformance. Our findings show that RAG with our optimal RAG pipeline\nconsistently outperforms zero-shot baselines across models, achieving higher\nscores for helpfulness, correctness, and detail with LLM-as-a-judge. These\nfindings demonstrate that our optimal RAG pipelines robustly enhance answer\nquality for a wide range of developer queries including both previously seen\nand novel questions across different LLMs",
      "url": "http://arxiv.org/abs/2507.16754v1",
      "published_time_eastern_timestamp": 1753202760.0
    },
    {
      "title": "Denoising-While-Completing Network (DWCNet): Robust Point Cloud\n  Completion Under Corruption",
      "summary": "Point cloud completion is crucial for 3D computer vision tasks in autonomous\ndriving, augmented reality, and robotics. However, obtaining clean and complete\npoint clouds from real-world environments is challenging due to noise and\nocclusions. Consequently, most existing completion networks -- trained on\nsynthetic data -- struggle with real-world degradations. In this work, we\ntackle the problem of completing and denoising highly corrupted partial point\nclouds affected by multiple simultaneous degradations. To benchmark robustness,\nwe introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which\nhighlights the limitations of current methods under diverse corruptions.\nBuilding on these insights, we propose DWCNet (Denoising-While-Completing\nNetwork), a completion framework enhanced with a Noise Management Module (NMM)\nthat leverages contrastive learning and self-attention to suppress noise and\nmodel structural relationships. DWCNet achieves state-of-the-art performance on\nboth clean and corrupted, synthetic and real-world datasets. The dataset and\ncode will be publicly available at\nhttps://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions",
      "url": "http://arxiv.org/abs/2507.16743v1",
      "published_time_eastern_timestamp": 1753202061.0
    },
    {
      "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
      "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.",
      "url": "http://arxiv.org/abs/2507.16725v1",
      "published_time_eastern_timestamp": 1753200492.0
    },
    {
      "title": "Enhancing Remote Sensing Vision-Language Models Through MLLM and\n  LLM-Based High-Quality Image-Text Dataset Generation",
      "summary": "The application of Vision-language foundation models (VLFMs) to remote\nsensing (RS) imagery has garnered significant attention due to their superior\ncapability in various downstream tasks. A key challenge lies in the scarcity of\nhigh-quality, large-scale, image-text paired training data. Recently, several\nworks introduced extensive image-text datasets for RS and trained their VLFMs.\nHowever, due to the rudimentary methods used for generating captions, the\nquality of datasets is suboptimal, requiring larger volumes of training data,\nwhile only yielding modest performance improvements. In this paper, we propose\na two-stage method named MpGI(Multi-Perspective Generation and Integration) for\ngenerating high-quality text captions for RS images. Firstly, we generate\ndistinct and detailed descriptions from different perspectives using\nRule-MLLM(Multimodal Large Language Model) Relay Generation and MLLMs\ngeneration methods. Next, we utilize Large Language Models (LLMs) to integrate\nthese diverse descriptions into comprehensive captions, capturing details from\nmultiple perspectives. Finally, we have created the HQRS-IT-210K dataset,\nincluding about 210,000 RS images and 1.3 million captions. We fine-tuned two\nVLFMs using our dataset: CLIP, a discriminative model, and CoCa, an\nimage-to-text generative model. This process resulted in our proposed HQRS-CLIP\nand RS-CoCa models. Experimental results demonstrate that HQRS-CLIP surpassed\nthe previous SOTA RS CLIP model in various downstream tasks while using only\n4.2\\% of the training data. RS-CoCa outperforms other advanced approaches\nacross benchmark datasets and can generate captions for RS images that rival or\neven exceed manual annotations. Dataset, pre-trained models, and codes will be\nreleased at https://github.com/YiguoHe/HQRS-210K-and-HQRS-CLIP.",
      "url": "http://arxiv.org/abs/2507.16716v1",
      "published_time_eastern_timestamp": 1753199693.0
    },
    {
      "title": "Error Detection Based on Generalized Successive Cancellation List\n  Decoding for Polar Codes",
      "summary": "Successive cancellation list (SCL) decoding has been widely adopted for polar\ncodes, which allows near maximum likelihood performance with sufficiently large\nlist size. In this work, we show that, if the list size is $2^\\gamma$, where\n$\\gamma$ is the fundamental quantity called mixing factor, then a modification\nto SCL decoding can implement Forney's generalized decoding rule. Hence, it\nprovides an efficient means to discard unreliable decisions. The performance\nachieved by short polar codes under the proposed generalized SCL decoding is\nanalyzed via Monte Carlo simulations.",
      "url": "http://arxiv.org/abs/2507.16699v1",
      "published_time_eastern_timestamp": 1753198438.0
    },
    {
      "title": "Linear codes arising from the point-hyperplane geometry -- Part II: the\n  twisted embedding",
      "summary": "Let $\\bar{\\Gamma}$ be the point-hyperplane geometry of a projective space\n$\\mathrm{PG(V)},$ where $V$ is a $(n+1)$-dimensional vector space over a finite\nfield $\\mathbb{F}_q$ of order $q.$ Suppose that $\\sigma$ is an automorphism of\n$\\mathbb{F}_q$ and consider the projective embedding $\\varepsilon_{\\sigma}$ of\n$\\bar{\\Gamma}$ into the projective space $\\mathrm{PG}(V\\otimes V^*)$ mapping\nthe point $([x],[\\xi])\\in \\bar{\\Gamma}$ to the projective point represented by\nthe pure tensor $x^{\\sigma}\\otimes \\xi$, with $\\xi(x)=0.$ In [I. Cardinali, L.\nGiuzzi, Linear codes arising from the point-hyperplane geometry -- part I: the\nSegre embedding (Jun. 2025). arXiv:2506.21309, doi:10.48550/ARXIV.2506.21309]\nwe focused on the case $\\sigma=1$ and we studied the projective code arising\nfrom the projective system $\\Lambda_1=\\varepsilon_{1}(\\bar{\\Gamma}).$ Here we\nfocus on the case $\\sigma\\not=1$ and we investigate the linear code ${\\mathcal\nC}(\\Lambda_{\\sigma})$ arising from the projective system\n$\\Lambda_{\\sigma}=\\varepsilon_{\\sigma}(\\bar{\\Gamma}).$ In particular, after\nhaving verified that $\\mathcal{C}( \\Lambda_{\\sigma})$ is a minimal code, we\ndetermine its parameters, its minimum distance as well as its automorphism\ngroup. We also give a (geometrical) characterization of its minimum and second\nlowest weight codewords and determine its maximum weight when $q$ and $n$ are\nboth odd.",
      "url": "http://arxiv.org/abs/2507.16694v1",
      "published_time_eastern_timestamp": 1753198222.0
    },
    {
      "title": "VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability\n  Prediction Models",
      "summary": "We present VulGuard, an automated tool designed to streamline the extraction,\nprocessing, and analysis of commits from GitHub repositories for Just-In-Time\nvulnerability prediction (JIT-VP) research. VulGuard automatically mines commit\nhistories, extracts fine-grained code changes, commit messages, and software\nengineering metrics, and formats them for downstream analysis. In addition, it\nintegrates several state-of-the-art vulnerability prediction models, allowing\nresearchers to train, evaluate, and compare models with minimal setup. By\nsupporting both repository-scale mining and model-level experimentation within\na unified framework, VulGuard addresses key challenges in reproducibility and\nscalability in software security research. VulGuard can also be easily\nintegrated into the CI/CD pipeline. We demonstrate the effectiveness of the\ntool in two influential open-source projects, FFmpeg and the Linux kernel,\nhighlighting its potential to accelerate real-world JIT-VP research and promote\nstandardized benchmarking. A demo video is available at:\nhttps://youtu.be/j96096-pxbs",
      "url": "http://arxiv.org/abs/2507.16685v1",
      "published_time_eastern_timestamp": 1753197524.0
    },
    {
      "title": "VulCoCo: A Simple Yet Effective Method for Detecting Vulnerable Code\n  Clones",
      "summary": "Code reuse is common in modern software development, but it can also spread\nvulnerabilities when developers unknowingly copy risky code. The code fragments\nthat preserve the logic of known vulnerabilities are known as vulnerable code\nclones (VCCs). Detecting those VCCs is a critical but challenging task.\nExisting VCC detection tools often rely on syntactic similarity or produce\ncoarse vulnerability predictions without clear explanations, limiting their\npractical utility. In this paper, we propose VulCoCo, a lightweight and\nscalable approach that combines embedding-based retrieval with large language\nmodel (LLM) validation. Starting from a set of known vulnerable functions, we\nretrieve syntactically or semantically similar candidate functions from a large\ncorpus and use an LLM to assess whether the candidates retain the\nvulnerability. Given that there is a lack of reproducible vulnerable code clone\nbenchmarks, we first construct a synthetic benchmark that spans various clone\ntypes.\n  Our experiments on the benchmark show that VulCoCo outperforms prior\nstate-of-the-art methods in terms of Precision@k and mean average precision\n(MAP). In addition, we also demonstrate VulCoCo's effectiveness in real-world\nprojects by submitting 400 pull requests (PRs) to 284 open-source projects.\nAmong them, 75 PRs were merged, and 15 resulted in newly published CVEs. We\nalso provide insights to inspire future work to further improve the precision\nof vulnerable code clone detection.",
      "url": "http://arxiv.org/abs/2507.16661v1",
      "published_time_eastern_timestamp": 1753196097.0
    },
    {
      "title": "Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and\n  Diverse Emotion Control",
      "summary": "Previous animatronic faces struggle to express emotions effectively due to\nhardware and software limitations. On the hardware side, earlier approaches\neither use rigid-driven mechanisms, which provide precise control but are\ndifficult to design within constrained spaces, or tendon-driven mechanisms,\nwhich are more space-efficient but challenging to control. In contrast, we\npropose a hybrid actuation approach that combines the best of both worlds. The\neyes and mouth-key areas for emotional expression-are controlled using rigid\nmechanisms for precise movement, while the nose and cheek, which convey subtle\nfacial microexpressions, are driven by strings. This design allows us to build\na compact yet versatile hardware platform capable of expressing a wide range of\nemotions. On the algorithmic side, our method introduces a self-modeling\nnetwork that maps motor actions to facial landmarks, allowing us to\nautomatically establish the relationship between blendshape coefficients for\ndifferent facial expressions and the corresponding motor control signals\nthrough gradient backpropagation. We then train a neural network to map speech\ninput to corresponding blendshape controls. With our method, we can generate\ndistinct emotional expressions such as happiness, fear, disgust, and anger,\nfrom any given sentence, each with nuanced, emotion-specific control signals-a\nfeature that has not been demonstrated in earlier systems. We release the\nhardware design and code at https://github.com/ZZongzheng0918/Morpheus-Hardware\nand https://github.com/ZZongzheng0918/Morpheus-Software.",
      "url": "http://arxiv.org/abs/2507.16645v1",
      "published_time_eastern_timestamp": 1753195369.0
    },
    {
      "title": "Benchmarking pig detection and tracking under diverse and challenging\n  conditions",
      "summary": "To ensure animal welfare and effective management in pig farming, monitoring\nindividual behavior is a crucial prerequisite. While monitoring tasks have\ntraditionally been carried out manually, advances in machine learning have made\nit possible to collect individualized information in an increasingly automated\nway. Central to these methods is the localization of animals across space\n(object detection) and time (multi-object tracking). Despite extensive research\nof these two tasks in pig farming, a systematic benchmarking study has not yet\nbeen conducted. In this work, we address this gap by curating two datasets:\nPigDetect for object detection and PigTrack for multi-object tracking. The\ndatasets are based on diverse image and video material from realistic barn\nconditions, and include challenging scenarios such as occlusions or bad\nvisibility. For object detection, we show that challenging training images\nimprove detection performance beyond what is achievable with randomly sampled\nimages alone. Comparing different approaches, we found that state-of-the-art\nmodels offer substantial improvements in detection quality over real-time\nalternatives. For multi-object tracking, we observed that SORT-based methods\nachieve superior detection performance compared to end-to-end trainable models.\nHowever, end-to-end models show better association performance, suggesting they\ncould become strong alternatives in the future. We also investigate\ncharacteristic failure cases of end-to-end models, providing guidance for\nfuture improvements. The detection and tracking models trained on our datasets\nperform well in unseen pens, suggesting good generalization capabilities. This\nhighlights the importance of high-quality training data. The datasets and\nresearch code are made publicly available to facilitate reproducibility, re-use\nand further development.",
      "url": "http://arxiv.org/abs/2507.16639v1",
      "published_time_eastern_timestamp": 1753195011.0
    },
    {
      "title": "A2Mamba: Attention-augmented State Space Models for Visual Recognition",
      "summary": "Transformers and Mamba, initially invented for natural language processing,\nhave inspired backbone architectures for visual recognition. Recent studies\nintegrated Local Attention Transformers with Mamba to capture both local\ndetails and global contexts. Despite competitive performance, these methods are\nlimited to simple stacking of Transformer and Mamba layers without any\ninteraction mechanism between them. Thus, deep integration between Transformer\nand Mamba layers remains an open problem. We address this problem by proposing\nA2Mamba, a powerful Transformer-Mamba hybrid network architecture, featuring a\nnew token mixer termed Multi-scale Attention-augmented State Space Model\n(MASS), where multi-scale attention maps are integrated into an\nattention-augmented SSM (A2SSM). A key step of A2SSM performs a variant of\ncross-attention by spatially aggregating the SSM's hidden states using the\nmulti-scale attention maps, which enhances spatial dependencies pertaining to a\ntwo-dimensional space while improving the dynamic modeling capabilities of\nSSMs. Our A2Mamba outperforms all previous ConvNet-, Transformer-, and\nMamba-based architectures in visual recognition tasks. For instance, A2Mamba-L\nachieves an impressive 86.1% top-1 accuracy on ImageNet-1K. In semantic\nsegmentation, A2Mamba-B exceeds CAFormer-S36 by 2.5% in mIoU, while exhibiting\nhigher efficiency. In object detection and instance segmentation with Cascade\nMask R-CNN, A2Mamba-S surpasses MambaVision-B by 1.2%/0.9% in AP^b/AP^m, while\nhaving 40% less parameters. Code is publicly available at\nhttps://github.com/LMMMEng/A2Mamba.",
      "url": "http://arxiv.org/abs/2507.16624v1",
      "published_time_eastern_timestamp": 1753193828.0
    },
    {
      "title": "Automatic Fine-grained Segmentation-assisted Report Generation",
      "summary": "Reliable end-to-end clinical report generation has been a longstanding goal\nof medical ML research. The end goal for this process is to alleviate\nradiologists' workloads and provide second opinions to clinicians or patients.\nThus, a necessary prerequisite for report generation models is a strong general\nperformance and some type of innate grounding capability, to convince\nclinicians or patients of the veracity of the generated reports. In this paper,\nwe present ASaRG (\\textbf{A}utomatic \\textbf{S}egmentation-\\textbf{a}ssisted\n\\textbf{R}eport \\textbf{G}eneration), an extension of the popular LLaVA\narchitecture that aims to tackle both of these problems. ASaRG proposes to fuse\nintermediate features and fine-grained segmentation maps created by specialist\nradiological models into LLaVA's multi-modal projection layer via simple\nconcatenation. With a small number of added parameters, our approach achieves a\n+0.89\\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA\nbaseline when using only intermediate features, and +2.77\\% performance gain\n($p<0.001$) when adding a combination of intermediate features and fine-grained\nsegmentation maps. Compared with COMG and ORID, two other report generation\nmethods that utilize segmentations, the performance gain amounts to 6.98\\% and\n6.28\\% in F1 score, respectively. ASaRG is not mutually exclusive with other\nchanges made to the LLaVA architecture, potentially allowing our method to be\ncombined with other advances in the field. Finally, the use of an arbitrary\nnumber of segmentations as part of the input demonstrably allows tracing\nelements of the report to the corresponding segmentation maps and verifying the\ngroundedness of assessments. Our code will be made publicly available at a\nlater date.",
      "url": "http://arxiv.org/abs/2507.16623v1",
      "published_time_eastern_timestamp": 1753193780.0
    },
    {
      "title": "Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian\n  Representation",
      "summary": "Accurate analysis of cardiac motion is crucial for evaluating cardiac\nfunction. While dynamic cardiac magnetic resonance imaging (CMR) can capture\ndetailed tissue motion throughout the cardiac cycle, the fine-grained 4D\ncardiac motion tracking remains challenging due to the homogeneous nature of\nmyocardial tissue and the lack of distinctive features. Existing approaches can\nbe broadly categorized into image based and representation-based, each with its\nlimitations. Image-based methods, including both raditional and deep\nlearning-based registration approaches, either struggle with topological\nconsistency or rely heavily on extensive training data. Representation-based\nmethods, while promising, often suffer from loss of image-level details. To\naddress these limitations, we propose Dynamic 3D Gaussian Representation\n(Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation\nwith implicit neural motion field modeling. Our method simultaneously optimizes\ncardiac structure and motion in a self-supervised manner, eliminating the need\nfor extensive training data or point-to-point correspondences. Through\ndifferentiable volumetric rendering, Dyna3DGR efficiently bridges continuous\nmotion representation with image-space alignment while preserving both\ntopological and temporal consistency. Comprehensive evaluations on the ACDC\ndataset demonstrate that our approach surpasses state-of-the-art deep\nlearning-based diffeomorphic registration methods in tracking accuracy. The\ncode will be available in https://github.com/windrise/Dyna3DGR.",
      "url": "http://arxiv.org/abs/2507.16608v1",
      "published_time_eastern_timestamp": 1753193210.0
    }
  ]
}
{
  "last_updated": "2025-05-20T02:17:46.215322-04:00",
  "papers": [
    {
      "title": "CIE: Controlling Language Model Text Generations Using Continuous\n  Signals",
      "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.",
      "url": "http://arxiv.org/abs/2505.13448v1",
      "published_time_eastern_timestamp": 1747677598.0
    },
    {
      "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement\n  Learning with Verifiable Rewards",
      "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.",
      "url": "http://arxiv.org/abs/2505.13445v1",
      "published_time_eastern_timestamp": 1747677571.0
    },
    {
      "title": "GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale\n  Synthetic Data Generation",
      "summary": "We present GrasMolmo, a generalizable open-vocabulary task-oriented grasping\n(TOG) model. GraspMolmo predicts semantically appropriate, stable grasps\nconditioned on a natural language instruction and a single RGB-D frame. For\ninstance, given \"pour me some tea\", GraspMolmo selects a grasp on a teapot\nhandle rather than its body. Unlike prior TOG methods, which are limited by\nsmall datasets, simplistic language, and uncluttered scenes, GraspMolmo learns\nfrom PRISM, a novel large-scale synthetic dataset of 379k samples featuring\ncluttered environments and diverse, realistic task descriptions. We fine-tune\nthe Molmo visual-language model on this data, enabling GraspMolmo to generalize\nto novel open-vocabulary instructions and objects. In challenging real-world\nevaluations, GraspMolmo achieves state-of-the-art results, with a 70%\nprediction success on complex tasks, compared to the 35% achieved by the next\nbest alternative. GraspMolmo also successfully demonstrates the ability to\npredict semantically correct bimanual grasps zero-shot. We release our\nsynthetic dataset, code, model, and benchmarks to accelerate research in\ntask-semantic robotic manipulation, which, along with videos, are available at\nhttps://abhaybd.github.io/GraspMolmo/.",
      "url": "http://arxiv.org/abs/2505.13441v1",
      "published_time_eastern_timestamp": 1747677546.0
    },
    {
      "title": "Recollection from Pensieve: Novel View Synthesis via Learning from\n  Uncalibrated Videos",
      "summary": "Currently almost all state-of-the-art novel view synthesis and reconstruction\nmodels rely on calibrated cameras or additional geometric priors for training.\nThese prerequisites significantly limit their applicability to massive\nuncalibrated data. To alleviate this requirement and unlock the potential for\nself-supervised training on large-scale uncalibrated videos, we propose a novel\ntwo-stage strategy to train a view synthesis model from only raw video frames\nor multi-view images, without providing camera parameters or other priors. In\nthe first stage, we learn to reconstruct the scene implicitly in a latent space\nwithout relying on any explicit 3D representation. Specifically, we predict\nper-frame latent camera and scene context features, and employ a view synthesis\nmodel as a proxy for explicit rendering. This pretraining stage substantially\nreduces the optimization complexity and encourages the network to learn the\nunderlying 3D consistency in a self-supervised manner. The learned latent\ncamera and implicit scene representation have a large gap compared with the\nreal 3D world. To reduce this gap, we introduce the second stage training by\nexplicitly predicting 3D Gaussian primitives. We additionally apply explicit\nGaussian Splatting rendering loss and depth projection loss to align the\nlearned latent representations with physically grounded 3D geometry. In this\nway, Stage 1 provides a strong initialization and Stage 2 enforces 3D\nconsistency - the two stages are complementary and mutually beneficial.\nExtensive experiments demonstrate the effectiveness of our approach, achieving\nhigh-quality novel view synthesis and accurate camera pose estimation, compared\nto methods that employ supervision with calibration, pose, or depth\ninformation. The code is available at https://github.com/Dwawayu/Pensieve.",
      "url": "http://arxiv.org/abs/2505.13440v1",
      "published_time_eastern_timestamp": 1747677545.0
    },
    {
      "title": "Understanding Complexity in VideoQA via Visual Program Generation",
      "summary": "We propose a data-driven approach to analyzing query complexity in Video\nQuestion Answering (VideoQA). Previous efforts in benchmark design have relied\non human expertise to design challenging questions, yet we experimentally show\nthat humans struggle to predict which questions are difficult for machine\nlearning models. Our automatic approach leverages recent advances in code\ngeneration for visual question answering, using the complexity of generated\ncode as a proxy for question difficulty. We demonstrate that this measure\ncorrelates significantly better with model performance than human estimates. To\noperationalize this insight, we propose an algorithm for estimating question\ncomplexity from code. It identifies fine-grained primitives that correlate with\nthe hardest questions for any given set of models, making it easy to scale to\nnew approaches in the future. Finally, to further illustrate the utility of our\nmethod, we extend it to automatically generate complex questions, constructing\na new benchmark that is 1.9 times harder than the popular NExT-QA.",
      "url": "http://arxiv.org/abs/2505.13429v1",
      "published_time_eastern_timestamp": 1747677314.0
    },
    {
      "title": "MM-PRM: Enhancing Multimodal Mathematical Reasoning with Scalable\n  Step-Level Supervision",
      "summary": "While Multimodal Large Language Models (MLLMs) have achieved impressive\nprogress in vision-language understanding, they still struggle with complex\nmulti-step reasoning, often producing logically inconsistent or partially\ncorrect solutions. A key limitation lies in the lack of fine-grained\nsupervision over intermediate reasoning steps. To address this, we propose\nMM-PRM, a process reward model trained within a fully automated, scalable\nframework. We first build MM-Policy, a strong multimodal model trained on\ndiverse mathematical reasoning data. Then, we construct MM-K12, a curated\ndataset of 10,000 multimodal math problems with verifiable answers, which\nserves as seed data. Leveraging a Monte Carlo Tree Search (MCTS)-based\npipeline, we generate over 700k step-level annotations without human labeling.\nThe resulting PRM is used to score candidate reasoning paths in the Best-of-N\ninference setup and achieves significant improvements across both in-domain\n(MM-K12 test set) and out-of-domain (OlympiadBench, MathVista, etc.)\nbenchmarks. Further analysis confirms the effectiveness of soft labels, smaller\nlearning rates, and path diversity in optimizing PRM performance. MM-PRM\ndemonstrates that process supervision is a powerful tool for enhancing the\nlogical robustness of multimodal reasoning systems. We release all our codes\nand data at https://github.com/ModalMinds/MM-PRM.",
      "url": "http://arxiv.org/abs/2505.13427v1",
      "published_time_eastern_timestamp": 1747677308.0
    },
    {
      "title": "G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language\n  Model via Reinforcement Learning",
      "summary": "Vision-Language Models (VLMs) excel in many direct multimodal tasks but\nstruggle to translate this prowess into effective decision-making within\ninteractive, visually rich environments like games. This ``knowing-doing'' gap\nsignificantly limits their potential as autonomous agents, as leading VLMs\noften performing badly in simple games. To address this, we introduce VLM-Gym,\na curated reinforcement learning (RL) environment featuring diverse visual\ngames with unified interfaces and adjustable, compositional difficulty,\nspecifically designed for scalable multi-game parallel training. Leveraging\nVLM-Gym, we train G0 models using pure RL-driven self-evolution, which\ndemonstrate emergent perception and reasoning patterns. To further mitigate\nchallenges arising from game diversity, we develop G1 models. G1 incorporates a\nperception-enhanced cold start prior to RL fine-tuning. Our resulting G1 models\nconsistently surpass their teacher across all games and outperform leading\nproprietary models like Claude-3.7-Sonnet-Thinking. Systematic analysis reveals\nan intriguing finding: perception and reasoning abilities mutually bootstrap\neach other throughout the RL training process. Source code including VLM-Gym\nand RL training are released at https://github.com/chenllliang/G1 to foster\nfuture research in advancing VLMs as capable interactive agents.",
      "url": "http://arxiv.org/abs/2505.13426v1",
      "published_time_eastern_timestamp": 1747677279.0
    },
    {
      "title": "FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language\n  Models with Emotional Synergy and Reasoning",
      "summary": "Facial Emotion Analysis (FEA) plays a crucial role in visual affective\ncomputing, aiming to infer a person's emotional state based on facial data.\nScientifically, facial expressions (FEs) result from the coordinated movement\nof facial muscles, which can be decomposed into specific action units (AUs)\nthat provide detailed emotional insights. However, traditional methods often\nstruggle with limited interpretability, constrained generalization and\nreasoning abilities. Recently, Multimodal Large Language Models (MLLMs) have\nshown exceptional performance in various visual tasks, while they still face\nsignificant challenges in FEA due to the lack of specialized datasets and their\ninability to capture the intricate relationships between FEs and AUs. To\naddress these issues, we introduce a novel FEA Instruction Dataset that\nprovides accurate and aligned FE and AU descriptions and establishes causal\nreasoning relationships between them, followed by constructing a new benchmark,\nFEABench. Moreover, we propose FEALLM, a novel MLLM architecture designed to\ncapture more detailed facial information, enhancing its capability in FEA\ntasks. Our model demonstrates strong performance on FEABench and impressive\ngeneralization capability through zero-shot evaluation on various datasets,\nincluding RAF-DB, AffectNet, BP4D, and DISFA, showcasing its robustness and\neffectiveness in FEA tasks. The dataset and code will be available at\nhttps://github.com/953206211/FEALLM.",
      "url": "http://arxiv.org/abs/2505.13419v1",
      "published_time_eastern_timestamp": 1747677135.0
    },
    {
      "title": "AdaptThink: Reasoning Models Can Learn When to Think",
      "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.",
      "url": "http://arxiv.org/abs/2505.13417v1",
      "published_time_eastern_timestamp": 1747677052.0
    },
    {
      "title": "Insufficient evidence for DMS and DMDS in the atmosphere of K2-18 b.\n  From a joint analysis of JWST NIRISS, NIRSpec, and MIRI observations",
      "summary": "Recent JWST observations of the temperate sub-Neptune K2-18 b have been\ninterpreted as suggestive of a liquid water ocean with possible biological\nactivity. Signatures of DMS and DMDS have been claimed in the near-infrared\n(using the NIRISS and NIRSpec instruments) and mid-infrared (using MIRI).\nHowever, the statistical significance of the atmospheric imprints of these\npotential biomarkers has yet to be quantified from a joint analysis of the\nentire planet spectrum. We test the robustness of the proposed DMS/DMDS\ndetections by simultaneously modeling the NIRISS and NIRSpec observations\njointly with the MIRI spectrum, considering different data reductions and\nmodeling choices. We use three well-tested pipelines to re-reduce the JWST\nobservations, and two retrieval codes to analyze the resulting transmission\nspectra as well as previously published data. Our joint analysis of the\npanchromatic (0.6 - 12 um) spectrum of K2-18 b finds insufficient evidence for\nthe presence of DMS and/or DMDS in the atmosphere of the planet. Furthermore,\nother molecules containing methyl functional groups (e.g., ethane) with\nabsorption bands similar to DMS/DMDS provide an equally good fit to the data.\nWe find that any marginal preferences are the result of limiting the number of\nmolecules considered in the model and oversensitivity to small changes between\ndata reductions. Our results confirm that there is no statistical significance\nfor DMS or DMDS in K2-18 b's atmosphere. While previous works have demonstrated\nthis on MIRI or NIRISS/NIRSpec observations alone, our analysis of the full\ntransmission spectrum does not support claims of potential biomarkers. Using\nthe best-fitting model including DMS/DMDS on the published data, we estimate\nthat ~25 more MIRI transits would be needed for a 3-sigma rejection of a flat\nline relative to DMS/DMDS features in the planet's mid-infrared transmission\nspectrum.",
      "url": "http://arxiv.org/abs/2505.13407v1",
      "published_time_eastern_timestamp": 1747676493.0
    },
    {
      "title": "R3: Robust Rubric-Agnostic Reward Models",
      "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3",
      "url": "http://arxiv.org/abs/2505.13388v1",
      "published_time_eastern_timestamp": 1747675743.0
    },
    {
      "title": "Thinkless: LLM Learns When to Think",
      "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless",
      "url": "http://arxiv.org/abs/2505.13379v1",
      "published_time_eastern_timestamp": 1747675456.0
    },
    {
      "title": "The SXS Collaboration's third catalog of binary black hole simulations",
      "summary": "We present a major update to the Simulating eXtreme Spacetimes (SXS)\nCollaboration's catalog of binary black hole simulations. Using highly\nefficient spectral methods implemented in the Spectral Einstein Code (SpEC), we\nhave nearly doubled the total number of binary configurations from 2,018 to\n3,756. The catalog now densely covers the parameter space with precessing\nsimulations up to mass ratio $q=8$ and dimensionless spins up to\n$|\\vec{\\chi}|\\le0.8$ with near-zero eccentricity. The catalog also includes\nsome simulations at higher mass ratios with moderate spin and more than 250\neccentric simulations. We have also deprecated and rerun some simulations from\nour previous catalog (e.g., simulations run with a much older version of SpEC\nor that had anomalously high errors in the waveform). The median waveform\ndifference (which is similar to the mismatch) between resolutions over the\nsimulations in the catalog is $4\\times10^{-4}$. The simulations have a median\nof 22 orbits, while the longest simulation has 148 orbits. We have corrected\neach waveform in the catalog to be in the binary's center-of-mass frame and\nexhibit gravitational-wave memory. We estimate the total CPU cost of all\nsimulations in the catalog to be 480,000,000 core-hours. We find that using\nspectral methods for binary black hole simulations is over 1,000 times more\nefficient than much shorter finite-difference simulations of comparable\naccuracy. The full catalog is publicly available through the sxs Python package\nand at https://data.black-holes.org .",
      "url": "http://arxiv.org/abs/2505.13378v1",
      "published_time_eastern_timestamp": 1747675314.0
    },
    {
      "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman\n  Modeling",
      "summary": "Diffusion-based generative models have demonstrated exceptional performance,\nyet their iterative sampling procedures remain computationally expensive. A\nprominent strategy to mitigate this cost is distillation, with offline\ndistillation offering particular advantages in terms of efficiency, modularity,\nand flexibility. In this work, we identify two key observations that motivate a\nprincipled distillation framework: (1) while diffusion models have been viewed\nthrough the lens of dynamical systems theory, powerful and underexplored tools\ncan be further leveraged; and (2) diffusion models inherently impose\nstructured, semantically coherent trajectories in latent space. Building on\nthese observations, we introduce the Koopman Distillation Model KDM, a novel\noffline distillation approach grounded in Koopman theory-a classical framework\nfor representing nonlinear dynamics linearly in a transformed space. KDM\nencodes noisy inputs into an embedded space where a learned linear operator\npropagates them forward, followed by a decoder that reconstructs clean samples.\nThis enables single-step generation while preserving semantic fidelity. We\nprovide theoretical justification for our approach: (1) under mild assumptions,\nthe learned diffusion dynamics admit a finite-dimensional Koopman\nrepresentation; and (2) proximity in the Koopman latent space correlates with\nsemantic similarity in the generated outputs, allowing for effective trajectory\nalignment. Empirically, KDM achieves state-of-the-art performance across\nstandard offline distillation benchmarks, improving FID scores by up to 40% in\na single generation step. All implementation details and code for the\nexperimental setups are provided in our GitHub -\nhttps://github.com/azencot-group/KDM, or in our project page -\nhttps://sites.google.com/view/koopman-distillation-model.",
      "url": "http://arxiv.org/abs/2505.13358v1",
      "published_time_eastern_timestamp": 1747673987.0
    },
    {
      "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on\n  Long Context Code Reasoning",
      "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation.",
      "url": "http://arxiv.org/abs/2505.13353v1",
      "published_time_eastern_timestamp": 1747673791.0
    },
    {
      "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative\n  Preference Optimization",
      "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.",
      "url": "http://arxiv.org/abs/2505.13346v1",
      "published_time_eastern_timestamp": 1747673435.0
    },
    {
      "title": "Occult: Optimizing Collaborative Communication across Experts for\n  Accelerated Parallel MoE Training and Inference",
      "summary": "Mixture-of-experts (MoE) architectures could achieve impressive computational\nefficiency with expert parallelism, which relies heavily on all-to-all\ncommunication across devices. Unfortunately, such communication overhead\ntypically constitutes a significant portion of the total runtime, hampering the\nscalability of distributed training and inference for modern MoE models\n(consuming over $40\\%$ runtime in large-scale training). In this paper, we\nfirst define collaborative communication to illustrate this intrinsic\nlimitation, and then propose system- and algorithm-level innovations to reduce\ncommunication costs. Specifically, given a pair of experts co-activated by one\ntoken, we call them \"collaborated\", which comprises $2$ cases as intra- and\ninter-collaboration, depending on whether they are kept on the same device. Our\npilot investigations reveal that augmenting the proportion of\nintra-collaboration can accelerate expert parallelism at scale. It motivates us\nto strategically optimize collaborative communication for accelerated MoE\ntraining and inference, dubbed Occult. Our designs are capable of either\ndelivering exact results with reduced communication cost or controllably\nminimizing the cost with collaboration pruning, materialized by modified\nfine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that\nOccult can be faster than popular state-of-the-art inference or training\nframeworks (more than $1.5\\times$ speed up across multiple tasks and models)\nwith comparable or superior quality compared to the standard fine-tuning. Code\nis available at\n$\\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$.",
      "url": "http://arxiv.org/abs/2505.13345v1",
      "published_time_eastern_timestamp": 1747673427.0
    },
    {
      "title": "SVAFD: A Secure and Verifiable Co-Aggregation Protocol for Federated\n  Distillation",
      "summary": "Secure Aggregation (SA) is an indispensable component of Federated Learning\n(FL) that concentrates on privacy preservation while allowing for robust\naggregation. However, most SA designs rely heavily on the unrealistic\nassumption of homogeneous model architectures. Federated Distillation (FD),\nwhich aggregates locally computed logits instead of model parameters,\nintroduces a promising alternative for cooperative training in heterogeneous\nmodel settings. Nevertheless, we recognize two major challenges in implementing\nSA for FD. (i) Prior SA designs encourage a dominant server, who is solely\nresponsible for collecting, aggregating and distributing. Such central\nauthority facilitates server to forge aggregation proofs or collude to bypass\nthe claimed security guarantees; (ii) Existing SA, tailored for FL models,\noverlook the intrinsic properties of logits, making them unsuitable for FD.\n  To address these challenges, we propose SVAFD, the first SA protocol that is\nspecifically designed for FD. At a high level, SVAFD incorporates two\ninnovations: (i) a multilateral co-aggregation method tha redefines the\nresponsibilities of clients and server. Clients autonomously evaluate and\naggregate logits shares locally with a lightweight coding scheme, while the\nserver handles ciphertext decoding and performs the task of generating\nverification proofs; (ii) a quality-aware knowledge filtration method that\nfacilitates biased logits exclusion against poisoning attacks. Moreover, SVAFD\nis resilient to stragglers and colluding clients, making it well-suited for\ndynamic networks in real-world applications. We have implemented the SVAFD\nprototype over four emerging FD architectures and evaluated it against\npoisoning and inference attacks. Results demonstrate that SVAFD improves model\naccuracy, making it a significant step forward in secure and verifiable\naggregation for heterogeneous FL systems.",
      "url": "http://arxiv.org/abs/2505.13319v1",
      "published_time_eastern_timestamp": 1747672227.0
    },
    {
      "title": "VesselGPT: Autoregressive Modeling of Vascular Geometry",
      "summary": "Anatomical trees are critical for clinical diagnosis and treatment planning,\nyet their complex and diverse geometry make accurate representation a\nsignificant challenge. Motivated by the latest advances in large language\nmodels, we introduce an autoregressive method for synthesizing anatomical\ntrees. Our approach first embeds vessel structures into a learned discrete\nvocabulary using a VQ-VAE architecture, then models their generation\nautoregressively with a GPT-2 model. This method effectively captures intricate\ngeometries and branching patterns, enabling realistic vascular tree synthesis.\nComprehensive qualitative and quantitative evaluations reveal that our\ntechnique achieves high-fidelity tree reconstruction with compact discrete\nrepresentations. Moreover, our B-spline representation of vessel cross-sections\npreserves critical morphological details that are often overlooked in previous'\nmethods parameterizations. To the best of our knowledge, this work is the first\nto generate blood vessels in an autoregressive manner. Code, data, and trained\nmodels will be made available.",
      "url": "http://arxiv.org/abs/2505.13318v1",
      "published_time_eastern_timestamp": 1747672226.0
    },
    {
      "title": "Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning\n  and Pretrain-Finetuning",
      "summary": "Semi-supervised learning (SSL) alleviates the cost of data labeling process\nby exploiting unlabeled data, and has achieved promising results on various\ntasks such as image classification. Meanwhile, the Pretrain-Finetuning paradigm\nhas garnered significant attention in recent years, and exploiting pre-trained\nmodels could also reduce the requirement of labeled data in downstream tasks.\nTherefore, a question naturally occurs: \\emph{When the labeled data is scarce\nin the target tasks, should we exploit unlabeled data or pre-trained models?}\nTo answer this question, we select pre-trained Vision-Language Models (VLMs) as\nrepresentative pretrain-finetuning instances and propose \\textit{Few-shot SSL}\n-- a framework that enables fair comparison between these two paradigms by\ncontrolling the amount of labeled data used. Extensive experiments across\nvarious settings demonstrate that pre-trained VLMs generally outperform SSL\nmethods in nearly all cases, except when the data has low resolution or lacks\nclear semantic structure. Therefore, we encourage future SSL research to\ncompare with pre-trained models and explore deeper integration, such as using\npre-trained knowledge to enhance pseudo-labeling. To support future research,\nwe release our unified reproduction and evaluation framework. Codes are\navailable at\nhttps://anonymous.4open.science/r/Rethinking-SSL-and-Pretrain-Finetuning-5566",
      "url": "http://arxiv.org/abs/2505.13317v1",
      "published_time_eastern_timestamp": 1747672160.0
    }
  ]
}
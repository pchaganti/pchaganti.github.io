{
  "last_updated": "2025-10-15T20:55:31.273622-04:00",
  "papers": [
    {
      "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
      "summary": "Existing Multimodal Large Language Models (MLLMs) suffer from increased\ninference costs due to the additional vision tokens introduced by image inputs.\nIn this work, we propose Visual Consistency Learning (ViCO), a novel training\nalgorithm that enables the model to represent images of varying semantic\ncomplexities using different numbers of vision tokens. The key idea behind our\nmethod is to employ multiple MLP connectors, each with a different image\ncompression ratio, to downsample the vision tokens based on the semantic\ncomplexity of the image. During training, we minimize the KL divergence between\nthe responses conditioned on different MLP connectors. At inference time, we\nintroduce an image router, termed Visual Resolution Router (ViR), that\nautomatically selects the appropriate compression rate for each image patch.\nCompared with existing dynamic high-resolution strategies, which adjust the\nnumber of visual tokens based on image resolutions, our method dynamically\nadapts the number of visual tokens according to semantic complexity.\nExperimental results demonstrate that our method can reduce the number of\nvision tokens by up to 50% while maintaining the model's perception, reasoning,\nand OCR capabilities. We hope this work will contribute to the development of\nmore efficient MLLMs. The code and models will be released to facilitate future\nresearch.",
      "url": "http://arxiv.org/abs/2510.12793v1",
      "published_time_eastern_timestamp": 1760464690.0
    },
    {
      "title": "Wavefront Coding for Accommodation-Invariant Near-Eye Displays",
      "summary": "We present a new computational near-eye display method that addresses the\nvergence-accommodation conflict problem in stereoscopic displays through\naccommodation-invariance. Our system integrates a refractive lens eyepiece with\na novel wavefront coding diffractive optical element, operating in tandem with\na pre-processing convolutional neural network. We employ end-to-end learning to\njointly optimize the wavefront-coding optics and the image pre-processing\nmodule. To implement this approach, we develop a differentiable retinal image\nformation model that accounts for limiting aperture and chromatic aberrations\nintroduced by the eye optics. We further integrate the neural transfer function\nand the contrast sensitivity function into the loss model to account for\nrelated perceptual effects. To tackle off-axis distortions, we incorporate\nposition dependency into the pre-processing module. In addition to conducting\nrigorous analysis based on simulations, we also fabricate the designed\ndiffractive optical element and build a benchtop setup, demonstrating\naccommodation-invariance for depth ranges of up to four diopters.",
      "url": "http://arxiv.org/abs/2510.12778v1",
      "published_time_eastern_timestamp": 1760464348.0
    },
    {
      "title": "What If : Understanding Motion Through Sparse Interactions",
      "summary": "Understanding the dynamics of a physical scene involves reasoning about the\ndiverse ways it can potentially change, especially as a result of local\ninteractions. We present the Flow Poke Transformer (FPT), a novel framework for\ndirectly predicting the distribution of local motion, conditioned on sparse\ninteractions termed \"pokes\". Unlike traditional methods that typically only\nenable dense sampling of a single realization of scene dynamics, FPT provides\nan interpretable directly accessible representation of multi-modal scene\nmotion, its dependency on physical interactions and the inherent uncertainties\nof scene dynamics. We also evaluate our model on several downstream tasks to\nenable comparisons with prior methods and highlight the flexibility of our\napproach. On dense face motion generation, our generic pre-trained model\nsurpasses specialized baselines. FPT can be fine-tuned in strongly\nout-of-distribution tasks such as synthetic datasets to enable significant\nimprovements over in-domain methods in articulated object motion estimation.\nAdditionally, predicting explicit motion distributions directly enables our\nmethod to achieve competitive performance on tasks like moving part\nsegmentation from pokes which further demonstrates the versatility of our FPT.\nCode and models are publicly available at\nhttps://compvis.github.io/flow-poke-transformer.",
      "url": "http://arxiv.org/abs/2510.12777v1",
      "published_time_eastern_timestamp": 1760464337.0
    },
    {
      "title": "PET Head Motion Estimation Using Supervised Deep Learning with Attention",
      "summary": "Head movement poses a significant challenge in brain positron emission\ntomography (PET) imaging, resulting in image artifacts and tracer uptake\nquantification inaccuracies. Effective head motion estimation and correction\nare crucial for precise quantitative image analysis and accurate diagnosis of\nneurological disorders. Hardware-based motion tracking (HMT) has limited\napplicability in real-world clinical practice. To overcome this limitation, we\npropose a deep-learning head motion correction approach with cross-attention\n(DL-HMC++) to predict rigid head motion from one-second 3D PET raw data.\nDL-HMC++ is trained in a supervised manner by leveraging existing dynamic PET\nscans with gold-standard motion measurements from external HMT. We evaluate\nDL-HMC++ on two PET scanners (HRRT and mCT) and four radiotracers (18F-FDG,\n18F-FPEB, 11C-UCB-J, and 11C-LSN3172176) to demonstrate the effectiveness and\ngeneralization of the approach in large cohort PET studies. Quantitative and\nqualitative results demonstrate that DL-HMC++ consistently outperforms\nstate-of-the-art data-driven motion estimation methods, producing motion-free\nimages with clear delineation of brain structures and reduced motion artifacts\nthat are indistinguishable from gold-standard HMT. Brain region of interest\nstandard uptake value analysis exhibits average difference ratios between\nDL-HMC++ and gold-standard HMT to be 1.2 plus-minus 0.5% for HRRT and 0.5\nplus-minus 0.2% for mCT. DL-HMC++ demonstrates the potential for data-driven\nPET head motion correction to remove the burden of HMT, making motion\ncorrection accessible to clinical populations beyond research settings. The\ncode is available at https://github.com/maxxxxxxcai/DL-HMC-TMI.",
      "url": "http://arxiv.org/abs/2510.12758v1",
      "published_time_eastern_timestamp": 1760463432.0
    },
    {
      "title": "A High-Level Feature Model to Predict the Encoding Energy of a Hardware\n  Video Encoder",
      "summary": "In today's society, live video streaming and user generated content streamed\nfrom battery powered devices are ubiquitous. Live streaming requires real-time\nvideo encoding, and hardware video encoders are well suited for such an\nencoding task. In this paper, we introduce a high-level feature model using\nGaussian process regression that can predict the encoding energy of a hardware\nvideo encoder. In an evaluation setup restricted to only P-frames and a single\nkeyframe, the model can predict the encoding energy with a mean absolute\npercentage error of approximately 9%. Further, we demonstrate with an ablation\nstudy that spatial resolution is a key high-level feature for encoding energy\nprediction of a hardware encoder. A practical application of our model is that\nit can be used to perform a prior estimation of the energy required to encode a\nvideo at various spatial resolutions, with different coding standards and codec\npresets.",
      "url": "http://arxiv.org/abs/2510.12754v1",
      "published_time_eastern_timestamp": 1760463225.0
    },
    {
      "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video\n  Super-Resolution",
      "summary": "Diffusion models have recently advanced video restoration, but applying them\nto real-world video super-resolution (VSR) remains challenging due to high\nlatency, prohibitive computation, and poor generalization to ultra-high\nresolutions. Our goal in this work is to make diffusion-based VSR practical by\nachieving efficiency, scalability, and real-time performance. To this end, we\npropose FlashVSR, the first diffusion-based one-step streaming framework\ntowards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408\nvideos on a single A100 GPU by combining three complementary innovations: (i) a\ntrain-friendly three-stage distillation pipeline that enables streaming\nsuper-resolution, (ii) locality-constrained sparse attention that cuts\nredundant computation while bridging the train-test resolution gap, and (iii) a\ntiny conditional decoder that accelerates reconstruction without sacrificing\nquality. To support large-scale training, we also construct VSR-120K, a new\ndataset with 120k videos and 180k images. Extensive experiments show that\nFlashVSR scales reliably to ultra-high resolutions and achieves\nstate-of-the-art performance with up to 12x speedup over prior one-step\ndiffusion VSR models. We will release the code, pretrained models, and dataset\nto foster future research in efficient diffusion-based VSR.",
      "url": "http://arxiv.org/abs/2510.12747v1",
      "published_time_eastern_timestamp": 1760462754.0
    },
    {
      "title": "Hey, wait a minute: on at-issue sensitivity in Language Models",
      "summary": "Evaluating the naturalness of dialogue in language models (LMs) is not\ntrivial: notions of 'naturalness' vary, and scalable quantitative metrics\nremain limited. This study leverages the linguistic notion of 'at-issueness' to\nassess dialogue naturalness and introduces a new method: Divide, Generate,\nRecombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)\ngenerates continuations for subparts using LMs, (iii) recombines the dialogue\nand continuations, and (iv) compares the likelihoods of the recombined\nsequences. This approach mitigates bias in linguistic analyses of LMs and\nenables systematic testing of discourse-sensitive behavior. Applying DGRC, we\nfind that LMs prefer to continue dialogue on at-issue content, with this effect\nenhanced in instruct-tuned models. They also reduce their at-issue preference\nwhen relevant cues (e.g., \"Hey, wait a minute\") are present. Although\ninstruct-tuning does not further amplify this modulation, the pattern reflects\na hallmark of successful dialogue dynamics.",
      "url": "http://arxiv.org/abs/2510.12740v1",
      "published_time_eastern_timestamp": 1760462240.0
    },
    {
      "title": "Clutch Control: An Attention-based Combinatorial Bandit for Efficient\n  Mutation in JavaScript Engine Fuzzing",
      "summary": "JavaScript engines are widely used in web browsers, PDF readers, and\nserver-side applications. The rise in concern over their security has led to\nthe development of several targeted fuzzing techniques. However, existing\napproaches use random selection to determine where to perform mutations in\nJavaScript code. We postulate that the problem of selecting better mutation\ntargets is suitable for combinatorial bandits with a volatile number of arms.\nThus, we propose CLUTCH, a novel deep combinatorial bandit that can observe\nvariable length JavaScript test case representations, using an attention\nmechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can\ndynamically adapt its exploration. We show that CLUTCH increases efficiency in\nJavaScript fuzzing compared to three state-of-the-art solutions by increasing\nthe number of valid test cases and coverage-per-testcase by, respectively,\n20.3% and 8.9% on average. In volatile and combinatorial settings we show that\nCLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%\nless regret in volatile and combinatorial settings, respectively.",
      "url": "http://arxiv.org/abs/2510.12732v1",
      "published_time_eastern_timestamp": 1760461851.0
    },
    {
      "title": "plasmonX: an Open-Source Code for Nanoplasmonics",
      "summary": "We present the first public release of plasmonX, a novel open-source code for\nsimulating the plasmonic response of complex nanostructures. The code supports\nboth fully atomistic and implicit descriptions of nanomaterials. In particular,\nit employs the frequency-dependent fluctuating charges ($\\omega$FQ) and dipoles\n($\\omega$FQF$\\mu$) models to describe the response properties of atomistic\nstructures, including simple and $d$-metals, graphene-based structures, and\nmulti-metal nanostructures. For implicit representations, the Boundary Element\nMethod is implemented in both the dielectric polarizable continuum model (DPCM)\nand integral equation formalism (IEF-PCM) variants. The distribution also\nincludes a post-processing module that enables analysis of electric\nfield-induced properties such as charge density and electric field patterns.",
      "url": "http://arxiv.org/abs/2510.12731v1",
      "published_time_eastern_timestamp": 1760461844.0
    },
    {
      "title": "Beyond Postconditions: Can Large Language Models infer Formal Contracts\n  for Automatic Software Verification?",
      "summary": "Automatic software verifiers have become increasingly effective at the task\nof checking software against (formal) specifications. Yet, their adoption in\npractice has been hampered by the lack of such specifications in real world\ncode. Large Language Models (LLMs) have shown promise in inferring formal\npostconditions from natural language hints embedded in code such as function\nnames, comments or documentation. Using the generated postconditions as\nspecifications in a subsequent verification, however, often leads verifiers to\nsuggest invalid inputs, hinting at potential issues that ultimately turn out to\nbe false alarms.\n  To address this, we revisit the problem of specification inference from\nnatural language in the context of automatic software verification. In the\nprocess, we introduce NL2Contract, the task of employing LLMs to translate\ninformal natural language into formal functional contracts, consisting of\npostconditions as well as preconditions. We introduce metrics to validate and\ncompare different NL2Contract approaches, using soundness, bug discriminative\npower of the generated contracts and their usability in the context of\nautomatic software verification as key metrics. We evaluate NL2Contract with\ndifferent LLMs and compare it to the task of postcondition generation\nnl2postcond. Our evaluation shows that (1) LLMs are generally effective at\ngenerating functional contracts sound for all possible inputs, (2) the\ngenerated contracts are sufficiently expressive for discriminating buggy from\ncorrect behavior, and (3) verifiers supplied with LLM inferred functional\ncontracts produce fewer false alarms than when provided with postconditions\nalone. Further investigations show that LLM inferred preconditions generally\nalign well with developers intentions which allows us to use automatic software\nverifiers to catch real-world bugs.",
      "url": "http://arxiv.org/abs/2510.12702v1",
      "published_time_eastern_timestamp": 1760459859.0
    }
  ]
}
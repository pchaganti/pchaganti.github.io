{
  "last_updated": "2025-07-13T23:58:45.534205-04:00",
  "papers": [
    {
      "title": "The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for\n  Mechanistic Interpretability?",
      "summary": "The concept of causal abstraction got recently popularised to demystify the\nopaque decision-making processes of machine learning models; in short, a neural\nnetwork can be abstracted as a higher-level algorithm if there exists a\nfunction which allows us to map between them. Notably, most interpretability\npapers implement these maps as linear functions, motivated by the linear\nrepresentation hypothesis: the idea that features are encoded linearly in a\nmodel's representations. However, this linearity constraint is not required by\nthe definition of causal abstraction. In this work, we critically examine the\nconcept of causal abstraction by considering arbitrarily powerful alignment\nmaps. In particular, we prove that under reasonable assumptions, any neural\nnetwork can be mapped to any algorithm, rendering this unrestricted notion of\ncausal abstraction trivial and uninformative. We complement these theoretical\nfindings with empirical evidence, demonstrating that it is possible to\nperfectly map models to algorithms even when these models are incapable of\nsolving the actual task; e.g., on an experiment using randomly initialised\nlanguage models, our alignment maps reach 100% interchange-intervention\naccuracy on the indirect object identification task. This raises the non-linear\nrepresentation dilemma: if we lift the linearity constraint imposed to\nalignment maps in causal abstraction analyses, we are left with no principled\nway to balance the inherent trade-off between these maps' complexity and\naccuracy. Together, these results suggest an answer to our title's question:\ncausal abstraction is not enough for mechanistic interpretability, as it\nbecomes vacuous without assumptions about how models encode information.\nStudying the connection between this information-encoding assumption and causal\nabstraction should lead to exciting future work.",
      "url": "http://arxiv.org/abs/2507.08802v1",
      "published_time_eastern_timestamp": 1752256795.0
    },
    {
      "title": "Lumos-1: On Autoregressive Video Generation from a Unified Model\n  Perspective",
      "summary": "Autoregressive large language models (LLMs) have unified a vast range of\nlanguage tasks, inspiring preliminary efforts in autoregressive video\ngeneration. Existing autoregressive video generators either diverge from\nstandard LLM architectures, depend on bulky external text encoders, or incur\nprohibitive latency due to next-token decoding. In this paper, we introduce\nLumos-1, an autoregressive video generator that retains the LLM architecture\nwith minimal architectural modifications. To inject spatiotemporal correlations\nin LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its\nimbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE\nscheme that preserves the original textual RoPE while providing comprehensive\nfrequency spectra and scaled 3D positions for modeling multimodal\nspatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy\nthat obeys intra-frame bidirectionality and inter-frame temporal causality.\nBased on this dependency strategy, we identify the issue of frame-wise loss\nimbalance caused by spatial information redundancy and solve it by proposing\nAutoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal\ntube masking during training with a compatible inference-time masking policy to\navoid quality degradation. By using memory-efficient training techniques, we\npre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on\nGenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code\nand models are available at https://github.com/alibaba-damo-academy/Lumos.",
      "url": "http://arxiv.org/abs/2507.08801v1",
      "published_time_eastern_timestamp": 1752256782.0
    },
    {
      "title": "Total/dual correlation/coherence, redundancy/synergy, complexity, and\n  O-information for real and complex valued multivariate data",
      "summary": "Firstly, assuming Gaussianity, equations for the following information theory\nmeasures are presented: total correlation/coherence (TC), dual total\ncorrelation/coherence (DTC), O-information, TSE complexity, and the\nredundancy-synergy index (RSI). Since these measures are functions of the\ncovariance matrix \"S\" and its inverse \"S^-1\", the associated Wishart and\ninverse-Wishart distributions are of note. The DTC is shown here to be the\nKullback-Leibler (KL) divergence for the inverse-Wishart pair \"(S^-1)\" and its\ndiagonal matrix \"diag(S^-1)\", shedding light on its interpretation as a measure\nof \"total partial correlation\", -lndetP, with test hypothesis H0: P=I, where\n\"P\" is the standardized inverse covariance (i.e. P=(D^-1/2)(S^-1)(D^-1/2), with\nD=diag(S^-1)). The second aim of this paper introduces a generalization of all\nthese measures for structured groups of variables. For instance, consider three\nor more groups, each consisting of three or more variables, with predominant\nredundancy within each group, but with synergistic interactions between groups.\nO-information will miss the between group synergy (since redundancy occurs more\noften in the system). In contrast, the structured O-information measure\npresented here will correctly report predominant synergy between groups. This\nis a relevant generalization towards structured multivariate information\nmeasures. A third aim is the presentation of a framework for quantifying the\ncontribution of \"connections\" between variables, to the system's TC, DTC,\nO-information, and TSE complexity. A fourth aim is to present a generalization\nof the redundancy-synergy index for quantifying the contribution of a group of\nvariables to the system's redundancy-synergy balance. Finally, it is shown that\nthe expressions derived here directly apply to data from several other\nelliptical distributions. All program codes, data files, and executables are\navailable.",
      "url": "http://arxiv.org/abs/2507.08773v1",
      "published_time_eastern_timestamp": 1752255304.0
    },
    {
      "title": "From One to More: Contextual Part Latents for 3D Generation",
      "summary": "Recent advances in 3D generation have transitioned from multi-view 2D\nrendering approaches to 3D-native latent diffusion frameworks that exploit\ngeometric priors in ground truth data. Despite progress, three key limitations\npersist: (1) Single-latent representations fail to capture complex multi-part\ngeometries, causing detail degradation; (2) Holistic latent coding neglects\npart independence and interrelationships critical for compositional design; (3)\nGlobal conditioning mechanisms lack fine-grained controllability. Inspired by\nhuman 3D design workflows, we propose CoPart - a part-aware diffusion framework\nthat decomposes 3D objects into contextual part latents for coherent multi-part\ngeneration. This paradigm offers three advantages: i) Reduces encoding\ncomplexity through part decomposition; ii) Enables explicit part relationship\nmodeling; iii) Supports part-level conditioning. We further develop a mutual\nguidance strategy to fine-tune pre-trained diffusion models for joint part\nlatent denoising, ensuring both geometric coherence and foundation model\npriors. To enable large-scale training, we construct Partverse - a novel 3D\npart dataset derived from Objaverse through automated mesh segmentation and\nhuman-verified annotations. Extensive experiments demonstrate CoPart's superior\ncapabilities in part-level editing, articulated object generation, and scene\ncomposition with unprecedented controllability.",
      "url": "http://arxiv.org/abs/2507.08772v1",
      "published_time_eastern_timestamp": 1752255198.0
    },
    {
      "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with\n  Chunk-Level Activation Sparsity",
      "summary": "To alleviate the computational burden of large language models (LLMs),\narchitectures with activation sparsity, represented by mixture-of-experts\n(MoE), have attracted increasing attention. However, the non-differentiable and\ninflexible routing of vanilla MoE hurts model performance. Moreover, while each\ntoken activates only a few parameters, these sparsely-activated architectures\nexhibit low chunk-level sparsity, indicating that the union of multiple\nconsecutive tokens activates a large ratio of parameters. Such a sparsity\npattern is unfriendly for acceleration under low-resource conditions (e.g.,\nend-side devices) and incompatible with mainstream acceleration techniques\n(e.g., speculative decoding). To address these challenges, we introduce a novel\nMoE architecture, BlockFFN, as well as its efficient training and deployment\ntechniques. Specifically, we use a router integrating ReLU activation and\nRMSNorm for differentiable and flexible routing. Next, to promote both\ntoken-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training\nobjectives are designed, making BlockFFN more acceleration-friendly. Finally,\nwe implement efficient acceleration kernels, combining activation sparsity and\nspeculative decoding for the first time. The experimental results demonstrate\nthe superior performance of BlockFFN over other MoE baselines, achieving over\n80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\\times$ speedup on\nreal end-side devices than dense models. All codes and checkpoints are\navailable publicly (https://github.com/thunlp/BlockFFN).",
      "url": "http://arxiv.org/abs/2507.08771v1",
      "published_time_eastern_timestamp": 1752254936.0
    },
    {
      "title": "Column Twisted Reed-Solomon Codes as MDS Codes",
      "summary": "In this paper, we study column twisted Reed-Solomon(TRS) codes. We establish\nsome conditions for column TRS codes to be MDS codes and show that the\ndimension of their Schur square codes is $2k$. Consequently, these TRS codes\nare not equivalent to Reed-Solomon(RS) codes. Moreover, this construction\nmethod provides more flexible parameters compared to previous twisted\ngeneralized Reed-Solomon(TGRS) code constructions. For large odd prime power\n$q$, different from the systematically constructed TGRS codes whose length was\npreviously limited to $\\frac{q+1}{2}$, our construction achieves code lengths\nup to $\\frac{q+3}{2}$. Finally, we present the dual codes of column TRS codes.\nThis paper provides a new approach to construct MDS codes by adding column\nvectors to generator matrix of RS codes.",
      "url": "http://arxiv.org/abs/2507.08755v1",
      "published_time_eastern_timestamp": 1752253679.0
    },
    {
      "title": "HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing\n  Enabling Multi-Granularity Interpretation and Cross-Domain Transfer",
      "summary": "Hierarchical land cover and land use (LCLU) classification aims to assign\npixel-wise labels with multiple levels of semantic granularity to remote\nsensing (RS) imagery. However, existing deep learning-based methods face two\nmajor challenges: 1) They predominantly adopt a flat classification paradigm,\nwhich limits their ability to generate end-to-end multi-granularity\nhierarchical predictions aligned with tree-structured hierarchies used in\npractice. 2) Most cross-domain studies focus on performance degradation caused\nby sensor or scene variations, with limited attention to transferring LCLU\nmodels to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop\nclassification). These limitations hinder the flexibility and generalization of\nLCLU models in practical applications. To address these challenges, we propose\nHieraRS, a novel hierarchical interpretation paradigm that enables\nmulti-granularity predictions and supports the efficient transfer of LCLU\nmodels to cross-domain tasks with heterogeneous tree-structured hierarchies. We\nintroduce the Bidirectional Hierarchical Consistency Constraint Mechanism\n(BHCCM), which can be seamlessly integrated into mainstream flat classification\nmodels to generate hierarchical predictions, while improving both semantic\nconsistency and classification accuracy. Furthermore, we present TransLU, a\ndual-branch cross-domain transfer framework comprising two key components:\nCross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment\n(CDSA). TransLU supports dynamic category expansion and facilitates the\neffective adaptation of LCLU models to heterogeneous hierarchies. In addition,\nwe construct MM-5B, a large-scale multi-modal hierarchical land use dataset\nfeaturing pixel-wise annotations. The code and MM-5B dataset will be released\nat: https://github.com/AI-Tianlong/HieraRS.",
      "url": "http://arxiv.org/abs/2507.08741v1",
      "published_time_eastern_timestamp": 1752252241.0
    },
    {
      "title": "RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for\n  Multi-Camera Vehicle Tracking",
      "summary": "The multi-camera vehicle tracking (MCVT) framework holds significant\npotential for smart city applications, including anomaly detection, traffic\ndensity estimation, and suspect vehicle tracking. However, current publicly\navailable datasets exhibit limitations, such as overly simplistic scenarios,\nlow-resolution footage, and insufficiently diverse conditions, creating a\nconsiderable gap between academic research and real-world scenario. To fill\nthis gap, we introduce RoundaboutHD, a comprehensive, high-resolution\nmulti-camera vehicle tracking benchmark dataset specifically designed to\nrepresent real-world roundabout scenarios. RoundaboutHD provides a total of 40\nminutes of labelled video footage captured by four non-overlapping,\nhigh-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle\nidentities are annotated across different camera views, offering rich\ncross-camera association data. RoundaboutHD offers temporal consistency video\nfootage and enhanced challenges, including increased occlusions and nonlinear\nmovement inside the roundabout. In addition to the full MCVT dataset, several\nsubsets are also available for object detection, single camera tracking, and\nimage-based vehicle re-identification (ReID) tasks. Vehicle model information\nand camera modelling/ geometry information are also included to support further\nanalysis. We provide baseline results for vehicle detection, single-camera\ntracking, image-based vehicle re-identification, and multi-camera tracking. The\ndataset and the evaluation code are publicly available at:\nhttps://github.com/siri-rouser/RoundaboutHD.git",
      "url": "http://arxiv.org/abs/2507.08729v1",
      "published_time_eastern_timestamp": 1752251427.0
    },
    {
      "title": "Constraining the survival of HCN during cometary impacts",
      "summary": "Cometary impacts have been invoked as an atmosphere-independent method of\nstockpiling hydrogen cyanide (HCN), a key prebiotic feedstock molecule, into\nenvironments favourable for the onset of prebiotic chemistry on the early\nEarth. This work revisits the prospects for cometary delivery of HCN through\nnew impacts simulations of idealised cometary bodies using the shock physics\ncode iSALE combined with simple chemical modelling. Using temperature and\npressure profiles for material within spherical, non-porous comets with a high\nresolution of Lagrangian tracer particles, we assess the survival rate of HCN\nacross a range of impact velocities, sizes and angles, assuming both steady\nstate and equilibrium chemistry. We find that HCN survival is extremely limited\nat impact velocities above the escape velocity of the Earth, unless the impact\noccurs at extreme obliquity ($\\theta \\sim 15^\\circ$). We present a\nparametrisation of the survival of HCN as a function of impact velocity, angle,\nand cometary diameter, which provides an upper limit to survival in more\nrealistic scenarios to aid with future studies investigating the role of comets\nin the origins of life. Although successful HCN delivery may be possible in our\nidealised model, we neglect to consider the effect of atmospheric passage and\nour results suggest that delivery alone is not likely to be sufficient for the\nonset of prebiotic chemistry.",
      "url": "http://arxiv.org/abs/2507.08727v1",
      "published_time_eastern_timestamp": 1752251250.0
    },
    {
      "title": "Carbon-Aware Workflow Scheduling with Fixed Mapping and Deadline\n  Constraint",
      "summary": "Large data and computing centers consume a significant share of the world's\nenergy consumption. A prominent subset of the workloads in such centers are\nworkflows with interdependent tasks, usually represented as directed acyclic\ngraphs (DAGs). To reduce the carbon emissions resulting from executing such\nworkflows in centers with a mixed (renewable and non-renewable) energy supply,\nit is advisable to move task executions to time intervals with sufficient green\nenergy when possible. To this end, we formalize the above problem as a\nscheduling problem with a given mapping and ordering of the tasks. We show that\nthis problem can be solved in polynomial time in the uniprocessor case. For at\nleast two processors, however, the problem becomes NP-hard. Hence, we propose a\nheuristic framework called CaWoSched that combines several greedy approaches\nwith local search. To assess the 16 heuristics resulting from different\ncombinations, we also devise a simple baseline algorithm and an exact ILP-based\nsolution. Our experimental results show that our heuristics provide significant\nsavings in carbon emissions compared to the baseline.",
      "url": "http://arxiv.org/abs/2507.08725v1",
      "published_time_eastern_timestamp": 1752251113.0
    },
    {
      "title": "Multilingual Multimodal Software Developer for Code Generation",
      "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.",
      "url": "http://arxiv.org/abs/2507.08719v1",
      "published_time_eastern_timestamp": 1752250793.0
    },
    {
      "title": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One\n  Engine",
      "summary": "Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset are publicly available at:\nhttps://github.com/hkw-xg/Great-MCD.",
      "url": "http://arxiv.org/abs/2507.08716v1",
      "published_time_eastern_timestamp": 1752250566.0
    },
    {
      "title": "Simulated non-Markovian Noise Resilience of Silicon-Based Spin Qubits\n  with Surface Code Error Correction",
      "summary": "We investigate the resilience of silicon-based spin qubits against\nnon-Markovian noise within the framework of quantum error correction. We\nconsider a realistic non-Markovian noise model that affects both the Larmor\nfrequency and exchange energy of qubits, allowing accurate simulations of noisy\nquantum circuits. We employ numerical emulation to assess the performance of\nthe distance-3 rotated surface code and its XZZX variant, using a logical qubit\ncoherence time metric based on Ramsey-like experiments. Our numerical results\nsuggest that quantum error correction converts non-Markovian physical noise\ninto Markovian logical noise, resulting in a quartic dependence of coherence\ntime between physical and logical qubits. Additionally, we analyze the effects\nof spatial noise correlations and sparse architectures, substantiating the\nrobustness of quantum error correction in silicon-based spin qubit systems.",
      "url": "http://arxiv.org/abs/2507.08713v1",
      "published_time_eastern_timestamp": 1752250264.0
    },
    {
      "title": "SGPMIL: Sparse Gaussian Process Multiple Instance Learning",
      "summary": "Multiple Instance Learning (MIL) offers a natural solution for settings where\nonly coarse, bag-level labels are available, without having access to\ninstance-level annotations. This is usually the case in digital pathology,\nwhich consists of gigapixel sized images. While deterministic attention-based\nMIL approaches achieve strong bag-level performance, they often overlook the\nuncertainty inherent in instance relevance. In this paper, we address the lack\nof uncertainty quantification in instance-level attention scores by introducing\n\\textbf{SGPMIL}, a new probabilistic attention-based MIL framework grounded in\nSparse Gaussian Processes (SGP). By learning a posterior distribution over\nattention scores, SGPMIL enables principled uncertainty estimation, resulting\nin more reliable and calibrated instance relevance maps. Our approach not only\npreserves competitive bag-level performance but also significantly improves the\nquality and interpretability of instance-level predictions under uncertainty.\nSGPMIL extends prior work by introducing feature scaling in the SGP predictive\nmean function, leading to faster training, improved efficiency, and enhanced\ninstance-level performance. Extensive experiments on multiple well-established\ndigital pathology datasets highlight the effectiveness of our approach across\nboth bag- and instance-level evaluations. Our code will be made publicly\navailable.",
      "url": "http://arxiv.org/abs/2507.08711v1",
      "published_time_eastern_timestamp": 1752250227.0
    },
    {
      "title": "Fine-tuning ORBGRAND with Very Few Channel Soft Values",
      "summary": "Guessing random additive noise decoding (GRAND) is a universal decoding\nparadigm that decodes by repeatedly testing error patterns until identifying a\ncodeword, where the ordering of tests is generated by the received channel\nvalues. On one hand, while testing error patterns in a descending order of\nposterior probabilities leads to maximum likelihood decoding, its\nimplementation complexity is prohibitive. On the other hand, testing error\npatterns with a prescribed set of error patterns permuted by the ranking among\nmagnitudes of log-likelihood ratios (i.e., ordered reliability bits, ORB)\nenables efficient implementation, but results in performance loss for\nfinite-length codes. Aiming at harnessing the strengths of these two\napproaches, this work proposes a fine-tuning method to improve ORBGRAND,\nadjusting the ordering of tests with the aid of very few exact channel soft\nvalues. This method is based on a metric for assessing the ``well-orderedness''\nof error patterns. The metric is studied via the lens of the asymptotic theory\nof integer partitioning, which provides highly accurate estimation in numerical\nexperiments. The metric then leads to an effective identification of\nfine-tuning to conduct, at the cost of a negligible increment of complexity.\nNumerical experiments demonstrate that the proposed fine-tuning method achieves\na substantial performance enhancement compared with ORBGRAND.",
      "url": "http://arxiv.org/abs/2507.08696v1",
      "published_time_eastern_timestamp": 1752248989.0
    },
    {
      "title": "LLMCup: Ranking-Enhanced Comment Updating with LLMs",
      "summary": "While comments are essential for enhancing code readability and\nmaintainability in modern software projects, developers are often motivated to\nupdate code but not comments, leading to outdated or inconsistent documentation\nthat hinders future understanding and maintenance. Recent approaches such as\nCUP and HebCup have attempted automatic comment updating using neural\nsequence-to-sequence models and heuristic rules, respectively. However, these\nmethods can miss or misinterpret crucial information during comment updating,\nresulting in inaccurate comments, and they often struggle with complex update\nscenarios. Given these challenges, a promising direction lies in leveraging\nlarge language models (LLMs), which have shown impressive performance in\nsoftware engineering tasks such as comment generation, code synthesis, and\nprogram repair. This suggests their strong potential to capture the logic\nbehind code modifications - an ability that is crucial for the task of comment\nupdating. Nevertheless, selecting an appropriate prompt strategy for an LLM on\neach update case remains challenging. To address this, we propose a novel\ncomment updating framework, LLMCup, which first uses multiple prompt strategies\nto provide diverse candidate updated comments via an LLM, and then employs a\nranking model, CupRank, to select the best candidate as final updated comment.\nExperimental results demonstrate the effectiveness of LLMCup, with improvements\nover state-of-the-art baselines (CUP and HebCup) by 49.0%-116.9% in Accuracy,\n10.8%-20% in BLEU-4, 4.6% in METEOR, 0.9%-1.9% in F1, and 2.1%-3.4% in\nSentenceBert similarity. Furthermore, a user study shows that comments updated\nby LLMCup sometimes surpass human-written updates, highlighting the importance\nof incorporating human evaluation in comment quality assessment.",
      "url": "http://arxiv.org/abs/2507.08671v1",
      "published_time_eastern_timestamp": 1752246687.0
    },
    {
      "title": "KELPS: A Framework for Verified Multi-Language Autoformalization via\n  Semantic-Syntactic Alignment",
      "summary": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.",
      "url": "http://arxiv.org/abs/2507.08665v1",
      "published_time_eastern_timestamp": 1752246306.0
    },
    {
      "title": "Introspection of Thought Helps AI Agents",
      "summary": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.",
      "url": "http://arxiv.org/abs/2507.08664v1",
      "published_time_eastern_timestamp": 1752246197.0
    },
    {
      "title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem\n  Proving via Reinforcement Learning",
      "summary": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2.",
      "url": "http://arxiv.org/abs/2507.08649v1",
      "published_time_eastern_timestamp": 1752245594.0
    },
    {
      "title": "NL in the Middle: Code Translation with LLMs and Intermediate\n  Representations",
      "summary": "Studies show that large language models (LLMs) produce buggy code\ntranslations. One avenue to improve translation accuracy is through\nintermediate representations, which could provide structured insights to guide\nthe model's understanding. We explore whether code translation using LLMs can\nbenefit from intermediate representations via natural language (NL) and\nabstract syntax trees (ASTs). Since prompt engineering greatly affects LLM\nperformance, we consider several ways to integrate these representations, from\none-shot to chain-of-thought (CoT) prompting. Using Open Gpt4 8X7B and\nspecialized StarCoder and CodeGen models on popular code translation benchmarks\n(CodeNet and AVATAR), we find that CoT with an intermediate NL summary performs\nbest, with an increase of 13.8% and 6.7%, respectively, in successful\ntranslations for the best-performing model (Open Gpt4 8X7B) compared to the\nzero-shot prompt.",
      "url": "http://arxiv.org/abs/2507.08627v1",
      "published_time_eastern_timestamp": 1752244161.0
    }
  ]
}
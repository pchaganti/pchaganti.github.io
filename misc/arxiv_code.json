{
  "last_updated": "2025-07-10T05:15:16.961581-04:00",
  "papers": [
    {
      "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
      "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.",
      "url": "http://arxiv.org/abs/2507.07105v1",
      "published_time_eastern_timestamp": 1752083959.0
    },
    {
      "title": "Does Data Scaling Lead to Visual Compositional Generalization?",
      "summary": "Compositional understanding is crucial for human intelligence, yet it remains\nunclear whether contemporary vision models exhibit it. The dominant machine\nlearning paradigm is built on the premise that scaling data and model sizes\nwill improve out-of-distribution performance, including compositional\ngeneralization. We test this premise through controlled experiments that\nsystematically vary data scale, concept diversity, and combination coverage. We\nfind that compositional generalization is driven by data diversity, not mere\ndata scale. Increased combinatorial coverage forces models to discover a\nlinearly factored representational structure, where concepts decompose into\nadditive components. We prove this structure is key to efficiency, enabling\nperfect generalization from few observed combinations. Evaluating pretrained\nmodels (DINO, CLIP), we find above-random yet imperfect performance, suggesting\npartial presence of this structure. Our work motivates stronger emphasis on\nconstructing diverse datasets for compositional generalization, and considering\nthe importance of representational structure that enables efficient\ncompositional learning. Code available at\nhttps://github.com/oshapio/visual-compositional-generalization.",
      "url": "http://arxiv.org/abs/2507.07102v1",
      "published_time_eastern_timestamp": 1752083943.0
    },
    {
      "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works,\n  and Why Gradient Accumulation Is Wasteful",
      "summary": "Conventional wisdom dictates that small batch sizes make language model\npretraining and fine-tuning unstable, motivating gradient accumulation, which\ntrades off the number of optimizer steps for a proportional increase in batch\nsize. While it is common to decrease the learning rate for smaller batch sizes,\nother hyperparameters are often held fixed. In this work, we revisit small\nbatch sizes all the way down to batch size one, and we propose a rule for\nscaling Adam hyperparameters to small batch sizes. We find that small batch\nsizes (1) train stably, (2) are consistently more robust to hyperparameter\nchoices, (3) achieve equal or better per-FLOP performance than larger batch\nsizes, and (4) notably enable stable language model training with vanilla SGD,\neven without momentum, despite storing no optimizer state. Building on these\nresults, we provide practical recommendations for selecting a batch size and\nsetting optimizer hyperparameters. We further recommend against gradient\naccumulation unless training on multiple devices with multiple model replicas,\nbottlenecked by inter-device bandwidth.",
      "url": "http://arxiv.org/abs/2507.07101v1",
      "published_time_eastern_timestamp": 1752083856.0
    },
    {
      "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
      "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.",
      "url": "http://arxiv.org/abs/2507.07095v1",
      "published_time_eastern_timestamp": 1752083524.0
    },
    {
      "title": "Boosting Parameter Efficiency in LLM-Based Recommendation through\n  Sophisticated Pruning",
      "summary": "LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec",
      "url": "http://arxiv.org/abs/2507.07064v1",
      "published_time_eastern_timestamp": 1752081970.0
    },
    {
      "title": "Quantum Spectral Clustering: Comparing Parameterized and Neuromorphic\n  Quantum Kernels",
      "summary": "We compare a parameterized quantum kernel (pQK) with a quantum leaky\nintegrate-and-fire (QLIF) neuromorphic computing approach that employs either\nthe Victor-Purpura or van Rossum kernel in a spectral clustering task, as well\nas the classical radial basis function (RBF) kernel. Performance evaluation\nincludes label-based classification and clustering metrics, as well as optimal\nnumber of clusters predictions for each dataset based on an elbow-like curve as\nis typically used in $K$-means clustering. The pQK encodes feature vectors\nthrough angle encoding with rotation angles scaled parametrically. Parameters\nare optimized through grid search to maximize kernel-target alignment,\nproducing a kernel that reflects distances in the feature space. The quantum\nneuromorphic approach uses population coding to transform data into spike\ntrains, which are then processed using temporal distance metrics. Kernel\nmatrices are used as input into a classical spectral clustering pipeline prior\nto performance evaluation. For the synthetic datasets and \\texttt{Iris}, the\nQLIF kernel typically achieves better classification and clustering performance\nthan pQK. However, on higher-dimensional datasets, such as a preprocessed\nversion of the Sloan Digital Sky Survey (\\texttt{SDSS}), pQK performed better,\nindicating a possible advantage in higher-dimensional regimes.",
      "url": "http://arxiv.org/abs/2507.07018v1",
      "published_time_eastern_timestamp": 1752079609.0
    },
    {
      "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge\n  Distillation",
      "summary": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill.",
      "url": "http://arxiv.org/abs/2507.07015v1",
      "published_time_eastern_timestamp": 1752079528.0
    },
    {
      "title": "Robust Containerization of the High Angular Resolution Functional\n  Imaging (HARFI) Pipeline",
      "summary": "Historically, functional magnetic resonance imaging (fMRI) of the brain has\nfocused primarily on gray matter, particularly the cortical gray matter and\nassociated nuclei. However, recent work has demonstrated that functional\nactivity in white matter also plays a meaningful role in both cognition and\nlearning. In previous work, we introduced the High Angular Resolution\nFunctional Imaging (HARFI) pipeline, which demonstrated both local and global\npatterns of functional correlation in white matter. Notably, HARFI enabled\nexploration of asymmetric voxel-wise correlation using odd-order spherical\nharmonics. Although the original implementation of HARFI was released via\nGitHub, adoption was limited due to the technical complexity of running the\nsource code. In this work, we present a robust and efficient containerized\nversion of the HARFI pipeline, enabling seamless execution across multiple\npublic datasets. Our goal is to facilitate broader and deeper exploration of\nfunctional white matter architecture, especially through the lens of high\nangular resolution functional correlations. The key innovation of this work is\nthe containerized implementation, which we have made available under a\npermissive open-source license to support reproducible and accessible research\npractices.",
      "url": "http://arxiv.org/abs/2507.07010v1",
      "published_time_eastern_timestamp": 1752079227.0
    },
    {
      "title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal\n  Preprocessing",
      "summary": "Electronic Health Records (EHR) are time-series relational databases that\nrecord patient interactions and medical events over time, serving as a critical\nresource for healthcare research and applications. However, privacy concerns\nand regulatory restrictions limit the sharing and utilization of such sensitive\ndata, necessitating the generation of synthetic EHR datasets. Unlike previous\nEHR synthesis methods, which typically generate medical records consisting of\nexpert-chosen features (e.g. a few vital signs or structured codes only), we\nintroduce RawMed, the first framework to synthesize multi-table, time-series\nEHR data that closely resembles raw EHRs. Using text-based representation and\ncompression techniques, RawMed captures complex structures and temporal\ndynamics with minimal preprocessing. We also propose a new evaluation framework\nfor multi-table time-series synthetic EHRs, assessing distributional\nsimilarity, inter-table relationships, temporal dynamics, and privacy.\nValidated on two open-source EHR datasets, RawMed outperforms baseline models\nin fidelity and utility. The code is available at\nhttps://github.com/eunbyeol-cho/RawMed.",
      "url": "http://arxiv.org/abs/2507.06996v1",
      "published_time_eastern_timestamp": 1752078142.0
    },
    {
      "title": "Are They All Good? Evaluating the Quality of CoTs in LLM-based Code\n  Generation",
      "summary": "Large language models (LLMs) have demonstrated impressive performance in code\ngeneration, particularly when augmented with chain-of-thought (CoT) prompting\ntechniques. They break down requirements into intermediate reasoning steps,\nwhich act as design rationales to guide LLMs in writing code like human\nprogrammers. Thus, the quality of these steps is crucial for ensuring the\ncorrectness and reliability of the generated code. However, little is known\nabout the quality of CoT generated by LLMs. To what extent can we trust the\nthoughts generated by LLMs? How good are they? This paper empirically explores\nthe external and internal factors of why LLMs generate unsatisfactory CoTs by\nanalyzing 1,023 failed code samples on two widely used code generation\nbenchmarks. We also evaluate their impact on code generation performance by\nanalyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting\nLLMs. Our study reveals three key findings: (1) External factors (53.60%), such\nas unclear requirements and lack of context, mainly affect CoT quality, while\ninternal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even\nwhen CoTs are correct, 18.5% of the generated code contains errors due to\ninstruction-following issues; conversely, 11.90% of correct code is paired with\nflawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when\ngiven detailed problem descriptions. These findings highlight key challenges in\nCoT-based code generation and suggest directions for improving LLM reasoning\nand reliability.",
      "url": "http://arxiv.org/abs/2507.06980v1",
      "published_time_eastern_timestamp": 1752077240.0
    },
    {
      "title": "Hallucinating 360Â°: Panoramic Street-View Generation via Local\n  Scenes Diffusion and Probabilistic Prompting",
      "summary": "Panoramic perception holds significant potential for autonomous driving,\nenabling vehicles to acquire a comprehensive 360{\\deg} surround view in a\nsingle shot. However, autonomous driving is a data-driven task. Complete\npanoramic data acquisition requires complex sampling systems and annotation\npipelines, which are time-consuming and labor-intensive. Although existing\nstreet view generation models have demonstrated strong data regeneration\ncapabilities, they can only learn from the fixed data distribution of existing\ndatasets and cannot achieve high-quality, controllable panoramic generation. In\nthis paper, we propose the first panoramic generation method Percep360 for\nautonomous driving. Percep360 enables coherent generation of panoramic data\nwith control signals based on the stitched panoramic data. Percep360 focuses on\ntwo key aspects: coherence and controllability. Specifically, to overcome the\ninherent information loss caused by the pinhole sampling process, we propose\nthe Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama\ngeneration as a spatially continuous diffusion process, bridging the gaps\nbetween different data distributions. Additionally, to achieve the controllable\ngeneration of panoramic images, we propose a Probabilistic Prompting Method\n(PPM). PPM dynamically selects the most relevant control cues, enabling\ncontrollable panoramic image generation. We evaluate the effectiveness of the\ngenerated images from three perspectives: image quality assessment (i.e.,\nno-reference and with reference), controllability, and their utility in\nreal-world Bird's Eye View (BEV) segmentation. Notably, the generated data\nconsistently outperforms the original stitched images in no-reference quality\nmetrics and enhances downstream perception models. The source code will be\npublicly available at https://github.com/Bryant-Teng/Percep360.",
      "url": "http://arxiv.org/abs/2507.06971v1",
      "published_time_eastern_timestamp": 1752076901.0
    },
    {
      "title": "No physics required! A visual-based introduction to GKP qubits for\n  computer scientists",
      "summary": "With the significance of continuous-variable quantum computing increasing\nthanks to the achievements of light-based quantum hardware, making it available\nto learner audiences outside physics has been an important yet seldom-tackled\nchallenge. Similarly, the rising focus on fault-tolerant quantum computing has\nshed light on quantum error correction schemes, turning it into the locus of\nattention for industry and academia alike. In this paper, we explore the widely\nadopted framework of quantum error correction based on continuous variable\nsystems and suggest a guide on building a self-contained learning session\ntargeting the famous Gottesman-Kitaev-Preskill (GKP) code through its geometric\nintuition.",
      "url": "http://arxiv.org/abs/2507.06943v1",
      "published_time_eastern_timestamp": 1752074775.0
    },
    {
      "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
      "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
      "url": "http://arxiv.org/abs/2507.06920v1",
      "published_time_eastern_timestamp": 1752073127.0
    },
    {
      "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction",
      "summary": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.",
      "url": "http://arxiv.org/abs/2507.06909v1",
      "published_time_eastern_timestamp": 1752072420.0
    },
    {
      "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection",
      "summary": "The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.",
      "url": "http://arxiv.org/abs/2507.06908v1",
      "published_time_eastern_timestamp": 1752072392.0
    },
    {
      "title": "CDC: Causal Domain Clustering for Multi-Domain Recommendation",
      "summary": "Multi-domain recommendation leverages domain-general knowledge to improve\nrecommendations across several domains. However, as platforms expand to dozens\nor hundreds of scenarios, training all domains in a unified model leads to\nperformance degradation due to significant inter-domain differences. Existing\ndomain grouping methods, based on business logic or data similarities, often\nfail to capture the true transfer relationships required for optimal grouping.\nTo effectively cluster domains, we propose Causal Domain Clustering (CDC). CDC\nmodels domain transfer patterns within a large number of domains using two\ndistinct effects: the Isolated Domain Affinity Matrix for modeling\nnon-interactive domain transfers, and the Hybrid Domain Affinity Matrix for\nconsidering dynamic domain synergy or interference under joint training. To\nintegrate these two transfer effects, we introduce causal discovery to\ncalculate a cohesion-based coefficient that adaptively balances their\ncontributions. A Co-Optimized Dynamic Clustering algorithm iteratively\noptimizes target domain clustering and source domain selection for training.\nCDC significantly enhances performance across over 50 domains on public\ndatasets and in industrial settings, achieving a 4.9% increase in online eCPM.\nCode is available at\nhttps://github.com/Chrissie-Law/Causal-Domain-Clustering-for-Multi-Domain-Recommendation",
      "url": "http://arxiv.org/abs/2507.06877v1",
      "published_time_eastern_timestamp": 1752070547.0
    },
    {
      "title": "On the Error Exponent Distribution of Code Ensembles over\n  Classical-Quantum Channels",
      "summary": "We show that the probability distribution of the error exponent in i.i.d.\ncode ensembles over classical-quantum (CQ) channels with arbitrary output\nstates accumulates above a threshold that is strictly larger than the CQ random\ncoding exponent (RCE) at low rates, while coinciding with it at rates close to\nthe mutual information of the channel. This result, combined with the work by\nDalai [1] and the recent ones by Renes [2] and Li and Yang [3], implies that\nthe ensemble distribution of error exponents concentrates around the CQ RCE in\nthe high rate regime. Moreover, in the same rate regime the threshold we derive\ncoincides with the ensemble-average of the exponent, that is, the typical\nrandom coding (TRC) exponent [4].",
      "url": "http://arxiv.org/abs/2507.06868v1",
      "published_time_eastern_timestamp": 1752070155.0
    },
    {
      "title": "OpenDPDv2: A Unified Learning and Optimization Framework for Neural\n  Network Digital Predistortion",
      "summary": "Neural network (NN)-based Digital Predistortion (DPD) stands out in improving\nsignal quality in wideband radio frequency (RF) power amplifiers (PAs)\nemploying complex modulation. However, NN DPDs usually rely on a large number\nof parameters for effective linearization and can significantly contribute to\nthe energy consumption of the digital back-end in RF systems. This paper\npresents OpenDPDv2, a unified framework for PA modeling, DPD learning, and\nmodel optimization to reduce power consumption while maintaining high\nlinearization performance. The optimization techniques feature a novel DPD\nalgorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The\ntop-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an\nAdjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude\n(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal\nsparsity of input signals and hidden neurons, the inference energy of our model\ncan be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM\nwith 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth\n256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,\ndatasets, and documentation are publicly accessible at:\nhttps://github.com/lab-emi/OpenDPD.",
      "url": "http://arxiv.org/abs/2507.06849v1",
      "published_time_eastern_timestamp": 1752069287.0
    },
    {
      "title": "Shifting from Ranking to Set Selection for Retrieval Augmented\n  Generation",
      "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR",
      "url": "http://arxiv.org/abs/2507.06838v1",
      "published_time_eastern_timestamp": 1752068136.0
    },
    {
      "title": "Comprehensive Evaluation of Prototype Neural Networks",
      "summary": "Prototype models are an important method for explainable artificial\nintelligence (XAI) and interpretable machine learning. In this paper, we\nperform an in-depth analysis of a set of prominent prototype models including\nProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive\nset of metrics. In addition to applying standard metrics from literature, we\npropose several new metrics to further complement the analysis of model\ninterpretability. In our experimentation, we apply the set of prototype models\non a diverse set of datasets including fine-grained classification, Non-IID\nsettings and multi-label classification to further contrast the performance.\nFurthermore, we also provide our code as an open-source library, which\nfacilitates simple application of the metrics itself, as well as extensibility\n- providing the option for easily adding new metrics and models.\nhttps://github.com/uos-sis/quanproto",
      "url": "http://arxiv.org/abs/2507.06819v1",
      "published_time_eastern_timestamp": 1752066501.0
    }
  ]
}
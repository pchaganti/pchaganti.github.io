{
  "last_updated": "2025-05-29T17:11:25.056363-04:00",
  "papers": [
    {
      "title": "Maximizing Confidence Alone Improves Reasoning",
      "summary": "Reinforcement learning (RL) has enabled machine learning models to achieve\nsignificant advances in many fields. Most recently, RL has empowered frontier\nlanguage models to solve challenging math, science, and coding problems.\nHowever, central to any RL algorithm is the reward function, and reward\nengineering is a notoriously difficult problem in any domain. In this paper, we\npropose RENT: Reinforcement Learning via Entropy Minimization -- a fully\nunsupervised RL method that requires no external reward or ground-truth\nanswers, and instead uses the model's entropy of its underlying distribution as\nan intrinsic reward. We find that by reinforcing the chains of thought that\nyield high model confidence on its generated answers, the model improves its\nreasoning ability. In our experiments, we showcase these improvements on an\nextensive suite of commonly-used reasoning benchmarks, including GSM8K,\nMATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and\nMistral families. The generality of our unsupervised learning method lends\nitself to applicability in a wide range of domains where external supervision\nis limited or unavailable.",
      "url": "http://arxiv.org/abs/2505.22660v1",
      "published_time_eastern_timestamp": 1748455177.0
    },
    {
      "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in\n  Learning to Reason",
      "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.",
      "url": "http://arxiv.org/abs/2505.22653v1",
      "published_time_eastern_timestamp": 1748455143.0
    },
    {
      "title": "WebDancer: Towards Autonomous Information Seeking Agency",
      "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent.",
      "url": "http://arxiv.org/abs/2505.22648v1",
      "published_time_eastern_timestamp": 1748455027.0
    },
    {
      "title": "SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes",
      "summary": "Industrial Control Systems (ICS) manage critical infrastructures like power\ngrids and water treatment plants. Cyberattacks on ICSs can disrupt operations,\ncausing severe economic, environmental, and safety issues. For example,\nundetected pollution in a water plant can put the lives of thousands at stake.\nICS researchers have increasingly turned to honeypots -- decoy systems designed\nto attract attackers, study their behaviors, and eventually improve defensive\nmechanisms. However, existing ICS honeypots struggle to replicate the ICS\nphysical process, making them susceptible to detection. Accurately simulating\nthe noise in ICS physical processes is challenging because different factors\nproduce it, including sensor imperfections and external interferences.\n  In this paper, we propose SimProcess, a novel framework to rank the fidelity\nof ICS simulations by evaluating how closely they resemble real-world and noisy\nphysical processes. It measures the simulation distance from a target system by\nestimating the noise distribution with machine learning models like Random\nForest. Unlike existing solutions that require detailed mathematical models or\nare limited to simple systems, SimProcess operates with only a timeseries of\nmeasurements from the real system, making it applicable to a broader range of\ncomplex dynamic systems. We demonstrate the framework's effectiveness through a\ncase study using real-world power grid data from the EPIC testbed. We compare\nthe performance of various simulation methods, including static and generative\nnoise techniques. Our model correctly classifies real samples with a recall of\nup to 1.0. It also identifies Gaussian and Gaussian Mixture as the best\ndistribution to simulate our power systems, together with a generative solution\nprovided by an autoencoder, thereby helping developers to improve honeypot\nfidelity. Additionally, we make our code publicly available.",
      "url": "http://arxiv.org/abs/2505.22638v1",
      "published_time_eastern_timestamp": 1748454863.0
    },
    {
      "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
      "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
      "url": "http://arxiv.org/abs/2505.22613v1",
      "published_time_eastern_timestamp": 1748453374.0
    },
    {
      "title": "BPMN to Smart Contract by Business Analyst",
      "summary": "This paper addresses the challenge of creating smart contracts for\napplications represented using Business Process Management and Notation (BPMN)\nmodels. In our prior work we presented a methodology that automates the\ngeneration of smart contracts from BPMN models. This approach abstracts the\nBPMN flow control, making it independent of the underlying blockchain\ninfrastructure, with only the BPMN task elements requiring coding. In\nsubsequent research, we enhanced our approach by adding support for nested\ntransactions and enabling a smart contract repair and/or upgrade. To empower\nBusiness Analysts (BAs) to generate smart contracts without relying on software\ndevelopers, we tackled the challenge of generating smart contracts from BPMN\nmodels without assistance of a software developer. We exploit the Decision\nModel and Notation (DMN) standard to represent the decisions and the business\nlogic of the BPMN task elements and amended our methodology for transformation\nof BPMN models into smart contracts to support also the generation script to\nrepresent the business logic represented by the DMN models. To support such\ntransformation, we describe how the BA documents, using the BPMN elements, the\nflow of information along with the flow of execution. Thus, if the BA is\nsuccessful in representing the blockchain application requirements using BPMN\nand DMN models, our methodology and the tool, called TABS, that we developed as\na proof of concept, is used to generate the smart contracts directly from those\nmodels without developer assistance.",
      "url": "http://arxiv.org/abs/2505.22612v1",
      "published_time_eastern_timestamp": 1748453318.0
    },
    {
      "title": "TPDE: A Fast Adaptable Compiler Back-End Framework",
      "summary": "Fast machine code generation is especially important for fast start-up\njust-in-time compilation, where the compilation time is part of the end-to-end\nlatency. However, widely used compiler frameworks like LLVM do not prioritize\nfast compilation and require an extra IR translation step increasing latency\neven further; and rolling a custom code generator is a substantial engineering\neffort, especially when targeting multiple architectures.\n  Therefore, in this paper, we present TPDE, a compiler back-end framework that\nadapts to existing code representations in SSA form. Using an IR-specific\nadapter providing canonical access to IR data structures and a specification of\nthe IR semantics, the framework performs one analysis pass and then performs\nthe compilation in just a single pass, combining instruction selection,\nregister allocation, and instruction encoding. The generated target\ninstructions are primarily derived code written in high-level language through\nLLVM's Machine IR, easing portability to different architectures while enabling\noptimizations during code generation.\n  To show the generality of our framework, we build a new back-end for LLVM\nfrom scratch targeting x86-64 and AArch64. Performance results on SPECint 2017\nshow that we can compile LLVM-IR 8--24x faster than LLVM -O0 while being on-par\nin terms of run-time performance. We also demonstrate the benefits of adapting\nto domain-specific IRs in JIT contexts, particularly WebAssembly and database\nquery compilation, where avoiding the extra IR translation further reduces\ncompilation latency.",
      "url": "http://arxiv.org/abs/2505.22610v1",
      "published_time_eastern_timestamp": 1748453153.0
    },
    {
      "title": "Tell me Habibi, is it Real or Fake?",
      "summary": "Deepfake generation methods are evolving fast, making fake media harder to\ndetect and raising serious societal concerns. Most deepfake detection and\ndataset creation research focuses on monolingual content, often overlooking the\nchallenges of multilingual and code-switched speech, where multiple languages\nare mixed within the same discourse. Code-switching, especially between Arabic\nand English, is common in the Arab world and is widely used in digital\ncommunication. This linguistic mixing poses extra challenges for deepfake\ndetection, as it can confuse models trained mostly on monolingual data. To\naddress this, we introduce \\textbf{ArEnAV}, the first large-scale\nArabic-English audio-visual deepfake dataset featuring intra-utterance\ncode-switching, dialectal variation, and monolingual Arabic content. It\n\\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our\ndataset is generated using a novel pipeline integrating four Text-To-Speech and\ntwo lip-sync models, enabling comprehensive analysis of multilingual multimodal\ndeepfake detection. We benchmark our dataset against existing monolingual and\nmultilingual datasets, state-of-the-art deepfake detection models, and a human\nevaluation, highlighting its potential to advance deepfake research. The\ndataset can be accessed\n\\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.",
      "url": "http://arxiv.org/abs/2505.22581v1",
      "published_time_eastern_timestamp": 1748451276.0
    },
    {
      "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified\n  Retrieval-Augmented Generation Systems",
      "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation.",
      "url": "http://arxiv.org/abs/2505.22571v1",
      "published_time_eastern_timestamp": 1748450791.0
    },
    {
      "title": "ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion\n  Models",
      "summary": "Recent advances in diffusion models have led to impressive image generation\ncapabilities, but aligning these models with human preferences remains\nchallenging. Reward-based fine-tuning using models trained on human feedback\nimproves alignment but often harms diversity, producing less varied outputs. In\nthis work, we address this trade-off with two contributions. First, we\nintroduce \\textit{combined generation}, a novel sampling strategy that applies\na reward-tuned diffusion model only in the later stages of the generation\nprocess, while preserving the base model for earlier steps. This approach\nmitigates early-stage overfitting and helps retain global structure and\ndiversity. Second, we propose \\textit{ImageReFL}, a fine-tuning method that\nimproves image diversity with minimal loss in quality by training on real\nimages and incorporating multiple regularizers, including diffusion and ReFL\nlosses. Our approach outperforms conventional reward tuning methods on standard\nquality and diversity metrics. A user study further confirms that our method\nbetter balances human preference alignment and visual diversity. The source\ncode can be found at https://github.com/ControlGenAI/ImageReFL .",
      "url": "http://arxiv.org/abs/2505.22569v1",
      "published_time_eastern_timestamp": 1748450707.0
    },
    {
      "title": "Toward Fully Neuromorphic Receivers for Ultra-Power Efficient\n  Communications",
      "summary": "Neuromorphic computing, inspired by biological neural systems, has emerged as\na promising approach for ultra-energy-efficient data processing by leveraging\nanalog neuron structures and spike-based computation. However, its application\nin communication systems remains largely unexplored, with existing efforts\nmainly focused on mapping isolated communication algorithms onto spiking\nnetworks, often accompanied by substantial, traditional computational overhead\ndue to transformations required to adapt problems to the spiking paradigm. In\nthis work, we take a fundamentally different route and, for the first time,\npropose a fully neuromorphic communication receiver by applying neuromorphic\nprinciples directly in the analog domain from the very start of the receiver\nprocessing chain. Specifically, we examine a simple transmission scenario: a\nBPSK receiver with repetition coding, and show that we can achieve joint\ndetection and decoding entirely through spiking signals. Our approach\ndemonstrates error-rate performance gains over conventional digital\nrealizations with power consumption on the order of microwatts, comparable with\na single very low-resolution Analog-to-Digital Converter (ADC) utilized in\ndigital receivers. To maintain performance under varying noise conditions, we\nalso introduce a novel noise-tracking mechanism that dynamically adjusts neural\nparameters during transmission. Finally, we discuss the key challenges and\ndirections toward ultra-efficient neuromorphic transceivers.",
      "url": "http://arxiv.org/abs/2505.22508v1",
      "published_time_eastern_timestamp": 1748447836.0
    },
    {
      "title": "Sparsification and Reconstruction from the Perspective of Representation\n  Geometry",
      "summary": "Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic\ninterpretability, aiming to identify interpretable monosemantic features.\nHowever, how does sparse encoding organize the representations of activation\nvector from language models? What is the relationship between this\norganizational paradigm and feature disentanglement as well as reconstruction\nperformance? To address these questions, we propose the SAEMA, which validates\nthe stratified structure of the representation by observing the variability of\nthe rank of the symmetric semipositive definite (SSPD) matrix corresponding to\nthe modal tensor unfolded along the latent tensor with the level of noise added\nto the residual stream. To systematically investigate how sparse encoding\nalters representational structures, we define local and global representations,\ndemonstrating that they amplify inter-feature distinctions by merging similar\nsemantic features and introducing additional dimensionality. Furthermore, we\nintervene the global representation from an optimization perspective, proving a\nsignificant causal relationship between their separability and the\nreconstruction performance. This study explains the principles of sparsity from\nthe perspective of representational geometry and demonstrates the impact of\nchanges in representational structure on reconstruction performance.\nParticularly emphasizes the necessity of understanding representations and\nincorporating representational constraints, providing empirical references for\ndeveloping new interpretable tools and improving SAEs. The code is available at\n\\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.",
      "url": "http://arxiv.org/abs/2505.22506v1",
      "published_time_eastern_timestamp": 1748447673.0
    },
    {
      "title": "ProCrop: Learning Aesthetic Image Cropping from Professional\n  Compositions",
      "summary": "Image cropping is crucial for enhancing the visual appeal and narrative\nimpact of photographs, yet existing rule-based and data-driven approaches often\nlack diversity or require annotated training data. We introduce ProCrop, a\nretrieval-based method that leverages professional photography to guide\ncropping decisions. By fusing features from professional photographs with those\nof the query image, ProCrop learns from professional compositions,\nsignificantly boosting performance. Additionally, we present a large-scale\ndataset of 242K weakly-annotated images, generated by out-painting professional\nimages and iteratively refining diverse crop proposals. This composition-aware\ndataset generation offers diverse high-quality crop proposals guided by\naesthetic principles and becomes the largest publicly available dataset for\nimage cropping. Extensive experiments show that ProCrop significantly\noutperforms existing methods in both supervised and weakly-supervised settings.\nNotably, when trained on the new dataset, our ProCrop surpasses previous\nweakly-supervised methods and even matches fully supervised approaches. Both\nthe code and dataset will be made publicly available to advance research in\nimage aesthetics and composition analysis.",
      "url": "http://arxiv.org/abs/2505.22490v1",
      "published_time_eastern_timestamp": 1748446724.0
    },
    {
      "title": "Single Domain Generalization for Alzheimer's Detection from 3D MRIs with\n  Pseudo-Morphological Augmentations and Contrastive Learning",
      "summary": "Although Alzheimer's disease detection via MRIs has advanced significantly\nthanks to contemporary deep learning models, challenges such as class\nimbalance, protocol variations, and limited dataset diversity often hinder\ntheir generalization capacity. To address this issue, this article focuses on\nthe single domain generalization setting, where given the data of one domain, a\nmodel is designed and developed with maximal performance w.r.t. an unseen\ndomain of distinct distribution. Since brain morphology is known to play a\ncrucial role in Alzheimer's diagnosis, we propose the use of learnable\npseudo-morphological modules aimed at producing shape-aware, anatomically\nmeaningful class-specific augmentations in combination with a supervised\ncontrastive learning module to extract robust class-specific representations.\nExperiments conducted across three datasets show improved performance and\ngeneralization capacity, especially under class imbalance and imaging protocol\nvariations. The source code will be made available upon acceptance at\nhttps://github.com/zobia111/SDG-Alzheimer.",
      "url": "http://arxiv.org/abs/2505.22465v1",
      "published_time_eastern_timestamp": 1748445496.0
    },
    {
      "title": "SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail\n  Voxels",
      "summary": "3D occupancy prediction has attracted much attention in the field of\nautonomous driving due to its powerful geometric perception and object\nrecognition capabilities. However, existing methods have not explored the most\nessential distribution patterns of voxels, resulting in unsatisfactory results.\nThis paper first explores the inter-class distribution and geometric\ndistribution of voxels, thereby solving the long-tail problem caused by the\ninter-class distribution and the poor performance caused by the geometric\ndistribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail\nOccupancy), which uses sparse head-tail voxel construction to accurately\nidentify and balance key voxels in the head and tail classes, while using\ndecoupled learning to reduce the model's bias towards the dominant (head)\ncategory and enhance the focus on the tail class. Experiments show that\nsignificant improvements have been made on multiple baselines: SHTOcc reduces\nGPU memory usage by 42.2%, increases inference speed by 58.6%, and improves\naccuracy by about 7%, verifying its effectiveness and efficiency. The code is\navailable at https://github.com/ge95net/SHTOcc",
      "url": "http://arxiv.org/abs/2505.22461v1",
      "published_time_eastern_timestamp": 1748445375.0
    },
    {
      "title": "Universal Domain Adaptation for Semantic Segmentation",
      "summary": "Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to\ntransfer knowledge from labeled source data to unlabeled target data. However,\ntraditional UDA-SS methods assume that category settings between source and\ntarget domains are known, which is unrealistic in real-world scenarios. This\nleads to performance degradation if private classes exist. To address this\nlimitation, we propose Universal Domain Adaptation for Semantic Segmentation\n(UniDA-SS), achieving robust adaptation even without prior knowledge of\ncategory settings. We define the problem in the UniDA-SS scenario as low\nconfidence scores of common classes in the target domain, which leads to\nconfusion with private classes. To solve this problem, we propose UniMAP:\nUniDA-SS with Image Matching and Prototype-based Distinction, a novel framework\ncomposed of two key components. First, Domain-Specific Prototype-based\nDistinction (DSPD) divides each class into two domain-specific prototypes,\nenabling finer separation of domain-specific features and enhancing the\nidentification of common classes across domains. Second, Target-based Image\nMatching (TIM) selects a source image containing the most common-class pixels\nbased on the target pseudo-label and pairs it in a batch to promote effective\nlearning of common classes. We also introduce a new UniDA-SS benchmark and\ndemonstrate through various experiments that UniMAP significantly outperforms\nbaselines. The code is available at\n\\href{https://github.com/KU-VGI/UniMAP}{this https URL}.",
      "url": "http://arxiv.org/abs/2505.22458v1",
      "published_time_eastern_timestamp": 1748445251.0
    },
    {
      "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO",
      "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT.",
      "url": "http://arxiv.org/abs/2505.22453v1",
      "published_time_eastern_timestamp": 1748445076.0
    },
    {
      "title": "On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene\n  Segmentation",
      "summary": "The emergence of large-scale pre-trained point cloud models has significantly\nadvanced 3D scene understanding, but adapting these models to specific\ndownstream tasks typically demands full fine-tuning, incurring high\ncomputational and storage costs. Parameter-efficient fine-tuning (PEFT)\ntechniques, successful in natural language processing and 2D vision tasks,\nwould underperform when naively applied to 3D point cloud models due to\nsignificant geometric and spatial distribution shifts. Existing PEFT methods\ncommonly treat points as orderless tokens, neglecting important local spatial\nstructures and global geometric contexts in 3D modeling. To bridge this gap, we\nintroduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT\nmodule specifically designed for 3D point cloud transformers. GEM explicitly\nintegrates fine-grained local positional encodings with a lightweight latent\nattention mechanism to capture comprehensive global context, thereby\neffectively addressing the spatial and geometric distribution mismatch.\nExtensive experiments demonstrate that GEM achieves performance comparable to\nor sometimes even exceeding full fine-tuning, while only updating 1.6% of the\nmodel's parameters, fewer than other PEFT methods. With significantly reduced\ntraining time and memory requirements, our approach thus sets a new benchmark\nfor efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point\ncloud models. Code will be released.",
      "url": "http://arxiv.org/abs/2505.22444v1",
      "published_time_eastern_timestamp": 1748444916.0
    },
    {
      "title": "Brightify: A tool for calculating brightness in neutron sources",
      "summary": "Brightness is a critical metric for optimizing the design of neutron sources\nand beamlines, yet there is no direct way to calculate brightness within most\nMonte Carlo packages used for neutron source simulation. In this paper, we\npresent Brightify, an open-source Python-based tool designed to calculate\nbrightness from Monte Carlo Particle List (MCPL) files, which can be extracted\nfrom several Monte Carlo simulation packages. Brightify provides an efficient\ncomputational approach to calculate brightness for any particle type and energy\nspectrum recorded in the MCPL file. It enables localized,\ndirectionally-resolved brightness evaluations by scanning across both spatial\nand angular domains, facilitating the identification of positions and\ndirections corresponding to maximum brightness. This functionality is\nparticularly valuable for identifying brightness hotspots and helping fine-tune\nthe design of neutron sources for optimal performance. We validate Brightify\nagainst standard methods, such as surface current tally and point estimator\ntally, and demonstrate its accuracy and adaptability, particularly in\nhigh-resolution analyses. By overcoming the limitations of traditional methods,\nBrightify streamlines neutron source re-optimization, reduces computational\nburden, and accelerates source development workflows. The full code is\navailable on the Brightify GitHub repository.",
      "url": "http://arxiv.org/abs/2505.22431v1",
      "published_time_eastern_timestamp": 1748444136.0
    },
    {
      "title": "RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network",
      "summary": "This paper presents a groundbreaking approach - the first online automatic\ngeometric calibration method for radar and camera systems. Given the\nsignificant data sparsity and measurement uncertainty in radar height data,\nachieving automatic calibration during system operation has long been a\nchallenge. To address the sparsity issue, we propose a Dual-Perspective\nrepresentation that gathers features from both frontal and bird's-eye views.\nThe frontal view contains rich but sensitive height information, whereas the\nbird's-eye view provides robust features against height uncertainty. We thereby\npropose a novel Selective Fusion Mechanism to identify and fuse reliable\nfeatures from both perspectives, reducing the effect of height uncertainty.\nMoreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism\nto explicitly find location correspondences through cross-modal matching.\nDuring the training phase, we also design a Noise-Resistant Matcher to provide\nbetter supervision and enhance the robustness of the matching mechanism against\nsparsity and height uncertainty. Our experimental results, tested on the\nnuScenes dataset, demonstrate that our method significantly outperforms\nprevious radar-camera auto-calibration methods, as well as existing\nstate-of-the-art LiDAR-camera calibration techniques, establishing a new\nbenchmark for future research. The code is available at\nhttps://github.com/nycu-acm/RC-AutoCalib.",
      "url": "http://arxiv.org/abs/2505.22427v1",
      "published_time_eastern_timestamp": 1748443951.0
    }
  ]
}
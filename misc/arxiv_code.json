{
  "last_updated": "2025-07-21T08:26:27.409959-04:00",
  "papers": [
    {
      "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation\n  Learning",
      "summary": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.",
      "url": "http://arxiv.org/abs/2507.14137v1",
      "published_time_eastern_timestamp": 1752861595.0
    },
    {
      "title": "OpenBEATs: A Fully Open-Source General-Purpose Audio Encoder",
      "summary": "Masked token prediction has emerged as a powerful pre-training objective\nacross language, vision, and speech, offering the potential to unify these\ndiverse modalities through a single pre-training task. However, its application\nfor general audio understanding remains underexplored, with BEATs being the\nonly notable example. BEATs has seen limited modifications due to the absence\nof open-source pre-training code. Furthermore, BEATs was trained only on\nAudioSet, restricting its broader downstream applicability. To address these\ngaps, we present OpenBEATs, an open-source framework that extends BEATs via\nmulti-domain audio pre-training. We conduct comprehensive evaluations across\nsix types of tasks, twenty five datasets, and three audio domains, including\naudio reasoning tasks such as audio question answering, entailment, and\ncaptioning. OpenBEATs achieves state-of-the-art performance on six bioacoustics\ndatasets, two environmental sound datasets and five reasoning datasets,\nperforming better than models exceeding a billion parameters at one-fourth\ntheir parameter size. These results demonstrate the effectiveness of\nmulti-domain datasets and masked token prediction task to learn general-purpose\naudio representations. To promote further research and reproducibility, we\nrelease all pre-training and evaluation code, pretrained and fine-tuned\ncheckpoints, and training logs at https://shikhar-s.github.io/OpenBEATs",
      "url": "http://arxiv.org/abs/2507.14129v1",
      "published_time_eastern_timestamp": 1752861466.0
    },
    {
      "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement\n  Learning",
      "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.",
      "url": "http://arxiv.org/abs/2507.14111v1",
      "published_time_eastern_timestamp": 1752860636.0
    },
    {
      "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based\n  Classification in Computed Tomography",
      "summary": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL",
      "url": "http://arxiv.org/abs/2507.14102v1",
      "published_time_eastern_timestamp": 1752859856.0
    },
    {
      "title": "Glucose-ML: A collection of longitudinal diabetes datasets for\n  development of robust AI solutions",
      "summary": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.",
      "url": "http://arxiv.org/abs/2507.14077v1",
      "published_time_eastern_timestamp": 1752857585.0
    },
    {
      "title": "Error Correcting Codes for Segmented Burst-Deletion Channels",
      "summary": "We study segmented burst-deletion channels motivated by the observation that\nsynchronization errors commonly occur in a bursty manner in real-world\nsettings. In this channel model, transmitted sequences are implicitly divided\ninto non-overlapping segments, each of which may experience at most one burst\nof deletions. In this paper, we develop error correction codes for segmented\nburst-deletion channels over arbitrary alphabets under the assumption that each\nsegment may contain only one burst of t-deletions. The main idea is to encode\nthe input subsequence corresponding to each segment using existing one-burst\ndeletion codes, with additional constraints that enable the decoder to identify\nsegment boundaries during the decoding process from the received sequence. The\nresulting codes achieve redundancy that scales as O(log b), where b is the\nlength of each segment.",
      "url": "http://arxiv.org/abs/2507.14070v1",
      "published_time_eastern_timestamp": 1752857362.0
    },
    {
      "title": "Formal Concept Analysis and Homotopical Combinatorics",
      "summary": "Formal Concept Analysis makes the fundamental observation that any complete\nlattice $(L, \\leq)$ is determined up to isomorphism by the restriction of the\nrelation ${\\leq} \\subseteq L \\times L$ to the set $J(L) \\times M(L)$, where\n$J(L)$ is the set of join-irreducible elements of $L$ and $M(L)$ is the set of\nmeet-irreducible elements of $L$.\n  For any finite lattice $L$ equipped with the action of a finite group $G$, we\nexplicitly describe this restricted relation for the lattice of transfer\nsystems $\\mathsf{Tr}(L)$ in terms of $L$ only. We apply this to give new\ncomputations of the number of transfer systems for certain finite groups, and\nto produce bounds on the number of transfer systems on certain families of\nabelian finite groups. We also provide computer code to enable other\nresearchers' use of these techniques.",
      "url": "http://arxiv.org/abs/2507.14068v1",
      "published_time_eastern_timestamp": 1752857191.0
    },
    {
      "title": "Bounds and Constructions of High-Memory Spatially-Coupled Codes",
      "summary": "In this paper, we apply the Clique Lov\\'asz Local Lemma to provide sufficient\nconditions on memory and lifting degree for removing certain harmful\ncombinatorial structures in spatially-coupled (SC) codes that negatively impact\ndecoding performance. Additionally, we present, for the first time, a\nconstructive algorithm based on the Moser-Tardos algorithm that ensures\npredictable performance. Furthermore, leveraging the properties of\nLLL-distribution and M-T-distribution, we establish the dependencies among the\nharmful structures during the construction process. We provide upper bounds on\nthe probability change of remaining harmful structures after eliminating some\nof them. In particular, the elimination of 4-cycles increases the probability\nof 6-cycles becoming active by at most a factor of $e^{8/3}$.",
      "url": "http://arxiv.org/abs/2507.14064v1",
      "published_time_eastern_timestamp": 1752856944.0
    },
    {
      "title": "Noradrenergic-inspired gain modulation attenuates the stability gap in\n  joint training",
      "summary": "Recent studies in continual learning have identified a transient drop in\nperformance on mastered tasks when assimilating new ones, known as the\nstability gap. Such dynamics contradict the objectives of continual learning,\nrevealing a lack of robustness in mitigating forgetting, and notably,\npersisting even under an ideal joint-loss regime. Examining this gap within\nthis idealized joint training context is critical to isolate it from other\nsources of forgetting. We argue that it reflects an imbalance between rapid\nadaptation and robust retention at task boundaries, underscoring the need to\ninvestigate mechanisms that reconcile plasticity and stability within continual\nlearning frameworks. Biological brains navigate a similar dilemma by operating\nconcurrently on multiple timescales, leveraging neuromodulatory signals to\nmodulate synaptic plasticity. However, artificial networks lack native\nmultitimescale dynamics, and although optimizers like momentum-SGD and Adam\nintroduce implicit timescale regularization, they still exhibit stability gaps.\nInspired by locus coeruleus mediated noradrenergic bursts, which transiently\nenhance neuronal gain under uncertainty to facilitate sensory assimilation, we\npropose uncertainty-modulated gain dynamics - an adaptive mechanism that\napproximates a two-timescale optimizer and dynamically balances integration of\nknowledge with minimal interference on previously consolidated information. We\nevaluate our mechanism on domain-incremental and class-incremental variants of\nthe MNIST and CIFAR benchmarks under joint training, demonstrating that\nuncertainty-modulated gain dynamics effectively attenuate the stability gap.\nFinally, our analysis elucidates how gain modulation replicates noradrenergic\nfunctions in cortical circuits, offering mechanistic insights into reducing\nstability gaps and enhance performance in continual learning tasks.",
      "url": "http://arxiv.org/abs/2507.14056v1",
      "published_time_eastern_timestamp": 1752856446.0
    },
    {
      "title": "Foundation Models as Class-Incremental Learners for Dermatological Image\n  Classification",
      "summary": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.",
      "url": "http://arxiv.org/abs/2507.14050v1",
      "published_time_eastern_timestamp": 1752855351.0
    },
    {
      "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing",
      "summary": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.",
      "url": "http://arxiv.org/abs/2507.14024v1",
      "published_time_eastern_timestamp": 1752853959.0
    },
    {
      "title": "Conformalized Regression for Continuous Bounded Outcomes",
      "summary": "Regression problems with bounded continuous outcomes frequently arise in\nreal-world statistical and machine learning applications, such as the analysis\nof rates and proportions. A central challenge in this setting is predicting a\nresponse associated with a new covariate value. Most of the existing\nstatistical and machine learning literature has focused either on point\nprediction of bounded outcomes or on interval prediction based on asymptotic\napproximations. We develop conformal prediction intervals for bounded outcomes\nbased on transformation models and beta regression. We introduce tailored\nnon-conformity measures based on residuals that are aligned with the underlying\nmodels, and account for the inherent heteroscedasticity in regression settings\nwith bounded outcomes. We present a theoretical result on asymptotic marginal\nand conditional validity in the context of full conformal prediction, which\nremains valid under model misspecification. For split conformal prediction, we\nprovide an empirical coverage analysis based on a comprehensive simulation\nstudy. The simulation study demonstrates that both methods provide valid\nfinite-sample predictive coverage, including settings with model\nmisspecification. Finally, we demonstrate the practical performance of the\nproposed conformal prediction intervals on real data and compare them with\nbootstrap-based alternatives.",
      "url": "http://arxiv.org/abs/2507.14023v1",
      "published_time_eastern_timestamp": 1752853908.0
    },
    {
      "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation",
      "summary": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.",
      "url": "http://arxiv.org/abs/2507.13985v1",
      "published_time_eastern_timestamp": 1752849954.0
    },
    {
      "title": "Secretive Hotplug Coded Caching",
      "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
      "url": "http://arxiv.org/abs/2507.13961v1",
      "published_time_eastern_timestamp": 1752848669.0
    },
    {
      "title": "Convergent transformations of visual representation in brains and models",
      "summary": "A fundamental question in cognitive neuroscience is what shapes visual\nperception: the external world's structure or the brain's internal\narchitecture. Although some perceptual variability can be traced to individual\ndifferences, brain responses to naturalistic stimuli evoke similar activity\npatterns across individuals, suggesting a convergent representational\nprinciple. Here, we test if this stimulus-driven convergence follows a common\ntrajectory across people and deep neural networks (DNNs) during its\ntransformation from sensory to high-level internal representations. We\nintroduce a unified framework that traces representational flow by combining\ninter-subject similarity with alignment to model hierarchies. Applying this\nframework to three independent fMRI datasets of visual scene perception, we\nreveal a cortex-wide network, conserved across individuals, organized into two\npathways: a medial-ventral stream for scene structure and a lateral-dorsal\nstream tuned for social and biological content. This functional organization is\ncaptured by the hierarchies of vision DNNs but not language models, reinforcing\nthe specificity of the visual-to-semantic transformation. These findings show a\nconvergent computational solution for visual encoding in both human and\nartificial vision, driven by the structure of the external world.",
      "url": "http://arxiv.org/abs/2507.13941v1",
      "published_time_eastern_timestamp": 1752848034.0
    },
    {
      "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization",
      "summary": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.",
      "url": "http://arxiv.org/abs/2507.13934v1",
      "published_time_eastern_timestamp": 1752847758.0
    },
    {
      "title": "Software architecture and manual for novel versatile CT image analysis\n  toolbox -- AnatomyArchive",
      "summary": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.",
      "url": "http://arxiv.org/abs/2507.13901v1",
      "published_time_eastern_timestamp": 1752845312.0
    },
    {
      "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative\n  Analysis of Methodologies",
      "summary": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.",
      "url": "http://arxiv.org/abs/2507.13875v1",
      "published_time_eastern_timestamp": 1752843281.0
    },
    {
      "title": "CodeEdu: A Multi-Agent Collaborative Platform for Personalized Coding\n  Education",
      "summary": "Large Language Models (LLMs) have demonstrated considerable potential in\nimproving coding education by providing support for code writing, explanation,\nand debugging. However, existing LLM-based approaches generally fail to assess\nstudents' abilities, design learning plans, provide personalized material\naligned with individual learning goals, and enable interactive learning.\nCurrent work mostly uses single LLM agents, which limits their ability to\nunderstand complex code repositories and schedule step-by-step tutoring. Recent\nresearch has shown that multi-agent LLMs can collaborate to solve complicated\nproblems in various domains like software engineering, but their potential in\nthe field of education remains unexplored. In this work, we introduce CodeEdu,\nan innovative multi-agent collaborative platform that combines LLMs with tool\nuse to provide proactive and personalized education in coding. Unlike static\npipelines, CodeEdu dynamically allocates agents and tasks to meet student\nneeds. Various agents in CodeEdu undertake certain functions specifically,\nincluding task planning, personalized material generation, real-time QA,\nstep-by-step tutoring, code execution, debugging, and learning report\ngeneration, facilitated with extensive external tools to improve task\nefficiency. Automated evaluations reveal that CodeEdu substantially enhances\nstudents' coding performance.",
      "url": "http://arxiv.org/abs/2507.13814v1",
      "published_time_eastern_timestamp": 1752835942.0
    },
    {
      "title": "Asymptotically Optimal Codes Correcting One Substring Edit",
      "summary": "The substring edit error is the operation of replacing a substring $u$ of $x$\nwith another string $v$, where the lengths of $u$ and $v$ are bounded by a\ngiven constant $k$. It encompasses localized insertions, deletions, and\nsubstitutions within a window. Codes correcting one substring edit have\nredundancy at least $\\log n+k$. In this paper, we construct codes correcting\none substring edit with redundancy $\\log n+O(\\log \\log n)$, which is\nasymptotically optimal.",
      "url": "http://arxiv.org/abs/2507.13808v1",
      "published_time_eastern_timestamp": 1752835027.0
    }
  ]
}
{
  "last_updated": "2025-11-13T13:17:28.658694-05:00",
  "papers": [
    {
      "title": "JaCoText: A Pretrained Model for Java Code-Text Generation",
      "summary": "Pretrained transformer-based models have shown high performance in natural language generation task. However, a new wave of interest has surged: automatic programming language generation. This task consists of translating natural language instructions to a programming code. Despite the fact that well-known pretrained models on language generation have achieved good performance in learning programming languages, effort is still needed in automatic code generation. In this paper, we introduce JaCoText, a model based on Transformers neural network. It aims to generate java source code from natural language text. JaCoText leverages advantages of both natural language and code generation models. More specifically, we study some findings from the state of the art and use them to (1) initialize our model from powerful pretrained models, (2) explore additional pretraining on our java dataset, (3) carry out experiments combining the unimodal and bimodal data in the training, and (4) scale the input and output length during the fine-tuning of the model. Conducted experiments on CONCODE dataset show that JaCoText achieves new state-of-the-art results.",
      "url": "http://arxiv.org/abs/2303.12869v1",
      "published_time_eastern_timestamp": 1679511685.0
    },
    {
      "title": "JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed Social Media Texts",
      "summary": "This paper reports about our work in the NLP Tool Contest @ICON-2017, shared task on Sentiment Analysis for Indian Languages (SAIL) (code mixed). To implement our system, we have used a machine learning algo-rithm called Multinomial Na√Øve Bayes trained using n-gram and SentiWordnet features. We have also used a small SentiWordnet for English and a small SentiWordnet for Bengali. But we have not used any SentiWordnet for Hindi language. We have tested our system on Hindi-English and Bengali-English code mixed social media data sets released for the contest. The performance of our system is very close to the best system participated in the contest. For both Bengali-English and Hindi-English runs, our system was ranked at the 3rd position out of all submitted runs and awarded the 3rd prize in the contest.",
      "url": "http://arxiv.org/abs/1802.05737v1",
      "published_time_eastern_timestamp": 1518724963.0
    },
    {
      "title": "pynucastro: an interface to nuclear reaction rates and code generator for reaction network equations",
      "summary": "pynucastro addresses two needs in the field of nuclear astrophysics: visual exploration of nuclear reaction rates or networks and automated code generation for integrating reaction network ODEs. pynucastro accomplishes this by interfacing with nuclear reaction rate parameterizations published by the JINA Reaclib project (Cyburt et al. 2010).",
      "url": "http://arxiv.org/abs/1803.08920v1",
      "published_time_eastern_timestamp": 1521580634.0
    },
    {
      "title": "Some punctured codes of several families of binary linear codes",
      "summary": "Two general constructions of linear codes with functions over finite fields have been extensively studied in the literature. The first one is given by $\\mathcal{C}(f)=\\left\\{ {\\rm Tr}(af(x)+bx)_{x \\in \\mathbb{F}_{q^m}^*}: a,b \\in \\mathbb{F}_{q^m} \\right\\}$, where $q$ is a prime power, $\\bF_{q^m}^*=\\bF_{q^m} \\setminus \\{0\\}$, $\\tr$ is the trace function from $\\bF_{q^m}$ to $\\bF_q$, and $f(x)$ is a function from $\\mathbb{F}_{q^m}$ to $\\mathbb{F}_{q^m}$ with $f(0)=0$. Almost bent functions, quadratic functions and some monomials on $\\bF_{2^m}$ were used in the first construction, and many families of binary linear codes with few weights were obtained in the literature. This paper studies some punctured codes of these binary codes. Several families of binary linear codes with few weights and new parameters are obtained in this paper. Several families of distance-optimal binary linear codes with new parameters are also produced in this paper.",
      "url": "http://arxiv.org/abs/2101.08425v1",
      "published_time_eastern_timestamp": 1611200798.0
    },
    {
      "title": "Understanding Code Patterns - Analysis, Interpretation & Measurement",
      "summary": "This research paper aims to find, analyze and understand code patterns in any software system and measure its quality by defining standards and proposing a formula for the same. Every code that is written can be divided into different code segments, each having its own impact on the overall system. We can analyze these code segments to get the code quality. The measures used in this paper include Lines of Code, Number of calls made by a module, Execution time, the system knowledge of user and developers, the use of generalization, inheritance, reusability and other object-oriented concepts. The entire software code is divided into code snippets, based on the logic that they implement. Each of these code snippets has an impact. This measure is called Impact Factor and is valued by the software developer and/or other system stakeholders. Efficiency = (Code Area / Execution Time) * Qr",
      "url": "http://arxiv.org/abs/1106.6159v1",
      "published_time_eastern_timestamp": 1309426398.0
    },
    {
      "title": "Robust Learning of Diverse Code Edits",
      "summary": "Software engineering activities frequently involve edits to existing code. However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements. In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm. Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity. Today's code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning. To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting. Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones. We show the generality of our approach on two model families (DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation and general problem-solving abilities post adaptation. We opensource the models, synthetic dataset, and implementation at https://aka.ms/nextcoder.",
      "url": "http://arxiv.org/abs/2503.03656v2",
      "published_time_eastern_timestamp": 1741192744.0
    },
    {
      "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs",
      "summary": "In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.",
      "url": "http://arxiv.org/abs/2502.19411v1",
      "published_time_eastern_timestamp": 1740596142.0
    },
    {
      "title": "SCC: Automatic Classification of Code Snippets",
      "summary": "Determining the programming language of a source code file has been considered in the research community; it has been shown that Machine Learning (ML) and Natural Language Processing (NLP) algorithms can be effective in identifying the programming language of source code files. However, determining the programming language of a code snippet or a few lines of source code is still a challenging task. Online forums such as Stack Overflow and code repositories such as GitHub contain a large number of code snippets. In this paper, we describe Source Code Classification (SCC), a classifier that can identify the programming language of code snippets written in 21 different programming languages. A Multinomial Naive Bayes (MNB) classifier is employed which is trained using Stack Overflow posts. It is shown to achieve an accuracy of 75% which is higher than that with Programming Languages Identification (PLI a proprietary online classifier of snippets) whose accuracy is only 55.5%. The average score for precision, recall and the F1 score with the proposed tool are 0.76, 0.75 and 0.75, respectively. In addition, it can distinguish between code snippets from a family of programming languages such as C, C++ and C#, and can also identify the programming language version such as C# 3.0, C# 4.0 and C# 5.0.",
      "url": "http://arxiv.org/abs/1809.07945v1",
      "published_time_eastern_timestamp": 1537505440.0
    },
    {
      "title": "On the largest minimum distances of [n,6] LCD codes",
      "summary": "Linear complementary dual (LCD) codes can be used to against side-channel attacks and fault noninvasive attacks. Let $d_{a}(n,6)$ and $d_{l}(n,6)$ be the minimum weights of all binary optimal linear codes and LCD codes with length $n$ and dimension 6, respectively.In this article, we aim to obtain the values of $d_{l}(n,6)$ for $n\\geq 51$ by investigating the nonexistence and constructions of LCD codes with given parameters. Suppose that $s \\ge 0$ and $0\\leq t\\leq 62$ are two integers and $n=63s+t$. Using the theories of defining vectors, generalized anti-codes, reduced codes and nested codes, we exactly determine $d_{l}(n,6)$ for $t \\notin\\{21,22,25,26,33,34,37,38,45,46\\}$, while we show that $d_{l}(n,6)\\in$$\\{d_{a}(n,6)$ $-1,d_{a}(n,6)\\}$ for $t\\in\\{21,22,26,34,37,38,46\\}$ and $ d_{l}(n,6)\\in$$ \\{d_{a}(n,6)-2,$ $d_{a}(n,6)-1\\}$ for$t\\in{25,33,45\\}$.",
      "url": "http://arxiv.org/abs/2406.02065v1",
      "published_time_eastern_timestamp": 1717487159.0
    },
    {
      "title": "EVOR: Evolving Retrieval for Code Generation",
      "summary": "Recently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge bases with a single source, limiting the adaptation capabilities of Large Language Models (LLMs) to domains they have insufficient knowledge of. In this work, we develop a novel pipeline, EVOR, that employs the synchronous evolution of both queries and diverse knowledge bases. On two realistic settings where the external knowledge is required to solve code generation tasks, we compile four new datasets associated with frequently updated libraries and long-tail programming languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR achieves two to four times of execution accuracy compared to other methods such as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We demonstrate that EVOR is flexible and can be easily combined with them to achieve further improvement. Further analysis reveals that EVOR benefits from the synchronous evolution of queries and documents and the diverse information sources in the knowledge base. We hope that our studies will inspire more insights into the design of advanced RACG pipelines in future research. Our model, code, and data are available at https://arks-codegen.github.io.",
      "url": "http://arxiv.org/abs/2402.12317v2",
      "published_time_eastern_timestamp": 1708364248.0
    },
    {
      "title": "DeCoMa: Detecting and Purifying Code Dataset Watermarks through Dual Channel Code Abstraction",
      "summary": "Watermarking is a technique to help identify the source of data points, which can be used to help prevent the misuse of protected datasets. Existing methods on code watermarking, leveraging the idea from the backdoor research, embed stealthy triggers as watermarks. Despite their high resilience against dilution attacks and backdoor detections, the robustness has not been fully evaluated. To fill this gap, we propose DeCoMa, a dual-channel approach to Detect and purify Code dataset waterMarks. To overcome the high barrier created by the stealthy and hidden nature of code watermarks, DeCoMa leverages dual-channel constraints on code to generalize and map code samples into standardized templates. Subsequently, DeCoMa extracts hidden watermarks by identifying outlier associations between paired elements within the standardized templates. Finally, DeCoMa purifies the watermarked dataset by removing all samples containing the detected watermark, enabling the silent appropriation of protected code. We conduct extensive experiments to evaluate the effectiveness and efficiency of DeCoMa, covering 14 types of code watermarks and 3 representative intelligent code tasks (a total of 14 scenarios). Experimental results demonstrate that DeCoMa achieves a stable recall of 100% in 14 code watermark detection scenarios, significantly outperforming the baselines. Additionally, DeCoMa effectively attacks code watermarks with embedding rates as low as 0.1%, while maintaining comparable model performance after training on the purified dataset. Furthermore, as DeCoMa requires no model training for detection, it achieves substantially higher efficiency than all baselines, with a speedup ranging from 31.5 to 130.9X. The results call for more advanced watermarking techniques for code models, while DeCoMa can serve as a baseline for future evaluation. Code is available at https://github.com/xiaoyuanpigo/DeCoMa",
      "url": "http://arxiv.org/abs/2504.07002v2",
      "published_time_eastern_timestamp": 1744215551.0
    },
    {
      "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging",
      "summary": "While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.",
      "url": "http://arxiv.org/abs/2410.01215v3",
      "published_time_eastern_timestamp": 1727841441.0
    },
    {
      "title": "Unified Framework for Quantum Code Embedding",
      "summary": "Given a Calderbank-Shor-Steane (CSS) code, it is sometimes necessary to modify the stabilizer code by adding an arbitrary number of physical qubits and parity checks. Motivations may include embedding a high-dimensional quantum low-density parity check (LDPC) code into finite-dimensional Euclidean space, or reducing the weights of an arbitrary CSS code. During this embedding, it is essential that the modified CSS code possesses an isomorphic set of logical qubits as the original CSS code. However, despite numerous explicit constructions, the conditions of when such a property holds true is not known in general. Therefore, using the language of homological algebra, we provide a unified framework that guarantees a natural isomorphism between the output and input codes. In particular, we explicitly show how previous works fit into our framework.",
      "url": "http://arxiv.org/abs/2507.05361v2",
      "published_time_eastern_timestamp": 1751911206.0
    },
    {
      "title": "Floquet codes without parent subsystem codes",
      "summary": "We propose a new class of error-correcting dynamic codes in two and three dimensions that has no explicit connection to any parent subsystem code. The two-dimensional code, which we call the CSS honeycomb code, is geometrically similar to that of the honeycomb code by Hastings and Haah, and also dynamically embeds an instantaneous toric code. However, unlike the honeycomb code it possesses an explicit CSS structure and its gauge checks do not form a subsystem code. Nevertheless, we show that our dynamic protocol conserves logical information and possesses a threshold for error correction. We generalize this construction to three dimensions and obtain a code that fault-tolerantly alternates between realizing two type-I fracton models, the checkerboard and the X-cube model. Finally, we show the compatibility of our CSS honeycomb code protocol and the honeycomb code by showing the possibility of randomly switching between the two protocols without information loss while still measuring error syndromes. We call this more general aperiodic structure `dynamic tree codes', which we also generalize to three dimensions. We construct a probabilistic finite automaton prescription that generates dynamic tree codes correcting any single-qubit Pauli errors and can be viewed as a step towards the development of practical fault-tolerant random codes.",
      "url": "http://arxiv.org/abs/2210.02468v4",
      "published_time_eastern_timestamp": 1664992802.0
    },
    {
      "title": "The DTFE public software: The Delaunay Tessellation Field Estimator code",
      "summary": "We present the DTFE public software, a code for reconstructing fields from a discrete set of samples/measurements using the maximum of information contained in the point distribution. The code is written in C++ using the CGAL library and is parallelized using OpenMP. The software was designed for the analysis of cosmological data but can be used in other fields where one must interpolate quantities given at a discrete point set. The software comes with a wide suite of options to facilitate the analysis of 2- and 3-dimensional data and of both numerical simulations and galaxy redshift surveys. For comparison purposes, the code also implements the TSC and SPH grid interpolation methods. The code comes with an extensive user guide detailing the program options, examples and the inner workings of the code. The DTFE public software and further information can be found at http://www.astro.rug.nl/~voronoi/DTFE/dtfe.html .",
      "url": "http://arxiv.org/abs/1105.0370v4",
      "published_time_eastern_timestamp": 1304352015.0
    },
    {
      "title": "A new three-dimensional general-relativistic hydrodynamics code",
      "summary": "We present a new three-dimensional general relativistic hydrodynamics code, the Whisky code. This code incorporates the expertise developed over the past years in the numerical solution of Einstein equations and of the hydrodynamics equations in a curved spacetime, and is the result of a collaboration of several European Institutes. We here discuss the ability of the code to carry out long-term accurate evolutions of the linear and nonlinear dynamics of isolated relativistic stars.",
      "url": "http://arxiv.org/abs/1004.3849v1",
      "published_time_eastern_timestamp": 1271919785.0
    },
    {
      "title": "Endomorphisms of Linear Block Codes",
      "summary": "The automorphism groups of various linear codes are extensively studied yielding insights into the respective code structure. This knowledge is used in, e.g., theoretical analysis and in improving decoding performance, motivating the analyses of endomorphisms of linear codes. In this work, we discuss the structure of the set of transformation matrices of code endomorphisms, defined as a generalization of code automorphisms, and provide an explicit construction of a bijective mapping between the image of an endomorphism and its canonical quotient space. Furthermore, we introduce a one-to-one mapping between the set of transformation matrices of endomorphisms and a larger linear block code enabling the use of well-known algorithms for the search for suitable endomorphisms. Additionally, we propose an approach to obtain unknown code endomorphisms based on automorphisms of the code. Furthermore, we consider ensemble decoding as a possible use case for endomorphisms by introducing endomorphism ensemble decoding. Interestingly, EED can improve decoding performance when other ensemble decoding schemes are not applicable.",
      "url": "http://arxiv.org/abs/2402.00562v2",
      "published_time_eastern_timestamp": 1706791757.0
    },
    {
      "title": "Projective error models: Stabilizer codes, Clifford codes, and weak stabilizer codes",
      "summary": "We introduce more general notions of Clifford codes and stabilizer codes, the latter we call weak stabilizer codes. This is all formulated in the language of projective representation theory of finite groups and we give a novel description of the detectable errors for a Clifford code. We give a complete characterization of when a Clifford code is also a weak stabilizer code in the case where the considered error model is a nice error basis. We also give examples of infinite families of non-stabilizer Clifford codes as well as examples of non-Clifford weak stabilizer codes. The latter of these types of examples is a class of codes that have not been studied in the same systematic framework as Clifford codes and stabilizer codes.",
      "url": "http://arxiv.org/abs/2506.01843v2",
      "published_time_eastern_timestamp": 1748881784.0
    },
    {
      "title": "LoRACode: LoRA Adapters for Code Embeddings",
      "summary": "Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations. To foster research in this area, we make our code and pre-trained models publicly available.",
      "url": "http://arxiv.org/abs/2503.05315v2",
      "published_time_eastern_timestamp": 1741344645.0
    },
    {
      "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
      "summary": "Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan & Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages. Project site with code and data: https://intercode-benchmark.github.io",
      "url": "http://arxiv.org/abs/2306.14898v3",
      "published_time_eastern_timestamp": 1687802390.0
    }
  ]
}
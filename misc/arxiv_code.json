{
  "last_updated": "2025-07-04T02:18:27.777769-04:00",
  "papers": [
    {
      "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer\n  Memory",
      "summary": "Dense 3D scene reconstruction from an ordered sequence or unordered image\ncollections is a critical step when bringing research in computer vision into\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\nan image pair densely into a shared coordinate system, subsequent methods\nmaintain an implicit memory to achieve dense 3D reconstruction from more\nimages. However, such implicit memory is limited in capacity and may suffer\nfrom information loss of earlier frames. We propose Point3R, an online\nframework targeting dense streaming 3D reconstruction. To be specific, we\nmaintain an explicit spatial pointer memory directly associated with the 3D\nstructure of the current scene. Each pointer in this memory is assigned a\nspecific 3D position and aggregates scene information nearby in the global\ncoordinate system into a changing spatial feature. Information extracted from\nthe latest frame interacts explicitly with this pointer memory, enabling dense\nintegration of the current observation into the global coordinate system. We\ndesign a 3D hierarchical position embedding to promote this interaction and\ndesign a simple yet effective fusion mechanism to ensure that our pointer\nmemory is uniform and efficient. Our method achieves competitive or\nstate-of-the-art performance on various tasks with low training costs. Code is\navailable at: https://github.com/YkiWu/Point3R.",
      "url": "http://arxiv.org/abs/2507.02863v1",
      "published_time_eastern_timestamp": 1751565596.0
    },
    {
      "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
      "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
      "url": "http://arxiv.org/abs/2507.02860v1",
      "published_time_eastern_timestamp": 1751565594.0
    },
    {
      "title": "Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation",
      "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.",
      "url": "http://arxiv.org/abs/2507.02856v1",
      "published_time_eastern_timestamp": 1751565542.0
    },
    {
      "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
      "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
      "url": "http://arxiv.org/abs/2507.02857v1",
      "published_time_eastern_timestamp": 1751565542.0
    },
    {
      "title": "MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs",
      "summary": "Recent advancements in the reasoning capabilities of large language models\n(LLMs) show that employing group relative policy optimization (GRPO) algorithm\nfor reinforcement learning (RL) training allows the models to use more\nthinking/reasoning tokens for generating better responses. However, LLMs can\ngenerate only a finite amount of tokens while maintaining attention to the\npreviously generated tokens. This limit, also known as the context size of an\nLLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.\nTo think beyond the limit of context size, an LLM must employ a modular\nthinking strategy to reason over multiple rounds. In this work, we propose\n$\\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL\ntraining method for generating thinking tokens in multiple rounds, effectively\nallowing the model to think with additional context size. We trained the\nopen-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient\nfine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our\nexperiments show 3.8\\% and 3.3\\% improvements over vanilla GRPO based training\nin the respective benchmarks. Furthermore, this improvement was achieved with\nonly 15\\% of samples, thus demonstrating sample efficiency of MOTIF. Our code\nand models are available at https://github.com/purbeshmitra/MOTIF and\nhttps://huggingface.co/purbeshmitra/MOTIF, respectively.",
      "url": "http://arxiv.org/abs/2507.02851v1",
      "published_time_eastern_timestamp": 1751565343.0
    },
    {
      "title": "LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge\n  Injection to All Users",
      "summary": "We describe a vulnerability in language models (LMs) trained with user\nfeedback, whereby a single user can persistently alter LM knowledge and\nbehavior given only the ability to provide prompts and upvote / downvote\nfeedback on LM outputs. To implement the attack, the attacker prompts the LM to\nstochastically output either a \"poisoned\" or benign response, then upvotes the\npoisoned response or downvotes the benign one. When feedback signals are used\nin a subsequent preference tuning behavior, LMs exhibit increased probability\nof producing poisoned responses even in contexts without malicious prompts. We\nshow that this attack can be used to (1) insert factual knowledge the model did\nnot previously possess, (2) modify code generation patterns in ways that\nintroduce exploitable security flaws, and (3) inject fake financial news. Our\nfinding both identifies a new qualitative feature of language model preference\ntuning (showing that it even highly restricted forms of preference data can be\nused to exert fine-grained control over behavior), and a new attack mechanism\nfor LMs trained with user feedback (extending work on pretraining-time data\npoisoning and deployment-time prompt injection).",
      "url": "http://arxiv.org/abs/2507.02850v1",
      "published_time_eastern_timestamp": 1751565340.0
    },
    {
      "title": "MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain\n  Disorder Diagnosis",
      "summary": "Recent evidence suggests that modeling higher-order interactions (HOIs) in\nfunctional magnetic resonance imaging (fMRI) data can enhance the diagnostic\naccuracy of machine learning systems. However, effectively extracting and\nutilizing HOIs remains a significant challenge. In this work, we propose\nMvHo-IB, a novel multi-view learning framework that integrates both pairwise\ninteractions and HOIs for diagnostic decision-making, while automatically\ncompressing task-irrelevant redundant information. MvHo-IB introduces several\nkey innovations: (1) a principled method that combines O-information from\ninformation theory with a matrix-based Renyi alpha-order entropy estimator to\nquantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to\neffectively utilize these interactions, and (3) a new multi-view learning\ninformation bottleneck objective to enhance representation learning.\nExperiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves\nstate-of-the-art performance, significantly outperforming previous methods,\nincluding recent hypergraph-based techniques. The implementation of MvHo-IB is\navailable at https://github.com/zky04/MvHo-IB.",
      "url": "http://arxiv.org/abs/2507.02847v1",
      "published_time_eastern_timestamp": 1751565243.0
    },
    {
      "title": "Legal Requirements Translation from Law",
      "summary": "Software systems must comply with legal regulations, which is a\nresource-intensive task, particularly for small organizations and startups\nlacking dedicated legal expertise. Extracting metadata from regulations to\nelicit legal requirements for software is a critical step to ensure compliance.\nHowever, it is a cumbersome task due to the length and complex nature of legal\ntext. Although prior work has pursued automated methods for extracting\nstructural and semantic metadata from legal text, key limitations remain: they\ndo not consider the interplay and interrelationships among attributes\nassociated with these metadata types, and they rely on manual labeling or\nheuristic-driven machine learning, which does not generalize well to new\ndocuments. In this paper, we introduce an approach based on textual entailment\nand in-context learning for automatically generating a canonical representation\nof legal text, encodable and executable as Python code. Our representation is\ninstantiated from a manually designed Python class structure that serves as a\ndomain-specific metamodel, capturing both structural and semantic legal\nmetadata and their interrelationships. This design choice reduces the need for\nlarge, manually labeled datasets and enhances applicability to unseen\nlegislation. We evaluate our approach on 13 U.S. state data breach notification\nlaws, demonstrating that our generated representations pass approximately 89.4%\nof test cases and achieve a precision and recall of 82.2 and 88.7,\nrespectively.",
      "url": "http://arxiv.org/abs/2507.02846v1",
      "published_time_eastern_timestamp": 1751565228.0
    },
    {
      "title": "Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context\n  Injection",
      "summary": "With the emergence of strong visual-language capabilities, multimodal large\nlanguage models (MLLMs) have demonstrated tremendous potential for real-world\napplications. However, the security vulnerabilities exhibited by the visual\nmodality pose significant challenges to deploying such models in open-world\nenvironments. Recent studies have successfully induced harmful responses from\ntarget MLLMs by encoding harmful textual semantics directly into visual inputs.\nHowever, in these approaches, the visual modality primarily serves as a trigger\nfor unsafe behavior, often exhibiting semantic ambiguity and lacking grounding\nin realistic scenarios. In this work, we define a novel setting: visual-centric\njailbreak, where visual information serves as a necessary component in\nconstructing a complete and realistic jailbreak context. Building on this\nsetting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates\ncontextual dialogue using four distinct visual-focused strategies, dynamically\ngenerating auxiliary images when necessary to construct a visual-centric\njailbreak scenario. To maximize attack effectiveness, it incorporates automatic\ntoxicity obfuscation and semantic refinement to produce a final attack prompt\nthat reliably triggers harmful responses from the target black-box MLLMs.\nSpecifically, VisCo achieves a toxicity score of 4.78 and an Attack Success\nRate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming\nthe baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The\ncode is available at https://github.com/Dtc7w3PQ/Visco-Attack.",
      "url": "http://arxiv.org/abs/2507.02844v1",
      "published_time_eastern_timestamp": 1751565192.0
    },
    {
      "title": "Revealing a transitional epoch of large-scale cosmic anisotropy in the\n  quasar distribution",
      "summary": "The Cosmological Principle posits that the Universe is isotropic on the\nlargest scales. While widely supported, this foundational assumption remains\ntestable. We analyse the angular distribution of over one million quasars from\nthe Gaia-unWISE catalogue using Renyi entropy, a multiscale statistical measure\nsensitive to higher-order clustering. Dividing the sample into three redshift\nbins, we find that both the low- and high-redshift distributions are\nstatistically consistent with isotropy. However, at intermediate redshift ($1\n\\leq z < 2.2$), we detect a statistically significant and scale-dependent\nanisotropy that persists under stringent masking, suggesting a physical origin.\nWe interpret this as evidence for a transitional epoch in cosmic history,\nduring which large-scale structures such as superclusters became prominent\nbefore their growth was gradually damped by the onset of accelerated expansion.\nThese findings position Renyi entropy as a powerful probe of cosmic evolution\nand highlight the potential thermodynamic links between structure formation,\nentropy dissipation, and the emergence of large-scale isotropy.",
      "url": "http://arxiv.org/abs/2507.02835v1",
      "published_time_eastern_timestamp": 1751564718.0
    },
    {
      "title": "Generalizing Verifiable Instruction Following",
      "summary": "A crucial factor for successful human and AI interaction is the ability of\nlanguage models or chatbots to follow human instructions precisely. A common\nfeature of instructions are output constraints like ``only answer with yes or\nno\" or ``mention the word `abrakadabra' at least 3 times\" that the user adds to\ncraft a more useful answer. Even today's strongest models struggle with\nfulfilling such constraints. We find that most models strongly overfit on a\nsmall set of verifiable constraints from the benchmarks that test these\nabilities, a skill called precise instruction following, and are not able to\ngeneralize well to unseen output constraints. We introduce a new benchmark,\nIFBench, to evaluate precise instruction following generalization on 58 new,\ndiverse, and challenging verifiable out-of-domain constraints. In addition, we\nperform an extensive analysis of how and on what data models can be trained to\nimprove precise instruction following generalization. Specifically, we\ncarefully design constraint verification modules and show that reinforcement\nlearning with verifiable rewards (RLVR) significantly improves instruction\nfollowing. In addition to IFBench, we release 29 additional new hand-annotated\ntraining constraints and verification functions, RLVR training prompts, and\ncode.",
      "url": "http://arxiv.org/abs/2507.02833v1",
      "published_time_eastern_timestamp": 1751564673.0
    },
    {
      "title": "HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity\n  Animatable Face Avatars",
      "summary": "We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for\nhigh-quality animatable face avatars. Creating such detailed face avatars from\nvideos is a challenging problem and has numerous applications in augmented and\nvirtual reality. While tremendous successes have been achieved for static\nfaces, animatable avatars from monocular videos still fall in the uncanny\nvalley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face\nthrough a collection of 3D Gaussian primitives. 3DGS excels at rendering static\nfaces, but the state-of-the-art still struggles with nonlinear deformations,\ncomplex lighting effects, and fine details. While most related works focus on\npredicting better Gaussian parameters from expression codes, we rethink the 3D\nGaussian representation itself and how to make it more expressive. Our insights\nlead to a novel extension of 3D Gaussians to high-dimensional multivariate\nGaussians, dubbed 'HyperGaussians'. The higher dimensionality increases\nexpressivity through conditioning on a learnable local embedding. However,\nsplatting HyperGaussians is computationally expensive because it requires\ninverting a high-dimensional covariance matrix. We solve this by\nreparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.\nThis trick boosts the efficiency so that HyperGaussians can be seamlessly\nintegrated into existing models. To demonstrate this, we plug in HyperGaussians\ninto the state-of-the-art in fast monocular face avatars: FlashAvatar. Our\nevaluation on 19 subjects from 4 face datasets shows that HyperGaussians\noutperform 3DGS numerically and visually, particularly for high-frequency\ndetails like eyeglass frames, teeth, complex facial movements, and specular\nreflections.",
      "url": "http://arxiv.org/abs/2507.02803v1",
      "published_time_eastern_timestamp": 1751562408.0
    },
    {
      "title": "Massive Interacting Binaries Enhance Feedback in Star-Forming Regions",
      "summary": "We present a new framework to incorporate feedback from massive interacting\nbinaries in simulations of star cluster formation. Our new feedback model adds\nbinary stellar evolution to the cluster formation code Torch, and couples it in\nAMUSE to the pre-existing modules for collisional stellar dynamics,\nmagnetohydrodynamics, and mechanical and radiative feedback. Our model accounts\nfor the effects of mass transfer on the stars' mass loss rates, their radiation\nspectra, and the timing of core-collapse supernovae. It also injects mass lost\nthrough non-conservative mass transfer and common envelope ejection into the\ninterstellar medium. We demonstrate the use of our feedback model through\nsimulations of isolated binaries in a gaseous medium, and of embedded clusters\nof massive binaries. Feedback from interacting binaries efficiently couples\nwith the surrounding interstellar medium. It increases the size of HII regions,\nincreases the kinetic and thermal energy of the gas, and increases the pressure\nwithin HII regions compared to models that use single star stellar evolution.\nThose differences arise from the ionizing radiation, which increases by three\norders of magnitude, resulting in HII regions that expand due to thermal\npressure rather than radiation pressure. The effects of stellar dynamics and\nthe gravitational potential of the background gas cause the evolution of\nindividual binaries to deviate from the predictions made by secular evolution,\nimpacting the subsequent feedback from the binary. We conclude that massive\ninteracting binaries are an important source of feedback in cluster-forming\nregions, and must be considered when studying the emerging timescales of young\nstar clusters.",
      "url": "http://arxiv.org/abs/2507.02780v1",
      "published_time_eastern_timestamp": 1751560991.0
    },
    {
      "title": "DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with\n  Self-Generated Cross-Modal Alignment",
      "summary": "We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model\n(LALM) designed for robust auditory perception and instruction-following,\nwithout requiring task-specific audio instruction-tuning. Recent LALMs\ntypically augment Large Language Models (LLMs) with auditory capabilities by\ntraining on large-scale, manually curated or LLM-synthesized audio-instruction\ndatasets. However, these approaches have often suffered from the catastrophic\nforgetting of the LLM's original language abilities. To address this, we\nrevisit the data construction pipeline and propose DeSTA, a self-generated\ncross-modal alignment strategy in which the backbone LLM generates its own\ntraining targets. This approach preserves the LLM's native language proficiency\nwhile establishing effective audio-text alignment, thereby enabling zero-shot\ngeneralization without task-specific tuning. Using DeSTA, we construct\nDeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training\nsamples derived from 7,000 hours of audio spanning 50 diverse datasets,\nincluding speech, environmental sounds, and music. DeSTA2.5-Audio achieves\nstate-of-the-art or competitive performance across a wide range of\naudio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,\nSpeech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate\nthat our self-generated strategy outperforms widely adopted data construction\nand training strategies in both auditory perception and instruction-following\ncapabilities. Our findings underscore the importance of carefully designed data\nconstruction in LALM development and offer practical insights for building\nrobust, general-purpose LALMs.",
      "url": "http://arxiv.org/abs/2507.02768v1",
      "published_time_eastern_timestamp": 1751560105.0
    },
    {
      "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
      "summary": "Recent work has shown that training loss scales as a power law with both\nmodel size and the number of tokens, and that achieving compute-optimal models\nrequires scaling model size and token count together. However, these scaling\nlaws assume an infinite supply of data and apply primarily in compute-bound\nsettings. As modern large language models increasingly rely on massive\ninternet-scale datasets, the assumption that they are compute-bound is becoming\nless valid. This shift highlights the need for architectures that prioritize\ntoken efficiency.\n  In this work, we investigate the use of the 2-simplicial Transformer, an\narchitecture that generalizes standard dot-product attention to trilinear\nfunctions through an efficient Triton kernel implementation. We demonstrate\nthat the 2-simplicial Transformer achieves better token efficiency than\nstandard Transformers: for a fixed token budget, similarly sized models\noutperform their dot-product counterparts on tasks involving mathematics,\ncoding, reasoning, and logic. We quantify these gains by demonstrating that\n$2$-simplicial attention changes the exponent in the scaling laws for knowledge\nand reasoning tasks compared to dot product attention.",
      "url": "http://arxiv.org/abs/2507.02754v1",
      "published_time_eastern_timestamp": 1751559394.0
    },
    {
      "title": "Partial Weakly-Supervised Oriented Object Detection",
      "summary": "The growing demand for oriented object detection (OOD) across various domains\nhas driven significant research in this area. However, the high cost of dataset\nannotation remains a major concern. Current mainstream OOD algorithms can be\nmainly categorized into three types: (1) fully supervised methods using\ncomplete oriented bounding box (OBB) annotations, (2) semi-supervised methods\nusing partial OBB annotations, and (3) weakly supervised methods using weak\nannotations such as horizontal boxes or points. However, these algorithms\ninevitably increase the cost of models in terms of annotation speed or\nannotation cost. To address this issue, we propose:(1) the first Partial\nWeakly-Supervised Oriented Object Detection (PWOOD) framework based on\npartially weak annotations (horizontal boxes or single points), which can\nefficiently leverage large amounts of unlabeled data, significantly\noutperforming weakly supervised algorithms trained with partially weak\nannotations, also offers a lower cost solution; (2) Orientation-and-Scale-aware\nStudent (OS-Student) model capable of learning orientation and scale\ninformation with only a small amount of orientation-agnostic or scale-agnostic\nweak annotations; and (3) Class-Agnostic Pseudo-Label Filtering strategy (CPF)\nto reduce the model's sensitivity to static filtering thresholds. Comprehensive\nexperiments on DOTA-v1.0/v1.5/v2.0 and DIOR datasets demonstrate that our PWOOD\nframework performs comparably to, or even surpasses, traditional\nsemi-supervised algorithms.",
      "url": "http://arxiv.org/abs/2507.02751v1",
      "published_time_eastern_timestamp": 1751559190.0
    },
    {
      "title": "Linear Attention with Global Context: A Multipole Attention Mechanism\n  for Vision and Physics",
      "summary": "Transformers have become the de facto standard for a wide range of tasks,\nfrom image classification to physics simulations. Despite their impressive\nperformance, the quadratic complexity of standard Transformers in both memory\nand time with respect to the input length makes them impractical for processing\nhigh-resolution inputs. Therefore, several variants have been proposed, the\nmost successful relying on patchification, downsampling, or coarsening\ntechniques, often at the cost of losing the finest-scale details. In this work,\nwe take a different approach. Inspired by state-of-the-art techniques in\n$n$-body numerical simulations, we cast attention as an interaction problem\nbetween grid points. We introduce the Multipole Attention Neural Operator\n(MANO), which computes attention in a distance-based multiscale fashion. MANO\nmaintains, in each attention head, a global receptive field and achieves linear\ntime and memory complexity with respect to the number of grid points. Empirical\nresults on image classification and Darcy flows demonstrate that MANO rivals\nstate-of-the-art models such as ViT and Swin Transformer, while reducing\nruntime and peak memory usage by orders of magnitude. We open source our code\nfor reproducibility at https://github.com/AlexColagrande/MANO.",
      "url": "http://arxiv.org/abs/2507.02748v1",
      "published_time_eastern_timestamp": 1751558726.0
    },
    {
      "title": "Prompt learning with bounding box constraints for medical image\n  segmentation",
      "summary": "Pixel-wise annotations are notoriously labourious and costly to obtain in the\nmedical domain. To mitigate this burden, weakly supervised approaches based on\nbounding box annotations-much easier to acquire-offer a practical alternative.\nVision foundation models have recently shown noteworthy segmentation\nperformance when provided with prompts such as points or bounding boxes. Prompt\nlearning exploits these models by adapting them to downstream tasks and\nautomating segmentation, thereby reducing user intervention. However, existing\nprompt learning approaches depend on fully annotated segmentation masks. This\npaper proposes a novel framework that combines the representational power of\nfoundation models with the annotation efficiency of weakly supervised\nsegmentation. More specifically, our approach automates prompt generation for\nfoundation models using only bounding box annotations. Our proposed\noptimization scheme integrates multiple constraints derived from box\nannotations with pseudo-labels generated by the prompted foundation model.\nExtensive experiments across multimodal datasets reveal that our weakly\nsupervised method achieves an average Dice score of 84.90% in a limited data\nsetting, outperforming existing fully-supervised and weakly-supervised\napproaches. The code is available at\nhttps://github.com/Minimel/box-prompt-learning-VFM.git",
      "url": "http://arxiv.org/abs/2507.02743v1",
      "published_time_eastern_timestamp": 1751558648.0
    },
    {
      "title": "A formal specification of the desired software behaviour of the Princess\n  Marijke lock complex",
      "summary": "The Princess Marijke lock complex is a large lock and water-protection\ninstallation in the Netherlands between the river Rhine and the\nAmsterdam-Rijnkanaal -- a large waterway connecting the Rhine to the port of\nAmsterdam. The lock complex consists of two independent locks and a moveable\nflood-protection barrier. Ensuring safe control of the lock complex is of\nutmost importance to guarantee both flood-protection and reliable ship\noperations. This paper gives a precise, formal description of the software\ncontrol of the lock complex in less than 400 lines of mCRL2 code. This\ndescription can act as a blueprint on how the software of this lock complex\nneeds to be constructed. Moreover, using model checking, 53 software\nrequirements are shown to be valid, ensuring that the formal description of the\nbehaviour is correct with regard to these properties and is unlikely to contain\nmistakes and oversights.",
      "url": "http://arxiv.org/abs/2507.02721v1",
      "published_time_eastern_timestamp": 1751556934.0
    },
    {
      "title": "The ESO SupJup Survey VIII. Chemical fingerprints of young L dwarf twins",
      "summary": "The potentially distinct formation pathways of exoplanets and brown dwarfs\nmay be imprinted in their elemental and isotopic ratios. This work is part of\nthe ESO SupJup Survey, which aims to disentangle the formation pathways of\nsuper-Jupiters and brown dwarfs through the analysis of their chemical and\nisotopic ratios. In this study, we characterize the atmospheres of two young L4\ndwarfs, 2MASS J03552337+1133437 (2M0355) and 2MASS J14252798-3650229 (2M1425),\nin the AB Doradus Moving Group. This involved constraining their chemical\ncomposition, $^{12}$CO/$^{13}$CO ratio, pressure-temperature profile, surface\ngravity, and rotational velocity. We have obtained high-resolution CRIRES+\nK-band spectra of these brown dwarfs, which we analyzed with an atmospheric\nretrieval pipeline. Atmospheric models were generated with the radiative\ntransfer code petitRADTRANS, for which we employed a free and equilibrium\nchemistry approach, which we coupled with the PyMultiNest sampling algorithm to\ndetermine the best fit. We report robust detections of $^{13}$CO (13.4 & 8.0\n$\\sigma$) and HF (11.6 & 15.8 $\\sigma$) in 2M0355 and 2M1425, respectively. The\nobjects have similar overall atmospheric properties, including\n$^{12}$CO/$^{13}$CO isotope ratios of $95.5\\substack{+6.8 \\\\ -6.4}$ for 2M0355\nand $109.6\\substack{+10.6 \\\\ -9.6}$ for 2M1425. In both brown dwarfs, we find\nonly tentative hints of H$_2^{18}$O (1.1 & 3.0 $\\sigma$), with lower limits of\nH$_2^{16}$O/H$_2^{18}$O $\\sim$ 1000. Both objects appear to be close to\nchemical equilibrium, considering the main spectral contributors. Future\nstudies will put them into the context of other objects observed as part of the\nESO SupJup Survey.",
      "url": "http://arxiv.org/abs/2507.02706v1",
      "published_time_eastern_timestamp": 1751555839.0
    }
  ]
}
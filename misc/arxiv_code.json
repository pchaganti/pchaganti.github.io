{
  "last_updated": "2025-08-15T05:13:17.564606-04:00",
  "papers": [
    {
      "title": "Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via\n  In-Context Learning",
      "summary": "This paper aims to model 3D human motion across domains, where a single model\nis expected to handle multiple modalities, tasks, and datasets. Existing\ncross-domain models often rely on domain-specific components and multi-stage\ntraining, which limits their practicality and scalability. To overcome these\nchallenges, we propose a new setting to train a unified cross-domain model\nthrough a single process, eliminating the need for domain-specific components\nand multi-stage training. We first introduce Pose-in-Context (PiC), which\nleverages in-context learning to create a pose-centric cross-domain model.\nWhile PiC generalizes across multiple pose-based tasks and datasets, it\nencounters difficulties with modality diversity, prompting strategy, and\ncontextual dependency handling. We thus propose Human-in-Context (HiC), an\nextension of PiC that broadens generalization across modalities, tasks, and\ndatasets. HiC combines pose and mesh representations within a unified\nframework, expands task coverage, and incorporates larger-scale datasets.\nAdditionally, HiC introduces a max-min similarity prompt sampling strategy to\nenhance generalization across diverse domains and a network architecture with\ndual-branch context injection for improved handling of contextual dependencies.\nExtensive experimental results show that HiC performs better than PiC in terms\nof generalization, data scale, and performance across a wide range of domains.\nThese results demonstrate the potential of HiC for building a unified\ncross-domain 3D human motion model with improved flexibility and scalability.\nThe source codes and models are available at\nhttps://github.com/BradleyWang0416/Human-in-Context.",
      "url": "http://arxiv.org/abs/2508.10897v1",
      "published_time_eastern_timestamp": 1755194363.0
    },
    {
      "title": "MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and\n  Multispectral Earth Observation Data",
      "summary": "Self-supervised learning holds great promise for remote sensing, but standard\nself-supervised methods must be adapted to the unique characteristics of Earth\nobservation data. We take a step in this direction by conducting a\ncomprehensive benchmark of fusion strategies and reconstruction target\nnormalization schemes for multimodal, multitemporal, and multispectral Earth\nobservation data. Based on our findings, we propose MAESTRO, a novel adaptation\nof the Masked Autoencoder, featuring optimized fusion strategies and a tailored\ntarget normalization scheme that introduces a spectral prior as a\nself-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO\nsets a new state-of-the-art on tasks that strongly rely on multitemporal\ndynamics, while remaining highly competitive on tasks dominated by a single\nmono-temporal modality. Code to reproduce all our experiments is available at\nhttps://github.com/ignf/maestro.",
      "url": "http://arxiv.org/abs/2508.10894v1",
      "published_time_eastern_timestamp": 1755194325.0
    },
    {
      "title": "Introducing CQ: A C-like API for Quantum Accelerated HPC",
      "summary": "In this paper we present CQ, a specification for a C-like API for quantum\naccelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written\nin C99, and built on top of the statevector simulator QuEST. CQ focuses on\nenabling the incremental integration of quantum computing into classical HPC\ncodes by supporting runtime offloading from languages such as C and Fortran. It\nprovides a way of describing and offloading quantum computations which is\ncompatible with strictly and strongly typed compiled languages, and gives the\nprogrammer fine-grained control over classical data movement. The CQ Simulated\nBackend (CQ-SimBE) provides both a way to demonstrate the usage and utility of\nCQ, and a space to experiment with new features such as support for analogue\nquantum computing. Both the CQ specification and CQ-SimBE are open-source, and\navailable in public repositories.",
      "url": "http://arxiv.org/abs/2508.10854v1",
      "published_time_eastern_timestamp": 1755192362.0
    },
    {
      "title": "Generalizable Federated Learning using Client Adaptive Focal Modulation",
      "summary": "Federated learning (FL) has proven essential for privacy-preserving,\ncollaborative training across distributed clients. Our prior work, TransFed,\nintroduced a robust transformer-based FL framework that leverages a\nlearn-to-adapt hypernetwork to generate personalized focal modulation layers\nper client, outperforming traditional methods in non-IID and cross-domain\nsettings. In this extended version, we propose AdaptFED, where we deepen the\ninvestigation of focal modulation in generalizable FL by incorporating: (1) a\nrefined adaptation strategy that integrates task-aware client embeddings to\npersonalize modulation dynamics further, (2) enhanced theoretical bounds on\nadaptation performance, and (3) broader empirical validation across additional\nmodalities, including time-series and multilingual data. We also introduce an\nefficient variant of TransFed that reduces server-client communication overhead\nvia low-rank hypernetwork conditioning, enabling scalable deployment in\nresource-constrained environments. Extensive experiments on eight diverse\ndatasets reaffirm the superiority of our method over state-of-the-art\nbaselines, particularly in source-free and cross-task federated setups. Our\nfindings not only extend the capabilities of focal modulation in FL but also\npave the way for more adaptive, scalable, and generalizable transformer-based\nfederated systems. The code is available at\nhttp://github.com/Tajamul21/TransFed",
      "url": "http://arxiv.org/abs/2508.10840v1",
      "published_time_eastern_timestamp": 1755191210.0
    },
    {
      "title": "Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning",
      "summary": "Current self-supervised stereo matching relies on the photometric consistency\nassumption, which breaks down in occluded regions due to ill-posed\ncorrespondences. To address this issue, we propose BaCon-Stereo, a simple yet\neffective contrastive learning framework for self-supervised stereo network\ntraining in both non-occluded and occluded regions. We adopt a teacher-student\nparadigm with multi-baseline inputs, in which the stereo pairs fed into the\nteacher and student share the same reference view but differ in target views.\nGeometrically, regions occluded in the student's target view are often visible\nin the teacher's, making it easier for the teacher to predict in these regions.\nThe teacher's prediction is rescaled to match the student's baseline and then\nused to supervise the student. We also introduce an occlusion-aware attention\nmap to better guide the student in learning occlusion completion. To support\ntraining, we synthesize a multi-baseline dataset BaCon-20k. Extensive\nexperiments demonstrate that BaCon-Stereo improves prediction in both occluded\nand non-occluded regions, achieves strong generalization and robustness, and\noutperforms state-of-the-art self-supervised methods on both KITTI 2015 and\n2012 benchmarks. Our code and dataset will be released upon paper acceptance.",
      "url": "http://arxiv.org/abs/2508.10838v1",
      "published_time_eastern_timestamp": 1755191027.0
    },
    {
      "title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
      "summary": "We present UI-Venus, a native UI agent that takes only screenshots as input\nbased on a multimodal large language model. UI-Venus achieves SOTA performance\non both UI grounding and navigation tasks using only several hundred thousand\nhigh-quality training samples through reinforcement finetune (RFT) based on\nQwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% /\n50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e.,\nScreenspot-V2 / Pro, surpassing the previous SOTA baselines including\nopen-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and\nplaning ability, we also evaluate it on the AndroidWorld, an online UI\nnavigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9%\nsuccess rate, also beating existing models.To achieve this, we introduce\ncarefully designed reward functions for both UI grounding and navigation tasks\nand corresponding efficient data cleaning strategies.To further boost\nnavigation performance, we propose Self-Evolving Trajectory History Alignment\n\\& Sparse Action Enhancement that refine historical reasoning traces and\nbalances the distribution of sparse but critical actions, leading to more\ncoherent planning and better generalization in complex UI tasks. Our\ncontributions include the publish of SOTA open-source UI agents, comprehensive\ndata cleaning protocols and a novel self-evolving framework for improving\nnavigation performance, which encourage further research and development in the\ncommunity. Code is available at https://github.com/antgroup/UI-Venus.",
      "url": "http://arxiv.org/abs/2508.10833v1",
      "published_time_eastern_timestamp": 1755190687.0
    },
    {
      "title": "Parity Cross-Resonance: A Multiqubit Gate",
      "summary": "We present a native three-qubit entangling gate that exploits engineered\ninteractions to realize control-control-target and control-target-target\noperations in a single coherent step. Unlike conventional decompositions into\nmultiple two-qubit gates, our hybrid optimization approach selectively\namplifies desired interactions while suppressing unwanted couplings, yielding\nrobust performance across the computational subspace and beyond. The new gate\ncan be classified as a cross-resonance gate. We show it can be utilized in\nseveral ways, for example, in GHZ triplet state preparation, Toffoli-class\nlogic demonstrations with many-body interactions, and in implementing a\ncontrolled-ZZ gate. The latter maps the parity of two data qubits directly onto\na measurement qubit, enabling faster and higher-fidelity stabilizer\nmeasurements in surface-code quantum error correction. In all these examples,\nwe show that the three-qubit gate performance remains robust across Hilbert\nspace sizes, as confirmed by testing under increasing total excitation numbers.\nThis work lays the foundation for co-designing circuit architectures and\ncontrol protocols that leverage native multiqubit interactions as core elements\nof next-generation superconducting quantum processors.",
      "url": "http://arxiv.org/abs/2508.10807v1",
      "published_time_eastern_timestamp": 1755188792.0
    },
    {
      "title": "Reduction of motion artifacts from photoplethysmography signals using\n  learned convolutional sparse coding",
      "summary": "Objective. Wearable devices with embedded photoplethysmography (PPG) enable\ncontinuous non-invasive monitoring of cardiac activity, offering a promising\nstrategy to reduce the global burden of cardiovascular diseases. However,\nmonitoring during daily life introduces motion artifacts that can compromise\nthe signals. Traditional signal decomposition techniques often fail with severe\nartifacts. Deep learning denoisers are more effective but have poorer\ninterpretability, which is critical for clinical acceptance. This study\nproposes a framework that combines the advantages of both signal decomposition\nand deep learning approaches. Approach. We leverage algorithm unfolding to\nintegrate prior knowledge about the PPG structure into a deep neural network,\nimproving its interpretability. A learned convolutional sparse coding model\nencodes the signal into a sparse representation using a learned dictionary of\nkernels that capture recurrent morphological patterns. The network is trained\nfor denoising using the PulseDB dataset and a synthetic motion artifact model\nfrom the literature. Performance is benchmarked with PPG during daily\nactivities using the PPG-DaLiA dataset and compared with two reference deep\nlearning methods. Main results. On the synthetic dataset, the proposed method,\non average, improved the signal-to-noise ratio (SNR) from -7.07 dB to 11.23 dB\nand reduced the heart rate mean absolute error (MAE) by 55%. On the PPG-DaLiA\ndataset, the MAE decreased by 23%. The proposed method obtained higher SNR and\ncomparable MAE to the reference methods. Significance. Our method effectively\nenhances the quality of PPG signals from wearable devices and enables the\nextraction of meaningful waveform features, which may inspire innovative tools\nfor monitoring cardiovascular diseases.",
      "url": "http://arxiv.org/abs/2508.10805v1",
      "published_time_eastern_timestamp": 1755188760.0
    },
    {
      "title": "Beyond \"Not Novel Enough\": Enriching Scholarly Critique with\n  LLM-Assisted Feedback",
      "summary": "Novelty assessment is a central yet understudied aspect of peer review,\nparticularly in high volume fields like NLP where reviewer capacity is\nincreasingly strained. We present a structured approach for automated novelty\nevaluation that models expert reviewer behavior through three stages: content\nextraction from submissions, retrieval and synthesis of related work, and\nstructured comparison for evidence based assessment. Our method is informed by\na large scale analysis of human written novelty reviews and captures key\npatterns such as independent claim verification and contextual reasoning.\nEvaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty\nassessments, the approach achieves 86.5% alignment with human reasoning and\n75.3% agreement on novelty conclusions - substantially outperforming existing\nLLM based baselines. The method produces detailed, literature aware analyses\nand improves consistency over ad hoc reviewer judgments. These results\nhighlight the potential for structured LLM assisted approaches to support more\nrigorous and transparent peer review without displacing human expertise. Data\nand code are made available.",
      "url": "http://arxiv.org/abs/2508.10795v1",
      "published_time_eastern_timestamp": 1755188317.0
    },
    {
      "title": "Enhancing Fairness in Autoencoders for Node-Level Graph Anomaly\n  Detection",
      "summary": "Graph anomaly detection (GAD) has become an increasingly important task\nacross various domains. With the rapid development of graph neural networks\n(GNNs), GAD methods have achieved significant performance improvements.\nHowever, fairness considerations in GAD remain largely underexplored. Indeed,\nGNN-based GAD models can inherit and amplify biases present in training data,\npotentially leading to unfair outcomes. While existing efforts have focused on\ndeveloping fair GNNs, most approaches target node classification tasks, where\nmodels often rely on simple layer architectures rather than autoencoder-based\nstructures, which are the most widely used architecturs for anomaly detection.\nTo address fairness in autoencoder-based GAD models, we propose\n\\textbf{D}is\\textbf{E}ntangled \\textbf{C}ounterfactual \\textbf{A}dversarial\n\\textbf{F}air (DECAF)-GAD, a framework that alleviates bias while preserving\nGAD performance. Specifically, we introduce a structural causal model (SCM) to\ndisentangle sensitive attributes from learned representations. Based on this\ncausal framework, we formulate a specialized autoencoder architecture along\nwith a fairness-guided loss function. Through extensive experiments on both\nsynthetic and real-world datasets, we demonstrate that DECAF-GAD not only\nachieves competitive anomaly detection performance but also significantly\nenhances fairness metrics compared to baseline GAD methods. Our code is\navailable at https://github.com/Tlhey/decaf_code.",
      "url": "http://arxiv.org/abs/2508.10785v1",
      "published_time_eastern_timestamp": 1755187935.0
    },
    {
      "title": "Ultra-High-Definition Reference-Based Landmark Image Super-Resolution\n  with Generative Diffusion Prior",
      "summary": "Reference-based Image Super-Resolution (RefSR) aims to restore a\nlow-resolution (LR) image by utilizing the semantic and texture information\nfrom an additional reference high-resolution (reference HR) image. Existing\ndiffusion-based RefSR methods are typically built upon ControlNet, which\nstruggles to effectively align the information between the LR image and the\nreference HR image. Moreover, current RefSR datasets suffer from limited\nresolution and poor image quality, resulting in the reference images lacking\nsufficient fine-grained details to support high-quality restoration. To\novercome the limitations above, we propose TriFlowSR, a novel framework that\nexplicitly achieves pattern matching between the LR image and the reference HR\nimage. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for\nUltra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios\nwith real-world degradation, in TriFlowSR, we design a Reference Matching\nStrategy to effectively match the LR image with the reference HR image.\nExperimental results show that our approach can better utilize the semantic and\ntexture information of the reference HR image compared to previous methods. To\nthe best of our knowledge, we propose the first diffusion-based RefSR pipeline\nfor ultra-high definition landmark scenarios under real-world degradation. Our\ncode and model will be available at https://github.com/nkicsl/TriFlowSR.",
      "url": "http://arxiv.org/abs/2508.10779v1",
      "published_time_eastern_timestamp": 1755187479.0
    },
    {
      "title": "Video-BLADE: Block-Sparse Attention Meets Step Distillation for\n  Efficient Video Generation",
      "summary": "Diffusion transformers currently lead the field in high-quality video\ngeneration, but their slow iterative denoising process and prohibitive\nquadratic attention costs for long sequences create significant inference\nbottlenecks. While both step distillation and sparse attention mechanisms have\nshown promise as independent acceleration strategies, effectively combining\nthese approaches presents critical challenges -- training-free integration\nyields suboptimal results, while separately training sparse attention after\nstep distillation requires prohibitively expensive high-quality video data. To\novercome these limitations, we propose BLADE, an innovative data-free joint\ntraining framework that introduces: (1) an Adaptive Block-Sparse Attention\n(ASA) mechanism for dynamically generating content-aware sparsity masks to\nfocus computation on salient spatiotemporal features, and (2) a sparsity-aware\nstep distillation paradigm built upon Trajectory Distribution Matching (TDM)\nthat directly incorporates sparsity into the distillation process rather than\ntreating it as a separate compression step, with fast convergence. We validate\nBLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework\ndemonstrates remarkable efficiency gains across different scales. On\nWan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference acceleration over a\n50-step baseline. Moreover, on models such as CogVideoX-5B with short video\nsequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the\nacceleration is accompanied by a consistent quality improvement. On the\nVBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from\n0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further\ncorroborated by superior ratings in human evaluations. Our code and model\nweights are publicly available at: http://ziplab.co/BLADE-Homepage/.",
      "url": "http://arxiv.org/abs/2508.10774v1",
      "published_time_eastern_timestamp": 1755187139.0
    },
    {
      "title": "Memorisation and forgetting in a learning Hopfield neural network:\n  bifurcation mechanisms, attractors and basins",
      "summary": "Despite explosive expansion of artificial intelligence based on artificial\nneural networks (ANNs), these are employed as \"black boxes'', as it is unclear\nhow, during learning, they form memories or develop unwanted features,\nincluding spurious memories and catastrophic forgetting. Much research is\navailable on isolated aspects of learning ANNs, but due to their high\ndimensionality and non-linearity, their comprehensive analysis remains a\nchallenge. In ANNs, knowledge is thought to reside in connection weights or in\nattractor basins, but these two paradigms are not linked explicitly. Here we\ncomprehensively analyse mechanisms of memory formation in an 81-neuron Hopfield\nnetwork undergoing Hebbian learning by revealing bifurcations leading to\nformation and destruction of attractors and their basin boundaries. We show\nthat, by affecting evolution of connection weights, the applied stimuli induce\na pitchfork and then a cascade of saddle-node bifurcations creating new\nattractors with their basins that can code true or spurious memories, and an\nabrupt disappearance of old memories (catastrophic forgetting). With successful\nlearning, new categories are represented by the basins of newly born point\nattractors, and their boundaries by the stable manifolds of new saddles. With\nthis, memorisation and forgetting represent two manifestations of the same\nmechanism. Our strategy to analyse high-dimensional learning ANNs is universal\nand applicable to recurrent ANNs of any form. The demonstrated mechanisms of\nmemory formation and of catastrophic forgetting shed light on the operation of\na wider class of recurrent ANNs and could aid the development of approaches to\nmitigate their flaws.",
      "url": "http://arxiv.org/abs/2508.10765v1",
      "published_time_eastern_timestamp": 1755186519.0
    },
    {
      "title": "FROGENT: An End-to-End Full-process Drug Design Agent",
      "summary": "Powerful AI tools for drug discovery reside in isolated web apps, desktop\nprograms, and code libraries. Such fragmentation forces scientists to manage\nincompatible interfaces and specialized scripts, which can be a cumbersome and\nrepetitive process. To address this issue, a Full-pROcess druG dEsign ageNT,\nnamed FROGENT, has been proposed. Specifically, FROGENT utilizes a Large\nLanguage Model and the Model Context Protocol to integrate multiple dynamic\nbiochemical databases, extensible tool libraries, and task-specific AI models.\nThis agentic framework allows FROGENT to execute complicated drug discovery\nworkflows dynamically, including component tasks such as target identification,\nmolecule generation and retrosynthetic planning. FROGENT has been evaluated on\neight benchmarks that cover various aspects of drug discovery, such as\nknowledge retrieval, property prediction, virtual screening, mechanistic\nanalysis, molecular design, and synthesis. It was compared against six\nincreasingly advanced ReAct-style agents that support code execution and\nliterature searches. Empirical results demonstrated that FROGENT triples the\nbest baseline performance in hit-finding and doubles it in interaction\nprofiling, significantly outperforming both the open-source model Qwen3-32B and\nthe commercial model GPT-4o. In addition, real-world cases have been utilized\nto validate the practicability and generalization of FROGENT. This development\nsuggests that streamlining the agentic drug discovery pipeline can\nsignificantly enhance researcher productivity.",
      "url": "http://arxiv.org/abs/2508.10760v1",
      "published_time_eastern_timestamp": 1755186353.0
    },
    {
      "title": "Forgery Guided Learning Strategy with Dual Perception Network for\n  Deepfake Cross-domain Detection",
      "summary": "The emergence of deepfake technology has introduced a range of societal\nproblems, garnering considerable attention. Current deepfake detection methods\nperform well on specific datasets, but exhibit poor performance when applied to\ndatasets with unknown forgery techniques. Moreover, as the gap between emerging\nand traditional forgery techniques continues to widen, cross-domain detection\nmethods that rely on common forgery traces are becoming increasingly\nineffective. This situation highlights the urgency of developing deepfake\ndetection technology with strong generalization to cope with fast iterative\nforgery techniques. To address these challenges, we propose a Forgery Guided\nLearning (FGL) strategy designed to enable detection networks to continuously\nadapt to unknown forgery techniques. Specifically, the FGL strategy captures\nthe differential information between known and unknown forgery techniques,\nallowing the model to dynamically adjust its learning process in real time. To\nfurther improve the ability to perceive forgery traces, we design a Dual\nPerception Network (DPNet) that captures both differences and relationships\namong forgery traces. In the frequency stream, the network dynamically\nperceives and extracts discriminative features across various forgery\ntechniques, establishing essential detection cues. These features are then\nintegrated with spatial features and projected into the embedding space. In\naddition, graph convolution is employed to perceive relationships across the\nentire feature space, facilitating a more comprehensive understanding of\nforgery trace correlations. Extensive experiments show that our approach\ngeneralizes well across different scenarios and effectively handles unknown\nforgery challenges, providing robust support for deepfake detection. Our code\nis available on https://github.com/vpsg-research/FGL.",
      "url": "http://arxiv.org/abs/2508.10741v1",
      "published_time_eastern_timestamp": 1755185241.0
    },
    {
      "title": "Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC\n  2025",
      "summary": "This paper presents a summary of the 2025 Sclera Segmentation Benchmarking\nCompetition (SSBC), which focused on the development of privacy-preserving\nsclera-segmentation models trained using synthetically generated ocular images.\nThe goal of the competition was to evaluate how well models trained on\nsynthetic data perform in comparison to those trained on real-world datasets.\nThe competition featured two tracks: $(i)$ one relying solely on synthetic data\nfor model development, and $(ii)$ one combining/mixing synthetic with (a\nlimited amount of) real-world data. A total of nine research groups submitted\ndiverse segmentation models, employing a variety of architectural designs,\nincluding transformer-based solutions, lightweight models, and segmentation\nnetworks guided by generative frameworks. Experiments were conducted across\nthree evaluation datasets containing both synthetic and real-world images,\ncollected under diverse conditions. Results show that models trained entirely\non synthetic data can achieve competitive performance, particularly when\ndedicated training strategies are employed, as evidenced by the top performing\nmodels that achieved $F_1$ scores of over $0.8$ in the synthetic data track.\nMoreover, performance gains in the mixed track were often driven more by\nmethodological choices rather than by the inclusion of real data, highlighting\nthe promise of synthetic data for privacy-aware biometric development. The code\nand data for the competition is available at:\nhttps://github.com/dariant/SSBC_2025.",
      "url": "http://arxiv.org/abs/2508.10737v1",
      "published_time_eastern_timestamp": 1755184618.0
    },
    {
      "title": "Traffic Intersection Simulation Using Turning Movement Count Data in\n  SUMO: A Case Study of Toronto Intersections",
      "summary": "Urban traffic simulation is vital in planning, modeling, and analyzing road\nnetworks. However, the realism of a simulation depends extensively on the\nquality of input data. This paper presents an intersection traffic simulation\ntool that leverages real-world vehicle turning movement count (TMC) data from\nthe City of Toronto to model traffic in an urban environment at an individual\nor multiple intersections using Simulation of Urban MObility (SUMO). The\nsimulation performed in this research focuses specifically on\nintersection-level traffic generation without creating full vehicle routes\nthrough the network. This also helps keep the network's complexity to a\nminimum. The simulated traffic is evaluated against actual data to show that\nthe simulation closely reproduces real intersection flows. This validates that\nthe real data can drive practical simulations, and these scenarios can replace\nsynthetic or random generated data, which is prominently used in developing new\ntraffic-related methodologies. This is the first tool to integrate TMC data\nfrom Toronto into SUMO via an easy-to-use Graphical User Interface. This work\ncontributes to the research and traffic planning community on data-driven\ntraffic simulation. It provides transportation engineers with a framework to\nevaluate intersection design and traffic signal optimization strategies using\nreadily available aggregate traffic data.",
      "url": "http://arxiv.org/abs/2508.10733v1",
      "published_time_eastern_timestamp": 1755184370.0
    },
    {
      "title": "Multi-Functional Polarization-Based Coverage Control through Static\n  Passive EMSs",
      "summary": "An innovative multi-functional static-passive electromagnetic skin (SP-EMS)\nsolution is proposed to simultaneously support, in reflection, two independent\nwave-manipulation functionalities with a single meta-atoms arrangement on the\nEMS aperture when illuminated by two EM sources operating at the same\nfrequency, but working in different polarization states. Towards this end, a\nsimple reference meta-atom is designed first to enable an accurate and\nindependent control of each polarization component of the local reflection\ntensor. Successively, the macro-scale synthesis of multi-polarization (MP)\nSP-EMSs (MP-SP-EMSs) is carried out by solving a global optimization problem\nwhere a cost function, which mathematically codes separate requirements for\neach polarization, is minimized with a customized version of the\nsystem-by-design (SbD) technique. Representative results from a set of\nnumerical and experimental tests are reported to assess the feasibility of a\nmulti-function EMS based on polarization diversity as well as the effectiveness\nand the robustness of the proposed method for the synthesis of MP-SP-EMSs.",
      "url": "http://arxiv.org/abs/2508.10730v1",
      "published_time_eastern_timestamp": 1755184282.0
    },
    {
      "title": "Dissecting Generalized Category Discovery: Multiplex Consensus under\n  Self-Deconstruction",
      "summary": "Human perceptual systems excel at inducing and recognizing objects across\nboth known and novel categories, a capability far beyond current machine\nlearning frameworks. While generalized category discovery (GCD) aims to bridge\nthis gap, existing methods predominantly focus on optimizing objective\nfunctions. We present an orthogonal solution, inspired by the human cognitive\nprocess for novel object understanding: decomposing objects into visual\nprimitives and establishing cross-knowledge comparisons. We propose ConGCD,\nwhich establishes primitive-oriented representations through high-level\nsemantic reconstruction, binding intra-class shared attributes via\ndeconstruction. Mirroring human preference diversity in visual processing,\nwhere distinct individuals leverage dominant or contextual cues, we implement\ndominant and contextual consensus units to capture class-discriminative\npatterns and inherent distributional invariants, respectively. A consensus\nscheduler dynamically optimizes activation pathways, with final predictions\nemerging through multiplex consensus integration. Extensive evaluations across\ncoarse- and fine-grained benchmarks demonstrate ConGCD's effectiveness as a\nconsensus-aware paradigm. Code is available at github.com/lytang63/ConGCD.",
      "url": "http://arxiv.org/abs/2508.10731v1",
      "published_time_eastern_timestamp": 1755184282.0
    },
    {
      "title": "EgoCross: Benchmarking Multimodal Large Language Models for Cross-Domain\n  Egocentric Video Question Answering",
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly pushed the frontier of egocentric video question answering\n(EgocentricQA). However, existing benchmarks and studies are mainly limited to\ncommon daily activities such as cooking and cleaning. In contrast, real-world\ndeployment inevitably encounters domain shifts, where target domains differ\nsubstantially in both visual style and semantic content. To bridge this gap, we\nintroduce \\textbf{EgoCross}, a comprehensive benchmark designed to evaluate the\ncross-domain generalization of MLLMs in EgocentricQA. EgoCross covers four\ndiverse and challenging domains, including surgery, industry, extreme sports,\nand animal perspective, representing realistic and high-impact application\nscenarios. It comprises approximately 1,000 QA pairs across 798 video clips,\nspanning four key QA tasks: prediction, recognition, localization, and\ncounting. Each QA pair provides both OpenQA and CloseQA formats to support\nfine-grained evaluation. Extensive experiments show that most existing MLLMs,\nwhether general-purpose or egocentric-specialized, struggle to generalize to\ndomains beyond daily life, highlighting the limitations of current models.\nFurthermore, we conduct several pilot studies, \\eg, fine-tuning and\nreinforcement learning, to explore potential improvements. We hope EgoCross and\nour accompanying analysis will serve as a foundation for advancing\ndomain-adaptive, robust egocentric video understanding. Data and codes will be\nreleased at:\n\\href{https://github.com/MyUniverse0726/EgoCross}{https://github.com/MyUniverse0726/EgoCross.}",
      "url": "http://arxiv.org/abs/2508.10729v1",
      "published_time_eastern_timestamp": 1755184280.0
    }
  ]
}
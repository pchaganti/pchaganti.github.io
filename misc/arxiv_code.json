{
  "last_updated": "2025-11-06T22:39:18.852317-05:00",
  "papers": [
    {
      "title": "Tracking and Understanding Object Transformations",
      "summary": "Real-world objects frequently undergo state transformations. From an apple\nbeing cut into pieces to a butterfly emerging from its cocoon, tracking through\nthese changes is important for understanding real-world objects and dynamics.\nHowever, existing methods often lose track of the target object after\ntransformation, due to significant changes in object appearance. To address\nthis limitation, we introduce the task of Track Any State: tracking objects\nthrough transformations while detecting and describing state changes,\naccompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we\npresent TubeletGraph, a zero-shot system that recovers missing objects after\ntransformation and maps out how object states are evolving over time.\nTubeletGraph first identifies potentially overlooked tracks, and determines\nwhether they should be integrated based on semantic and proximity priors. Then,\nit reasons about the added tracks and generates a state graph describing each\nobserved transformation. TubeletGraph achieves state-of-the-art tracking\nperformance under transformations, while demonstrating deeper understanding of\nobject transformations and promising capabilities in temporal grounding and\nsemantic reasoning for complex object transformations. Code, additional\nresults, and the benchmark dataset are available at\nhttps://tubelet-graph.github.io.",
      "url": "http://arxiv.org/abs/2511.04678v1",
      "published_time_eastern_timestamp": 1762455570.0
    },
    {
      "title": "KGB-evolution: a relativistic $N$-body code for kinetic gravity braiding\n  models",
      "summary": "We present KGB-evolution, a relativistic $N$-body simulation code that\nextends the $k$-evolution code by incorporating an effective field theory\nparameterization of kinetic gravity braiding, while also including the\n$k$-essence model as a limiting case. As a first step, we implement the\nlinearized dark energy stress-energy tensor and scalar field equations,\nproviding the groundwork for a future full Horndeski theory extension. We\nvalidate KGB-evolution by comparing its power spectra against linear\npredictions from hi$\\_$class, finding excellent agreement on large scales at\nlow redshifts and over all scales at high redshifts. We demonstrate that\nnonlinear growth of matter and metric perturbations on small scales drives the\nlinearized dark energy field into a nonlinear clustering regime, which in turn\nfeeds back on the growth of cosmic structure. In contrast to the $k$-essence\nlimit, a nonzero braiding considerably amplifies this backreaction, producing a\nsignificantly stronger alteration of structure formation in the kinetic gravity\nbraiding model.",
      "url": "http://arxiv.org/abs/2511.04676v1",
      "published_time_eastern_timestamp": 1762455495.0
    },
    {
      "title": "InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual\n  Generation",
      "summary": "We introduce InfinityStar, a unified spacetime autoregressive framework for\nhigh-resolution image and dynamic video synthesis. Building on the recent\nsuccess of autoregressive modeling in both vision and language, our purely\ndiscrete approach jointly captures spatial and temporal dependencies within a\nsingle architecture. This unified design naturally supports a variety of\ngeneration tasks such as text-to-image, text-to-video, image-to-video, and long\ninteractive video synthesis via straightforward temporal autoregression.\nExtensive experiments demonstrate that InfinityStar scores 83.74 on VBench,\noutperforming all autoregressive models by large margins, even surpassing some\ndiffusion competitors like HunyuanVideo. Without extra optimizations, our model\ngenerates a 5s, 720p video approximately 10x faster than leading\ndiffusion-based methods. To our knowledge, InfinityStar is the first discrete\nautoregressive video generator capable of producing industrial level 720p\nvideos. We release all code and models to foster further research in efficient,\nhigh-quality video generation.",
      "url": "http://arxiv.org/abs/2511.04675v1",
      "published_time_eastern_timestamp": 1762455483.0
    },
    {
      "title": "On the Exoplanet Yield of Gaia Astrometry",
      "summary": "We re-examine the expected yield of Gaia astrometric planet detections using\nupdated models for giant-planet occurrence, the local stellar population, and\nGaia's demonstrated astrometric precision. Our analysis combines a\nsemi-analytic model that clarifies key scaling relations with more realistic\nMonte Carlo simulations. We predict $7{,}500 \\pm 2{,}100$ planet discoveries in\nthe 5-year dataset (DR4) and $120{,}000 \\pm 22{,}000$ over the full 10-year\nmission (DR5), with the dominant error arising from uncertainties in\ngiant-planet occurrence. We evaluate the sensitivity of these forecasts to the\ndetection threshold and the desired precision for measurements of planet masses\nand orbital parameters. Roughly $1{,}900 \\pm 540$ planets in DR4 and $38{,}000\n\\pm 7{,}300$ planets in DR5 should have masses and orbital periods determined\nto better than $20$%. Most detections will be super-Jupiters ($3$ - $13 M_{\\rm\nJ}$) on $2$ - $5$AU orbits around GKM-type stars ($0.4$ - $1.3 M_\\odot$) within\n$500$ pc. Unresolved binary stars will lead to spurious planet detections, but\nwe estimate that genuine planets will outnumber them by a factor of $5$ or\nmore. An exception is planets around M-dwarfs with $a < 1$AU, for which the\nfalse-positive rate is expected to be about $50$%. To support community\npreparation for upcoming data releases, we provide mock catalogs of Gaia\nexoplanets and planet-impostor binaries.",
      "url": "http://arxiv.org/abs/2511.04673v1",
      "published_time_eastern_timestamp": 1762455420.0
    },
    {
      "title": "Twist and higher modes of a complex scalar field at the threshold of\n  collapse",
      "summary": "We investigate the threshold of collapse of a massless complex scalar field\nin axisymmetric spacetimes under the ansatz of Choptuik et al. 2004, in which a\nsymmetry depending on the azimuthal parameter $m$ is imposed on the scalar\nfield. This allows for both non-vanishing twist and angular momentum. We extend\nearlier work to include higher angular modes. Using the pseudospectral code\nbamps with a new adapted symmetry reduction method, which we call $m$-cartoon,\nand a generalized twist-compatible apparent horizon finder, we evolve\nnear-critical initial data to the verge of black hole formation for the lowest\nnontrivial modes, $m=1$ and $m=2$. For $m=1$ we recover discrete\nself-similarity with echoing period $\\Delta\\simeq0.42$ and power-law scaling\nwith exponent $\\gamma\\simeq0.11$, consistent with earlier work. For $m=2$ we\nfind that universality is maintained within this nonzero fixed-$m$ symmetry\nclass but with smaller period and critical exponents, $\\Delta\\simeq0.09$ and\n$\\gamma\\simeq0.035$, establishing an explicit dependence of the critical\nsolution on the angular mode. Analysis of the relation between the angular\nmomentum and the mass of apparent horizons at the instant of formation,\n$J_{\\mathrm{AH}}{-}M_{\\mathrm{AH}}$, shows that the effect of angular momentum\nis minimal at the threshold, with\n$\\chi_{\\mathrm{AH}}=J_{\\mathrm{AH}}/M_{\\mathrm{AH}}^2\\to0$, and, therefore,\nexcludes extremal black holes for the families under consideration. Our results\ndemonstrate that while universality and DSS hold within each $m$-sector, the\ncritical universal values vary with $m$, and neither extremality nor\nbifurcation occur in the complex scalar field model within the families\nconsidered here.",
      "url": "http://arxiv.org/abs/2511.04649v1",
      "published_time_eastern_timestamp": 1762454402.0
    },
    {
      "title": "Random Construction of Quantum LDPC Codes",
      "summary": "We propose a method for modifying orthogonal sparse matrix pairs used in CSS\ncodes while preserving their matrix row and column weight distributions, which\nplay a crucial role in determining the performance of belief-propagation\ndecoding. Unlike simple row or column permutations that merely reorder existing\nelements, the proposed local modification introduces genuine structural\nrandomness through small $2\\times2$ cross-swap operations followed by\ninteger-linear-program-based local repairs that restore orthogonality. By\napplying this procedure repeatedly in a random manner, ensembles of randomized\nquantum LDPC codes can be constructed. The computational complexity of each\nrepair depends only on the maximum row and column weights and is independent of\nthe overall matrix size, ensuring scalability to large code blocks.",
      "url": "http://arxiv.org/abs/2511.04634v1",
      "published_time_eastern_timestamp": 1762453885.0
    },
    {
      "title": "Students' Acceptance of Arduino Technology Integration in Student-Led\n  Science Inquiry: Insights from the Technology Acceptance Model",
      "summary": "This study examines high school students' acceptance of Arduino technology in\na student-led, inquiry-based science class, using the extended Technology\nAcceptance Model (TAM2) as a guiding framework. Through qualitative analysis of\ninterviews and classroom observations, we explored how students perceived\nArduino's usefulness and ease of use. Going beyond traditional quantitative TAM\nstudies, this qualitative TAM research provides a nuanced, in-depth\nunderstanding of the contextual factors shaping technology acceptance. Key\nfindings reveal that acceptance was driven not only by instrumental factors\nlike job relevance and output quality but also by the unique sociocultural\ncontext of the Korean education system, where technology use was perceived as\nvaluable for university admissions (subjective norm and image). Critically,\nunlike earlier research that emphasized programming challenges, participants in\nthis study found Arduino accessible and intuitive, thanks to integrated visual\nblock-coding tools. These findings highlight the importance of both\ntechnological design and pedagogical support in shaping students' experiences.\nImplications for science curriculum design, teacher preparation, and equitable\ntechnology integration in secondary education are discussed.",
      "url": "http://arxiv.org/abs/2511.04614v1",
      "published_time_eastern_timestamp": 1762452435.0
    },
    {
      "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration\n  from a Baseline Paper",
      "summary": "Understanding the current capabilities and risks of AI Scientist systems is\nessential for ensuring trustworthy and sustainable AI-driven scientific\nprogress while preserving the integrity of the academic ecosystem. To this end,\nwe develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system\nthat mimics the core research workflow of a novice student researcher: Given\nthe baseline paper from the human mentor, it analyzes its limitations,\nformulates novel hypotheses for improvement, validates them through rigorous\nexperimentation, and writes a paper with the results. Unlike previous\napproaches that assume full automation or operate on small-scale code, Jr. AI\nScientist follows a well-defined research workflow and leverages modern coding\nagents to handle complex, multi-file implementations, leading to scientifically\nvaluable contributions. For evaluation, we conducted automated assessments\nusing AI Reviewers, author-led evaluations, and submissions to Agents4Science,\na venue dedicated to AI-driven scientific contributions. The findings\ndemonstrate that Jr. AI Scientist generates papers receiving higher review\nscores than existing fully automated systems. Nevertheless, we identify\nimportant limitations from both the author evaluation and the Agents4Science\nreviews, indicating the potential risks of directly applying current AI\nScientist systems and key challenges for future research. Finally, we\ncomprehensively report various risks identified during development. We hope\nthese insights will deepen understanding of current progress and risks in AI\nScientist development.",
      "url": "http://arxiv.org/abs/2511.04583v1",
      "published_time_eastern_timestamp": 1762450669.0
    },
    {
      "title": "Regular fat linear sets",
      "summary": "In this work, we introduce $(r,i)$-regular fat linear sets, which are defined\nas linear sets containing exactly $r$ points of weight $i$ and all other points\nof weight one. This notion generalizes and unifies existing constructions;\nscattered linear sets, clubs, and other previously studied families are special\ncases. We present new classes of regular fat linear sets in PG$(k-1,q^n)$ for\ncomposite $n$ and study their equivalence classes. Finally, we show that\nregular fat linear sets naturally yield three-weight rank-metric codes, which\nwe use to obtain bounds on their parameters.",
      "url": "http://arxiv.org/abs/2511.04581v1",
      "published_time_eastern_timestamp": 1762450630.0
    },
    {
      "title": "Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic\n  Alignment",
      "summary": "Vision-Language-Action (VLA) models have emerged as a powerful framework that\nunifies perception, language, and control, enabling robots to perform diverse\ntasks through multimodal understanding. However, current VLA models typically\ncontain massive parameters and rely heavily on large-scale robot data\npretraining, leading to high computational costs during training, as well as\nlimited deployability for real-time inference. Moreover, most training\nparadigms often degrade the perceptual representations of the vision-language\nbackbone, resulting in overfitting and poor generalization to downstream tasks.\nIn this work, we present Evo-1, a lightweight VLA model that reduces\ncomputation and improves deployment efficiency, while maintaining strong\nperformance without pretraining on robot data. Evo-1 builds on a native\nmultimodal Vision-Language model (VLM), incorporating a novel cross-modulated\ndiffusion transformer along with an optimized integration module, together\nforming an effective architecture. We further introduce a two-stage training\nparadigm that progressively aligns action with perception, preserving the\nrepresentations of the VLM. Notably, with only 0.77 billion parameters, Evo-1\nachieves state-of-the-art results on the Meta-World and RoboTwin suite,\nsurpassing the previous best models by 12.4% and 6.9%, respectively, and also\nattains a competitive result of 94.8% on LIBERO. In real-world evaluations,\nEvo-1 attains a 78% success rate with high inference frequency and low memory\noverhead, outperforming all baseline methods. We release code, data, and model\nweights to facilitate future research on lightweight and efficient VLA models.",
      "url": "http://arxiv.org/abs/2511.04555v1",
      "published_time_eastern_timestamp": 1762448869.0
    }
  ]
}
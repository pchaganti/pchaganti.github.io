{
  "last_updated": "2026-01-19T07:29:35.654050-05:00",
  "papers": [
    {
      "title": "UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation",
      "summary": "Despite recent progress, medical foundation models still struggle to unify visual understanding and generation, as these tasks have inherently conflicting goals: semantic abstraction versus pixel-level reconstruction. Existing approaches, typically based on parameter-shared autoregressive architectures, frequently lead to compromised performance in one or both tasks. To address this, we present UniX, a next-generation unified medical foundation model for chest X-ray understanding and generation. UniX decouples the two tasks into an autoregressive branch for understanding and a diffusion branch for high-fidelity generation. Crucially, a cross-modal self-attention mechanism is introduced to dynamically guide the generation process with understanding features. Coupled with a rigorous data cleaning pipeline and a multi-stage training strategy, this architecture enables synergistic collaboration between tasks while leveraging the strengths of diffusion models for superior generation. On two representative benchmarks, UniX achieves a 46.1% improvement in understanding performance (Micro-F1) and a 24.2% gain in generation quality (FD-RadDino), using only a quarter of the parameters of LLM-CXR. By achieving performance on par with task-specific models, our work establishes a scalable paradigm for synergistic medical image understanding and generation. Codes and models are available at https://github.com/ZrH42/UniX.",
      "url": "http://arxiv.org/abs/2601.11522v1",
      "published_time_eastern_timestamp": 1768589998.0
    },
    {
      "title": "Empirical Coordination over Markov Channel with Independent Source",
      "summary": "We study joint source-channel coding over Markov channels through the empirical coordination framework. More specifically, we aim at determining the empirical distributions of source and channel symbols that can be induced by a coding scheme. We consider strictly causal encoders that generate channel inputs, without access to the past channel states, henceforth driving the current Markov state evolution. Our main result is the single-letter inner and outer bounds of the set of achievable joint distributions, coordinating all the symbols in the network. To establish the inner bound, we introduce a new notion of typicality, the input-driven Markov typicality, and develop its fundamental properties. Contrary to the classical block-Markov coding schemes that rely on blockwise independence for discrete memoryless channels, our analysis directly exploits the Markov channel structure and improves beyond the independence-based arguments.",
      "url": "http://arxiv.org/abs/2601.11520v1",
      "published_time_eastern_timestamp": 1768589946.0
    },
    {
      "title": "On a C*-Diagonal Generated by the Toric Code",
      "summary": "We study the abelian sub-C*-algebra of the CAR algebra generated by the start and face opertors of Kitaev's toric code. We show that it is a C*-diagonal equivalent to the canonical diagonal of the CAR algebra.",
      "url": "http://arxiv.org/abs/2601.11511v1",
      "published_time_eastern_timestamp": 1768589212.0
    },
    {
      "title": "Applying Formal Methods Tools to an Electronic Warfare Codebase (Experience report)",
      "summary": "While using formal methods offers advantages over unit testing, their steep learning curve can be daunting to developers and can be a major impediment to widespread adoption. To support integration into an industrial software engineering workflow, a tool must provide useful information and must be usable with relatively minimal user effort. In this paper, we discuss our experiences associated with identifying and applying formal methods tools on an electronic warfare (EW) system with stringent safety requirements and present perspectives on formal methods tools from EW software engineers who are proficient in development yet lack formal methods training. In addition to a difference in mindset between formal methods and unit testing approaches, some formal methods tools use terminology or annotations that differ from their target programming language, creating another barrier to adoption. Input/output contracts, objects in memory affected by a function, and loop invariants can be difficult to grasp and use. In addition to usability, our findings include a comparison of vulnerabilities detected by different tools. Finally, we present suggestions for improving formal methods usability including better documentation of capabilities, decreased manual effort, and improved handling of library code.",
      "url": "http://arxiv.org/abs/2601.11510v1",
      "published_time_eastern_timestamp": 1768589179.0
    },
    {
      "title": "Coding Schemes for the Noisy Torn Paper Channel",
      "summary": "To make DNA a suitable medium for archival data storage, it is essential to consider the decay process of the strands observed in DNA storage systems. This paper studies the decay process as a probabilistic noisy torn paper channel (TPC), which first corrupts the bits of the transmitted sequence in a probabilistic manner by substitutions, then breaks the sequence into a set of noisy unordered substrings. The present work devises coding schemes for the noisy TPC by embedding markers in the transmitted sequence. We investigate the use of static markers and markers connected to the data in the form of hash functions. These two tools have also been recently exploited to tackle the noiseless TPC. Simulations show that static markers excel at higher substitution probabilities, while data-dependent markers are superior at lower noise levels. Both approaches achieve reconstruction rates exceeding $99\\%$ with no false decodings observed, primarily limited by computational resources.",
      "url": "http://arxiv.org/abs/2601.11501v1",
      "published_time_eastern_timestamp": 1768588368.0
    },
    {
      "title": "Convergence Properties of Good Quantum Codes for Classical Communication",
      "summary": "An important part of the information theory folklore had been about the output statistics of codes that achieve the capacity and how the empirical distributions compare to the output distributions induced by the optimal input in the channel capacity problem. Results for a variety of such empirical output distributions of good codes have been known in the literature, such as the comparison of the output distribution of the code to the optimal output distribution in vanishing and non-vanishing error probability cases. Motivated by these, we aim to achieve similar results for the quantum codes that are used for classical communication, that is the setting in which the classical messages are communicated through quantum codewords that pass through a noisy quantum channel. We first show the uniqueness of the optimal output distribution, to be able to talk more concretely about the optimal output distribution. Then, we extend the vanishing error probability results to the quantum case, by using techniques that are close in spirit to the classical case. We also extend non-vanishing error probability results to the quantum case on block codes, by using the second-order converses for such codes based on hypercontractivity results for the quantum generalized depolarizing semi-groups.",
      "url": "http://arxiv.org/abs/2601.11498v1",
      "published_time_eastern_timestamp": 1768587725.0
    },
    {
      "title": "Influence of secondary neutrons on alpha-particle induced reaction cross section measurement below the Coulomb barrier",
      "summary": "The influence of the secondary neutrons on measurements of alpha-particle activation cross sections below the Coulomb barrier was studied for the $^\\mathrm{nat}$Pt($α$,x)$^\\mathrm{195m}$Pt reaction. We characterized the secondary neutron field by using the particle transport simulation code PHITS, and estimated the $^\\mathrm{nat}$Pt(n,x)$^\\mathrm{195m}$Pt yields by using the characterized neutron spectra and the $^\\mathrm{nat}$Pt(n,x)$^{195m}$Pt cross sections in the JENDL-5/A library. We confirmed that the unexpectedly high $^\\mathrm{nat}$Pt($α$,x)$^\\mathrm{195m}$Pt cross sections below the Coulomb barrier measured by us are explained well by the $^\\mathrm{nat}$Pt(n,x)$^\\mathrm{195m}$Pt reaction induced by the secondary neutrons. This indicates that the secondary neutron effect is sometimes not negligible even in low energy charged-particle activation cross section measurements. We also studied the influence of the secondary light charged particles by the same approach, and confirmed that their influence is negligible.",
      "url": "http://arxiv.org/abs/2601.11497v1",
      "published_time_eastern_timestamp": 1768587577.0
    },
    {
      "title": "CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation",
      "summary": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM-based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics-vs-Expert correlation (MvE) using clinician ratings on 175 \"disagreement\" cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT-CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman~0.70), while CRG shows negative correlation; and BERTScore-F1 is least sensitive to factual error injection. We will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.",
      "url": "http://arxiv.org/abs/2601.11488v1",
      "published_time_eastern_timestamp": 1768586959.0
    },
    {
      "title": "A Genetic Algorithm for Generating Extreme Examples in Arithmetic Dynamics",
      "summary": "We describe a genetic algorithm to find extreme examples in the arithmetic of dynamical systems. The algorithm is applied to four problems: small (non-zero) canonical heights, many rational preperiodic points, long rational cycles, and long rational tails. Data is provided for extreme examples generated for polynomials up to degree 13 and rational functions up to degree 5. This work significantly expands the known examples of extreme behavior for several of the conjectured behaviors in arithmetic dynamics and provides a foundation from which to begin a more advanced application of machine learning techniques in the creation of extreme examples for arithmetic dynamics.",
      "url": "http://arxiv.org/abs/2601.11482v1",
      "published_time_eastern_timestamp": 1768586607.0
    },
    {
      "title": "Globally Optimal Contour Deformations with Neural Networks",
      "summary": "In this article, we explore the use of contour deformation for the numerical evaluation of Feynman integrals after sector decomposition. In existing codes, the contour of integration is determined heuristically for each phase-space point by sampling the integrand. In this work, we introduce a method for choosing the contour deformation for an entire phase-space region using only an initial sampling or training step. We demonstrate that the resulting integrand has a lower variance than that obtained with heuristic methods and show that optimising a contour to reduce the estimated error of a Quasi-Monte Carlo sample is an ill-defined problem. The a priori knowledge of the integration path obtained in this work can be used to improve the speed of conventional integration methods or be leveraged for integration using neural networks, where, crucially, it removes the need to retrain the neural network for each phase-space point. The techniques described in this work can be adapted to other problems where a non-trivial integration path has to be chosen subject to a set of constraints.",
      "url": "http://arxiv.org/abs/2601.11448v1",
      "published_time_eastern_timestamp": 1768583557.0
    },
    {
      "title": "When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models",
      "summary": "Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the scores generally improves the score-matching loss and model likelihood, it fails to consistently enhance perceptual quality metrics such as FID on image datasets. We confirm this observation across a breadth of aggregation rules using Deep Ensembles, Monte Carlo Dropout, on CIFAR-10 and FFHQ. We attempt to explain this discrepancy by investigating possible explanations, such as the link between score estimation and image quality. We also look into tabular data through random forests, and find that one aggregation strategy outperforms the others. Finally, we provide theoretical insights into the summing of score models, which shed light not only on ensembling but also on several model composition techniques (e.g. guidance).",
      "url": "http://arxiv.org/abs/2601.11444v1",
      "published_time_eastern_timestamp": 1768583245.0
    },
    {
      "title": "Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation method that dynamically updates the language model's parameters during inference to improve RAG system performance in specialized domains. Our method introduces a simple yet effective approach where the model learns to predict retrieved content, enabling automatic parameter adjustment to the target domain. Through extensive experiments across six specialized domains, we demonstrate that TTARAG achieves substantial performance improvements over baseline RAG systems. Code available at https://github.com/sunxin000/TTARAG.",
      "url": "http://arxiv.org/abs/2601.11443v1",
      "published_time_eastern_timestamp": 1768583221.0
    },
    {
      "title": "Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models",
      "summary": "Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause conflicts. In contrast, we shift our attention to Hierarchical Orthogonal Residual SprEad of the information matrix, which reduces noisy gradients and enables more stable edits from a different perspective. We demonstrate the effectiveness of our method HORSE through a clear theoretical comparison with several popular methods and extensive experiments conducted on two datasets across multiple LLMs. The results show that HORSE maintains precise massive editing across diverse scenarios. The code is available at https://github.com/XiaojieGu/HORSE",
      "url": "http://arxiv.org/abs/2601.11441v1",
      "published_time_eastern_timestamp": 1768582939.0
    },
    {
      "title": "Inter-patient ECG Arrhythmia Classification with LGNs and LUTNs",
      "summary": "Deep Differentiable Logic Gate Networks (LGNs) and Lookup Table Networks (LUTNs) are demonstrated to be suitable for the automatic classification of electrocardiograms (ECGs) using the inter-patient paradigm. The methods are benchmarked using the MIT-BIH arrhythmia data set, achieving up to 94.28% accuracy and a $jκ$ index of 0.683 on a four-class classification problem. Our models use between 2.89k and 6.17k FLOPs, including preprocessing and readout, which is three to six orders of magnitude less compared to SOTA methods. A novel preprocessing method is utilized that attains superior performance compared to existing methods for both the mixed-patient and inter-patient paradigms. In addition, a novel method for training the Lookup Tables (LUTs) in LUTNs is devised that uses the Boolean equation of a multiplexer (MUX). Additionally, rate coding was utilized for the first time in these LGNs and LUTNs, enhancing the performance of LGNs. Furthermore, it is the first time that LGNs and LUTNs have been benchmarked on the MIT-BIH arrhythmia dataset using the inter-patient paradigm. Using an Artix 7 FPGA, between 2000 and 2990 LUTs were needed, and between 5 to 7 mW (i.e. 50 pJ to 70 pJ per inference) was estimated for running these models. The performance in terms of both accuracy and $jκ$-index is significantly higher compared to previous LGN results. These positive results suggest that one can utilize LGNs and LUTNs for the detection of arrhythmias at extremely low power and high speeds in heart implants or wearable devices, even for patients not included in the training set.",
      "url": "http://arxiv.org/abs/2601.11433v1",
      "published_time_eastern_timestamp": 1768582536.0
    },
    {
      "title": "The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents",
      "summary": "Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.",
      "url": "http://arxiv.org/abs/2601.11421v1",
      "published_time_eastern_timestamp": 1768581725.0
    },
    {
      "title": "Qihe: A General-Purpose Static Analysis Framework for Verilog",
      "summary": "In the past decades, static analysis has thrived in software, facilitating applications in bug detection, security, and program understanding. These advanced analyses are largely underpinned by general-purpose static analysis frameworks, which offer essential infrastructure to streamline their development. Conversely, hardware lacks such a framework, which overshadows the promising opportunities for sophisticated static analysis in hardware, hindering achievements akin to those witnessed in software. We thus introduce Qihe, the first general-purpose static analysis framework for Verilog -- a highly challenging endeavor given the absence of precedents in hardware. Qihe features an analysis-oriented front end, a Verilog-specific IR, and a suite of diverse fundamental analyses that capture essential hardware-specific characteristics -- such as bit-vector arithmetic, register synchronization, and digital component concurrency -- and enable the examination of intricate hardware data and control flows. These fundamental analyses are designed to support a wide array of hardware analysis clients. To validate Qihe's utility, we further developed a set of clients spanning bug detection, security, and program understanding. Our preliminary experimental results are highly promising; for example, Qihe uncovered 9 previously unknown bugs in popular real-world hardware projects (averaging 1.5K+ GitHub stars), all of which were confirmed by developers; moreover, Qihe successfully identified 18 bugs beyond the capabilities of existing static analyses for Verilog bug detection (i.e., linters), and detected 16 vulnerabilities in real-world hardware programs. By open-sourcing Qihe, which comprises over 100K lines of code, we aim to inspire further innovation and applications of sophisticated static analysis for hardware, aspiring to foster a similarly vibrant ecosystem that software analysis enjoys.",
      "url": "http://arxiv.org/abs/2601.11408v1",
      "published_time_eastern_timestamp": 1768580796.0
    },
    {
      "title": "Efficient Channel Autoencoders for Wideband Communications leveraging Walsh-Hadamard interleaving",
      "summary": "This paper investigates how end-to-end (E2E) channel autoencoders (AEs) can achieve energy-efficient wideband communications by leveraging Walsh-Hadamard (WH) interleaved converters. WH interleaving enables high sampling rate analog-digital conversion with reduced power consumption using an analog WH transformation. We demonstrate that E2E-trained neural coded modulation can transparently adapt to the WH-transceiver hardware without requiring algorithmic redesign. Focusing on the short block length regime, we train WH-domain AEs and benchmark them against standard neural and conventional baselines, including 5G Polar codes. We quantify the system-level energy tradeoffs among baseband compute, channel signal-to-noise ratio (SNR), and analog converter power. Our analysis shows that the proposed WH-AE system can approach conventional Polar code SNR performance within 0.14dB while consuming comparable or lower system power. Compared to the best neural baseline, WH-AE achieves, on average, 29% higher energy efficiency (in bit/J) for the same reliability. These findings establish WH-domain learning as a viable path to energy-efficient, high-throughput wideband communications by explicitly balancing compute complexity, SNR, and analog power consumption.",
      "url": "http://arxiv.org/abs/2601.11407v1",
      "published_time_eastern_timestamp": 1768580450.0
    },
    {
      "title": "Understanding Help Seeking for Digital Privacy, Safety, and Security",
      "summary": "The complexity of navigating digital privacy, safety, and security threats often falls directly on users. This leads to users seeking help from family and peers, platforms and advice guides, dedicated communities, and even large language models (LLMs). As a precursor to improving resources across this ecosystem, our community needs to understand what help seeking looks like in the wild. To that end, we blend qualitative coding with LLM fine-tuning to sift through over one billion Reddit posts from the last four years to identify where and for what users seek digital privacy, safety, or security help. We isolate three million relevant posts with 93% precision and recall and automatically annotate each with the topics discussed (e.g., security tools, privacy configurations, scams, account compromise, content moderation, and more). We use this dataset to understand the scope and scale of help seeking, the communities that provide help, and the types of help sought. Our work informs the development of better resources for users (e.g., user guides or LLM help-giving agents) while underscoring the inherent challenges of supporting users through complex combinations of threats, platforms, mitigations, context, and emotions.",
      "url": "http://arxiv.org/abs/2601.11398v1",
      "published_time_eastern_timestamp": 1768579802.0
    },
    {
      "title": "An exciting approach to theoretical spectroscopy",
      "summary": "Theoretical spectroscopy, and more generally, electronic-structure theory, are powerful concepts for describing the complex many-body interactions in materials. They comprise a variety of methods that can capture all aspects, from ground-state properties to lattice excitations to different types of light-matter interaction, including time-resolved variants. Modern electronic-structure codes implement either a few or several of these methods. Among them, exciting is an all-electron full-potential package that has a very rich portfolio of all levels of theory, with a particular focus on excitations. It implements the linearized augmented planewave plus local orbital basis, which is known as the gold standard for solving the Kohn-Sham equations of density-functional theory. Based on this, it also offers benchmark-quality results for a wide range of excited-state methods. In this review, we provide a comprehensive overview of the features implemented in exciting in recent years, accompanied by short summaries on the state of the art of the underlying methodologies. They comprise density-functional theory and time-dependent density-functional theory, density-functional perturbation theory for phonons and electron-phonon coupling, many-body perturbation theory in terms of the $GW$ approach and the Bethe-Salpeter equation. Moreover, we capture resonant inelastic x-ray scattering, pump-probe spectroscopy as well as exciton-phonon coupling. Finally, we cover workflows and a view on data and machine learning. All aspects are demonstrated with examples for scientific relevant materials.",
      "url": "http://arxiv.org/abs/2601.11388v1",
      "published_time_eastern_timestamp": 1768578989.0
    },
    {
      "title": "Polar Orbit Decoding: Universal Parallel Soft Decoding via Automorphism Orbits",
      "summary": "Binary linear block codes (BLBCs) form the foundation of modern communication systems, yet no single code family simultaneously optimizes all performance aspects. This leads to the widely used multi-code architecture in the standard, significantly increasing the hardware complexity since multiple decoders are required in each piece of equipment. A universal decoding framework based on polar transformations has recently been proposed to unify BLBC decoding under polar-style decoders, but its parallelization has not yet been discussed. In this work, we propose Polar Orbit Decoding (POD), a universal parallel decoding framework for BLBCs. We identify that the automorphisms of BLBCs generate an orbit of permutations that induce diverse decoding trajectories with identical dynamic-frozen constraints after the polar transformations. By decoding over this automorphism orbit in parallel, POD achieves substantial latency-performance tradeoffs without requiring frozen-set readaptation or extra exhaustive permutation searches. Moreover, to enable efficient orbit traversal in the implementation, we represent the automorphism group in a base and strong generating set (BSGS) form using Schreier-Sims algorithms, making offline systematic computation accessible in polynomial time. Simulation results on extended BCH and extended Golay codes demonstrate that POD can achieve maximum-likelihood performance while significantly reducing the decoding latency compared to conventional successive cancellation list decoding.",
      "url": "http://arxiv.org/abs/2601.11373v1",
      "published_time_eastern_timestamp": 1768577401.0
    }
  ]
}
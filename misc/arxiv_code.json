{
  "last_updated": "2026-02-23T20:16:20.426959-05:00",
  "papers": [
    {
      "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning",
      "summary": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.",
      "url": "http://arxiv.org/abs/2602.18429v1",
      "published_time_eastern_timestamp": 1771613587.0
    },
    {
      "title": "SPQ: An Ensemble Technique for Large Language Model Compression",
      "summary": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/",
      "url": "http://arxiv.org/abs/2602.18420v1",
      "published_time_eastern_timestamp": 1771613056.0
    },
    {
      "title": "Exploiting Completeness Perception with Diffusion Transformer for Unified 3D MRI Synthesis",
      "summary": "Missing data problems, such as missing modalities in multi-modal brain MRI and missing slices in cardiac MRI, pose significant challenges in clinical practice. Existing methods rely on external guidance to supply detailed missing state for instructing generative models to synthesize missing MRIs. However, manual indicators are not always available or reliable in real-world scenarios due to the unpredictable nature of clinical environments. Moreover, these explicit masks are not informative enough to provide guidance for improving semantic consistency. In this work, we argue that generative models should infer and recognize missing states in a self-perceptive manner, enabling them to better capture subtle anatomical and pathological variations. Towards this goal, we propose CoPeDiT, a general-purpose latent diffusion model equipped with completeness perception for unified synthesis of 3D MRIs. Specifically, we incorporate dedicated pretext tasks into our tokenizer, CoPeVAE, empowering it to learn completeness-aware discriminative prompts, and design MDiT3D, a specialized diffusion transformer architecture for 3D MRI synthesis, that effectively uses the learned prompts as guidance to enhance semantic consistency in 3D space. Comprehensive evaluations on three large-scale MRI datasets demonstrate that CoPeDiT significantly outperforms state-of-the-art methods, achieving superior robustness, generalizability, and flexibility. The code is available at https://github.com/JK-Liu7/CoPeDiT .",
      "url": "http://arxiv.org/abs/2602.18400v1",
      "published_time_eastern_timestamp": 1771610739.0
    },
    {
      "title": "Qualitative Coding Analysis through Open-Source Large Language Models: A User Study and Design Recommendations",
      "summary": "Qualitative data analysis is labor-intensive, yet the privacy risks associated with commercial Large Language Models (LLMs) often preclude their use in sensitive research. To address this, we introduce ChatQDA, an on-device framework powered by open-source LLMs designed for privacy-preserving open coding. Our mixed-methods user study reveals that while participants rated the system highly for usability and perceived efficiency, they exhibited \"conditional trust\", valuing the tool for surface-level extraction while questioning its interpretive nuance and consistency. Furthermore, despite the technical security of local deployment, participants reported epistemic uncertainty regarding data protection, suggesting that invisible security measures are insufficient to foster trust. We conclude with design recommendations for local-first analysis tools that prioritize verifiable privacy and methodological rigor.",
      "url": "http://arxiv.org/abs/2602.18352v1",
      "published_time_eastern_timestamp": 1771607042.0
    },
    {
      "title": "A method to derive self-consistent NLTE astrophysical parameters for 4 million high-resolution 4MOST stellar spectra in half a day with invertible neural networks",
      "summary": "Modern spectroscopic surveys obtain spectra for millions of stars. However, classical spectroscopic methods can often be computationally expensive, rendering them impractical for the analysis of large datasets. We introduce a novel simulation-based deep-learning approach for the efficient analysis of high-resolution stellar spectra to be obtained with the upcoming high-resolution 4MOST spectrograph. We used a suite of synthetic non-local thermodynamic equilibrium (NLTE) spectra generated with Turbospectrum to mimic 4MOST observations and trained a conditional invertible neural network (cINN) for the purpose of predicting self-consistently stellar surface parameters and chemical abundances. The cINN is a neural network architecture that estimates full posterior distributions for the target stellar properties, providing an intrinsic uncertainty estimate. We evaluated the predictive performance of the trained cINN model on both synthetic data and observed spectra of stars. We found that our new cINN trained on NLTE synthetic spectra is capable of recovering stellar parameters with average errors ($Ïƒ$) of $33$ K for $T_\\mathrm{eff}$, $0.16$ dex for $\\log(g)$, and $0.12$ dex for [Fe/H], $0.1$ dex for [Ca/Fe], $0.11$ for [Mg/Fe], and $0.51$ dex for [Li/Fe], respectively, at a signal to noise ratio of 250 per Angstrom. From the analysis of the observed spectra of Gaia-ESO / 4MOST / PLATO benchmark stars, we verified that our NLTE estimates for stellar parameters and abundances are consistent with results obtained with the independent code TSFitPy. We conclude that the NLTE cINN is robust and can, theoretically, evaluate 4 million high-resolution 4MOST spectra in less than a day, using GPU acceleration.",
      "url": "http://arxiv.org/abs/2602.18340v1",
      "published_time_eastern_timestamp": 1771606208.0
    },
    {
      "title": "Recoverable systems and the maximal hard-core model on the triangular lattice",
      "summary": "In a previous paper (arXiv:2510.19746), we have studied the maximal hard-code model on the square lattice ${\\mathbb Z}^2$ from the perspective of recoverable systems. Here we extend this study to the case of the triangular lattice ${\\mathbb A}$. The following results are obtained:\n  (1) We derive bounds on the capacity of the associated recoverable system on ${\\mathbb A}$;\n  (2) We show non-uniqueness of Gibbs measures in the high-activity regime;\n  (3) We characterize extremal periodic Gibbs measures for sufficiently low values of activity.",
      "url": "http://arxiv.org/abs/2602.18310v1",
      "published_time_eastern_timestamp": 1771603742.0
    },
    {
      "title": "Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation",
      "summary": "Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an \"in the wild\" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.",
      "url": "http://arxiv.org/abs/2602.18309v1",
      "published_time_eastern_timestamp": 1771603651.0
    },
    {
      "title": "ReqElicitGym: An Evaluation Environment for Interview Competence in Conversational Requirements Elicitation",
      "summary": "With the rapid improvement of LLMs' coding capabilities, the bottleneck of LLM-based automated software development is shifting from generating correct code to eliciting users' requirements. Despite growing interest, the interview competence of LLMs in conversational requirements elicitation remains fully underexplored. Existing evaluations often depend on a few scenarios, real user interaction, and subjective human scoring, which hinders systematic and quantitative comparison. To address these challenges, we propose ReqElicitGym, an interactive and automatic evaluation environment for assessing interview competence in conversational requirements elicitation. Specifically, ReqElicitGym introduces a new evaluation dataset and designs both an interactive oracle user and a task evaluator. The dataset contains 101 website requirements elicitation scenarios spanning 10 application types. Both the oracle user and the task evaluator achieve high agreement with real users and expert judgment. Using our ReqElicitGym, any automated conversational requirements elicitation approach (e.g., LLM-based agents) can be evaluated in a reproducible and quantitative manner through interaction with the environment. Based on our ReqElicitGym, we conduct a systematic empirical study on seven representative LLMs, and the results show that current LLMs still exhibit limited interview competence in uncovering implicit requirements. Particularly, they elicit less than half of the users' implicit requirements, and their effective elicitation questions often emerge in later turns of the dialogue. Besides, we found LLMs can elicit interaction and content implicit requirements, but consistently struggle with style-related requirements. We believe ReqElicitGym will facilitate the evaluation and development of automated conversational requirements elicitation.",
      "url": "http://arxiv.org/abs/2602.18306v1",
      "published_time_eastern_timestamp": 1771603333.0
    },
    {
      "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory",
      "summary": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.",
      "url": "http://arxiv.org/abs/2602.18297v1",
      "published_time_eastern_timestamp": 1771602630.0
    },
    {
      "title": "GR-Athena++: Binary Neutron Star Merger Simulations with Neutrino Transport",
      "summary": "We present general-relativistic radiation magnetohydrodynamics simulations of binary neutron star mergers performed with GR-Athena++. Neutrino transport is treated using a moment-based, energy-integrated scheme (M1), augmented by neutrino number density evolution (N0). Our implementation is validated through an extensive suite of standard tests and demonstrated to perform robustly under adaptive mesh refinement. As a first application, we simulate the gravitational collapse of a uniformly rotating, magnetized neutron star, demonstrating stable radiation evolution through apparent-horizon formation using a novel excision technique based on the tapering of state vector evolution inside the horizon. To further test robustness in highly dynamic environments, we apply our code to two demanding binary neutron star merger scenarios. We investigate a long-lived remnant with the DD2 equation of state, evolved with full general-relativistic magnetohydrodynamics and M1 neutrino transport. Following this, a gravitational collapse scenario with the SFHo equation of state is explored. We showcase long-term stable evolution on neutrino cooling time-scales, demonstrating robust handling of excision and stable evolution of the post-collapse accretion phase in three-dimensional mergers with magnetic fields and neutrino radiation.",
      "url": "http://arxiv.org/abs/2602.18290v1",
      "published_time_eastern_timestamp": 1771601832.0
    },
    {
      "title": "Detecting PowerShell-based Fileless Cryptojacking Attacks Using Machine Learning",
      "summary": "With the emergence of remote code execution (RCE) vulnerabilities in ubiquitous libraries and advanced social engineering techniques, threat actors have started conducting widespread fileless cryptojacking attacks. These attacks have become effective with stealthy techniques based on PowerShell-based exploitation in Windows OS environments. Even if attacks are detected and malicious scripts removed, processes may remain operational on victim endpoints, creating a significant challenge for detection mechanisms. In this paper, we conducted an experimental study with a collected dataset on detecting PowerShell-based fileless cryptojacking scripts. The results showed that Abstract Syntax Tree (AST)-based fine-tuned CodeBERT achieved a high recall rate, proving the importance of the use of AST integration and fine-tuned pre-trained models for programming language.",
      "url": "http://arxiv.org/abs/2602.18285v1",
      "published_time_eastern_timestamp": 1771601535.0
    },
    {
      "title": "PRISM: Parallel Reward Integration with Symmetry for MORL",
      "summary": "This work studies heterogeneous Multi-Objective Reinforcement Learning (MORL), where objectives can differ sharply in temporal frequency. Such heterogeneity allows dense objectives to dominate learning, while sparse long-horizon rewards receive weak credit assignment, leading to poor sample efficiency. We propose a Parallel Reward Integration with Symmetry (PRISM) algorithm that enforces reflectional symmetry as an inductive bias in aligning reward channels. PRISM introduces ReSymNet, a theory-motivated model that reconciles temporal-frequency mismatches across objectives, using residual blocks to learn a scaled opportunity value that accelerates exploration while preserving the optimal policy. We also propose SymReg, a reflectional equivariance regulariser that enforces agent mirroring and constrains policy search to a reflection-equivariant subspace. This restriction provably reduces hypothesis complexity and improves generalisation. Across MuJoCo benchmarks, PRISM consistently outperforms both a sparse-reward baseline and an oracle trained with full dense rewards, improving Pareto coverage and distributional balance: it achieves hypervolume gains exceeding 100\\% over the baseline and up to 32\\% over the oracle. The code is at \\href{https://github.com/EVIEHub/PRISM}{https://github.com/EVIEHub/PRISM}.",
      "url": "http://arxiv.org/abs/2602.18277v1",
      "published_time_eastern_timestamp": 1771599762.0
    },
    {
      "title": "Many Tools, Few Exploitable Vulnerabilities: A Survey of 246 Static Code Analyzers for Security",
      "summary": "Static security analysis is a widely used technique for detecting software vulnerabilities across a wide range of weaknesses, application domains, and programming languages. While prior work surveyed static analyzes for specific weaknesses or application domains, no overview of the entire security landscape exists. We present a systematic literature review of 246 static security analyzers concerning their targeted vulnerabilities, application domains, analysis techniques, evaluation methods, and limitations. We observe that most analyzers focus on a limited set of weaknesses, that the vulnerabilities they detect are rarely exploitable, and that evaluations use custom benchmarks that are too small to enable robust assessment.",
      "url": "http://arxiv.org/abs/2602.18270v1",
      "published_time_eastern_timestamp": 1771599138.0
    },
    {
      "title": "Construction of Cyclic Codes over a Class of Matrix Rings",
      "summary": "Let $ \\mathbb F_2[u]/ \\langle u^k \\rangle= \\mathbb F_2+u\\mathbb F_2+u^2\\mathbb F_2+\\cdots+u^{k-1}\\mathbb F_2 ,$ where $u^k=0$ for a positive integer $k$, and $\\mathcal{R}=M_4 (\\mathbb F_2( u)/ \\langle u^k \\rangle)$ be the finite noncommutative non-chain matrix ring of order $4\\times4$. This paper presents the construction of cyclic codes over the finite field $\\mathbb F_{16}$ via the considered matrix ring $\\mathcal{R}$. In this connection, first, we discuss the structure of the ring $\\mathcal{R}$ and show that $\\mathcal{R}$ is isomorphic to the ring $( \\mathbb F_{16}+ v\\mathbb F_{16} + v^2\\mathbb F_{16} + v^3\\mathbb F_{16}) + u(\\mathbb F_{16} + v\\mathbb F_{16} + v^2\\mathbb F_{16} + v^3\\mathbb F_{16}) + u^2(\\mathbb F_{16} + v\\mathbb F_{16} + v^2\\mathbb F_{16}+ v^3\\mathbb F_{16}) + \\cdots + u^{k-1}(\\mathbb F_{16} + v\\mathbb F_{16} + v^2\\mathbb F_{16} + v^3\\mathbb F_{16})$ where $v^4=0, u^k=0, u^iv^j=v^ju^i$ for $i \\in \\{1,\\dots, k-1\\}$ and $j \\in \\{1, 2, 3\\}$. Then, we establish the form of ideals of the ring $\\mathcal{R}$ and related cyclic codes over $\\mathcal{R}$. Further, we show that these cyclic codes can be written as the direct sums of $\\mathcal{R}$-submodules of $\\frac{\\mathcal{R}[x]}{<x^n-1>}$, and derive the formula for the cardinality of cyclic codes over $\\mathcal{R}$. Then, we consider the Euclidean and Hermitian duals of the derived cyclic codes over $\\mathcal{R}$. Under the module isometry for $\\mathcal{R}$, we use the Bachoc map and the Gray map, which takes a derived cyclic code over $\\mathcal{R}$ to $\\mathbb F_{16}$. Finally, we provide some non-trivial examples of linear codes over $\\mathbb F_{16}$ with good parameters that support our derived results and compare a few codes with existing codes in the literature.",
      "url": "http://arxiv.org/abs/2602.18255v1",
      "published_time_eastern_timestamp": 1771598535.0
    },
    {
      "title": "Variational Distributional Neuron",
      "summary": "We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This \"contraction\" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze \"collapse\" modes and the conditions for a \"living neuron\", then extend the contribution over time via autoregressive priors over the latent, per unit.",
      "url": "http://arxiv.org/abs/2602.18250v1",
      "published_time_eastern_timestamp": 1771598153.0
    },
    {
      "title": "Reflections on the Future of Statistics Education in a Technological Era",
      "summary": "Keeping pace with rapidly evolving technology is a key challenge in teaching statistics. To equip students with essential skills for the modern workplace, educators must integrate relevant technologies into the statistical curriculum where possible. University-level statistics education has experienced substantial technological change, particularly in the tools and practices that underpin teaching and learning. Statistical programming has become central to many courses, with R widely used and Python increasingly incorporated into statistics and data analytics programmes. Additionally, coding practices, database management, and machine learning now feature within some statistics curricula. Looking ahead, we anticipate a growing emphasis on artificial intelligence (AI), particularly the pedagogical implications of generative AI tools such as ChatGPT. In this article, we explore these technological developments and discuss strategies for their integration into contemporary statistics education.",
      "url": "http://arxiv.org/abs/2602.18242v1",
      "published_time_eastern_timestamp": 1771597575.0
    },
    {
      "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning",
      "summary": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.",
      "url": "http://arxiv.org/abs/2602.18232v1",
      "published_time_eastern_timestamp": 1771596802.0
    },
    {
      "title": "[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games",
      "summary": "Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.",
      "url": "http://arxiv.org/abs/2602.18230v1",
      "published_time_eastern_timestamp": 1771596691.0
    },
    {
      "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps",
      "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime",
      "url": "http://arxiv.org/abs/2602.18201v1",
      "published_time_eastern_timestamp": 1771593928.0
    },
    {
      "title": "Improving Sampling for Masked Diffusion Models via Information Gain",
      "summary": "Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.",
      "url": "http://arxiv.org/abs/2602.18176v1",
      "published_time_eastern_timestamp": 1771590363.0
    }
  ]
}
{
  "last_updated": "2025-10-08T17:10:40.461992-04:00",
  "papers": [
    {
      "title": "Human3R: Everyone Everywhere All at Once",
      "summary": "We present Human3R, a unified, feed-forward framework for online 4D\nhuman-scene reconstruction, in the world frame, from casually captured\nmonocular videos. Unlike previous approaches that rely on multi-stage\npipelines, iterative contact-aware refinement between humans and scenes, and\nheavy dependencies, e.g., human detection, depth estimation, and SLAM\npre-processing, Human3R jointly recovers global multi-person SMPL-X bodies\n(\"everyone\"), dense 3D scene (\"everywhere\"), and camera trajectories in a\nsingle forward pass (\"all-at-once\"). Our method builds upon the 4D online\nreconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,\nto strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct\nreadout of multiple SMPL-X bodies. Human3R is a unified model that eliminates\nheavy dependencies and iterative refinement. After being trained on the\nrelatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it\nachieves superior performance with remarkable efficiency: it reconstructs\nmultiple humans in a one-shot manner, along with 3D scenes, in one stage, at\nreal-time speed (15 FPS) with a low memory footprint (8 GB). Extensive\nexperiments demonstrate that Human3R delivers state-of-the-art or competitive\nperformance across tasks, including global human motion estimation, local human\nmesh recovery, video depth estimation, and camera pose estimation, with a\nsingle unified model. We hope that Human3R will serve as a simple yet strong\nbaseline, be easily extended for downstream applications.Code available in\nhttps://fanegg.github.io/Human3R",
      "url": "http://arxiv.org/abs/2510.06219v1",
      "published_time_eastern_timestamp": 1759859992.0
    },
    {
      "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a\n  Challenging Benchmark",
      "summary": "Most existing benchmarks for egocentric vision understanding focus primarily\non daytime scenarios, overlooking the low-light conditions that are inevitable\nin real-world applications. To investigate this gap, we present EgoNight, the\nfirst comprehensive benchmark for nighttime egocentric vision, with visual\nquestion answering (VQA) as the core task. A key feature of EgoNight is the\nintroduction of day-night aligned videos, which enhance night annotation\nquality using the daytime data and reveal clear performance gaps between\nlighting conditions. To achieve this, we collect both synthetic videos rendered\nby Blender and real-world recordings, ensuring that scenes and actions are\nvisually and temporally aligned. Leveraging these paired videos, we construct\nEgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and\nrefinement through extensive human verification. Each QA pair is double-checked\nby annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs\nacross 90 videos, spanning 12 diverse QA types, with more than 300 hours of\nhuman work. Evaluations of state-of-the-art multimodal large language models\n(MLLMs) reveal substantial performance drops when transferring from day to\nnight, underscoring the challenges of reasoning under low-light conditions.\nBeyond VQA, EgoNight also introduces two auxiliary tasks, day-night\ncorrespondence retrieval and egocentric depth estimation at night, that further\nexplore the boundaries of existing models. We believe EgoNight-VQA provides a\nstrong foundation for advancing application-driven egocentric vision research\nand for developing models that generalize across illumination domains. All the\ndata and code will be made available upon acceptance.",
      "url": "http://arxiv.org/abs/2510.06218v1",
      "published_time_eastern_timestamp": 1759859987.0
    },
    {
      "title": "Tensor time series change-point detection in cryptocurrency network data",
      "summary": "Financial fraud has been growing exponentially in recent years. The rise of\ncryptocurrencies as an investment asset has simultaneously seen a parallel\ngrowth in cryptocurrency scams. To detect possible cryptocurrency fraud, and in\nparticular market manipulation, previous research focused on the detection of\nchanges in the network of trades; however, market manipulators are now trading\nacross multiple cryptocurrency platforms, making their detection more\ndifficult. Hence, it is important to consider the identification of changes\nacross several trading networks or a `network of networks' over time. To this\nend, in this article, we propose a new change-point detection method in the\nnetwork structure of tensor-variate data. This new method, labeled TenSeg,\nfirst employs a tensor decomposition, and second detects multiple change-points\nin the second-order (cross-covariance or network) structure of the decomposed\ndata. It allows for change-point detection in the presence of frequent changes\nof possibly small magnitudes and is computationally fast. We apply our method\nto several simulated datasets and to a cryptocurrency dataset, which consists\nof network tensor-variate data from the Ethereum blockchain. We demonstrate\nthat our approach substantially outperforms other state-of-the-art change-point\ntechniques, and the detected change-points in the Ethereum data set coincide\nwith changes across several trading networks or a `network of networks' over\ntime. Finally, all the relevant \\textsf{R} code implementing the method in the\narticle are available on https://github.com/Anastasiou-Andreas/TenSeg.",
      "url": "http://arxiv.org/abs/2510.06211v1",
      "published_time_eastern_timestamp": 1759859917.0
    },
    {
      "title": "EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern\n  Coding Model",
      "summary": "Recent advances in control robot methods, from end-to-end\nvision-language-action frameworks to modular systems with predefined\nprimitives, have advanced robots' ability to follow natural language\ninstructions. Nonetheless, many approaches still struggle to scale to diverse\nenvironments, as they often rely on large annotated datasets and offer limited\ninterpretability.In this work, we introduce EmbodiedCoder, a training-free\nframework for open-world mobile robot manipulation that leverages coding models\nto directly generate executable robot trajectories. By grounding high-level\ninstructions in code, EmbodiedCoder enables flexible object geometry\nparameterization and manipulation trajectory synthesis without additional data\ncollection or fine-tuning.This coding-based paradigm provides a transparent and\ngeneralizable way to connect perception with manipulation. Experiments on real\nmobile robots show that EmbodiedCoder achieves robust performance across\ndiverse long-term tasks and generalizes effectively to novel objects and\nenvironments.Our results demonstrate an interpretable approach for bridging\nhigh-level reasoning and low-level control, moving beyond fixed primitives\ntoward versatile robot intelligence. See the project page at:\nhttps://anonymous.4open.science/w/Embodied-Coder/",
      "url": "http://arxiv.org/abs/2510.06207v1",
      "published_time_eastern_timestamp": 1759859882.0
    },
    {
      "title": "Modulation Discovery with Differentiable Digital Signal Processing",
      "summary": "Modulations are a critical part of sound design and music production,\nenabling the creation of complex and evolving audio. Modern synthesizers\nprovide envelopes, low frequency oscillators (LFOs), and more parameter\nautomation tools that allow users to modulate the output with ease. However,\ndetermining the modulation signals used to create a sound is difficult, and\nexisting sound-matching / parameter estimation systems are often\nuninterpretable black boxes or predict high-dimensional framewise parameter\nvalues without considering the shape, structure, and routing of the underlying\nmodulation curves. We propose a neural sound-matching approach that leverages\nmodulation extraction, constrained control signal parameterizations, and\ndifferentiable digital signal processing (DDSP) to discover the modulations\npresent in a sound. We demonstrate the effectiveness of our approach on highly\nmodulated synthetic and real audio samples, its applicability to different DDSP\nsynth architectures, and investigate the trade-off it incurs between\ninterpretability and sound-matching accuracy. We make our code and audio\nsamples available and provide the trained DDSP synths in a VST plugin.",
      "url": "http://arxiv.org/abs/2510.06204v1",
      "published_time_eastern_timestamp": 1759859784.0
    },
    {
      "title": "Latent Speech-Text Transformer",
      "summary": "Auto-regressive speech-text models are typically pre-trained on a large\nnumber of interleaved sequences of text tokens and raw speech encoded as speech\ntokens using vector quantization. These models have demonstrated\nstate-of-the-art performance in speech-to-speech understanding and generation\nbenchmarks, together with promising scaling laws, primarily enabled by the\nrepresentational alignment between text and speech. Nevertheless, they suffer\nfrom shortcomings, partly owing to the disproportionately longer sequences of\nspeech tokens in contrast to textual tokens. This results in a large compute\nimbalance between modalities during pre-training as well as during inference,\nand a potential hindrance to effectively aligning speech and text, ultimately\ntranslating to several orders of magnitude slower scaling laws. We introduce\nthe Latent Speech-Text Transformer (LST), which makes pre-training speech-text\nmodels more data-efficient by dynamically and inexpensively aggregating speech\ntokens into latent speech patches. These patches serve as higher-level units\nthat can either align with corresponding textual units to aid capability\ntransfer or even encapsulate common speech sequences like silences to be more\ncompute-efficient. We show that LST outperforms vanilla approaches on\nspeech-to-speech as well as text-to-text benchmarks in both data- and\ncompute-controlled settings, the former indicating more effective\nrepresentational alignment and the latter indicating steeper scaling laws for\nspeech-text models. On HellaSwag story completion, LST achieves 6.5% absolute\ngain in speech accuracy under compute-controlled training and 5.3% under\ndata-controlled training, while also improving text performance. We will\nrelease our models, code, and the evaluation data to facilitate further\nresearch.",
      "url": "http://arxiv.org/abs/2510.06195v1",
      "published_time_eastern_timestamp": 1759859528.0
    },
    {
      "title": "Overlap-aware segmentation for topological reconstruction of obscured\n  objects",
      "summary": "The separation of overlapping objects presents a significant challenge in\nscientific imaging. While deep learning segmentation-regression algorithms can\npredict pixel-wise intensities, they typically treat all regions equally rather\nthan prioritizing overlap regions where attribution is most ambiguous. Recent\nadvances in instance segmentation show that weighting regions of pixel overlap\nin training can improve segmentation boundary predictions in regions of\noverlap, but this idea has not yet been extended to segmentation regression. We\naddress this with Overlap-Aware Segmentation of ImageS (OASIS): a new\nsegmentation-regression framework with a weighted loss function designed to\nprioritize regions of object-overlap during training, enabling extraction of\npixel intensities and topological features from heavily obscured objects. We\ndemonstrate OASIS in the context of the MIGDAL experiment, which aims to\ndirectly image the Migdal effect--a rare process where electron emission is\ninduced by nuclear scattering--in a low-pressure optical time projection\nchamber. This setting poses an extreme test case, as the target for\nreconstruction is a faint electron recoil track which is often heavily-buried\nwithin the orders-of-magnitude brighter nuclear recoil track. Compared to\nunweighted training, OASIS improves median intensity reconstruction errors from\n-32% to -14% for low-energy electron tracks (4-5 keV) and improves topological\nintersection-over-union scores from 0.828 to 0.855. These performance gains\ndemonstrate OASIS's ability to recover obscured signals in overlap-dominated\nregions. The framework provides a generalizable methodology for scientific\nimaging where pixels represent physical quantities and overlap obscures\nfeatures of interest. All code is openly available to facilitate cross-domain\nadoption.",
      "url": "http://arxiv.org/abs/2510.06194v1",
      "published_time_eastern_timestamp": 1759859521.0
    },
    {
      "title": "On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond",
      "summary": "This paper formally studies generation processes, including auto-regressive\nnext-token prediction and masked diffusion, that abstract beyond architectural\nspecifics. At this level of abstraction, we quantify their benefits and\nlimitations through measurable criteria such as computational hardness and\nlearnability. In particular, we demonstrate that allowing generation to proceed\nbeyond autoregression and current masked diffusion, with capabilities to\nrewrite and length-variable edit, can bring significant theoretical and\nempirical advantages, with important implications for frontier LLMs that aspire\nto tackle increasingly hard problems and work universally across domains beyond\nnatural language, such as coding and science.",
      "url": "http://arxiv.org/abs/2510.06190v1",
      "published_time_eastern_timestamp": 1759859370.0
    },
    {
      "title": "Automated Program Repair of Uncompilable Student Code",
      "summary": "A significant portion of student programming submissions in CS1 learning\nenvironments are uncompilable, limiting their use in student modeling and\ndownstream knowledge tracing. Traditional modeling pipelines often exclude\nthese cases, discarding observations of student learning. This study\ninvestigates automated program repair as a strategy to recover uncompilable\ncode while preserving students' structural intent for use in student modeling.\nWithin this framework, we assess large language models (LLMs) as repair agents,\nincluding GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash\n(Google), under high- and low-context prompting conditions. Repairs were\nevaluated for compilability, edit distance, and preservation of students'\noriginal structure and logic. We find that while all three LLMs are capable of\nproducing compilable repairs, their behavior diverges in how well they preserve\nstudents' control flow and code structure, which affects their pedagogical\nutility. By recovering uncompilable submissions, this work enables richer and\nmore comprehensive analyses of learners' coding processes and development over\ntime.",
      "url": "http://arxiv.org/abs/2510.06187v1",
      "published_time_eastern_timestamp": 1759859193.0
    },
    {
      "title": "RECODE-H: A Benchmark for Research Code Development with Interactive\n  Human Feedback",
      "summary": "Large language models (LLMs) show the promise in supporting scientific\nresearch implementation, yet their ability to generate correct and executable\ncode remains limited. Existing works largely adopt one-shot settings, ignoring\nthe iterative and feedback-driven nature of realistic workflows of scientific\nresearch development. To address this gap, we present RECODE-H, a benchmark of\n102 tasks from research papers and repositories that evaluates LLM agents\nthrough multi-turn interactions with LLM-simulated human feedback. It includes\nstructured instructions,unit tests, and a five-level feedback hierarchy to\nreflect realistic researcher-agent collaboration. We further present\nReCodeAgent, a framework that integrates feedback into iterative code\ngeneration. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,\nDeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer\nfeedback, while also highlighting ongoing challenges in the generation of\ncomplex research code. RECODE-H establishes a foundation for developing\nadaptive, feedback-driven LLM agents in scientific research implementation",
      "url": "http://arxiv.org/abs/2510.06186v1",
      "published_time_eastern_timestamp": 1759859135.0
    },
    {
      "title": "Probabilistic Guarantees to Explicit Constructions: Local Properties of\n  Linear Codes",
      "summary": "We present a general framework for derandomizing random linear codes with\nrespect to a broad class of permutation-invariant properties, known as local\nproperties, which encompass several standard notions such as distance,\nlist-decoding, list-recovery, and perfect hashing. Our approach extends the\nclassical Alon-Edmonds-Luby (AEL) construction through a modified formalism of\nlocal coordinate-wise linear (LCL) properties, introduced by Levi, Mosheiff,\nand Shagrithaya (2025). The main theorem demonstrates that if random linear\ncodes satisfy the complement of an LCL property $\\mathcal{P}$ with high\nprobability, then one can construct explicit codes satisfying the complement of\n$\\mathcal{P}$ as well, with an enlarged yet constant alphabet size. This gives\nthe first explicit constructions for list recovery, as well as special cases\n(e.g., list recovery with erasures, zero-error list recovery, perfect hash\nmatrices), with parameters matching those of random linear codes. More broadly,\nour constructions realize the full range of parameters associated with these\nproperties at the same level of optimality as in the random setting, thereby\noffering a systematic pathway from probabilistic guarantees to explicit codes\nthat attain them. Furthermore, our derandomization of random linear codes also\nadmits efficient (list) decoding via recently developed expander-based\ndecoders.",
      "url": "http://arxiv.org/abs/2510.06185v1",
      "published_time_eastern_timestamp": 1759859120.0
    },
    {
      "title": "Batched high-rate logical operations for quantum LDPC codes",
      "summary": "High-rate quantum LDPC (qLDPC) codes reduce memory overhead by densely\npacking many logical qubits into a single block of physical qubits. Here we\nextend this concept to high-rate computation by constructing \\emph{batched}\nfault-tolerant operations that apply the same logical gate across many code\nblocks in parallel. By leveraging shared physical resources to execute many\nlogical operations in parallel, these operations realize high rates in\nspace-time and significantly reduce computational costs. For \\emph{arbitrary}\nCSS qLDPC codes, we build batched gadgets with \\emph{constant space-time\noverhead} (assuming fast classical computation) for (i) single-shot error\ncorrection, state preparation, and code surgeries (ii) code switching, and\n(iii) addressable Clifford gates. Using these batched gadgets we also construct\nparallel non-Clifford gates with low space-time cost. We outline principles for\ndesigning parallel quantum algorithms optimized for a batched architecture, and\nshow in particular how lattice Hamiltonian dynamical simulations can be\ncompiled efficiently. We also propose a near-term implementation using new\nself-dual Bivariate-Bicycle codes with high encoding rates ($\\sim 1/10$),\ntransversal Clifford gates, and global $T$ gates via parallel magic state\ncultivation, enabling Hamiltonian simulations with a lower space-time cost than\nanalogous surface-code protocols and low-rate qLDPC protocols. These results\nopen new paths toward scalable quantum computation via co-design of parallel\nquantum algorithms and high-rate fault-tolerant protocols.",
      "url": "http://arxiv.org/abs/2510.06159v1",
      "published_time_eastern_timestamp": 1759857970.0
    },
    {
      "title": "Resolving Star Cluster Formation in Galaxy Simulations with Cosmic Ray\n  Feedback",
      "summary": "Star clusters host the massive stars responsible for feedback in star-forming\ngalaxies. Stellar feedback shapes the interstellar medium (ISM), affecting the\nformation of future star clusters. To self-consistently capture the interplay\nbetween feedback and star formation, a model must resolve the parsec-scale star\nformation sites and the multiphase ISM. Additionally, the dynamical impact of\ncosmic rays (CRs) on star formation rates (SFRs) must also be considered. We\npresent the first simulations of the formation of an ensemble of star clusters\nwith dynamically-coupled CRs, near-individual star particles, and a\nfeedback-regulated ISM. We analyze tallbox simulations performed using the\nCRISP model in the moving-mesh code AREPO. We apply varied implementations of\nCR transport under the theory of self-confinement. We find that CRs\nsimultaneously reduce the SFR, the power law slope of the cluster mass\nfunction, and the cluster formation efficiency. Each simulation is compatible\nwith observations, and CR feedback tends to move results along observed star\ncluster relations. We see only modest changes in cluster radius and velocity\ndispersions, but significant differences in the virial parameters. Ultimately,\nthe primary impact of CRs is to reduce SFRs. Lower SFRs imply fewer supernovae,\nand consequently a lower turbulent energy budget for gas. Star clusters formed\nin a CR-regulated ISM have lower velocity dispersions, and are therefore more\nbound under self-gravity. The effective clustering of SNe is unchanged by CRs.\nThrough this work, we demonstrate the predictive power of the CRISP feedback\nmodel, despite this idealized setup.",
      "url": "http://arxiv.org/abs/2510.06134v1",
      "published_time_eastern_timestamp": 1759856924.0
    },
    {
      "title": "lm-Meter: Unveiling Runtime Inference Latency for On-Device Language\n  Models",
      "summary": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications, but their prevalent cloud-based deployment raises growing\nconcerns around data privacy and long-term sustainability. Running LLMs locally\non mobile and edge devices (on-device LLMs) offers the promise of enhanced\nprivacy, reliability, and reduced communication costs. However, realizing this\nvision remains challenging due to substantial memory and compute demands, as\nwell as limited visibility into performance-efficiency trade-offs on\nresource-constrained hardware. We propose lm-Meter, the first lightweight,\nonline latency profiler tailored for on-device LLM inference. lm-Meter captures\nfine-grained, real-time latency at both phase (e.g., embedding, prefill,\ndecode, softmax, sampling) and kernel levels without auxiliary devices. We\nimplement lm-Meter on commercial mobile platforms and demonstrate its high\nprofiling accuracy with minimal system overhead, e.g., only 2.58% throughput\nreduction in prefill and 0.99% in decode under the most constrained Powersave\ngovernor. Leveraging lm-Meter, we conduct comprehensive empirical studies\nrevealing phase- and kernel-level bottlenecks in on-device LLM inference,\nquantifying accuracy-efficiency trade-offs, and identifying systematic\noptimization opportunities. lm-Meter provides unprecedented visibility into the\nruntime behavior of LLMs on constrained platforms, laying the foundation for\ninformed optimization and accelerating the democratization of on-device LLM\nsystems. Code and tutorials are available at\nhttps://github.com/amai-gsu/LM-Meter.",
      "url": "http://arxiv.org/abs/2510.06126v1",
      "published_time_eastern_timestamp": 1759856730.0
    },
    {
      "title": "Multimodal Feature Prototype Learning for Interpretable and\n  Discriminative Cancer Survival Prediction",
      "summary": "Survival analysis plays a vital role in making clinical decisions. However,\nthe models currently in use are often difficult to interpret, which reduces\ntheir usefulness in clinical settings. Prototype learning presents a potential\nsolution, yet traditional methods focus on local similarities and static\nmatching, neglecting the broader tumor context and lacking strong semantic\nalignment with genomic data. To overcome these issues, we introduce an\ninnovative prototype-based multimodal framework, FeatProto, aimed at enhancing\ncancer survival prediction by addressing significant limitations in current\nprototype learning methodologies within pathology. Our framework establishes a\nunified feature prototype space that integrates both global and local features\nof whole slide images (WSI) with genomic profiles. This integration facilitates\ntraceable and interpretable decision-making processes. Our approach includes\nthree main innovations: (1) A robust phenotype representation that merges\ncritical patches with global context, harmonized with genomic data to minimize\nlocal bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that\nsustains stable cross-modal associations and employs a wandering mechanism to\nadapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype\nmatching scheme designed to capture global centrality, local typicality, and\ncohort-level trends, thereby refining prototype inference. Comprehensive\nevaluations on four publicly available cancer datasets indicate that our method\nsurpasses current leading unimodal and multimodal survival prediction\ntechniques in both accuracy and interoperability, providing a new perspective\non prototype learning for critical medical applications. Our source code is\navailable at https://github.com/JSLiam94/FeatProto.",
      "url": "http://arxiv.org/abs/2510.06113v1",
      "published_time_eastern_timestamp": 1759855792.0
    },
    {
      "title": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction\n  Interpretations",
      "summary": "Open Source Software (OSS) has become a very important and crucial\ninfrastructure worldwide because of the value it provides. OSS typically\ndepends on contributions from developers across diverse backgrounds and levels\nof experience. Making safe changes, such as fixing a bug or implementing a new\nfeature, can be challenging, especially in object-oriented systems where\ncomponents are interdependent. Static analysis and defect-prediction tools\nproduce metrics (e.g., complexity,coupling) that flag potentially fault-prone\ncomponents, but these signals are often hard for contributors new or unfamiliar\nwith the codebase to interpret. Large Language Models (LLMs) have shown strong\nperformance on software engineering tasks such as code summarization and\ndocumentation generation. Building on this progress, we investigate whether\nLLMs can translate fault-prediction metrics into clear, human-readable risk\nexplanations and actionable guidance to help OSS contributors plan and review\ncode modifications. We outline explanation types that an LLM-generated\nassistant could provide (descriptive, contextual, and actionable explanations).\nWe also outline our next steps to assess usefulness through a task-based study\nwith OSS contributors, comparing metric-only baselines to LLM-generated\nexplanations on decision quality, time-to-completion, and error rates",
      "url": "http://arxiv.org/abs/2510.06104v1",
      "published_time_eastern_timestamp": 1759854961.0
    },
    {
      "title": "The Valley of Code Reasoning: Scaling Knowledge Distillation of Large\n  Language Models",
      "summary": "Distilling the thinking traces of a Large Language Model (LLM) with reasoning\ncapabilities into a smaller model has been proven effective. Yet, there is a\nscarcity of work done on how model performances scale with the quantity of\ndistillation data. In this work, we study the scaling trend of distilling\ncompetitive coding skills on two small non-reasoning LLMs. We validate the\nhypothesis that there is a $\\textit{valley of code reasoning}$: downstream\nperformance on competitive coding first drops as data quantity increases, then\nit steadily increases in a sharper-than-log-linear fashion. Having identified\nthe trend, we further fine-tune the models at two different distillation stages\non the same data to ground conclusions on their respective learning phases. We\nlearn that across stages in the low and medium-low data regimes, small models\nbenefit significantly from easier coding questions than from harder ones. We\nalso find that, surprisingly, the correctness of outputs in training data makes\nno difference to distillation outcomes. Our work represents a step forward in\nunderstanding the training dynamics of code reasoning distillation outside\nintuition",
      "url": "http://arxiv.org/abs/2510.06101v1",
      "published_time_eastern_timestamp": 1759854729.0
    },
    {
      "title": "Compact Multi-level-prior Tensor Representation for Hyperspectral Image\n  Super-resolution",
      "summary": "Fusing a hyperspectral image with a multispectral image acquired over the\nsame scene, \\textit{i.e.}, hyperspectral image super-resolution, has become a\npopular computational way to access the latent high-spatial-spectral-resolution\nimage. To date, a variety of fusion methods have been proposed, among which the\ntensor-based ones have testified that multiple priors, such as multidimensional\nlow-rankness and spatial total variation at multiple levels, effectively drive\nthe fusion process. However, existing tensor-based models can only effectively\nleverage one or two priors at one or two levels, since simultaneously\nincorporating multi-level priors inevitably increases model complexity. This\nintroduces challenges in both balancing the weights of different priors and\noptimizing multi-block structures. Concerning this, we present a novel\nhyperspectral super-resolution model compactly characterizing these multi-level\npriors of hyperspectral images within the tensor framework. Firstly, the\nproposed model decouples the spectral low-rankness and spatial priors by\ncasting the latent high-spatial-spectral-resolution image into spectral\nsubspace and spatial maps via block term decomposition. Secondly, these spatial\nmaps are stacked as the spatial tensor encoding the high-order spatial\nlow-rankness and smoothness priors, which are co-modeled via the proposed\nnon-convex mode-shuffled tensor correlated total variation. Finally, we draw\ninspiration from the linearized alternating direction method of multipliers to\ndesign an efficient algorithm to optimize the resulting model, theoretically\nproving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments\non multiple datasets demonstrate the effectiveness of the proposed algorithm.\nThe code implementation will be available from https://github.com/WongYinJ.",
      "url": "http://arxiv.org/abs/2510.06098v1",
      "published_time_eastern_timestamp": 1759854394.0
    },
    {
      "title": "Recursive construction and enumeration of self-orthogonal and self-dual\n  codes over finite commutative chain rings of even characteristic - I",
      "summary": "Let $\\mathscr{R}_{e,m}$ denote a finite commutative chain ring of even\ncharacteristic with maximal ideal $\\langle u \\rangle$ of nilpotency index $e\n\\geq 3,$ Teichm$\\ddot{u}$ller set $\\mathcal{T}_{m},$ and residue field\n$\\mathscr{R}_{e,m}/\\langle u \\rangle$ of order $2^m.$ Suppose that $2 \\in\n\\langle u^{\\kappa}\\rangle \\setminus \\langle u^{\\kappa+1}\\rangle$ for some odd\ninteger $\\kappa$ with $3 \\leq \\kappa \\leq e.$ In this paper, we first develop a\nrecursive method to construct a self-orthogonal code $\\mathscr{D}_e$ of type\n$\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_e\\}$ and length $n$ over\n$\\mathscr{R}_{e,m}$ from a chain $\\mathcal{C}^{(1)}\\subseteq \\mathcal{C}^{(2)}\n\\subseteq \\cdots \\subseteq \\mathcal{C}^{(\\lceil \\frac{e}{2} \\rceil)} $ of\nself-orthogonal codes of length $n$ over $\\mathcal{T}_{m},$ and vice versa,\nsubject to certain conditions, where $\\lambda_1,\\lambda_2,\\ldots,\\lambda_e$ are\nnon-negative integers satisfying\n$2\\lambda_1+2\\lambda_2+\\cdots+2\\lambda_{e-i+1}+\\lambda_{e-i+2}+\\lambda_{e-i+3}+\\cdots+\\lambda_i\n\\leq n$ for $\\lceil \\frac{e+1}{2} \\rceil \\leq i\\leq e,$ and\n  $\\lfloor \\cdot \\rfloor$ and $\\lceil \\cdot \\rceil$ denote the floor and\nceiling functions, respectively. This construction ensures that\n$Tor_i(\\mathscr{D}_e)=\\mathcal{C}^{(i)}$ for $1 \\leq i \\leq \\lceil \\frac{e}{2}\n\\rceil.$\n  With the help of this recursive construction method and by applying results\nfrom group theory and finite geometry, we obtain explicit enumeration formulae\nfor all self-orthogonal and self-dual codes of an arbitrary length over\n$\\mathscr{R}_{e,m}.$ We also illustrate these results with some examples.",
      "url": "http://arxiv.org/abs/2510.06082v1",
      "published_time_eastern_timestamp": 1759853351.0
    },
    {
      "title": "Recursive construction and enumeration of self-orthogonal and self-dual\n  codes over finite commutative chain rings of even characteristic - II",
      "summary": "Let $\\mathcal{R}_{e,m}$ be a finite commutative chain ring of even\ncharacteristic with maximal ideal $\\langle u \\rangle$ of nilpotency index $e\n\\geq 2,$ Teichm$\\ddot{u}$ller set $\\mathcal{T}_{m},$ and residue field\n$\\mathcal{R}_{e,m}/\\langle u \\rangle$ of order $2^m.$ Suppose that $2 \\in\n\\langle u^{\\kappa}\\rangle \\setminus \\langle u^{\\kappa+1}\\rangle$ for some even\npositive integer $ \\kappa \\leq e.$ In this paper, we provide a recursive method\nto construct a self-orthogonal code $\\mathcal{C}_e$ of type $\\{\\lambda_1,\n\\lambda_2, \\ldots, \\lambda_e\\}$ and length $n$ over $\\mathcal{R}_{e,m}$ from a\nchain $\\mathcal{D}^{(1)}\\subseteq \\mathcal{D}^{(2)} \\subseteq \\cdots \\subseteq\n\\mathcal{D}^{(\\lceil \\frac{e}{2} \\rceil)}$ of self-orthogonal codes of length\n$n$ over $\\mathcal{T}_{m},$ and vice versa, where $\\dim\n\\mathcal{D}^{(i)}=\\lambda_1+\\lambda_2+\\cdots+\\lambda_i$ for $1 \\leq i \\leq\n\\lceil \\frac{e}{2} \\rceil,$ the codes $\\mathcal{D}^{(\\lfloor \\frac{e+1}{2}\n\\rfloor-\\kappa)},\\mathcal{D}^{(\\lfloor \\frac{e+1}{2} \\rfloor\n-\\kappa+1)},\\ldots,\\mathcal{D}^{(\\lfloor \\frac{e}{2}\\rfloor-\\lfloor\n\\frac{\\kappa}{2} \\rfloor)}$ satisfy certain additional conditions, and\n$\\lambda_1,\\lambda_2,\\ldots,\\lambda_e$ are non-negative integers satisfying\n$2\\lambda_1+2\\lambda_2+\\cdots+2\\lambda_{e-i+1}+\\lambda_{e-i+2}+\\lambda_{e-i+3}+\\cdots+\\lambda_i\n\\leq n$ for $\\lceil \\frac{e+1}{2} \\rceil \\leq i\\leq e.$ This construction\nguarantees that $Tor_i(\\mathcal{C}_e)=\\mathcal{D}^{(i)}$ for $1 \\leq i \\leq\n\\lceil \\frac{e}{2} \\rceil.$ By employing this recursive construction method,\ntogether with the results from group theory and finite geometry, we derive\nexplicit enumeration formulae for all self-orthogonal and self-dual codes of an\narbitrary length over $\\mathcal{R}_{e,m}.$ We also demonstrate these results\nthrough examples.",
      "url": "http://arxiv.org/abs/2510.06069v1",
      "published_time_eastern_timestamp": 1759852742.0
    }
  ]
}
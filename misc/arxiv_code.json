{
  "last_updated": "2025-11-01T23:41:32.627108-04:00",
  "papers": [
    {
      "title": "Scaling Image Geo-Localization to Continent Level",
      "summary": "Determining the precise geographic location of an image at a global scale\nremains an unsolved challenge. Standard image retrieval techniques are\ninefficient due to the sheer volume of images (>100M) and fail when coverage is\ninsufficient. Scalable solutions, however, involve a trade-off: global\nclassification typically yields coarse results (10+ kilometers), while\ncross-view retrieval between ground and aerial imagery suffers from a domain\ngap and has been primarily studied on smaller regions. This paper introduces a\nhybrid approach that achieves fine-grained geo-localization across a large\ngeographic expanse the size of a continent. We leverage a proxy classification\ntask during training to learn rich feature representations that implicitly\nencode precise location information. We combine these learned prototypes with\nembeddings of aerial imagery to increase robustness to the sparsity of\nground-level data. This enables direct, fine-grained retrieval over areas\nspanning multiple countries. Our extensive evaluation demonstrates that our\napproach can localize within 200m more than 68\\% of queries of a dataset\ncovering a large part of Europe. The code is publicly available at\nhttps://scaling-geoloc.github.io.",
      "url": "http://arxiv.org/abs/2510.26795v1",
      "published_time_eastern_timestamp": 1761847175.0
    },
    {
      "title": "The Quest for Generalizable Motion Generation: Data, Model, and\n  Evaluation",
      "summary": "Despite recent advances in 3D human motion generation (MoGen) on standard\nbenchmarks, existing models still face a fundamental bottleneck in their\ngeneralization capability. In contrast, adjacent generative fields, most\nnotably video generation (ViGen), have demonstrated remarkable generalization\nin modeling human behaviors, highlighting transferable insights that MoGen can\nleverage. Motivated by this observation, we present a comprehensive framework\nthat systematically transfers knowledge from ViGen to MoGen across three key\npillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a\nlarge-scale dataset comprising 228,000 high-quality motion samples that\nintegrates high-fidelity optical MoCap data with semantically annotated motions\nfrom web videos and synthesized samples generated by state-of-the-art ViGen\nmodels. The dataset includes both text-motion pairs and text-video-motion\ntriplets, substantially expanding semantic diversity. Second, we propose\nViMoGen, a flow-matching-based diffusion transformer that unifies priors from\nMoCap data and ViGen models through gated multimodal conditioning. To enhance\nefficiency, we further develop ViMoGen-light, a distilled variant that\neliminates video generation dependencies while preserving strong\ngeneralization. Finally, we present MBench, a hierarchical benchmark designed\nfor fine-grained evaluation across motion quality, prompt fidelity, and\ngeneralization ability. Extensive experiments show that our framework\nsignificantly outperforms existing approaches in both automatic and human\nevaluations. The code, data, and benchmark will be made publicly available.",
      "url": "http://arxiv.org/abs/2510.26794v1",
      "published_time_eastern_timestamp": 1761847167.0
    },
    {
      "title": "Gistify! Codebase-Level Understanding via Runtime Execution",
      "summary": "As coding agents are increasingly deployed in large codebases, the need to\nautomatically design challenging, codebase-level evaluation is central. We\npropose Gistify, a task where a coding LLM must create a single, minimal,\nself-contained file that can reproduce a specific functionality of a codebase.\nThe coding LLM is given full access to a codebase along with a specific\nentrypoint (e.g., a python command), and the generated file must replicate the\noutput of the same command ran under the full codebase, while containing only\nthe essential components necessary to execute the provided command. Success on\nGistify requires both structural understanding of the codebase, accurate\nmodeling of its execution flow as well as the ability to produce potentially\nlarge code patches. Our findings show that current state-of-the-art models\nstruggle to reliably solve Gistify tasks, especially ones with long executions\ntraces.",
      "url": "http://arxiv.org/abs/2510.26790v1",
      "published_time_eastern_timestamp": 1761847106.0
    },
    {
      "title": "Defeating the Training-Inference Mismatch via FP16",
      "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often\nsuffers from instability due to the numerical mismatch between the training and\ninference policies. While prior work has attempted to mitigate this issue\nthrough algorithmic corrections or engineering alignments, we show that its\nroot cause lies in the floating point precision itself. The widely adopted\nBF16, despite its large dynamic range, introduces large rounding errors that\nbreaks the consistency between training and inference. In this work, we\ndemonstrate that simply reverting to \\textbf{FP16} effectively eliminates this\nmismatch. The change is simple, fully supported by modern frameworks with only\na few lines of code change, and requires no modification to the model\narchitecture or learning algorithm. Our results suggest that using FP16\nuniformly yields more stable optimization, faster convergence, and stronger\nperformance across diverse tasks, algorithms and frameworks. We hope these\nfindings motivate a broader reconsideration of precision trade-offs in RL\nfine-tuning.",
      "url": "http://arxiv.org/abs/2510.26788v1",
      "published_time_eastern_timestamp": 1761847091.0
    },
    {
      "title": "HEIR: Learning Graph-Based Motion Hierarchies",
      "summary": "Hierarchical structures of motion exist across research fields, including\ncomputer vision, graphics, and robotics, where complex dynamics typically arise\nfrom coordinated interactions among simpler motion components. Existing methods\nto model such dynamics typically rely on manually-defined or heuristic\nhierarchies with fixed motion primitives, limiting their generalizability\nacross different tasks. In this work, we propose a general hierarchical motion\nmodeling method that learns structured, interpretable motion relationships\ndirectly from data. Our method represents observed motions using graph-based\nhierarchies, explicitly decomposing global absolute motions into\nparent-inherited patterns and local motion residuals. We formulate hierarchy\ninference as a differentiable graph learning problem, where vertices represent\nelemental motions and directed edges capture learned parent-child dependencies\nthrough graph neural networks. We evaluate our hierarchical reconstruction\napproach on three examples: 1D translational motion, 2D rotational motion, and\ndynamic 3D scene deformation via Gaussian splatting. Experimental results show\nthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,\nand produces more realistic and interpretable deformations compared to the\nbaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,\ndata-driven hierarchical modeling paradigm, our method offers a formulation\napplicable to a broad range of motion-centric tasks. Project Page:\nhttps://light.princeton.edu/HEIR/",
      "url": "http://arxiv.org/abs/2510.26786v1",
      "published_time_eastern_timestamp": 1761847060.0
    },
    {
      "title": "LLMs Process Lists With General Filter Heads",
      "summary": "We investigate the mechanisms underlying a range of list-processing tasks in\nLLMs, and we find that LLMs have learned to encode a compact, causal\nrepresentation of a general filtering operation that mirrors the generic\n\"filter\" function of functional programming. Using causal mediation analysis on\na diverse set of list-processing tasks, we find that a small number of\nattention heads, which we dub filter heads, encode a compact representation of\nthe filtering predicate in their query states at certain tokens. We demonstrate\nthat this predicate representation is general and portable: it can be extracted\nand reapplied to execute the same filtering operation on different collections,\npresented in different formats, languages, or even in tasks. However, we also\nidentify situations where transformer LMs can exploit a different strategy for\nfiltering: eagerly evaluating if an item satisfies the predicate and storing\nthis intermediate result as a flag directly in the item representations. Our\nresults reveal that transformer LMs can develop human-interpretable\nimplementations of abstract computational operations that generalize in ways\nthat are surprisingly similar to strategies used in traditional functional\nprogramming patterns.",
      "url": "http://arxiv.org/abs/2510.26784v1",
      "published_time_eastern_timestamp": 1761847037.0
    },
    {
      "title": "Surpassing state of the art on AMD area estimation from RGB fundus\n  images through careful selection of U-Net architectures and loss functions\n  for class imbalance",
      "summary": "Age-related macular degeneration (AMD) is one of the leading causes of\nirreversible vision impairment in people over the age of 60. This research\nfocuses on semantic segmentation for AMD lesion detection in RGB fundus images,\na non-invasive and cost-effective imaging technique. The results of the ADAM\nchallenge - the most comprehensive AMD detection from RGB fundus images\nresearch competition and open dataset to date - serve as a benchmark for our\nevaluation. Taking the U-Net connectivity as a base of our framework, we\nevaluate and compare several approaches to improve the segmentation model's\narchitecture and training pipeline, including pre-processing techniques,\nencoder (backbone) deep network types of varying complexity, and specialized\nloss functions to mitigate class imbalances on image and pixel levels. The main\noutcome of this research is the final configuration of the AMD detection\nframework, which outperforms all the prior ADAM challenge submissions on the\nmulti-class segmentation of different AMD lesion types in non-invasive RGB\nfundus images. The source code used to conduct the experiments presented in\nthis paper is made freely available.",
      "url": "http://arxiv.org/abs/2510.26778v1",
      "published_time_eastern_timestamp": 1761846946.0
    },
    {
      "title": "Approximate quantum error correction, eigenstate thermalization and the\n  chaos bound",
      "summary": "Quantum error correction, thermalization, and quantum chaos are fundamental\naspects of quantum many-body physics that have each developed largely\nindependently, despite their deep conceptual overlap. In this work, we\nestablish a precise link between all three in systems that satisfy the\neigenstate thermalization hypothesis (ETH) and exhibit a well-defined hierarchy\nof time scales between dissipation and scrambling. Building on the ETH matrix\nansatz and the structure of the out-of-time-order correlator (OTOC), we show\nthat the chaos bound directly constrains the error of an approximate quantum\nerror-correcting code. This establishes a quantitative relation between\ninformation scrambling, thermalization, and correctability. Furthermore, we\nderive bounds on dynamical fluctuations around the infinite-time average and on\nfluctuation-dissipation relations, expressed in terms of both the code error\nand the Lyapunov exponent. Our results reveal how the limits of quantum chaos\nconstrain information preservation in thermalizing quantum systems.",
      "url": "http://arxiv.org/abs/2510.26758v1",
      "published_time_eastern_timestamp": 1761846537.0
    },
    {
      "title": "Running VLAs at Real-time Speed",
      "summary": "In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate\nand at most 480Hz trajectory frequency using a single consumer GPU. This\nenables dynamic and real-time tasks that were previously believed to be\nunattainable by large VLA models. To achieve it, we introduce a bag of\nstrategies to eliminate the overheads in model inference. The real-world\nexperiment shows that the pi0 policy with our strategy achieves a 100% success\nrate in grasping a falling pen task. Based on the results, we further propose a\nfull streaming inference framework for real-time robot control of VLA. Code is\navailable at https://github.com/Dexmal/realtime-vla.",
      "url": "http://arxiv.org/abs/2510.26742v1",
      "published_time_eastern_timestamp": 1761845894.0
    },
    {
      "title": "Using Copilot Agent Mode to Automate Library Migration: A Quantitative\n  Assessment",
      "summary": "Keeping software systems up to date is essential to avoid technical debt,\nsecurity vulnerabilities, and the rigidity typical of legacy systems. However,\nupdating libraries and frameworks remains a time consuming and error-prone\nprocess. Recent advances in Large Language Models (LLMs) and agentic coding\nsystems offer new opportunities for automating such maintenance tasks. In this\npaper, we evaluate the update of a well-known Python library, SQLAlchemy,\nacross a dataset of ten client applications. For this task, we use the Github's\nCopilot Agent Mode, an autonomous AI systema capable of planning and executing\nmulti-step migration workflows. To assess the effectiveness of the automated\nmigration, we also introduce Migration Coverage, a metric that quantifies the\nproportion of API usage points correctly migrated. The results of our study\nshow that the LLM agent was capable of migrating functionalities and API usages\nbetween SQLAlchemy versions (migration coverage: 100%, median), but failed to\nmaintain the application functionality, leading to a low test-pass rate\n(39.75%, median).",
      "url": "http://arxiv.org/abs/2510.26699v1",
      "published_time_eastern_timestamp": 1761843913.0
    },
    {
      "title": "LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits",
      "summary": "Low-Rank Adaptation (LoRA) has become a popular technique for\nparameter-efficient fine-tuning of large language models (LLMs). In many\nreal-world scenarios, multiple adapters are loaded simultaneously to enable LLM\ncustomization for personalized user experiences or to support a diverse range\nof tasks. Although each adapter is lightweight in isolation, their aggregate\ncost becomes substantial at scale. To address this, we propose LoRAQuant, a\nmixed-precision post-training quantization method tailored to LoRA.\nSpecifically, LoRAQuant reparameterizes each adapter by singular value\ndecomposition (SVD) to concentrate the most important information into specific\nrows and columns. This makes it possible to quantize the important components\nto higher precision, while quantizing the rest to ultra-low bitwidth. We\nconduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B\nmodels on mathematical reasoning, coding, and summarization tasks. Results show\nthat our LoRAQuant uses significantly lower bits than other quantization\nmethods, but achieves comparable or even higher performance.",
      "url": "http://arxiv.org/abs/2510.26690v1",
      "published_time_eastern_timestamp": 1761843562.0
    },
    {
      "title": "A Proposed Framework for Quantifying AI-to-Clinical Translation: The\n  Algorithm-to-Outcome Concordance (AOC) Metric",
      "summary": "Background: The rapid evolution of personalized neoantigen vaccines has been\naccelerated by artificial intelligence (AI)-based prediction models. Yet, a\nconsistent framework to evaluate the translational fidelity between\ncomputational predictions and clinical outcomes remains lacking. Methods: This\nsystematic synthesis analyzed six melanoma vaccine trials conducted between\n2017 and 2025 across mRNA, peptide, and dendritic cell platforms. We introduced\nthe Algorithm-to-Outcome Concordance (AOC) metric - a quantitative measure\nlinking model performance (AUC) with clinical efficacy (HR/ORR) - and\nintegrated mechanistic, economic, and regulatory perspectives. Results:\nSimulated AOC values across studies ranged from 0.42-0.79, suggesting\nheterogeneous concordance between algorithmic prediction and observed outcomes.\nHigh tumor mutational burden and clonal neoantigen dominance correlated with\nimproved translational fidelity. Economic modeling suggested that achieving AOC\n>0.7 could reduce ICER below $100,000/QALY. Conclusions: This framework\nquantitatively bridges AI-driven neoantigen prediction with clinical\ntranslation, offering a reproducible metric for future personalized vaccine\nvalidation and regulatory standardization. This study presents AOC as a\nhypothesis-generating tool, with all computations based on simulated or\naggregated trial data for demonstration purposes only.",
      "url": "http://arxiv.org/abs/2510.26685v1",
      "published_time_eastern_timestamp": 1761843340.0
    },
    {
      "title": "Process-based Indicators of Vulnerability Re-Introducing Code Changes:\n  An Exploratory Case Study",
      "summary": "Software vulnerabilities often persist or re-emerge even after being fixed,\nrevealing the complex interplay between code evolution and socio-technical\nfactors. While source code metrics provide useful indicators of\nvulnerabilities, software engineering process metrics can uncover patterns that\nlead to their introduction. Yet few studies have explored whether process\nmetrics can reveal risky development activities over time -- insights that are\nessential for anticipating and mitigating software vulnerabilities. This work\nhighlights the critical role of process metrics along with code changes in\nunderstanding and mitigating vulnerability reintroduction. We move beyond\nfile-level prediction and instead analyze security fixes at the commit level,\nfocusing not only on whether a single fix introduces a vulnerability but also\non the longer sequences of changes through which vulnerabilities evolve and\nre-emerge. Our approach emphasizes that reintroduction is rarely the result of\none isolated action, but emerges from cumulative development activities and\nsocio-technical conditions. To support this analysis, we conducted a case study\non the ImageMagick project by correlating longitudinal process metrics such as\nbus factor, issue density, and issue spoilage with vulnerability reintroduction\nactivities, encompassing 76 instances of reintroduced vulnerabilities. Our\nfindings show that reintroductions often align with increased issue spoilage\nand fluctuating issue density, reflecting short-term inefficiencies in issue\nmanagement and team responsiveness. These observations provide a foundation for\nbroader studies that combine process and code metrics to predict risky fixes\nand strengthen software security.",
      "url": "http://arxiv.org/abs/2510.26676v1",
      "published_time_eastern_timestamp": 1761842736.0
    },
    {
      "title": "Tidal disruption events with SPH-EXA: resolving the return of the stream",
      "summary": "In a tidal disruption event (TDE), a star is disrupted by the tidal field of\na massive black hole, creating a debris stream that returns to the black hole,\nforms an accretion flow, and powers a luminous flare. Over the last few\ndecades, several numerical studies have concluded that shock-induced\ndissipation occurs as the stream returns to pericentre (i.e.,\npre-self-intersection), resulting in efficient circularisation of the debris.\nHowever, the efficacy of these shocks is the subject of intense debate. We\npresent high-resolution simulations (up to 10^10 particles) of the disruption\nof a solar-like star by a 10^6M_sun black hole with the new, GPU-based,\nsmoothed-particle hydrodynamics code SPH-EXA, including the relativistic\napsidal precession of the stellar debris orbits; our simulations run from\ninitial disruption to the moment of stream self-intersection. With 10^8\nparticles - corresponding to the highest-resolution SPH simulations of TDEs in\nthe pre-existing literature - we find significant, in-plane spreading of the\ndebris as the stream returns through pericenter, in line with previous works\nthat suggested this is a significant source of dissipation and luminous\nemission. However, with increasing resolution this effect is dramatically\ndiminished, and with 10^10 particles there is effectively no change between the\nincoming and the outgoing stream widths. Our results demonstrate that the\nparadigm of significant dissipation of kinetic energy during pericentre passage\nis incorrect, and instead it is likely that debris circularisation is mediated\nby the originally proposed, stream-stream collision scenario.",
      "url": "http://arxiv.org/abs/2510.26663v1",
      "published_time_eastern_timestamp": 1761841850.0
    },
    {
      "title": "BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric\n  Brain MRI",
      "summary": "Assessing the severity of artifacts in pediatric brain Magnetic Resonance\nImaging (MRI) is critical for diagnostic accuracy, especially in low-field\nsystems where the signal-to-noise ratio is reduced. Manual quality assessment\nis time-consuming and subjective, motivating the need for robust automated\nsolutions. In this work, we propose BRIQA (Balanced Reweighting in Image\nQuality Assessment), which addresses class imbalance in artifact severity\nlevels. BRIQA uses gradient-based loss reweighting to dynamically adjust\nper-class contributions and employs a rotating batching scheme to ensure\nconsistent exposure to underrepresented classes. Through experiments, no single\narchitecture performs best across all artifact types, emphasizing the\nimportance of architectural diversity. The rotating batching configuration\nimproves performance across metrics by promoting balanced learning when\ncombined with cross-entropy loss. BRIQA improves average macro F1 score from\n0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098),\nPositioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012)\nartifact severity classification. The code is available at\nhttps://github.com/BioMedIA-MBZUAI/BRIQA.",
      "url": "http://arxiv.org/abs/2510.26661v1",
      "published_time_eastern_timestamp": 1761841749.0
    },
    {
      "title": "Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in\n  Dynamic Environments",
      "summary": "This paper presents a hierarchical path-planning and control framework that\ncombines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with\na low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller\nfor continuous actuation. The high-level module selects behaviors and\nsub-goals; the low-level module executes smooth velocity commands. We design a\npractical reward shaping scheme (direction, distance, obstacle avoidance,\naction smoothness, collision penalty, time penalty, and progress), together\nwith a LiDAR-based safety gate that prevents unsafe motions. The system is\nimplemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics,\nincluding success rate, collision rate, path efficiency, and re-planning\nefficiency, in dynamic and partially observable environments. Experiments show\nimproved success rate and sample efficiency over single-algorithm baselines\n(DQN or TD3 alone) and rule-based planners, with better generalization to\nunseen obstacle configurations and reduced abrupt control changes. Code and\nevaluation scripts are available at the project repository.",
      "url": "http://arxiv.org/abs/2510.26646v1",
      "published_time_eastern_timestamp": 1761840721.0
    },
    {
      "title": "Curly Flow Matching for Learning Non-gradient Field Dynamics",
      "summary": "Modeling the transport dynamics of natural processes from population-level\nobservations is a ubiquitous problem in the natural sciences. Such models rely\non key assumptions about the underlying process in order to enable faithful\nlearning of governing dynamics that mimic the actual system behavior. The de\nfacto assumption in current approaches relies on the principle of least action\nthat results in gradient field dynamics and leads to trajectories minimizing an\nenergy functional between two probability measures. However, many real-world\nsystems, such as cell cycles in single-cell RNA, are known to exhibit\nnon-gradient, periodic behavior, which fundamentally cannot be captured by\ncurrent state-of-the-art methods such as flow and bridge matching. In this\npaper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is\ncapable of learning non-gradient field dynamics by designing and solving a\nSchr\\\"odinger bridge problem with a non-zero drift reference process -- in\nstark contrast to typical zero-drift reference processes -- which is\nconstructed using inferred velocities in addition to population snapshot data.\nWe showcase Curly-FM by solving the trajectory inference problems for single\ncells, computational fluid dynamics, and ocean currents with approximate\nvelocities. We demonstrate that Curly-FM can learn trajectories that better\nmatch both the reference process and population marginals. Curly-FM expands\nflow matching models beyond the modeling of populations and towards the\nmodeling of known periodic behavior in physical systems. Our code repository is\naccessible at: https://github.com/kpetrovicc/curly-flow-matching.git",
      "url": "http://arxiv.org/abs/2510.26645v1",
      "published_time_eastern_timestamp": 1761840699.0
    },
    {
      "title": "Toward Automated Security Risk Detection in Large Software Using Call\n  Graph Analysis",
      "summary": "Threat modeling plays a critical role in the identification and mitigation of\nsecurity risks; however, manual approaches are often labor intensive and prone\nto error. This paper investigates the automation of software threat modeling\nthrough the clustering of call graphs using density-based and community\ndetection algorithms, followed by an analysis of the threats associated with\nthe identified clusters. The proposed method was evaluated through a case study\nof the Splunk Forwarder Operator (SFO), wherein selected clustering metrics\nwere applied to the software's call graph to assess pertinent code-density\nsecurity weaknesses. The results demonstrate the viability of the approach and\nunderscore its potential to facilitate systematic threat assessment. This work\ncontributes to the advancement of scalable, semi-automated threat modeling\nframeworks tailored for modern cloud-native environments.",
      "url": "http://arxiv.org/abs/2510.26620v1",
      "published_time_eastern_timestamp": 1761839039.0
    },
    {
      "title": "Aeolus: A Multi-structural Flight Delay Dataset",
      "summary": "We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed\nto advance research on flight delay prediction and support the development of\nfoundation models for tabular data. Existing datasets in this domain are\ntypically limited to flat tabular structures and fail to capture the\nspatiotemporal dynamics inherent in delay propagation. Aeolus addresses this\nlimitation by providing three aligned modalities: (i) a tabular dataset with\nrich operational, meteorological, and airportlevel features for over 50 million\nflights; (ii) a flight chain module that models delay propagation along\nsequential flight legs, capturing upstream and downstream dependencies; and\n(iii) a flight network graph that encodes shared aircraft, crew, and airport\nresource connections, enabling cross-flight relational reasoning. The dataset\nis carefully constructed with temporal splits, comprehensive features, and\nstrict leakage prevention to support realistic and reproducible machine\nlearning evaluation. Aeolus supports a broad range of tasks, including\nregression, classification, temporal structure modeling, and graph learning,\nserving as a unified benchmark across tabular, sequential, and graph\nmodalities. We release baseline experiments and preprocessing tools to\nfacilitate adoption. Aeolus fills a key gap for both domain-specific modeling\nand general-purpose structured data research.Our source code and data can be\naccessed at https://github.com/Flnny/Delay-data",
      "url": "http://arxiv.org/abs/2510.26616v1",
      "published_time_eastern_timestamp": 1761838903.0
    },
    {
      "title": "Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives",
      "summary": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.",
      "url": "http://arxiv.org/abs/2510.26606v1",
      "published_time_eastern_timestamp": 1761838513.0
    }
  ]
}
{
  "last_updated": "2025-09-19T14:16:10.679809-04:00",
  "papers": [
    {
      "title": "Lost in Translation? Vocabulary Alignment for Source-Free Domain\n  Adaptation in Open-Vocabulary Semantic Segmentation",
      "summary": "We introduce VocAlign, a novel source-free domain adaptation framework\nspecifically designed for VLMs in open-vocabulary semantic segmentation. Our\nmethod adopts a student-teacher paradigm enhanced with a vocabulary alignment\nstrategy, which improves pseudo-label generation by incorporating additional\nclass concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to\nfine-tune the model, preserving its original capabilities while minimizing\ncomputational overhead. In addition, we propose a Top-K class selection\nmechanism for the student model, which significantly reduces memory\nrequirements while further improving adaptation performance. Our approach\nachieves a notable 6.11 mIoU improvement on the CityScapes dataset and\ndemonstrates superior performance on zero-shot segmentation benchmarks, setting\na new standard for source-free adaptation in the open-vocabulary setting.",
      "url": "http://arxiv.org/abs/2509.15225v1",
      "published_time_eastern_timestamp": 1758218398.0
    },
    {
      "title": "Calibration-Aware Prompt Learning for Medical Vision-Language Models",
      "summary": "Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable\nperformance across diverse medical imaging tasks by leveraging large-scale\nimage-text pretraining. However, their confidence calibration is largely\nunexplored, and so remains a significant challenge. As such, miscalibrated\npredictions can lead to overconfident errors, undermining clinical trust and\ndecision-making reliability. To address this, we introduce CalibPrompt, the\nfirst framework to calibrate Med-VLMs during prompt tuning. CalibPrompt\noptimizes a small set of learnable prompts with carefully designed calibration\nobjectives under scarce labeled data regime. First, we study a regularizer that\nattempts to align the smoothed accuracy with the predicted model confidences.\nSecond, we introduce an angular separation loss to maximize textual feature\nproximity toward improving the reliability in confidence estimates of\nmultimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs\nand five diverse medical imaging datasets reveal that CalibPrompt consistently\nimproves calibration without drastically affecting clean accuracy. Our code is\navailable at https://github.com/iabh1shekbasu/CalibPrompt.",
      "url": "http://arxiv.org/abs/2509.15226v1",
      "published_time_eastern_timestamp": 1758218398.0
    },
    {
      "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based\n  Monocular Depth Estimation",
      "summary": "Event cameras capture sparse, high-temporal-resolution visual information,\nmaking them particularly suitable for challenging environments with high-speed\nmotion and strongly varying lighting conditions. However, the lack of large\ndatasets with dense ground-truth depth annotations hinders learning-based\nmonocular depth estimation from event data. To address this limitation, we\npropose a cross-modal distillation paradigm to generate dense proxy labels\nleveraging a Vision Foundation Model (VFM). Our strategy requires an event\nstream spatially aligned with RGB frames, a simple setup even available\noff-the-shelf, and exploits the robustness of large-scale VFMs. Additionally,\nwe propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2),\nor deriving from it a novel recurrent architecture to infer depth from\nmonocular event cameras. We evaluate our approach with synthetic and real-world\ndatasets, demonstrating that i) our cross-modal paradigm achieves competitive\nperformance compared to fully supervised methods without requiring expensive\ndepth annotations, and ii) our VFM-based models achieve state-of-the-art\nperformance.",
      "url": "http://arxiv.org/abs/2509.15224v1",
      "published_time_eastern_timestamp": 1758218391.0
    },
    {
      "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform\n  Data",
      "summary": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that\noperate GUIs autonomously, showing great potential, yet progress is limited by\nthe lack of large-scale, open-source computer use data and foundation models.\nIn this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It\noffers a large-scale dataset spanning 6 operating systems and 3 task domains,\nbuilt via a closed-loop pipeline uniting automated agents with human experts.\nTrained on this scaled-up data, ScaleCUA can operate seamlessly across\nplatforms. Specifically, it delivers strong gains over baselines (+26.6 on\nWebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art\nresults (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on\nWebArena-Lite-v2). These findings underscore the power of data-driven scaling\nfor general-purpose computer use agents. We will release data, models, and code\nto advance future research: https://github.com/OpenGVLab/ScaleCUA.",
      "url": "http://arxiv.org/abs/2509.15221v1",
      "published_time_eastern_timestamp": 1758218362.0
    },
    {
      "title": "Lightweight and Accurate Multi-View Stereo with Confidence-Aware\n  Diffusion Model",
      "summary": "To reconstruct the 3D geometry from calibrated images, learning-based\nmulti-view stereo (MVS) methods typically perform multi-view depth estimation\nand then fuse depth maps into a mesh or point cloud. To improve the\ncomputational efficiency, many methods initialize a coarse depth map and then\ngradually refine it in higher resolutions. Recently, diffusion models achieve\ngreat success in generation tasks. Starting from a random noise, diffusion\nmodels gradually recover the sample with an iterative denoising process. In\nthis paper, we propose a novel MVS framework, which introduces diffusion models\nin MVS. Specifically, we formulate depth refinement as a conditional diffusion\nprocess. Considering the discriminative characteristic of depth estimation, we\ndesign a condition encoder to guide the diffusion process. To improve\nefficiency, we propose a novel diffusion network combining lightweight 2D U-Net\nand convolutional GRU. Moreover, we propose a novel confidence-based sampling\nstrategy to adaptively sample depth hypotheses based on the confidence\nestimated by diffusion model. Based on our novel MVS framework, we propose two\nnovel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive\nperformance with state-of-the-art efficiency in run-time and GPU memory.\nCasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and\nETH3D. Code is available at: https://github.com/cvg/diffmvs.",
      "url": "http://arxiv.org/abs/2509.15220v1",
      "published_time_eastern_timestamp": 1758218359.0
    },
    {
      "title": "LNE-Blocking: An Efficient Framework for Contamination Mitigation\n  Evaluation on Large Language Models",
      "summary": "The problem of data contamination is now almost inevitable during the\ndevelopment of large language models (LLMs), with the training data commonly\nintegrating those evaluation benchmarks even unintentionally. This problem\nsubsequently makes it hard to benchmark LLMs fairly. Instead of constructing\ncontamination-free datasets (quite hard), we propose a novel framework,\n\\textbf{LNE-Blocking}, to restore model performance prior to contamination on\npotentially leaked datasets. Our framework consists of two components:\ncontamination detection and disruption operation. For the prompt, the framework\nfirst uses the contamination detection method, \\textbf{LNE}, to assess the\nextent of contamination in the model. Based on this, it adjusts the intensity\nof the disruption operation, \\textbf{Blocking}, to elicit non-memorized\nresponses from the model. Our framework is the first to efficiently restore the\nmodel's greedy decoding performance. This comes with a strong performance on\nmultiple datasets with potential leakage risks, and it consistently achieves\nstable recovery results across different models and varying levels of data\ncontamination. We release the code at https://github.com/RuijieH/LNE-Blocking\nto facilitate research.",
      "url": "http://arxiv.org/abs/2509.15218v1",
      "published_time_eastern_timestamp": 1758218356.0
    },
    {
      "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction",
      "summary": "Trajectory prediction is a critical task in computer vision and autonomous\nsystems, playing a key role in autonomous driving, robotics, surveillance, and\nvirtual reality. Existing methods often rely on complete and noise-free\nobservational data, overlooking the challenges associated with out-of-sight\nobjects and the inherent noise in sensor data caused by limited camera\ncoverage, obstructions, and the absence of ground truth for denoised\ntrajectories. These limitations pose safety risks and hinder reliable\nprediction in real-world scenarios. In this extended work, we present\nadvancements in Out-of-Sight Trajectory (OST), a novel task that predicts the\nnoise-free visual trajectories of out-of-sight objects using noisy sensor data.\nBuilding on our previous research, we broaden the scope of Out-of-Sight\nTrajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending\nits applicability to autonomous driving, robotics, surveillance, and virtual\nreality. Our enhanced Vision-Positioning Denoising Module leverages camera\ncalibration to establish a vision-positioning mapping, addressing the lack of\nvisual references, while effectively denoising noisy sensor data in an\nunsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB\ndatasets, our approach achieves state-of-the-art performance in both trajectory\ndenoising and prediction, significantly surpassing previous baselines.\nAdditionally, we introduce comparisons with traditional denoising methods, such\nas Kalman filtering, and adapt recent trajectory prediction models to our task,\nproviding a comprehensive benchmark. This work represents the first initiative\nto integrate vision-positioning projection for denoising noisy sensor\ntrajectories of out-of-sight agents, paving the way for future advances. The\ncode and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST",
      "url": "http://arxiv.org/abs/2509.15219v1",
      "published_time_eastern_timestamp": 1758218356.0
    },
    {
      "title": "Geometric Image Synchronization with Deep Watermarking",
      "summary": "Synchronization is the task of estimating and inverting geometric\ntransformations (e.g., crop, rotation) applied to an image. This work\nintroduces SyncSeal, a bespoke watermarking method for robust image\nsynchronization, which can be applied on top of existing watermarking methods\nto enhance their robustness against geometric transformations. It relies on an\nembedder network that imperceptibly alters images and an extractor network that\npredicts the geometric transformation to which the image was subjected. Both\nnetworks are end-to-end trained to minimize the error between the predicted and\nground-truth parameters of the transformation, combined with a discriminator to\nmaintain high perceptual quality. We experimentally validate our method on a\nwide variety of geometric and valuemetric transformations, demonstrating its\neffectiveness in accurately synchronizing images. We further show that our\nsynchronization can effectively upgrade existing watermarking methods to\nwithstand geometric transformations to which they were previously vulnerable.",
      "url": "http://arxiv.org/abs/2509.15208v1",
      "published_time_eastern_timestamp": 1758218214.0
    },
    {
      "title": "FlowRL: Matching Reward Distributions for LLM Reasoning",
      "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.",
      "url": "http://arxiv.org/abs/2509.15207v1",
      "published_time_eastern_timestamp": 1758218196.0
    },
    {
      "title": "Circuit-based chatacterization of finite-temperature quantum phases and\n  self-correcting quantum memory",
      "summary": "Quantum phases at zero temperature can be characterized as equivalence\nclasses under local unitary transformations: two ground states within a gapped\nphase can be transformed into each other via a local unitary circuit. We\ngeneralize this circuit-based characterization of phases to systems at\nfinite-temperature thermal equilibrium described by Gibbs states. We construct\na channel circuit that approximately transforms one Gibbs state into another\nprovided the two are connected by a path in parameter space along which a\ncertain correlation-decay condition holds. For finite-dimensional systems of\nlinear size $L$ and approximation error $\\epsilon$, the locality of the circuit\nis ${\\rm polylog}({\\rm poly}(L)/\\epsilon)$. The correlation-decay condition,\nwhich we specify, is expected to be satisfied in the interior of many\nnoncritical thermal phases, including those displaying discrete symmetry\nbreaking and topological order. As an application, we show that any system in\nthe same thermal phase as a zero-temperature topological code coherently\npreserves quantum information for a macroscopically long time, establishing\nself-correction as a universal property of thermal phases. As part of the\nproof, we provide explicit encoding and decoding channel circuits to encode\ninformation into, and decode it from, a system in thermal equilibrium.",
      "url": "http://arxiv.org/abs/2509.15204v1",
      "published_time_eastern_timestamp": 1758218115.0
    },
    {
      "title": "Strong converse exponent of channel interconversion",
      "summary": "In their seminal work, Bennett et al. [IEEE Trans. Inf. Theory (2002)] showed\nthat, with sufficient shared randomness, one noisy channel can simulate another\nat a rate equal to the ratio of their capacities. We establish that when coding\nabove this channel interconversion capacity, the exact strong converse exponent\nis characterized by a simple optimization involving the difference of the\ncorresponding R\\'enyi channel capacities with H\\\"older dual parameters. We\nfurther extend this result to the entanglement-assisted interconversion of\nclassical-quantum channels, showing that the strong converse exponent is\nlikewise determined by differences of sandwiched R\\'enyi channel capacities.\nThe converse bound is obtained by relaxing to non-signaling assisted codes and\napplying H\\\"older duality together with the data processing inequality for\nR\\'enyi divergences. Achievability is proven by concatenating refined channel\ncoding and simulation protocols that go beyond first-order capacities,\nattaining an exponentially small conversion error, remaining robust under small\nvariations in the input distribution, and tolerating a sublinear gap between\nthe conversion rates.",
      "url": "http://arxiv.org/abs/2509.15200v1",
      "published_time_eastern_timestamp": 1758218041.0
    },
    {
      "title": "Orion: Fuzzing Workflow Automation",
      "summary": "Fuzz testing is one of the most effective techniques for finding software\nvulnerabilities. While modern fuzzers can generate inputs and monitor\nexecutions automatically, the overall workflow, from analyzing a codebase, to\nconfiguring harnesses, to triaging results, still requires substantial manual\neffort. Prior attempts focused on single stages such as harness synthesis or\ninput minimization, leaving researchers to manually connect the pieces into a\ncomplete fuzzing campaign.\n  We introduce Orion, a framework that automates the the manual bottlenecks of\nfuzzing by integrating LLM reasoning with traditional tools, allowing campaigns\nto scale to settings where human effort alone was impractical. Orion uses LLMs\nfor code reasoning and semantic guidance, while relying on deterministic tools\nfor verification, iterative refinement, and tasks that require precision.\nAcross our benchmark suite, Orion reduces human effort by 46-204x depending on\nthe workflow stage, and we demonstrate its effectiveness through the discovery\nof two previously unknown vulnerabilities in the widely used open-source clib\nlibrary.",
      "url": "http://arxiv.org/abs/2509.15195v1",
      "published_time_eastern_timestamp": 1758217926.0
    },
    {
      "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot\n  Spatio-Temporal Video Grounding",
      "summary": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal\ntube of a video, as specified by the input text query. In this paper, we\nutilize multimodal large language models (MLLMs) to explore a zero-shot\nsolution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to\ndynamically assign special tokens, referred to as \\textit{grounding tokens},\nfor grounding the text query; and (2) MLLMs often suffer from suboptimal\ngrounding due to the inability to fully integrate the cues in the text query\n(\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we\npropose a MLLM-based zero-shot framework for STVG, which includes novel\ndecomposed spatio-temporal highlighting (DSTH) and temporal-augmented\nassembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH\nstrategy first decouples the original query into attribute and action\nsub-queries for inquiring the existence of the target both spatially and\ntemporally. It then uses a novel logit-guided re-attention (LRA) module to\nlearn latent variables as spatial and temporal prompts, by regularizing token\npredictions for each sub-query. These prompts highlight attribute and action\ncues, respectively, directing the model's attention to reliable spatial and\ntemporal related visual regions. In addition, as the spatial grounding by the\nattribute sub-query should be temporally consistent, we introduce the TAS\nstrategy to assemble the predictions using the original video frames and the\ntemporal-augmented frames as inputs to help improve temporal consistency. We\nevaluate our method on various MLLMs, and show that it outperforms SOTA methods\non three common STVG benchmarks.\n  The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.",
      "url": "http://arxiv.org/abs/2509.15178v1",
      "published_time_eastern_timestamp": 1758216950.0
    },
    {
      "title": "A Race Bias Free Face Aging Model for Reliable Kinship Verification",
      "summary": "The age gap in kinship verification addresses the time difference between the\nphotos of the parent and the child. Moreover, their same-age photos are often\nunavailable, and face aging models are racially biased, which impacts the\nlikeness of photos. Therefore, we propose a face aging GAN model, RA-GAN,\nconsisting of two new modules, RACEpSp and a feature mixer, to produce racially\nunbiased images. The unbiased synthesized photos are used in kinship\nverification to investigate the results of verifying same-age parent-child\nimages. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an\naverage of 13.14\\% across all age groups, and CUSP-GAN in the 60+ age group by\n9.1\\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects'\nidentities better than SAM-GAN and CUSP-GAN across all age groups.\nAdditionally, we demonstrate that transforming parent and child images from the\nKinFaceW-I and KinFaceW-II datasets to the same age can enhance the\nverification accuracy across all age groups. The accuracy increases with our\nRA-GAN for the kinship relationships of father-son and father-daughter,\nmother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41,\nrespectively, on KinFaceW-I. Additionally, the accuracy for the relationships\nof father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on\nKinFaceW-II, respectively. The code is available\nat~\\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}",
      "url": "http://arxiv.org/abs/2509.15177v1",
      "published_time_eastern_timestamp": 1758216860.0
    },
    {
      "title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images\n  Pretrained Model",
      "summary": "This paper explores the transfer of knowledge from general vision models\npretrained on 2D natural images to improve 3D medical image segmentation. We\nfocus on the semi-supervised setting, where only a few labeled 3D medical\nimages are available, along with a large set of unlabeled images. To tackle\nthis, we propose a model-agnostic framework that progressively distills\nknowledge from a 2D pretrained model to a 3D segmentation model trained from\nscratch. Our approach, M&N, involves iterative co-training of the two models\nusing pseudo-masks generated by each other, along with our proposed learning\nrate guided sampling that adaptively adjusts the proportion of labeled and\nunlabeled data in each training batch to align with the models' prediction\naccuracy and stability, minimizing the adverse effect caused by inaccurate\npseudo-masks. Extensive experiments on multiple publicly available datasets\ndemonstrate that M&N achieves state-of-the-art performance, outperforming\nthirteen existing semi-supervised segmentation approaches under all different\nsettings. Importantly, ablation studies show that M&N remains model-agnostic,\nallowing seamless integration with different architectures. This ensures its\nadaptability as more advanced models emerge. The code is available at\nhttps://github.com/pakheiyeung/M-N.",
      "url": "http://arxiv.org/abs/2509.15167v1",
      "published_time_eastern_timestamp": 1758215872.0
    },
    {
      "title": "Locally recoverable codes with multiple recovering sets from maximal\n  curves",
      "summary": "In this paper, we present a construction of locally recoverable codes (LRCs)\nwith multiple recovery sets using algebraic curves with many rational points.\nBy leveraging separable morphisms between smooth projective curves and\nexpanding the class of curves previously considered, we significantly\ngeneralize and enhance the framework. Our approach corrects certain\ninaccuracies in the existing literature while extending results to a broader\nrange of curves, thereby achieving better parameters and wider applicability.\nIn addition, the constructions presented here result in LRCs with large\navailability.",
      "url": "http://arxiv.org/abs/2509.15163v1",
      "published_time_eastern_timestamp": 1758215610.0
    },
    {
      "title": "Mind the Gap: Data Rewriting for Stable Off-Policy Supervised\n  Fine-Tuning",
      "summary": "Supervised fine-tuning (SFT) of large language models can be viewed as an\noff-policy learning problem, where expert demonstrations come from a fixed\nbehavior policy while training aims to optimize a target policy. Importance\nsampling is the standard tool for correcting this distribution mismatch, but\nlarge policy gaps lead to high variance and training instability. Existing\napproaches mitigate this issue using KL penalties or clipping, which passively\nconstrain updates rather than actively reducing the gap. We propose a simple\nyet effective data rewriting framework that proactively shrinks the policy gap\nby keeping correct solutions as on-policy data and rewriting incorrect ones\nwith guided re-solving, falling back to expert demonstrations only when needed.\nThis aligns the training distribution with the target policy before\noptimization, reducing importance sampling variance and stabilizing off-policy\nfine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate\nconsistent and significant gains over both vanilla SFT and the state-of-the-art\nDynamic Fine-Tuning (DFT) approach. The data and code will be released at\nhttps://github.com/NKU-HLT/Off-Policy-SFT.",
      "url": "http://arxiv.org/abs/2509.15157v1",
      "published_time_eastern_timestamp": 1758214950.0
    },
    {
      "title": "MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label\n  Augmentation",
      "summary": "Ensuring factual consistency and reliable reasoning remains a critical\nchallenge for medical vision-language models. We introduce MEDFACT-R1, a\ntwo-stage framework that integrates external knowledge grounding with\nreinforcement learning to improve the factual medical reasoning. The first\nstage uses pseudo-label supervised fine-tuning (SFT) to incorporate external\nfactual expertise; while the second stage applies Group Relative Policy\nOptimization (GRPO) with four tailored factual reward signals to encourage\nself-consistent reasoning. Across three public medical QA benchmarks,\nMEDFACT-R1 delivers up to 22.5% absolute improvement in factual accuracy over\nprevious state-of-the-art methods. Ablation studies highlight the necessity of\npseudo-label SFT cold start and validate the contribution of each GRPO reward,\nunderscoring the synergy between knowledge grounding and RL-driven reasoning\nfor trustworthy medical AI. Codes are released at\nhttps://github.com/Garfieldgengliang/MEDFACT-R1.",
      "url": "http://arxiv.org/abs/2509.15154v1",
      "published_time_eastern_timestamp": 1758214799.0
    },
    {
      "title": "Code Less to Code More: Streamlining Language Server Protocol and Type\n  System Development for Language Families",
      "summary": "Developing editing support for $L$ languages in $E$ editors is complex and\ntime-consuming. Some languages do not provide dedicated editors, while others\noffer a single native editor. The $\\textit{language server protocol}$ (LSP)\nreduces the language-editor combinations $L \\times E$ to $L + E$, where a\nsingle language server communicates with editors via LSP plugins. However,\noverlapping implementations of linguistic components remain an issue. Existing\nlanguage workbenches struggle with modularity, reusability, and leveraging type\nsystems for language server generation. In this work, we propose: (i) Typelang,\na family of domain-specific languages for modular, composable, and reusable\ntype system implementation, (ii) a modular language server generation process,\nproducing servers for languages built in a modular workbench, (iii) the\nvariant-oriented programming paradigm and a cross-artifact coordination layer\nto manage interdependent software variants, and (iv) an LSP plugin generator,\nreducing $E$ to $1$ by automating plugin creation for multiple editors. To\nsimplify editing support for language families, each language artifact\nintegrates its own Typelang variant, used to generate language servers. This\nreduces combinations to $T \\times 1$, where $T = L$ represents the number of\ntype systems. Further reuse of language artifacts across languages lowers this\nto $N \\times 1$, where $N << T$, representing unique type systems. We implement\nTypelang in Neverlang, generating language servers for each artifact and LSP\nplugins for three editors. Empirical evaluation shows a 93.48% reduction in\ncharacters needed for type system implementation and 100% automation of LSP\nplugin generation, significantly lowering effort for editing support in\nlanguage families, especially when artifacts are reused.",
      "url": "http://arxiv.org/abs/2509.15150v1",
      "published_time_eastern_timestamp": 1758214621.0
    },
    {
      "title": "A1: Asynchronous Test-Time Scaling via Conformal Prediction",
      "summary": "Large language models (LLMs) benefit from test-time scaling, but existing\nmethods face significant challenges, including severe synchronization overhead,\nmemory bottlenecks, and latency, especially during speculative decoding with\nlong reasoning chains. We introduce A1 (Asynchronous Test-Time Scaling), a\nstatistically guaranteed adaptive inference framework that addresses these\nchallenges. A1 refines arithmetic intensity to identify synchronization as the\ndominant bottleneck, proposes an online calibration strategy to enable\nasynchronous inference, and designs a three-stage rejection sampling pipeline\nthat supports both sequential and parallel scaling. Through experiments on the\nMATH, AMC23, AIME24, and AIME25 datasets, across various draft-target model\nfamilies, we demonstrate that A1 achieves a remarkable 56.7x speedup in\ntest-time scaling and a 4.14x improvement in throughput, all while maintaining\naccurate rejection-rate control, reducing latency and memory overhead, and no\naccuracy loss compared to using target model scaling alone. These results\nposition A1 as an efficient and principled solution for scalable LLM inference.\nWe have released the code at\nhttps://github.com/menik1126/asynchronous-test-time-scaling.",
      "url": "http://arxiv.org/abs/2509.15148v1",
      "published_time_eastern_timestamp": 1758214509.0
    }
  ]
}
{
  "last_updated": "2025-10-16T17:10:33.203571-04:00",
  "papers": [
    {
      "title": "BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis\n  for Fast and Accurate Multi-Hop Reasoning",
      "summary": "As retrieval-augmented generation (RAG) tackles complex tasks, increasingly\nexpanded contexts offer richer information, but at the cost of higher latency\nand increased cognitive load on the model. To mitigate this bottleneck,\nespecially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a\nuniversal, lightweight compressor that distills relevant evidence for a given\nquery from retrieved documents into a concise summary for seamless integration\ninto in-context RAG. Using seed data consisting of relatively short contexts\n(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression\nof extended contexts exceeding 10k words across a wide range of scenarios.\nFurthermore, BRIEF-Pro offers flexible user control over summary length by\nallowing users to specify the desired number of sentences. Experiments on four\nopen-domain multi-hop question-answering datasets show that BRIEF-Pro generates\nmore concise and relevant summaries, enhancing performance across small, large,\nand proprietary language models. With the 70B reader model, 32x compression by\nBRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,\nwhile requiring only 23% of its computational overhead.",
      "url": "http://arxiv.org/abs/2510.13799v1",
      "published_time_eastern_timestamp": 1760551065.0
    },
    {
      "title": "NoisePrints: Distortion-Free Watermarks for Authorship in Private\n  Diffusion Models",
      "summary": "With the rapid adoption of diffusion models for visual content generation,\nproving authorship and protecting copyright have become critical. This\nchallenge is particularly important when model owners keep their models private\nand may be unwilling or unable to handle authorship issues, making third-party\nverification essential. A natural solution is to embed watermarks for later\nverification. However, existing methods require access to model weights and\nrely on computationally heavy procedures, rendering them impractical and\nnon-scalable. To address these challenges, we propose , a lightweight\nwatermarking scheme that utilizes the random seed used to initialize the\ndiffusion process as a proof of authorship without modifying the generation\nprocess. Our key observation is that the initial noise derived from a seed is\nhighly correlated with the generated visual content. By incorporating a hash\nfunction into the noise sampling process, we further ensure that recovering a\nvalid seed from the content is infeasible. We also show that sampling an\nalternative seed that passes verification is infeasible, and demonstrate the\nrobustness of our method under various manipulations. Finally, we show how to\nuse cryptographic zero-knowledge proofs to prove ownership without revealing\nthe seed. By keeping the seed secret, we increase the difficulty of watermark\nremoval. In our experiments, we validate NoisePrints on multiple\nstate-of-the-art diffusion models for images and videos, demonstrating\nefficient verification using only the seed and output, without requiring access\nto model weights.",
      "url": "http://arxiv.org/abs/2510.13793v1",
      "published_time_eastern_timestamp": 1760550645.0
    },
    {
      "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for\n  Generalist Robot Policy",
      "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
      "url": "http://arxiv.org/abs/2510.13778v1",
      "published_time_eastern_timestamp": 1760549405.0
    },
    {
      "title": "From Random to Explicit via Subspace Designs With Applications to Local\n  Properties and Matroids",
      "summary": "In coding theory, a common question is to understand the threshold rates of\nvarious local properties of codes, such as their list decodability and list\nrecoverability. A recent work Levi, Mosheiff, and Shagrithaya (FOCS 2025) gave\na novel unified framework for calculating the threshold rates of local\nproperties for random linear and random Reed--Solomon codes.\n  In this paper, we extend their framework to studying the local properties of\nsubspace designable codes, including explicit folded Reed-Solomon and\nunivariate multiplicity codes. Our first main result is a local equivalence\nbetween random linear codes and (nearly) optimal subspace design codes up to an\narbitrarily small rate decrease. We show any local property of random linear\ncodes applies to all subspace design codes. As such, we give the first explicit\nconstruction of folded linear codes that simultaneously attain all local\nproperties of random linear codes. Conversely, we show that any local property\nwhich applies to all subspace design codes also applies to random linear codes.\n  Our second main result is an application to matroid theory. We show that the\ncorrectable erasure patterns in a maximally recoverable tensor code can be\nidentified in deterministic polynomial time, assuming a positive answer to a\nmatroid-theoretic question due to Mason (1981). This improves on a result of\nJackson and Tanigawa (JCTB 2024) who gave a complexity characterization of\n$\\mathsf{RP} \\cap \\mathsf{coNP}$ assuming a stronger conjecture. Our result\nalso applies to the generic bipartite rigidity and matrix completion matroids.\n  As a result of additional interest, we study the existence and limitations of\nsubspace designs. In particular, we tighten the analysis of family of subspace\ndesigns constructioned by Guruswami and Kopparty (Combinatorica 2016) and show\nthat better subspace designs do not exist over algebraically closed fields.",
      "url": "http://arxiv.org/abs/2510.13777v1",
      "published_time_eastern_timestamp": 1760549299.0
    },
    {
      "title": "Polarization dynamics of X-ray synchrotron emission from a multi-zone\n  blazar jet",
      "summary": "The polarization of X-ray synchrotron emission in blazars offers a direct\nprobe into the magnetic field geometry and particle acceleration processes\noperating in relativistic jets. We use particle-in-cell simulations of magnetic\nreconnection and magnetized turbulence, coupled to polarization-sensitive\nradiative transfer code, to interpret IXPE observations of Mrk 421 during a\nhigh flux state recorded in December of 2023. To evaluate the fitness of the\ntheoretical scenarios, we rely on a quantitative comparison the statistical\nproperties of simulated and observed X-ray flux and polarization light curves\nusing five evaluation metrics, rather than attempting to fit individual data\npoints. We propose a multi-zone model where jet emission is represented as the\nsum of the radiative output of many independent cells, each described by a\nsimulation run viewed at different orientations. Comparison of ensembles of\nsimulated Stokes-parameter light curves with IXPE data shows that magnetic\nreconnection dominated models provide the best match to the observed X-ray flux\nand polarization dynamics. The optimal configuration corresponds to N = 15\nemitting cells, which reproduces the observed amplitudes and timescales of the\nX-ray flux and polarization variations. Magnetized turbulence models\nunderpredict both the flux and polarization variability. Our results indicate\nthat a multi-zone, reconnection-powered emission scenario can describe the\nX-ray polarization behavior of Mrk 421 and establish a quantitative framework\nfor testing theoretical models against IXPE observations of other\nhigh-synchrotron-peaked blazars.",
      "url": "http://arxiv.org/abs/2510.13776v1",
      "published_time_eastern_timestamp": 1760549239.0
    },
    {
      "title": "Combinatorial Bounds for List Recovery via Discrete Brascamp--Lieb\n  Inequalities",
      "summary": "In coding theory, the problem of list recovery asks one to find all codewords\n$c$ of a given code $C$ which such that at least $1-\\rho$ fraction of the\nsymbols of $c$ lie in some predetermined set of $\\ell$ symbols for each\ncoordinate of the code. A key question is bounding the maximum possible list\nsize $L$ of such codewords for the given code $C$.\n  In this paper, we give novel combinatorial bounds on the list recoverability\nof various families of linear and folded linear codes, including random linear\ncodes, random Reed--Solomon codes, explicit folded Reed--Solomon codes, and\nexplicit univariate multiplicity codes. Our main result is that in all of these\nsettings, we show that for code of rate $R$, when $\\rho = 1 - R - \\epsilon$\napproaches capacity, the list size $L$ is at most\n$(\\ell/(R+\\epsilon))^{O(R/\\epsilon)}$. These results also apply in the\naverage-radius regime. Our result resolves a long-standing open question on\nwhether $L$ can be bounded by a polynomial in $\\ell$. In the zero-error regime,\nour bound on $L$ perfectly matches known lower bounds.\n  The primary technique is a novel application of a discrete entropic\nBrascamp--Lieb inequality to the problem of list recovery, allowing us to\nrelate the local structure of each coordinate with the global structure of the\nrecovered list. As a result of independent interest, we show that a recent\nresult by Chen and Zhang (STOC 2025) on the list decodability of folded\nReed--Solomon codes can be generalized into a novel Brascamp--Lieb type\ninequality.",
      "url": "http://arxiv.org/abs/2510.13775v1",
      "published_time_eastern_timestamp": 1760549231.0
    },
    {
      "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of\n  Robust Spatial Representations",
      "summary": "Forecasting urban phenomena such as housing prices and public health\nindicators requires the effective integration of various geospatial data.\nCurrent methods primarily utilize task-specific models, while recent foundation\nmodels for spatial representations often support only limited modalities and\nlack multimodal fusion capabilities. To overcome these challenges, we present\nUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal\nFusion (SMF). The framework employs modality-specific encoders to process\ndifferent types of inputs, including street view imagery, remote sensing data,\ncartographic maps, and points of interest (POIs) data. These multimodal inputs\nare integrated via a Transformer-based fusion module that learns unified\nrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwide\ndemonstrates UrbanFusion's strong generalization and predictive performance\ncompared to state-of-the-art GeoAI models. Specifically, it 1) outperforms\nprior foundation models on location-encoding, 2) allows multimodal input during\ninference, and 3) generalizes well to regions unseen during training.\nUrbanFusion can flexibly utilize any subset of available modalities for a given\nlocation during both pretraining and inference, enabling broad applicability\nacross diverse data availability scenarios. All source code is available at\nhttps://github.com/DominikM198/UrbanFusion.",
      "url": "http://arxiv.org/abs/2510.13774v1",
      "published_time_eastern_timestamp": 1760549184.0
    },
    {
      "title": "Scaling Vision Transformers for Functional MRI with Flat Maps",
      "summary": "A key question for adapting modern deep learning architectures to functional\nMRI (fMRI) is how to represent the data for model input. To bridge the modality\ngap between fMRI and natural images, we transform the 4D volumetric fMRI data\ninto videos of 2D fMRI activity flat maps. We train Vision Transformers on 2.3K\nhours of fMRI flat map videos from the Human Connectome Project using the\nspatiotemporal masked autoencoder (MAE) framework. We observe that masked fMRI\nmodeling performance improves with dataset size according to a strict power\nscaling law. Downstream classification benchmarks show that our model learns\nrich representations supporting both fine-grained state decoding across\nsubjects, as well as subject-specific trait decoding across changes in brain\nstate. This work is part of an ongoing open science project to build foundation\nmodels for fMRI data. Our code and datasets are available at\nhttps://github.com/MedARC-AI/fmri-fm.",
      "url": "http://arxiv.org/abs/2510.13768v1",
      "published_time_eastern_timestamp": 1760548500.0
    },
    {
      "title": "Uni-MMMU: A Massive Multi-discipline Multimodal Unified Benchmark",
      "summary": "Unified multimodal models aim to jointly enable visual understanding and\ngeneration, yet current benchmarks rarely examine their true integration.\nExisting evaluations either treat the two abilities in isolation or overlook\ntasks that inherently couple them. To address this gap, we present Uni-MMMU, a\ncomprehensive and discipline-aware benchmark that systematically unfolds the\nbidirectional synergy between generation and understanding across eight\nreasoning-centric domains, including science, coding, mathematics, and puzzles.\nEach task is bidirectionally coupled, demanding models to (i) leverage\nconceptual understanding to guide precise visual synthesis, or (ii) utilize\ngeneration as a cognitive scaffold for analytical reasoning. Uni-MMMU\nincorporates verifiable intermediate reasoning steps, unique ground truths, and\na reproducible scoring protocol for both textual and visual outputs. Through\nextensive evaluation of state-of-the-art unified, generation-only, and\nunderstanding-only models, we reveal substantial performance disparities and\ncross-modal dependencies, offering new insights into when and how these\nabilities reinforce one another, and establishing a reliable foundation for\nadvancing unified models.",
      "url": "http://arxiv.org/abs/2510.13759v1",
      "published_time_eastern_timestamp": 1760548235.0
    },
    {
      "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
      "summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for\nstructured visuals like charts and diagrams, as pixel-based perception lacks a\nmechanism for verification. To address this, we propose to leverage derendering\n-- the process of reverse-engineering visuals into executable code -- as a new\nmodality for verifiable visual reasoning. Specifically, we propose RECODE, an\nagentic framework that first generates multiple candidate programs to reproduce\nthe input image. It then uses a critic to select the most faithful\nreconstruction and iteratively refines the code. This process not only\ntransforms an ambiguous perceptual task into a verifiable, symbolic problem,\nbut also enables precise calculations and logical inferences later on. On\nvarious visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,\nRECODE significantly outperforms methods that do not leverage code or only use\ncode for drawing auxiliary lines or cropping. Our work demonstrates that\ngrounding visual perception in executable code provides a new path toward more\naccurate and verifiable multimodal reasoning.",
      "url": "http://arxiv.org/abs/2510.13756v1",
      "published_time_eastern_timestamp": 1760547937.0
    },
    {
      "title": "UniCalli: A Unified Diffusion Framework for Column-Level Generation and\n  Recognition of Chinese Calligraphy",
      "summary": "Computational replication of Chinese calligraphy remains challenging.\nExisting methods falter, either creating high-quality isolated characters while\nignoring page-level aesthetics like ligatures and spacing, or attempting page\nsynthesis at the expense of calligraphic correctness. We introduce\n\\textbf{UniCalli}, a unified diffusion framework for column-level recognition\nand generation. Training both tasks jointly is deliberate: recognition\nconstrains the generator to preserve character structure, while generation\nprovides style and layout priors. This synergy fosters concept-level\nabstractions that improve both tasks, especially in limited-data regimes. We\ncurated a dataset of over 8,000 digitized pieces, with ~4,000 densely\nannotated. UniCalli employs asymmetric noising and a rasterized box map for\nspatial priors, trained on a mix of synthetic, labeled, and unlabeled data. The\nmodel achieves state-of-the-art generative quality with superior ligature\ncontinuity and layout fidelity, alongside stronger recognition. The framework\nsuccessfully extends to other ancient scripts, including Oracle bone\ninscriptions and Egyptian hieroglyphs. Code and data can be viewed in\n\\href{https://github.com/EnVision-Research/UniCalli}{this URL}.",
      "url": "http://arxiv.org/abs/2510.13745v1",
      "published_time_eastern_timestamp": 1760547127.0
    },
    {
      "title": "Dynamics and Observational Signatures of Core-Collapse Supernovae with\n  Central Engines: Hydrodynamics Simulations with Monte Carlo Post-Processing",
      "summary": "A long-lived central engine embedded in expanding supernova ejecta can alter\nthe dynamics and observational signatures of the event, producing an unusually\nluminous, energetic, and/or rapidly-evolving transient. We use two-dimensional\nhydrodynamics simulations to study the effect of a central energy source,\nvarying the amount, rate, and isotropy of the energy deposition. We\npost-process the results with a time-dependent Monte Carlo radiation transport\ncode to extract observational signatures. The engine excavates a bubble at the\ncentre of the ejecta, which becomes Rayleigh-Taylor unstable. Sufficiently\npowerful engines are able to break through the edge of the bubble and\naccelerate, shred, and compositionally mix the entire ejecta. The breakout of\nthe engine-driven wind occurs at distinct rupture points, and the outflowing\nhigh-velocity gas may eventually give rise to radio emission. The dynamical\nimpact of the engine leads to faster rising optical light curves, with photon\nescape facilitated by the faster expansion of the ejecta and the opening of\nlow-density channels. For models with strong engines, the spectra are initially\nhot and featureless, but later evolve to resemble those of broad-line Ic\nsupernovae. Under certain conditions, line emission from ionized, low-velocity\nmaterial near the centre of the ejecta may be able to escape and produce narrow\nemission similar to that seen in interacting supernovae. We discuss how\nvariability in the engine energy reservoir and injection rate could give rise\nto a heterogeneous set of events spanning multiple observational classes,\nincluding the fast blue optical transients, broad-line Ic supernovae, and\nsuperluminous supernovae.",
      "url": "http://arxiv.org/abs/2510.13741v1",
      "published_time_eastern_timestamp": 1760546860.0
    },
    {
      "title": "Multi-Scale High-Resolution Logarithmic Grapher Module for Efficient\n  Vision GNNs",
      "summary": "Vision graph neural networks (ViG) have demonstrated promise in vision tasks\nas a competitive alternative to conventional convolutional neural nets (CNN)\nand transformers (ViTs); however, common graph construction methods, such as\nk-nearest neighbor (KNN), can be expensive on larger images. While methods such\nas Sparse Vision Graph Attention (SVGA) have shown promise, SVGA's fixed step\nscale can lead to over-squashing and missing multiple connections to gain the\nsame information that could be gained from a long-range link. Through this\nobservation, we propose a new graph construction method, Logarithmic Scalable\nGraph Construction (LSGC) to enhance performance by limiting the number of\nlong-range links. To this end, we propose LogViG, a novel hybrid CNN-GNN model\nthat utilizes LSGC. Furthermore, inspired by the successes of multi-scale and\nhigh-resolution architectures, we introduce and apply a high-resolution branch\nand fuse features between our high-resolution and low-resolution branches for a\nmulti-scale high-resolution Vision GNN network. Extensive experiments show that\nLogViG beats existing ViG, CNN, and ViT architectures in terms of accuracy,\nGMACs, and parameters on image classification and semantic segmentation tasks.\nOur smallest model, Ti-LogViG, achieves an average top-1 accuracy on\nImageNet-1K of 79.9% with a standard deviation of 0.2%, 1.7% higher average\naccuracy than Vision GNN with a 24.3% reduction in parameters and 35.3%\nreduction in GMACs. Our work shows that leveraging long-range links in graph\nconstruction for ViGs through our proposed LSGC can exceed the performance of\ncurrent state-of-the-art ViGs. Code is available at\nhttps://github.com/mmunir127/LogViG-Official.",
      "url": "http://arxiv.org/abs/2510.13740v1",
      "published_time_eastern_timestamp": 1760546829.0
    },
    {
      "title": "Comparing subgrid models for cosmic ray diffusion in a magnetized\n  isolated galaxy simulation",
      "summary": "Galactic cosmic rays (CRs) play a crucial role in galaxy formation and\nevolution by altering gas dynamics and chemistry across multiple scales.\nTypical numerical simulations of CR transport assume a constant diffusion\ncoefficient for the entire galaxy, despite both numerical and theoretical\nstudies showing that it can change by orders of magnitude depending on the\nphase of the interstellar medium. Only a few simulations exist that\nself-consistently calculate CR transport with diffusion, streaming, and\nadvection by the background gas. In this study we explore three subgrid models\nfor CR diffusion, based on popular theories of CR transport. We post-process an\nisolated, star-forming MHD galactic disk simulated using the RAMSES code. The\nresulting diffusion coefficients depend solely on the subgrid turbulent kinetic\nenergy and the MHD state variables of the plasma. We use these models to\ncalculate coefficients for vertical transport. We find that they depend\ncritically on the local magnetic field tilt angle. Across models, our resulting\ndiffusion coefficients range from $10^{26}~\\rm cm^2s^{-1}$ to $10^{31}~\\rm\ncm^2s^{-1}$, and yield CR energy densities at the midplane from $1$ to $100\n~\\rm eV cm^{-3}$, suggesting varied degrees of backreaction on their\nenvironment. Using simple approximations, we show that the gamma ray luminosity\nof the galaxy depends primarily on the gas surface density and the turbulent\nconfinement of CRs by the galactic corona.",
      "url": "http://arxiv.org/abs/2510.13737v1",
      "published_time_eastern_timestamp": 1760546691.0
    },
    {
      "title": "The collision and merger products of stars do not look alike: A\n  magnetohydrodynamics comparison",
      "summary": "A significant fraction of stars experience close interactions, including\ncollisions resulting from gravitational encounters and mergers within close\nbinary systems. These processes can produce more massive stars that may give\nrise to relatively rare objects such as blue stragglers. Distinguishing the\noutcomes of collisions and mergers is challenging yet essential for\ninterpreting observations. This study utilizes the magnetohydrodynamics code\nAREPO to simulate collisions and mergers of $5$ to $10 \\,\\mathrm{M}_{\\odot}$\nmain-sequence stars, systematically comparing the properties of the resulting\nproducts. Both collisions and mergers yield more massive, strongly magnetized,\nrapidly and differentially rotating stars with cores enriched in hydrogen, but\nnotable quantitative differences emerge. Merger products exhibit core hydrogen\nfractions up to $10\\%$ higher than those of collision products. In both\nscenarios, turbulent mixing amplifies magnetic field energies by $9$ to $12$\norders of magnitude. However, magnetic fields in small-impact-parameter\ncollision products display small-scale reversals that may dissipate over time,\nwhereas merger products and large-impact-parameter collision products develop\nlarge-scale ordered, potentially long-lived magnetic fields. Additionally, only\nmerger products display magnetically driven, bipolar outflows with radial\nvelocities exceeding $300$ to $400 \\,\\mathrm{km}\\,\\mathrm{s}^{-1}$. These\ndistinctions may result in divergent long-term evolutionary outcomes, which\nwarrant further investigation in future studies.",
      "url": "http://arxiv.org/abs/2510.13736v1",
      "published_time_eastern_timestamp": 1760546616.0
    },
    {
      "title": "Assessing the Geographic Generalization and Physical Consistency of\n  Generative Models for Climate Downscaling",
      "summary": "Kilometer-scale weather data is crucial for real-world applications but\nremains computationally intensive to produce using traditional weather\nsimulations. An emerging solution is to use deep learning models, which offer a\nfaster alternative for climate downscaling. However, their reliability is still\nin question, as they are often evaluated using standard machine learning\nmetrics rather than insights from atmospheric and weather physics. This paper\nbenchmarks recent state-of-the-art deep learning models and introduces\nphysics-inspired diagnostics to evaluate their performance and reliability,\nwith a particular focus on geographic generalization and physical consistency.\nOur experiments show that, despite the seemingly strong performance of models\nsuch as CorrDiff, when trained on a limited set of European geographies (e.g.,\ncentral Europe), they struggle to generalize to other regions such as Iberia,\nMorocco in the south, or Scandinavia in the north. They also fail to accurately\ncapture second-order variables such as divergence and vorticity derived from\npredicted velocity fields. These deficiencies appear even in in-distribution\ngeographies, indicating challenges in producing physically consistent\npredictions. We propose a simple initial solution: introducing a power spectral\ndensity loss function that empirically improves geographic generalization by\nencouraging the reconstruction of small-scale physical structures. The code for\nreproducing the experimental results can be found at\nhttps://github.com/CarloSaccardi/PSD-Downscaling",
      "url": "http://arxiv.org/abs/2510.13722v1",
      "published_time_eastern_timestamp": 1760545589.0
    },
    {
      "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete\n  Flow Matching",
      "summary": "Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal\nretrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal\nfoundation model that achieves unified modeling through discrete flow\nparadigms. By leveraging metric-induced probability paths and kinetic optimal\nvelocities, NExT-OMNI natively supports any-to-any understanding and generation\nwith enhanced response efficiency, while enabling broader application scenarios\nthrough concise unified representations rather than task-decoupled designs.\nTrained on large-scale interleaved text, image, video, and audio data,\nNExT-OMNI delivers competitive performance on multimodal generation and\nunderstanding benchmarks, while outperforming prior unified models in\nmulti-turn multimodal interaction and cross-modal retrieval, highlighting its\narchitectural advantages as a next-generation multimodal foundation model. To\nadvance further research, we release training details, data protocols, and\nopen-source both the code and model checkpoints.",
      "url": "http://arxiv.org/abs/2510.13721v1",
      "published_time_eastern_timestamp": 1760545518.0
    },
    {
      "title": "Training LLM Agents to Empower Humans",
      "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.",
      "url": "http://arxiv.org/abs/2510.13709v1",
      "published_time_eastern_timestamp": 1760544573.0
    },
    {
      "title": "VC-Dimension vs Degree: An Uncertainty Principle for Boolean Functions",
      "summary": "In this paper, we uncover a new uncertainty principle that governs the\ncomplexity of Boolean functions. This principle manifests as a fundamental\ntrade-off between two central measures of complexity: a combinatorial\ncomplexity of its supported set, captured by its Vapnik-Chervonenkis dimension\n($\\mathrm{VC}(f)$), and its algebraic structure, captured by its polynomial\ndegree over various fields. We establish two primary inequalities that\nformalize this trade-off:$\\mathrm{VC}(f)+\\deg(f)\\ge n,$ and\n$\\mathrm{VC}(f)+\\deg_{\\mathbb{F}_2}(f)\\ge n$. In particular, these results\nrecover the classical uncertainty principle on the discrete hypercube, as well\nas the Sziklai--Weiner's bound in the case of $\\mathbb{F}_2$.",
      "url": "http://arxiv.org/abs/2510.13705v1",
      "published_time_eastern_timestamp": 1760544144.0
    },
    {
      "title": "On Pretraining for Project-Level Code Completion",
      "summary": "Repository-level pretraining is commonly used to enable large language models\nfor code to leverage codebase-wide context. This enhances their ability to\ngenerate accurate and context-aware code completions. In this work, we\ninvestigate how different repository-processing strategies affect in-context\nlearning in OpenCoder, a 1.5B-parameter model. We extend its context window\nfrom 4,096 to 16,384 tokens by training on additional 1B tokens of curated\nrepository-level data. Despite relying on a smaller dataset than competing\nmodels (which often use hundreds of billions of tokens), our model achieves\ncomparable performance on the Long Code Arena benchmark. We find that various\nrepository-processing techniques yield similarly strong results, with the\nprimary gain coming from adapting to a new rotary positional embedding (RoPE)\nscaling parameter. Finally, we show that a simpler file-level training approach\nat the original sequence length remains highly effective, opening up\nrepository-level code completion research to settings with more constrained\ndata and compute resources.",
      "url": "http://arxiv.org/abs/2510.13697v1",
      "published_time_eastern_timestamp": 1760543719.0
    }
  ]
}
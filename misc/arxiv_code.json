{
  "last_updated": "2025-07-30T23:59:47.475933-04:00",
  "papers": [
    {
      "title": "Viser: Imperative, Web-based 3D Visualization in Python",
      "summary": "We present Viser, a 3D visualization library for computer vision and\nrobotics. Viser aims to bring easy and extensible 3D visualization to Python:\nwe provide a comprehensive set of 3D scene and 2D GUI primitives, which can be\nused independently with minimal setup or composed to build specialized\ninterfaces. This technical report describes Viser's features, interface, and\nimplementation. Key design choices include an imperative-style API and a\nweb-based viewer, which improve compatibility with modern programming patterns\nand workflows.",
      "url": "http://arxiv.org/abs/2507.22885v1",
      "published_time_eastern_timestamp": 1753898371.0
    },
    {
      "title": "Automatically discovering heuristics in a complex SAT solver with large\n  language models",
      "summary": "Satisfiability problem (SAT) is a cornerstone of computational complexity\nwith broad industrial applications, and it remains challenging to optimize\nmodern SAT solvers in real-world settings due to their intricate architectures.\nWhile automatic configuration frameworks have been developed, they rely on\nmanually constrained search spaces and yield limited performance gains. This\nwork introduces a novel paradigm which effectively optimizes complex SAT\nsolvers via Large Language Models (LLMs), and a tool called AutoModSAT is\ndeveloped. Three fundamental challenges are addressed in order to achieve\nsuperior performance: (1) LLM-friendly solver: Systematic guidelines are\nproposed for developing a modularized solver to meet LLMs' compatibility,\nemphasizing code simplification, information share and bug reduction; (2)\nAutomatic prompt optimization: An unsupervised automatic prompt optimization\nmethod is introduced to advance the diversity of LLMs' output; (3) Efficient\nsearch strategy: We design a presearch strategy and an EA evolutionary\nalgorithm for the final efficient and effective discovery of heuristics.\nExtensive experiments across a wide range of datasets demonstrate that\nAutoModSAT achieves 50% performance improvement over the baseline solver and\nachieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,\nAutoModSAT attains a 20% speedup on average compared to parameter-tuned\nalternatives of the SOTA solvers, showcasing the enhanced capability in\nhandling complex problem instances. This work bridges the gap between AI-driven\nheuristics discovery and mission-critical system optimization, and provides\nboth methodological advancements and empirically validated results for\nnext-generation complex solver development.",
      "url": "http://arxiv.org/abs/2507.22876v1",
      "published_time_eastern_timestamp": 1753897945.0
    },
    {
      "title": "TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning",
      "summary": "Large pre-trained models achieve remarkable performance in vision tasks but\nare impractical for fine-tuning due to high computational and storage costs.\nParameter-Efficient Fine-Tuning (PEFT) methods mitigate this issue by updating\nonly a subset of parameters; however, most existing approaches are\ntask-agnostic, failing to fully exploit task-specific adaptations, which leads\nto suboptimal efficiency and performance. To address this limitation, we\npropose Task-Relevant Parameter and Token Selection (TR-PTS), a task-driven\nframework that enhances both computational efficiency and accuracy.\nSpecifically, we introduce Task-Relevant Parameter Selection, which utilizes\nthe Fisher Information Matrix (FIM) to identify and fine-tune only the most\ninformative parameters in a layer-wise manner, while keeping the remaining\nparameters frozen. Simultaneously, Task-Relevant Token Selection dynamically\npreserves the most informative tokens and merges redundant ones, reducing\ncomputational overhead. By jointly optimizing parameters and tokens, TR-PTS\nenables the model to concentrate on task-discriminative information. We\nevaluate TR-PTS on benchmark, including FGVC and VTAB-1k, where it achieves\nstate-of-the-art performance, surpassing full fine-tuning by 3.40% and 10.35%,\nrespectively. The code are available at https://github.com/synbol/TR-PTS.",
      "url": "http://arxiv.org/abs/2507.22872v1",
      "published_time_eastern_timestamp": 1753897633.0
    },
    {
      "title": "Tracking research software outputs in the UK",
      "summary": "Research software is crucial in the research process and the growth of Open\nScience underscores the importance of accessing research artifacts, like data\nand code, raising traceability challenges among outputs. While it is a clear\nprinciple that research code, along with other essential outputs, should be\nrecognised as artifacts of the research process, the how of this principle\nremains variable. This study examines where UK academic institutions store and\nregister software as a unique research output, searching the UKRI's Gateway to\nResearch (GtR) metadata for publicly funded research software in the UK. The\nquantity of software reported as research outcomes remains low in proportion to\nother categories. Artifact sharing appears low, with one-quarter of the\nreported software having no links and 45% having either a missing or erroneous\nURL. Of the valid URLs, we find the single largest category is Public\nCommercial Code Repository, with GitHub being the host of 18% of all publicly\nfunded research software listed. These observations are contrasted with past\nfindings from 2023 and finally, we discuss the lack of artifact sharing in UK\nresearch, with resulting implications for the maintenance and evolution of\nresearch software. Without dissemination, research software risks demotion to a\ntransient artifact, useful only to meet short term research demands but\nultimately lost to the broader enterprise of science.",
      "url": "http://arxiv.org/abs/2507.22871v1",
      "published_time_eastern_timestamp": 1753897607.0
    },
    {
      "title": "Repair-R1: Better Test Before Repair",
      "summary": "APR (Automated Program Repair) aims to automatically locate program defects,\ngenerate patches and validate the repairs. Existing techniques for APR are\noften combined with LLMs (Large Language Models), which leverages the\ncode-related knowledge of LLMs to improve repair effectiveness. Current\nLLM-based APR methods typically utilize test cases only during the inference\nstage, adopting an iterative approach that performs repair first and validates\nit through test execution afterward. This conventional paradigm neglects two\nimportant aspects: the potential contribution of test cases in the training\nphase, and the possibility of leveraging testing prior to repair. To address\nthis, we propose Repair-R1, which introduces test cases into the model's\ntraining phase and shifts test generation to precede repair. The model is\nrequired to first generate discriminative test cases that can distinguish\ndefective behaviors, and then perform repair based on these tests. This enables\nthe model to better locate defects and understand the underlying causes of\ndefects, thereby improving repair effectiveness. We implement Repair-R1 with\nthree different backbone models, using RL (reinforcement learning) to\nco-optimize test generation and bug repair. Experimental results on four widely\nadopted benchmarks demonstrate the superiority of Repair-R1. Specially,\ncompared to vanilla models, Repair-R1 improves repair success rate by 2.68\\% to\n48.29\\%, test generation success rate by 16.38\\% to 53.28\\%, and test coverage\nby 0.78\\% to 53.96\\%. We publish the code and weights at\nhttps://github.com/Tomsawyerhu/APR-RL and\nhttps://huggingface.co/tomhu/Qwen3-4B-RL-5000-step.",
      "url": "http://arxiv.org/abs/2507.22853v1",
      "published_time_eastern_timestamp": 1753896245.0
    },
    {
      "title": "Robust Contract with Career Concerns",
      "summary": "An employer contracts with a worker to incentivize efforts whose productivity\ndepends on ability; the worker then enters a market that pays him contingent on\nability evaluation. With non-additive monitoring technology, the\ninterdependence between market expectations and worker efforts can lead to\nmultiple equilibria (contrasting Holmstrom (1982/1999); Gibbons and Murphy\n(1992)). We identify a sufficient and necessary criterion for the employer to\nface such strategic uncertainty--one linked to skill-effort complementarity, a\npervasive feature of labor markets. To fully implement work, the employer\noptimally creates private wage discrimination to iteratively eliminate\npessimistic market expectations and low worker efforts. Our result suggests\nthat present contractual privacy, employers' coordination motives generate\nwithin-group pay inequality. The comparative statics further explain several\nstylized facts about residual wage dispersion.",
      "url": "http://arxiv.org/abs/2507.22852v1",
      "published_time_eastern_timestamp": 1753896178.0
    },
    {
      "title": "DEQSE Quantum IDE Extension: Integrated Tool for Quantum Software\n  Engineering",
      "summary": "This paper presents a tool that simplifies quantum software development by\nunifying circuit design, code generation, and execution within a single\ncross-platform environment that supports iterative development. Implemented as\nopen source, the DEQSE Quantum IDE Extension has been developed to provide\nquantum functionalities within the Visual Studio Code environment, including\nproject creator, code runner, code converter, and embedded quantum circuit\nsimulator. Furthermore, the system provides capabilities that facilitate\niterative development and support learning, distinguishing it from other\navailable Visual Studio Code Extensions for quantum computing.",
      "url": "http://arxiv.org/abs/2507.22843v1",
      "published_time_eastern_timestamp": 1753894814.0
    },
    {
      "title": "PAF-Net: Phase-Aligned Frequency Decoupling Network for Multi-Process\n  Manufacturing Quality Prediction",
      "summary": "Accurate quality prediction in multi-process manufacturing is critical for\nindustrial efficiency but hindered by three core challenges: time-lagged\nprocess interactions, overlapping operations with mixed periodicity, and\ninter-process dependencies in shared frequency bands. To address these, we\npropose PAF-Net, a frequency decoupled time series prediction framework with\nthree key innovations: (1) A phase-correlation alignment method guided by\nfrequency domain energy to synchronize time-lagged quality series, resolving\ntemporal misalignment. (2) A frequency independent patch attention mechanism\npaired with Discrete Cosine Transform (DCT) decomposition to capture\nheterogeneous operational features within individual series. (3) A frequency\ndecoupled cross attention module that suppresses noise from irrelevant\nfrequencies, focusing exclusively on meaningful dependencies within shared\nbands. Experiments on 4 real-world datasets demonstrate PAF-Net's superiority.\nIt outperforms 10 well-acknowledged baselines by 7.06% lower MSE and 3.88%\nlower MAE. Our code is available at\nhttps://github.com/StevenLuan904/PAF-Net-Official.",
      "url": "http://arxiv.org/abs/2507.22840v1",
      "published_time_eastern_timestamp": 1753894602.0
    },
    {
      "title": "ScreenCoder: Advancing Visual-to-Code Generation for Front-End\n  Automation via Modular Multimodal Agents",
      "summary": "Automating the transformation of user interface (UI) designs into front-end\ncode holds significant promise for accelerating software development and\ndemocratizing design workflows. While recent large language models (LLMs) have\ndemonstrated progress in text-to-code generation, many existing approaches rely\nsolely on natural language prompts, limiting their effectiveness in capturing\nspatial layout and visual design intent. In contrast, UI development in\npractice is inherently multimodal, often starting from visual sketches or\nmockups. To address this gap, we introduce a modular multi-agent framework that\nperforms UI-to-code generation in three interpretable stages: grounding,\nplanning, and generation. The grounding agent uses a vision-language model to\ndetect and label UI components, the planning agent constructs a hierarchical\nlayout using front-end engineering priors, and the generation agent produces\nHTML/CSS code via adaptive prompt-based synthesis. This design improves\nrobustness, interpretability, and fidelity over end-to-end black-box methods.\nFurthermore, we extend the framework into a scalable data engine that\nautomatically produces large-scale image-code pairs. Using these synthetic\nexamples, we fine-tune and reinforce an open-source VLM, yielding notable gains\nin UI understanding and code quality. Extensive experiments demonstrate that\nour approach achieves state-of-the-art performance in layout accuracy,\nstructural coherence, and code correctness. Our code is made publicly available\nat https://github.com/leigest519/ScreenCoder.",
      "url": "http://arxiv.org/abs/2507.22827v1",
      "published_time_eastern_timestamp": 1753893681.0
    },
    {
      "title": "DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent\n  Diffusion",
      "summary": "Deep neural networks have demonstrated remarkable success across numerous\ntasks, yet they remain vulnerable to Trojan (backdoor) attacks, raising serious\nconcerns about their safety in real-world mission-critical applications. A\ncommon countermeasure is trigger inversion -- reconstructing malicious\n\"shortcut\" patterns (triggers) inserted by an adversary during training.\nCurrent trigger-inversion methods typically search the full pixel space under\nspecific assumptions but offer no assurances that the estimated trigger is more\nthan an adversarial perturbation that flips the model output. Here, we propose\na data-free, zero-shot trigger-inversion strategy that restricts the search\nspace while avoiding strong assumptions on trigger appearance. Specifically, we\nincorporate a diffusion-based generator guided by the target classifier;\nthrough iterative generation, we produce candidate triggers that align with the\ninternal representations the model relies on for malicious behavior. Empirical\nevaluations, both quantitative and qualitative, show that our approach\nreconstructs triggers that effectively distinguish clean versus Trojaned\nmodels. DISTIL surpasses alternative methods by high margins, achieving up to\n7.1% higher accuracy on the BackdoorBench dataset and a 9.4% improvement on\ntrojaned object detection model scanning, offering a promising new direction\nfor reliable backdoor defense without reliance on extensive data or strong\nprior assumptions about triggers. The code is available at\nhttps://github.com/AdaptiveMotorControlLab/DISTIL.",
      "url": "http://arxiv.org/abs/2507.22813v1",
      "published_time_eastern_timestamp": 1753893073.0
    },
    {
      "title": "Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource\n  Settings",
      "summary": "Accurate fetal biometric measurements, such as abdominal circumference, play\na vital role in prenatal care. However, obtaining high-quality ultrasound\nimages for these measurements heavily depends on the expertise of sonographers,\nposing a significant challenge in low-income countries due to the scarcity of\ntrained personnel. To address this issue, we leverage FetalCLIP, a\nvision-language model pretrained on a curated dataset of over 210,000 fetal\nultrasound image-caption pairs, to perform automated fetal ultrasound image\nquality assessment (IQA) on blind-sweep ultrasound data. We introduce\nFetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank\nAdaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN\nand Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of\n0.757. Moreover, we show that an adapted segmentation model, when repurposed\nfor classification, further improves performance, achieving an F1 score of\n0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal\nultrasound foundation models can enable task-specific adaptations, advancing\nprenatal care in resource-limited settings. The experimental code is available\nat: https://github.com/donglihe-hub/FetalCLIP-IQA.",
      "url": "http://arxiv.org/abs/2507.22802v1",
      "published_time_eastern_timestamp": 1753891769.0
    },
    {
      "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
      "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
      "url": "http://arxiv.org/abs/2507.22801v1",
      "published_time_eastern_timestamp": 1753891441.0
    },
    {
      "title": "An Uncertainty Principle for Probabilistic Computation in the Retina",
      "summary": "We introduce a probabilistic model of early visual processing, beginning with\nthe interaction between a light wavefront and the retina. We argue that\nperception originates not with deterministic transduction, but with\nprobabilistic threshold crossings shaped by quantum photon arrival statistics\nand biological variability. We formalize this with an uncertainty relation, \\(\n\\Delta \\alpha \\cdot \\Delta t \\geq \\eta \\), through the transformation of light\ninto symbolic neural code through the layered retinal architecture. Our model\nis supported by previous experimental results, which show intrinsic variability\nin retinal responses even under fixed stimuli. We contrast this with a\nclassical null hypothesis of deterministic encoding and propose experiments to\nfurther test our uncertainty relation. By re-framing the retina as a\nprobabilistic measurement device, we lay the foundation for future models of\ncortical dynamics rooted in quantum-like computation. We are not claiming that\nthe brain could be working as a quantum-system, but rather putting forth the\nargument that the brain as a classical system could still implement\nquantum-inspired computations. We define quantum-inspired computation as a\nscheme that includes both probabilistic and time-sensitive computation, clearly\nseparating it from classically implementable probabilistic systems.",
      "url": "http://arxiv.org/abs/2507.22785v1",
      "published_time_eastern_timestamp": 1753890673.0
    },
    {
      "title": "Confined Circumstellar Material as a Dust Formation Site in Type II\n  Supernovae",
      "summary": "We propose a model for dust formation in Type II supernovae (SNe) interacting\nwith confined circumstellar material (CSM), motivated by recent time-domain\nsurveys that have revealed a substantial fraction of SN progenitors to be\nsurrounded by CSM ejected shortly before core-collapse. We simulate the pre-SN\nmass eruption and the resulting confined CSM using the open-source code CHIPS,\nand follow the subsequent evolution of the SN ejecta and its interaction with\nthe CSM. We show that a cold dense shell (CDS) is formed at the radiative shock\nunder a wide range of conditions and later undergoes rapid adiabatic cooling\nduring free expansion, leading to efficient dust condensation. The resulting\ndust mass ranges from $\\sim10^{-3}\\,M_\\odot$ to $0.1\\,M_\\odot$, depending on\nthe mass and spatial extent of the CSM. We further calculate the infrared (IR)\nemission from the newly formed dust and find broad consistency with\nobservations of SN~1998S. Notably, the IR light curve exhibits a rapid rise\nwithin $\\lesssim10\\,{\\rm d}$, closely resembling that of kilonovae (KNe). This\nsuggests that dust emission powered by confined CSM interaction may be also\ndiscovered in KN searches. Moreover, the high-density environment of the CDS\nmay allow dust grains to grow to larger sizes, enhancing their survivability\nagainst destruction by reverse shocks propagating from the interstellar medium\nat later times.",
      "url": "http://arxiv.org/abs/2507.22763v1",
      "published_time_eastern_timestamp": 1753889010.0
    },
    {
      "title": "A Linear N-Point Solver for Structure and Motion from Asynchronous\n  Tracks",
      "summary": "Structure and continuous motion estimation from point correspondences is a\nfundamental problem in computer vision that has been powered by well-known\nalgorithms such as the familiar 5-point or 8-point algorithm. However, despite\ntheir acclaim, these algorithms are limited to processing point correspondences\noriginating from a pair of views each one representing an instantaneous capture\nof the scene. Yet, in the case of rolling shutter cameras, or more recently,\nevent cameras, this synchronization breaks down. In this work, we present a\nunified approach for structure and linear motion estimation from 2D point\ncorrespondences with arbitrary timestamps, from an arbitrary set of views. By\nformulating the problem in terms of first-order dynamics and leveraging a\nconstant velocity motion model, we derive a novel, linear point incidence\nrelation allowing for the efficient recovery of both linear velocity and 3D\npoints with predictable degeneracies and solution multiplicities. Owing to its\ngeneral formulation, it can handle correspondences from a wide range of sensing\nmodalities such as global shutter, rolling shutter, and event cameras, and can\neven combine correspondences from different collocated sensors. We validate the\neffectiveness of our solver on both simulated and real-world data, where we\nshow consistent improvement across all modalities when compared to recent\napproaches. We believe our work opens the door to efficient structure and\nmotion estimation from asynchronous data. Code can be found at\nhttps://github.com/suhang99/AsyncTrack-Motion-Solver.",
      "url": "http://arxiv.org/abs/2507.22733v1",
      "published_time_eastern_timestamp": 1753887226.0
    },
    {
      "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in\n  Retrieval-Augmented Reasoning for LLMs",
      "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.",
      "url": "http://arxiv.org/abs/2507.22716v1",
      "published_time_eastern_timestamp": 1753885784.0
    },
    {
      "title": "Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints",
      "summary": "Shape-from-Template (SfT) refers to the class of methods that reconstruct the\n3D shape of a deforming object from images/videos using a 3D template.\nTraditional SfT methods require point correspondences between images and the\ntexture of the 3D template in order to reconstruct 3D shapes from images/videos\nin real time. Their performance severely degrades when encountered with severe\nocclusions in the images because of the unavailability of correspondences. In\ncontrast, modern SfT methods use a correspondence-free approach by\nincorporating deep neural networks to reconstruct 3D objects, thus requiring\nhuge amounts of data for supervision. Recent advances use a fully unsupervised\nor self-supervised approach by combining differentiable physics and graphics to\ndeform 3D template to match input images. In this paper, we propose an\nunsupervised SfT which uses only image observations: color features, gradients\nand silhouettes along with a mesh inextensibility constraint to reconstruct at\na $400\\times$ faster pace than (best-performing) unsupervised SfT. Moreover,\nwhen it comes to generating finer details and severe occlusions, our method\noutperforms the existing methodologies by a large margin. Code is available at\nhttps://github.com/dvttran/nsft.",
      "url": "http://arxiv.org/abs/2507.22699v1",
      "published_time_eastern_timestamp": 1753884579.0
    },
    {
      "title": "H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity",
      "summary": "Different from existing federated fine-tuning (FFT) methods for foundation\nmodels, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-explored\nscenario where clients exhibit double heterogeneity in model architectures and\ndownstream tasks. This hybrid heterogeneity introduces two significant\nchallenges: 1) heterogeneous matrix aggregation, where clients adopt different\nlarge-scale foundation models based on their task requirements and resource\nlimitations, leading to dimensional mismatches during LoRA parameter\naggregation; and 2) multi-task knowledge interference, where local shared\nparameters, trained with both task-shared and task-specific knowledge, cannot\nensure only task-shared knowledge is transferred between clients. To address\nthese challenges, we propose H2Tune, a federated foundation model fine-tuning\nwith hybrid heterogeneity. Our framework H2Tune consists of three key\ncomponents: (i) sparsified triple matrix decomposition to align hidden\ndimensions across clients through constructing rank-consistent middle matrices,\nwith adaptive sparsification based on client resources; (ii) relation-guided\nmatrix layer alignment to handle heterogeneous layer structures and\nrepresentation capabilities; and (iii) alternating task-knowledge\ndisentanglement mechanism to decouple shared and specific knowledge of local\nmodel parameters through alternating optimization. Theoretical analysis proves\na convergence rate of O(1/\\sqrt{T}). Extensive experiments show our method\nachieves up to 15.4% accuracy improvement compared to state-of-the-art\nbaselines. Our code is available at\nhttps://anonymous.4open.science/r/H2Tune-1407.",
      "url": "http://arxiv.org/abs/2507.22633v1",
      "published_time_eastern_timestamp": 1753879998.0
    },
    {
      "title": "Bridging the Gap in Missing Modalities: Leveraging Knowledge\n  Distillation and Style Matching for Brain Tumor Segmentation",
      "summary": "Accurate and reliable brain tumor segmentation, particularly when dealing\nwith missing modalities, remains a critical challenge in medical image\nanalysis. Previous studies have not fully resolved the challenges of tumor\nboundary segmentation insensitivity and feature transfer in the absence of key\nimaging modalities. In this study, we introduce MST-KDNet, aimed at addressing\nthese critical issues. Our model features Multi-Scale Transformer Knowledge\nDistillation to effectively capture attention weights at various resolutions,\nDual-Mode Logit Distillation to improve the transfer of knowledge, and a Global\nStyle Matching Module that integrates feature matching with adversarial\nlearning. Comprehensive experiments conducted on the BraTS and FeTS 2024\ndatasets demonstrate that MST-KDNet surpasses current leading methods in both\nDice and HD95 scores, particularly in conditions with substantial modality\nloss. Our approach shows exceptional robustness and generalization potential,\nmaking it a promising candidate for real-world clinical applications. Our\nsource code is available at https://github.com/Quanato607/MST-KDNet.",
      "url": "http://arxiv.org/abs/2507.22626v1",
      "published_time_eastern_timestamp": 1753879604.0
    },
    {
      "title": "Multilingual Political Views of Large Language Models: Identification\n  and Steering",
      "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.",
      "url": "http://arxiv.org/abs/2507.22623v1",
      "published_time_eastern_timestamp": 1753879355.0
    }
  ]
}
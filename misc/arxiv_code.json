{
  "last_updated": "2025-08-20T23:35:38.541770-04:00",
  "papers": [
    {
      "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
      "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
      "url": "http://arxiv.org/abs/2508.14896v1",
      "published_time_eastern_timestamp": 1755712791.0
    },
    {
      "title": "Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in\n  Milliseconds",
      "summary": "Reconstructing 3D human bodies from sparse views has been an appealing topic,\nwhich is crucial to broader the related applications. In this paper, we propose\na quite challenging but valuable task to reconstruct the human body from only\ntwo images, i.e., the front and back view, which can largely lower the barrier\nfor users to create their own 3D digital humans. The main challenges lie in the\ndifficulty of building 3D consistency and recovering missing information from\nthe highly sparse input. We redesign a geometry reconstruction model based on\nfoundation reconstruction models to predict consistent point clouds even input\nimages have scarce overlaps with extensive human data training. Furthermore, an\nenhancement algorithm is applied to supplement the missing color information,\nand then the complete human point clouds with colors can be obtained, which are\ndirectly transformed into 3D Gaussians for better rendering quality.\nExperiments show that our method can reconstruct the entire human in 190 ms on\na single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,\ndemonstrating state-of-the-art performance on the THuman2.0 and cross-domain\ndatasets. Additionally, our method can complete human reconstruction even with\nimages captured by low-cost mobile devices, reducing the requirements for data\ncollection. Demos and code are available at\nhttps://hustvl.github.io/Snap-Snap/.",
      "url": "http://arxiv.org/abs/2508.14892v1",
      "published_time_eastern_timestamp": 1755712751.0
    },
    {
      "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
      "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.",
      "url": "http://arxiv.org/abs/2508.14879v1",
      "published_time_eastern_timestamp": 1755712215.0
    }
  ]
}
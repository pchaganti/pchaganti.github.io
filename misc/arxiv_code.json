{
  "last_updated": "2025-06-01T05:11:51.046837-04:00",
  "papers": [
    {
      "title": "TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models",
      "summary": "Image-text models excel at image-level tasks but struggle with detailed\nvisual understanding. While these models provide strong visual-language\nalignment, segmentation models like SAM2 offer precise spatial boundaries for\nobjects. To this end, we propose TextRegion, a simple, effective, and\ntraining-free framework that combines the strengths of image-text models and\nSAM2 to generate powerful text-aligned region tokens. These tokens enable\ndetailed visual understanding while preserving open-vocabulary capabilities.\nThey can be directly applied to various downstream tasks, including open-world\nsemantic segmentation, referring expression comprehension, and grounding. We\nconduct extensive evaluations and consistently achieve superior or competitive\nperformance compared to state-of-the-art training-free methods. Additionally,\nour framework is compatible with many image-text models, making it highly\npractical and easily extensible as stronger models emerge. Code is available\nat: https://github.com/avaxiao/TextRegion.",
      "url": "http://arxiv.org/abs/2505.23769v1",
      "published_time_eastern_timestamp": 1748541599.0
    },
    {
      "title": "Turbulence in Primordial Dark Matter Halos and Its Impact on the First\n  Star Formation",
      "summary": "We present high-resolution simulations of the first star-forming clouds in 15\nminihalos with masses ranging from $\\sim 10^5$ to $10^7\\ \\text{M}_{\\odot}$ at\nredshifts $z \\sim 17 - 20$, using the \\texttt{GIZMO} code. Our simulations\nincorporate detailed primordial gas physics and adopt initial conditions from\nthe state-of-the-art TNG cosmological simulations. To achieve the required\nresolution, we apply a particle-splitting technique that increases the\nresolution of the original TNG data by a factor of $\\sim 10^5$, reaching gas\nand dark matter particle masses of $0.2\\ \\text{M}_{\\odot}$ and $80\\\n\\text{M}_{\\odot}$, respectively. This enables us to resolve gas accretion\nduring the early assembly of minihalos and to capture the emergence of strong\nturbulent flows. We find that turbulence, driven by gas infall into the dark\nmatter potential wells, is predominantly supersonic, with characteristic Mach\nnumbers ranging from $1.8$ to $4.2$, increasing with halo mass. The supersonic\nturbulence effectively fragments the central gas cloud into multiple dense\nclumps, some of which form gravitationally bound cores and begin to collapse\ninto the first stars. Our results suggest that supersonic turbulence is a\ncommon feature in minihalos and plays a key role in generating clumpy\nstar-forming clouds, with important implications for the initial mass function\nof the first stars.",
      "url": "http://arxiv.org/abs/2505.23768v1",
      "published_time_eastern_timestamp": 1748541598.0
    },
    {
      "title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
      "summary": "The rapid advancement of large Vision-Language Models (VLMs) has propelled\nthe development of pure-vision-based GUI Agents, capable of perceiving and\noperating Graphical User Interfaces (GUI) to autonomously fulfill user\ninstructions. However, existing approaches usually adopt an offline learning\nframework, which faces two core limitations: (1) heavy reliance on high-quality\nmanual annotations for element grounding and action supervision, and (2)\nlimited adaptability to dynamic and interactive environments. To address these\nlimitations, we propose ZeroGUI, a scalable, online learning framework for\nautomating GUI Agent training at Zero human cost. Specifically, ZeroGUI\nintegrates (i) VLM-based automatic task generation to produce diverse training\ngoals from the current environment state, (ii) VLM-based automatic reward\nestimation to assess task success without hand-crafted evaluation functions,\nand (iii) two-stage online reinforcement learning to continuously interact with\nand learn from GUI environments. Experiments on two advanced GUI Agents\n(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance\nacross OSWorld and AndroidLab environments. The code is available at\nhttps://github.com/OpenGVLab/ZeroGUI.",
      "url": "http://arxiv.org/abs/2505.23762v1",
      "published_time_eastern_timestamp": 1748541591.0
    },
    {
      "title": "Model Immunization from a Condition Number Perspective",
      "summary": "Model immunization aims to pre-train models that are difficult to fine-tune\non harmful tasks while retaining their utility on other non-harmful tasks.\nThough prior work has shown empirical evidence for immunizing text-to-image\nmodels, the key understanding of when immunization is possible and a precise\ndefinition of an immunized model remain unclear. In this work, we propose a\nframework, based on the condition number of a Hessian matrix, to analyze model\nimmunization for linear models. Building on this framework, we design an\nalgorithm with regularization terms to control the resulting condition numbers\nafter pre-training. Empirical results on linear models and non-linear deep-nets\ndemonstrate the effectiveness of the proposed algorithm on model immunization.\nThe code is available at\nhttps://github.com/amberyzheng/model-immunization-cond-num.",
      "url": "http://arxiv.org/abs/2505.23760v1",
      "published_time_eastern_timestamp": 1748541588.0
    },
    {
      "title": "Impromptu VLA: Open Weights and Open Data for Driving\n  Vision-Language-Action Models",
      "summary": "Vision-Language-Action (VLA) models for autonomous driving show promise but\nfalter in unstructured corner case scenarios, largely due to a scarcity of\ntargeted benchmarks. To address this, we introduce Impromptu VLA. Our core\ncontribution is the Impromptu VLA Dataset: over 80,000 meticulously curated\nvideo clips, distilled from over 2M source clips sourced from 8 open-source\nlarge-scale datasets. This dataset is built upon our novel taxonomy of four\nchallenging unstructured categories and features rich, planning-oriented\nquestion-answering annotations and action trajectories. Crucially, experiments\ndemonstrate that VLAs trained with our dataset achieve substantial performance\ngains on established benchmarks--improving closed-loop NeuroNCAP scores and\ncollision rates, and reaching near state-of-the-art L2 accuracy in open-loop\nnuScenes trajectory prediction. Furthermore, our Q&A suite serves as an\neffective diagnostic, revealing clear VLM improvements in perception,\nprediction, and planning. Our code, data and models are available at\nhttps://github.com/ahydchh/Impromptu-VLA.",
      "url": "http://arxiv.org/abs/2505.23757v1",
      "published_time_eastern_timestamp": 1748541586.0
    },
    {
      "title": "ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks",
      "summary": "Recent progress in large language models (LLMs) has enabled tool-augmented\nagents capable of solving complex real-world tasks through step-by-step\nreasoning. However, existing evaluations often focus on general-purpose or\nmultimodal scenarios, leaving a gap in domain-specific benchmarks that assess\ntool-use capabilities in complex remote sensing use cases. We present ThinkGeo,\nan agentic benchmark designed to evaluate LLM-driven agents on remote sensing\ntasks via structured tool use and multi-step planning. Inspired by\ntool-interaction paradigms, ThinkGeo includes human-curated queries spanning a\nwide range of real-world applications such as urban planning, disaster\nassessment and change analysis, environmental monitoring, transportation\nanalysis, aviation monitoring, recreational infrastructure, and industrial site\nanalysis. Each query is grounded in satellite or aerial imagery and requires\nagents to reason through a diverse toolset. We implement a ReAct-style\ninteraction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,\nQwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise\nexecution metrics and final answer correctness. Our analysis reveals notable\ndisparities in tool accuracy and planning consistency across models. ThinkGeo\nprovides the first extensive testbed for evaluating how tool-enabled LLMs\nhandle spatial reasoning in remote sensing. Our code and dataset are publicly\navailable",
      "url": "http://arxiv.org/abs/2505.23752v1",
      "published_time_eastern_timestamp": 1748541578.0
    },
    {
      "title": "To Trust Or Not To Trust Your Vision-Language Model's Prediction",
      "summary": "Vision-Language Models (VLMs) have demonstrated strong capabilities in\naligning visual and textual modalities, enabling a wide range of applications\nin multimodal understanding and generation. While they excel in zero-shot and\ntransfer learning scenarios, VLMs remain susceptible to misclassification,\noften yielding confident yet incorrect predictions. This limitation poses a\nsignificant risk in safety-critical domains, where erroneous predictions can\nlead to severe consequences. In this work, we introduce TrustVLM, a\ntraining-free framework designed to address the critical challenge of\nestimating when VLM's predictions can be trusted. Motivated by the observed\nmodality gap in VLMs and the insight that certain concepts are more distinctly\nrepresented in the image embedding space, we propose a novel confidence-scoring\nfunction that leverages this space to improve misclassification detection. We\nrigorously evaluate our approach across 17 diverse datasets, employing 4\narchitectures and 2 VLMs, and demonstrate state-of-the-art performance, with\nimprovements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95\ncompared to existing baselines. By improving the reliability of the model\nwithout requiring retraining, TrustVLM paves the way for safer deployment of\nVLMs in real-world applications. The code will be available at\nhttps://github.com/EPFL-IMOS/TrustVLM.",
      "url": "http://arxiv.org/abs/2505.23745v1",
      "published_time_eastern_timestamp": 1748541541.0
    },
    {
      "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters\n  is All You Need",
      "summary": "Deep neural networks (DNNs) often underperform in real-world, dynamic\nsettings where data distributions change over time. Domain Incremental Learning\n(DIL) offers a solution by enabling continual model adaptation, with\nParameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce\nknowledge conflicts. However, existing PIDIL methods struggle with parameter\nselection accuracy, especially as the number of domains and corresponding\nclasses grows. To address this, we propose SOYO, a lightweight framework that\nimproves domain selection in PIDIL. SOYO introduces a Gaussian Mixture\nCompressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior\ndomain data efficiently, while a Multi-level Domain Feature Fusion Network\n(MDFN) enhances domain feature extraction. Our framework supports multiple\nParameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks\nsuch as image classification, object detection, and speech enhancement.\nExperimental results on six benchmarks demonstrate SOYO's consistent\nsuperiority over existing baselines, showcasing its robustness and adaptability\nin complex, evolving environments. The codes will be released in\nhttps://github.com/qwangcv/SOYO.",
      "url": "http://arxiv.org/abs/2505.23744v1",
      "published_time_eastern_timestamp": 1748541537.0
    },
    {
      "title": "MAGREF: Masked Guidance for Any-Reference Video Generation",
      "summary": "Video generation has made substantial strides with the emergence of deep\ngenerative models, especially diffusion-based approaches. However, video\ngeneration based on multiple reference subjects still faces significant\nchallenges in maintaining multi-subject consistency and ensuring high\ngeneration quality. In this paper, we propose MAGREF, a unified framework for\nany-reference video generation that introduces masked guidance to enable\ncoherent multi-subject video synthesis conditioned on diverse reference images\nand a textual prompt. Specifically, we propose (1) a region-aware dynamic\nmasking mechanism that enables a single model to flexibly handle various\nsubject inference, including humans, objects, and backgrounds, without\narchitectural changes, and (2) a pixel-wise channel concatenation mechanism\nthat operates on the channel dimension to better preserve appearance features.\nOur model delivers state-of-the-art video generation quality, generalizing from\nsingle-subject training to complex multi-subject scenarios with coherent\nsynthesis and precise control over individual subjects, outperforming existing\nopen-source and commercial baselines. To facilitate evaluation, we also\nintroduce a comprehensive multi-subject video benchmark. Extensive experiments\ndemonstrate the effectiveness of our approach, paving the way for scalable,\ncontrollable, and high-fidelity multi-subject video synthesis. Code and model\ncan be found at: https://github.com/MAGREF-Video/MAGREF",
      "url": "http://arxiv.org/abs/2505.23742v1",
      "published_time_eastern_timestamp": 1748541495.0
    },
    {
      "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
      "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state $Z$ that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state $Z$. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor.",
      "url": "http://arxiv.org/abs/2505.23734v1",
      "published_time_eastern_timestamp": 1748541424.0
    },
    {
      "title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning",
      "summary": "Existing reasoning segmentation approaches typically fine-tune multimodal\nlarge language models (MLLMs) using image-text pairs and corresponding mask\nlabels. However, they exhibit limited generalization to out-of-distribution\nscenarios without an explicit reasoning process. Although recent efforts\nleverage reinforcement learning through group-relative policy optimization\n(GRPO) to enhance reasoning ability, they often suffer from overthinking -\nproducing uniformly verbose reasoning chains irrespective of task complexity.\nThis results in elevated computational costs and limited control over reasoning\nquality. To address this problem, we propose PixelThink, a simple yet effective\nscheme that integrates externally estimated task difficulty and internally\nmeasured model uncertainty to regulate reasoning generation within a\nreinforcement learning paradigm. The model learns to compress reasoning length\nin accordance with scene complexity and predictive confidence. To support\ncomprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark\nwith annotated reasoning references and difficulty scores, along with a suite\nof metrics designed to assess segmentation accuracy, reasoning quality, and\nefficiency jointly. Experimental results demonstrate that the proposed approach\nimproves both reasoning efficiency and overall segmentation performance. Our\nwork contributes novel perspectives towards efficient and interpretable\nmultimodal understanding. The code and model will be publicly available.",
      "url": "http://arxiv.org/abs/2505.23727v1",
      "published_time_eastern_timestamp": 1748541349.0
    },
    {
      "title": "Don't Take the Premise for Granted: Evaluating the Premise Critique\n  Ability of Large Language Models",
      "summary": "Large language models (LLMs) have witnessed rapid advancements, demonstrating\nremarkable capabilities. However, a notable vulnerability persists: LLMs often\nuncritically accept flawed or contradictory premises, leading to inefficient\nreasoning and unreliable outputs. This emphasizes the significance of\npossessing the \\textbf{Premise Critique Ability} for LLMs, defined as the\ncapacity to proactively identify and articulate errors in input premises. Most\nexisting studies assess LLMs' reasoning ability in ideal settings, largely\nignoring their vulnerabilities when faced with flawed premises. Thus, we\nintroduce the \\textbf{Premise Critique Bench (PCBench)}, designed by\nincorporating four error types across three difficulty levels, paired with\nmulti-faceted evaluation metrics. We conducted systematic evaluations of 15\nrepresentative LLMs. Our findings reveal: (1) Most models rely heavily on\nexplicit prompts to detect errors, with limited autonomous critique; (2)\nPremise critique ability depends on question difficulty and error type, with\ndirect contradictions being easier to detect than complex or procedural errors;\n(3) Reasoning ability does not consistently correlate with the premise critique\nability; (4) Flawed premises trigger overthinking in reasoning models, markedly\nlengthening responses due to repeated attempts at resolving conflicts. These\ninsights underscore the urgent need to enhance LLMs' proactive evaluation of\ninput validity, positioning premise critique as a foundational capability for\ndeveloping reliable, human-centric systems. The code is available at\nhttps://github.com/MLGroupJLU/Premise_Critique.",
      "url": "http://arxiv.org/abs/2505.23715v1",
      "published_time_eastern_timestamp": 1748540984.0
    },
    {
      "title": "SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid\n  Methods",
      "summary": "This paper addresses the critical need for high-quality evaluation datasets\nin low-resource languages to advance cross-lingual transfer. While\ncross-lingual transfer offers a key strategy for leveraging multilingual\npretraining to expand language technologies to understudied and typologically\ndiverse languages, its effectiveness is dependent on quality and suitable\nbenchmarks. We release new sense-annotated datasets of sentences containing\npolysemous words, spanning nine low-resource languages across diverse language\nfamilies and scripts. To facilitate dataset creation, the paper presents a\ndemonstrably beneficial semi-automatic annotation method. The utility of the\ndatasets is demonstrated through Word-in-Context (WiC) formatted experiments\nthat evaluate transfer on these low-resource languages. Results highlight the\nimportance of targeted dataset creation and evaluation for effective polysemy\ndisambiguation in low-resource settings and transfer studies. The released\ndatasets and code aim to support further research into fair, robust, and truly\nmultilingual NLP.",
      "url": "http://arxiv.org/abs/2505.23714v1",
      "published_time_eastern_timestamp": 1748540888.0
    },
    {
      "title": "SocialMaze: A Benchmark for Evaluating Social Reasoning in Large\n  Language Models",
      "summary": "Large language models (LLMs) are increasingly applied to socially grounded\ntasks, such as online community moderation, media content analysis, and social\nreasoning games. Success in these contexts depends on a model's social\nreasoning ability - the capacity to interpret social contexts, infer others'\nmental states, and assess the truthfulness of presented information. However,\nthere is currently no systematic evaluation framework that comprehensively\nassesses the social reasoning capabilities of LLMs. Existing efforts often\noversimplify real-world scenarios and consist of tasks that are too basic to\nchallenge advanced models. To address this gap, we introduce SocialMaze, a new\nbenchmark specifically designed to evaluate social reasoning. SocialMaze\nsystematically incorporates three core challenges: deep reasoning, dynamic\ninteraction, and information uncertainty. It provides six diverse tasks across\nthree key settings: social reasoning games, daily-life interactions, and\ndigital community platforms. Both automated and human validation are used to\nensure data quality. Our evaluation reveals several key insights: models vary\nsubstantially in their ability to handle dynamic interactions and integrate\ntemporally evolving information; models with strong chain-of-thought reasoning\nperform better on tasks requiring deeper inference beyond surface-level cues;\nand model reasoning degrades significantly under uncertainty. Furthermore, we\nshow that targeted fine-tuning on curated reasoning examples can greatly\nimprove model performance in complex social scenarios. The dataset is publicly\navailable at: https://huggingface.co/datasets/MBZUAI/SocialMaze",
      "url": "http://arxiv.org/abs/2505.23713v1",
      "published_time_eastern_timestamp": 1748540856.0
    },
    {
      "title": "CLDTracker: A Comprehensive Language Description for Visual Tracking",
      "summary": "VOT remains a fundamental yet challenging task in computer vision due to\ndynamic appearance changes, occlusions, and background clutter. Traditional\ntrackers, relying primarily on visual cues, often struggle in such complex\nscenarios. Recent advancements in VLMs have shown promise in semantic\nunderstanding for tasks like open-vocabulary detection and image captioning,\nsuggesting their potential for VOT. However, the direct application of VLMs to\nVOT is hindered by critical limitations: the absence of a rich and\ncomprehensive textual representation that semantically captures the target\nobject's nuances, limiting the effective use of language information;\ninefficient fusion mechanisms that fail to optimally integrate visual and\ntextual features, preventing a holistic understanding of the target; and a lack\nof temporal modeling of the target's evolving appearance in the language\ndomain, leading to a disconnect between the initial description and the\nobject's subsequent visual changes. To bridge these gaps and unlock the full\npotential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive\nLanguage Description framework for robust visual Tracking. Our tracker\nintroduces a dual-branch architecture consisting of a textual and a visual\nbranch. In the textual branch, we construct a rich bag of textual descriptions\nderived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with\nsemantic and contextual cues to address the lack of rich textual\nrepresentation. Experiments on six standard VOT benchmarks demonstrate that\nCLDTracker achieves SOTA performance, validating the effectiveness of\nleveraging robust and temporally-adaptive vision-language representations for\ntracking. Code and models are publicly available at:\nhttps://github.com/HamadYA/CLDTracker",
      "url": "http://arxiv.org/abs/2505.23704v1",
      "published_time_eastern_timestamp": 1748540370.0
    },
    {
      "title": "Dyn-HTE: High-temperature expansion of the dynamic Matsubara spin\n  correlator",
      "summary": "The high-temperature series expansion for quantum spin models is a\nwell-established tool to compute thermodynamic quantities and equal-time spin\ncorrelations, in particular for frustrated interactions. We extend the scope of\nthis expansion to the dynamic Matsubara spin-spin correlator and develop a\nfully analytic algorithm to compute its expansion coefficients. We focus on\nHeisenberg models with a single coupling constant J and spin lengths S=1/2,1.\nThe expansion coefficients up to 12th order in J/T are precomputed on all\npossible ~10^6 graphs embeddable in arbitrary lattices and are provided under\nhttps://github.com/bsbierski/Dyn-HTE. This enables calculation of static\nmomentum-resolved susceptibilities for arbitrary site-pairs or wavevectors. We\ntest our results for the S=1/2 Heisenberg chain and on the triangular lattice\nmodel. Moreover, the analytic frequency dependence in the expansion allows for\nstable analytic continuation to the real-frequency dynamic structure factor.\nThis important application is discussed in a companion letter.",
      "url": "http://arxiv.org/abs/2505.23699v1",
      "published_time_eastern_timestamp": 1748540258.0
    },
    {
      "title": "Data-to-Dashboard: Multi-Agent LLM Framework for Insightful\n  Visualization in Enterprise Analytics",
      "summary": "The rapid advancement of LLMs has led to the creation of diverse agentic\nsystems in data analysis, utilizing LLMs' capabilities to improve insight\ngeneration and visualization. In this paper, we present an agentic system that\nautomates the data-to-dashboard pipeline through modular LLM agents capable of\ndomain detection, concept extraction, multi-perspective analysis generation,\nand iterative self-reflection. Unlike existing chart QA systems, our framework\nsimulates the analytical reasoning process of business analysts by retrieving\ndomain-relevant knowledge and adapting to diverse datasets without relying on\nclosed ontologies or question templates.\n  We evaluate our system on three datasets across different domains.\nBenchmarked against GPT-4o with a single-prompt baseline, our approach shows\nimproved insightfulness, domain relevance, and analytical depth, as measured by\ntailored evaluation metrics and qualitative human assessment.\n  This work contributes a novel modular pipeline to bridge the path from raw\ndata to visualization, and opens new opportunities for human-in-the-loop\nvalidation by domain experts in business analytics. All code can be found here:\nhttps://github.com/77luvC/D2D_Data2Dashboard",
      "url": "http://arxiv.org/abs/2505.23695v1",
      "published_time_eastern_timestamp": 1748539935.0
    },
    {
      "title": "Grounded Reinforcement Learning for Visual Reasoning",
      "summary": "While reinforcement learning (RL) over chains of thought has significantly\nadvanced language models in tasks such as mathematics and coding, visual\nreasoning introduces added complexity by requiring models to direct visual\nattention, interpret perceptual inputs, and ground abstract reasoning in\nspatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement\nLearning), a vision-language model trained with RL to explicitly anchor each\nreasoning step to specific visual coordinates. Inspired by human visual\ndecision-making, ViGoRL learns to produce spatially grounded reasoning traces,\nguiding visual attention to task-relevant regions at each step. When\nfine-grained exploration is required, our novel multi-turn RL framework enables\nthe model to dynamically zoom into predicted coordinates as reasoning unfolds.\nAcross a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK\nfor spatial reasoning, V*bench for visual search, and ScreenSpot and\nVisualWebArena for web-based grounding--ViGoRL consistently outperforms both\nsupervised fine-tuning and conventional RL baselines that lack explicit\ngrounding mechanisms. Incorporating multi-turn RL with zoomed-in visual\nfeedback significantly improves ViGoRL's performance on localizing small GUI\nelements and visual search, achieving 86.4% on V*Bench. Additionally, we find\nthat grounding amplifies other visual behaviors such as region exploration,\ngrounded subgoal setting, and visual verification. Finally, human evaluations\nshow that the model's visual references are not only spatially accurate but\nalso helpful for understanding model reasoning steps. Our results show that\nvisually grounded RL is a strong paradigm for imbuing models with\ngeneral-purpose visual reasoning.",
      "url": "http://arxiv.org/abs/2505.23678v1",
      "published_time_eastern_timestamp": 1748539226.0
    },
    {
      "title": "ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in\n  Lung Cancer",
      "summary": "Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer\n(NSCLC) remains a critical unmet need. Existing radiomics and deep\nlearning-based predictive models rely primarily on pre-treatment imaging to\npredict categorical response outcomes, limiting their ability to capture the\ncomplex morphological and textural transformations induced by immunotherapy.\nThis study introduces ImmunoDiff, an anatomy-aware diffusion model designed to\nsynthesize post-treatment CT scans from baseline imaging while incorporating\nclinically relevant constraints. The proposed framework integrates anatomical\npriors, specifically lobar and vascular structures, to enhance fidelity in CT\nsynthesis. Additionally, we introduce a novel cbi-Adapter, a conditioning\nmodule that ensures pairwise-consistent multimodal integration of imaging and\nclinical data embeddings, to refine the generative process. Additionally, a\nclinical variable conditioning mechanism is introduced, leveraging demographic\ndata, blood-based biomarkers, and PD-L1 expression to refine the generative\nprocess. Evaluations on an in-house NSCLC cohort treated with immune checkpoint\ninhibitors demonstrate a 21.24% improvement in balanced accuracy for response\nprediction and a 0.03 increase in c-index for survival prediction. Code will be\nreleased soon.",
      "url": "http://arxiv.org/abs/2505.23675v1",
      "published_time_eastern_timestamp": 1748539180.0
    },
    {
      "title": "Quantum-Based Software Engineering",
      "summary": "Quantum computing has demonstrated potential for solving computationally\nintensive problems more efficiently than classical methods. Many software\nengineering tasks, such as test case selection, static analysis, code clone\ndetection, and defect prediction, involve complex optimization, search, or\nclassification, making them candidates for quantum enhancement. In this paper,\nwe propose Quantum-Based Software Engineering (QBSE), a potential research\ndirection for applying quantum computing to classical software engineering\nproblems. We outline its scope, clarify its distinction from quantum software\nengineering (QSE), and identify key problem types that may benefit from quantum\noptimization, search, and learning techniques. We also summarize existing\nresearch efforts that remain fragmented. Finally, we sketch a preliminary\nresearch agenda that may help guide the future development of QBSE as a\nstructured and meaningful direction within software engineering.",
      "url": "http://arxiv.org/abs/2505.23674v1",
      "published_time_eastern_timestamp": 1748539178.0
    }
  ]
}
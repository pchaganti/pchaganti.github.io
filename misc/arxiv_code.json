{
  "last_updated": "2025-07-03T17:11:12.584771-04:00",
  "papers": [
    {
      "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
      "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
      "url": "http://arxiv.org/abs/2507.01945v1",
      "published_time_eastern_timestamp": 1751478950.0
    },
    {
      "title": "CI-VID: A Coherent Interleaved Text-Video Dataset",
      "summary": "Text-to-video (T2V) generation has recently attracted considerable attention,\nresulting in the development of numerous high-quality datasets that have\npropelled progress in this area. However, existing public datasets are\nprimarily composed of isolated text-video (T-V) pairs and thus fail to support\nthe modeling of coherent multi-clip video sequences. To address this\nlimitation, we introduce CI-VID, a dataset that moves beyond isolated\ntext-to-video (T2V) generation toward text-and-video-to-video (TV2V)\ngeneration, enabling models to produce coherent, multi-scene video sequences.\nCI-VID contains over 340,000 samples, each featuring a coherent sequence of\nvideo clips with text captions that capture both the individual content of each\nclip and the transitions between them, enabling visually and textually grounded\ngeneration. To further validate the effectiveness of CI-VID, we design a\ncomprehensive, multi-dimensional benchmark incorporating human evaluation,\nVLM-based assessment, and similarity-based metrics. Experimental results\ndemonstrate that models trained on CI-VID exhibit significant improvements in\nboth accuracy and content consistency when generating video sequences. This\nfacilitates the creation of story-driven content with smooth visual transitions\nand strong temporal coherence, underscoring the quality and practical utility\nof the CI-VID dataset We release the CI-VID dataset and the accompanying code\nfor data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID",
      "url": "http://arxiv.org/abs/2507.01938v1",
      "published_time_eastern_timestamp": 1751478481.0
    },
    {
      "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic\n  Observations",
      "summary": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.",
      "url": "http://arxiv.org/abs/2507.01930v1",
      "published_time_eastern_timestamp": 1751478257.0
    },
    {
      "title": "evMLP: An Efficient Event-Driven MLP Architecture for Vision",
      "summary": "Deep neural networks have achieved remarkable results in computer vision\ntasks. In the early days, Convolutional Neural Networks (CNNs) were the\nmainstream architecture. In recent years, Vision Transformers (ViTs) have\nbecome increasingly popular. In addition, exploring applications of multi-layer\nperceptrons (MLPs) has provided new perspectives for research into vision model\narchitectures. In this paper, we present evMLP accompanied by a simple\nevent-driven local update mechanism. The proposed evMLP can independently\nprocess patches on images or feature maps via MLPs. We define changes between\nconsecutive frames as \"events\". Under the event-driven local update mechanism,\nevMLP selectively processes patches where events occur. For sequential image\ndata (e.g., video processing), this approach improves computational performance\nby avoiding redundant computations. Through ImageNet image classification\nexperiments, evMLP attains accuracy competitive with state-of-the-art models.\nMore significantly, experimental results on multiple video datasets demonstrate\nthat evMLP reduces computational cost via its event-driven local update\nmechanism while maintaining output consistency with its non-event-driven\nbaseline. The code and trained models are available at\nhttps://github.com/i-evi/evMLP.",
      "url": "http://arxiv.org/abs/2507.01927v1",
      "published_time_eastern_timestamp": 1751477810.0
    },
    {
      "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
      "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof \\textit{action tokens} that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
      "url": "http://arxiv.org/abs/2507.01925v1",
      "published_time_eastern_timestamp": 1751477692.0
    },
    {
      "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with\n  Visual Reasoning",
      "summary": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly.",
      "url": "http://arxiv.org/abs/2507.01908v1",
      "published_time_eastern_timestamp": 1751476941.0
    },
    {
      "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
      "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.",
      "url": "http://arxiv.org/abs/2507.01903v1",
      "published_time_eastern_timestamp": 1751476760.0
    },
    {
      "title": "Simulation and analysis of turbulent flame and its effect on the wall of\n  aero engine combustor",
      "summary": "The main objective of this study is to simulate the behavior of the reactive\nflow of the turbulent flame in aeronautical combustion chamber of the\nALLISON-T56 turboprop, and contribute to the analysis of flame structure and\ndetermine for given pressure and temperature of fresh gas the behavior of the\nthermodynamic parameters of combustion. The numerical approach is based on the\nresolution of basic equations of turbulent combustion using Ansys-Fluent code\nwhere the turbulence model K-e is chosen, the geometry of the combustion\nchamber is made using Ansys-workbench software. Thereafter, we simulate the\ntransient temperature field through the wall of a tubular combustion chamber,\nand the characterization of the thermal expansion, the thermoelastic stresses\nand strains with the physical properties of refractory materials. The obtained\nresults are then compared with the results of the scientific literature",
      "url": "http://arxiv.org/abs/2507.01892v1",
      "published_time_eastern_timestamp": 1751475889.0
    },
    {
      "title": "Improving GANs by leveraging the quantum noise from real hardware",
      "summary": "We propose a novel approach to generative adversarial networks (GANs) in\nwhich the standard i.i.d. Gaussian latent prior is replaced or hybridized with\na quantum-correlated prior derived from measurements of a 16-qubit entangling\ncircuit. Each latent sample is generated by grouping repeated shots per qubit\ninto a binary fraction, applying the inverse Gaussian CDF to obtain a\n16-dimensional Gaussian vector whose joint copula reflects genuine quantum\nentanglement, and then projecting into the high-dimensional space via a fixed\nrandom matrix. By pre-sampling tens of millions of bitstrings, either from a\nnoiseless simulator or from IBM hardware, we build large pools of independent\nbut internally quantum-correlated latents. We integrate this prior into three\nrepresentative architectures (WGAN, SNGAN, BigGAN) on CIFAR-10, making no\nchanges to the neural network structure or training hyperparameters. The hybrid\nlatent representations incorporating hardware-derived noise consistently lower\nthe FID relative to both the classical baseline and the simulator variant,\nespecially when the quantum component constitutes a substantial fraction of the\nprior. In addition, we execute on the QPU in parallel to not only save\ncomputing time but also further decrease the FID up to 17% in BigGAN. These\nresults indicate that intrinsic quantum randomness and device-specific\nimperfections can provide a structured inductive bias that enhances GAN\nperformance. Our work demonstrates a practical pipeline for leveraging noisy\nquantum hardware to enrich deep-generative modeling, opening a new interface\nbetween quantum information and machine learning. All code and data are\navailable at https://github.com/Neon8988/GAN_QN.git.",
      "url": "http://arxiv.org/abs/2507.01886v1",
      "published_time_eastern_timestamp": 1751475369.0
    },
    {
      "title": "Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for\n  Semi-Supervised Lifelong Person Re-Identification",
      "summary": "Current lifelong person re-identification (LReID) methods predominantly rely\non fully labeled data streams. However, in real-world scenarios where\nannotation resources are limited, a vast amount of unlabeled data coexists with\nscarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)\nproblem where LReID methods suffer severe performance degradation. Existing\nLReID methods, even when combined with semi-supervised strategies, suffer from\nlimited long-term adaptation performance due to struggling with the noisy\nknowledge occurring during unlabeled data utilization. In this paper, we\npioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing\nPrototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key\ninnovation lies in establishing a self-reinforcing cycle between dynamic\nprototype-guided pseudo-label generation and new-old knowledge collaborative\npurification to enhance the utilization of unlabeled data. Specifically,\nlearnable identity prototypes are introduced to dynamically capture the\nidentity distributions and generate high-quality pseudo-labels. Then, the\ndual-knowledge cooperation scheme integrates current model specialization and\nhistorical model generalization, refining noisy pseudo-labels. Through this\ncyclic design, reliable pseudo-labels are progressively mined to improve\ncurrent-stage learning and ensure positive knowledge propagation over long-term\nlearning. Experiments on the established Semi-LReID benchmarks show that our\nSPRED achieves state-of-the-art performance. Our source code is available at\nhttps://github.com/zhoujiahuan1991/ICCV2025-SPRED",
      "url": "http://arxiv.org/abs/2507.01884v1",
      "published_time_eastern_timestamp": 1751475219.0
    },
    {
      "title": "Generalized ODE reduction algorithm for bounded degree transformation",
      "summary": "The integrability problem of rational first-order ODEs\n$y^{\\prime}=\\frac{M(x,y)}{N(x,y)}$, where $M,N \\in \\mathbb{R}[x,y]$ is a\nlong-term research focus in the area of dynamical systems, physics, etc.\nAlthough the computer algebra system such as Mathematica, Maple has developed\nstandard algorithms to tackle its first integral expressed by Liouvillian or\nspecial function, this problem is quite difficult and the general method\nrequires specifying a tight degree bound for the Darboux polynomial. Computing\nthe bounded degree first integral, in general, is very expensive for a computer\nalgebra system\\cite{duarte2021efficient}\\cite{cheze2020symbolic} and becomes\nimpractical for ODE of large size. In \\cite{huang2025algorithm}, we have\nproposed an algorithm to find the inverse of a local rational transformation $y\n\\to \\frac{A(x,y)}{B(x,y)}$ that transforms a rational ODE to a simpler and more\ntractable structure $y^{\\prime}=\\sum_{i=0}^nf_i(x)y^i$, whose integrability\nunder linear transformation $\\left\\{x \\to F(x),y \\to P(x)y+Q(x)\\right\\}$ can be\ndetected by Maple efficiently \\cite{CHEBTERRAB2000204}\\cite{cheb2000first}. In\nthat paper we have also mentioned when $M(x,y),N(x,y)$ of the reducible\nstructure are not coprime, canceling the common factors in $y$ will alter the\nstructure which makes that algorithm fail. In this paper, we consider this\nissue. We conclude that with the exact tight degree bound for the polynomial\n$A(x,y)$ given, we have an efficient algorithm to compute such transformation\nand the reduced ODE for \"quite a lot of\" cases where $M,N$ are not coprime. We\nhave also implemented this algorithm in Maple and the code is available in\nresearchgate.",
      "url": "http://arxiv.org/abs/2507.01878v1",
      "published_time_eastern_timestamp": 1751474893.0
    },
    {
      "title": "A hierarchical invariant for line bundles and its applications in\n  algebraic geometry codes",
      "summary": "We introduce the notion of hierarchical depth for line bundles on smooth\nprojective surfaces, defined via filtrations by line subbundles with successive\nquotients supported on effective divisors. This invariant helps to investigate\nboth the algebraic and geometric complexity of line bundles through discrete\nstepwise constructions. We study some of its basic properties, including\nfunctorial behavior under restriction to curves and compatibility with\nampleness and base-point freeness. Applying this framework to algebraic\ngeometry (AG) codes, we show that hierarchical filtrations yield natural code\nfamilies whose combinatorial parameters (dimension, minimum distance) evolve\npredictably across the filtration.",
      "url": "http://arxiv.org/abs/2507.01859v1",
      "published_time_eastern_timestamp": 1751473291.0
    },
    {
      "title": "MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time\n  Image Enhancement on Mobile Devices",
      "summary": "Recent advancements in deep neural networks have driven significant progress\nin image enhancement (IE). However, deploying deep learning models on\nresource-constrained platforms, such as mobile devices, remains challenging due\nto high computation and memory demands. To address these challenges and\nfacilitate real-time IE on mobile, we introduce an extremely lightweight\nConvolutional Neural Network (CNN) framework with around 4K parameters. Our\napproach integrates reparameterization with an Incremental Weight Optimization\nstrategy to ensure efficiency. Additionally, we enhance performance with a\nFeature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,\noptimized with a Local Variance-Weighted loss. With this efficient framework,\nwe are the first to achieve real-time IE inference at up to 1,100 frames per\nsecond (FPS) while delivering competitive image quality, achieving the best\ntrade-off between speed and performance across multiple IE tasks. The code will\nbe available at https://github.com/AVC2-UESTC/MobileIE.git.",
      "url": "http://arxiv.org/abs/2507.01838v1",
      "published_time_eastern_timestamp": 1751471624.0
    },
    {
      "title": "Autoadaptive Medical Segment Anything Model",
      "summary": "Medical image segmentation is a key task in the imaging workflow, influencing\nmany image-based decisions. Traditional, fully-supervised segmentation models\nrely on large amounts of labeled training data, typically obtained through\nmanual annotation, which can be an expensive, time-consuming, and error-prone\nprocess. This signals a need for accurate, automatic, and annotation-efficient\nmethods of training these models. We propose ADA-SAM (automated,\ndomain-specific, and adaptive segment anything model), a novel multitask\nlearning framework for medical image segmentation that leverages class\nactivation maps from an auxiliary classifier to guide the predictions of the\nsemi-supervised segmentation branch, which is based on the Segment Anything\n(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient\nfeedback mechanism to create a learnable connection between the segmentation\nand classification branches by using the segmentation gradients to guide and\nimprove the classification predictions. We validate ADA-SAM on real-world\nclinical data collected during rehabilitation trials, and demonstrate that our\nproposed method outperforms both fully-supervised and semi-supervised baselines\nby double digits in limited label settings. Our code is available at:\nhttps://github.com/tbwa233/ADA-SAM.",
      "url": "http://arxiv.org/abs/2507.01828v1",
      "published_time_eastern_timestamp": 1751471072.0
    },
    {
      "title": "TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning\n  Agents",
      "summary": "We present a novel approach to knowledge transfer in model-based\nreinforcement learning, addressing the critical challenge of deploying large\nworld models in resource-constrained environments. Our method efficiently\ndistills a high-capacity multi-task agent (317M parameters) into a compact\nmodel (1M parameters) on the MT30 benchmark, significantly improving\nperformance across diverse tasks. Our distilled model achieves a\nstate-of-the-art normalized score of 28.45, surpassing the original 1M\nparameter model score of 18.93. This improvement demonstrates the ability of\nour distillation technique to capture and consolidate complex multi-task\nknowledge. We further optimize the distilled model through FP16 post-training\nquantization, reducing its size by $\\sim$50\\%. Our approach addresses practical\ndeployment limitations and offers insights into knowledge representation in\nlarge world models, paving the way for more efficient and accessible multi-task\nreinforcement learning systems in robotics and other resource-constrained\napplications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.",
      "url": "http://arxiv.org/abs/2507.01823v1",
      "published_time_eastern_timestamp": 1751470729.0
    },
    {
      "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding",
      "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.",
      "url": "http://arxiv.org/abs/2507.01802v1",
      "published_time_eastern_timestamp": 1751469689.0
    },
    {
      "title": "HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing\n  Supervision",
      "summary": "3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the\nphysical world and perform spatial reasoning. Answer-centric supervision is a\ncommonly used training method for 3D VQA models. Many models that utilize this\nstrategy have achieved promising results in 3D VQA tasks. However, the\nanswer-centric approach only supervises the final output of models and allows\nmodels to develop reasoning pathways freely. The absence of supervision on the\nreasoning pathway enables the potential for developing superficial shortcuts\nthrough common patterns in question-answer pairs. Moreover, although\nslow-thinking methods advance large language models, they suffer from\nunderthinking. To address these issues, we propose \\textbf{HCNQA}, a 3D VQA\nmodel leveraging a hierarchical concentration narrowing supervision method. By\nmimicking the human process of gradually focusing from a broad area to specific\nobjects while searching for answers, our method guides the model to perform\nthree phases of concentration narrowing through hierarchical supervision. By\nsupervising key checkpoints on a general reasoning pathway, our method can\nensure the development of a rational and effective reasoning pathway. Extensive\nexperimental results demonstrate that our method can effectively ensure that\nthe model develops a rational reasoning pathway and performs better. The code\nis available at https://github.com/JianuoZhu/HCNQA.",
      "url": "http://arxiv.org/abs/2507.01800v1",
      "published_time_eastern_timestamp": 1751469608.0
    },
    {
      "title": "How Do Vision-Language Models Process Conflicting Information Across\n  Modalities?",
      "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.",
      "url": "http://arxiv.org/abs/2507.01790v1",
      "published_time_eastern_timestamp": 1751469314.0
    },
    {
      "title": "Reconfigurable Intelligent Surface aided\n  Integrated-Navigation-and-Communication in Urban Canyons: A Satellite\n  Selection Approach",
      "summary": "This study investigates the application of a simultaneous transmitting and\nreflecting reconfigurable intelligent surface (STAR-RIS)-aided\nmedium-Earth-orbit (MEO) satellite network for providing both global\npositioning services and communication services in the urban canyons, where the\ndirect satellite-user links are obstructed. Superposition coding (SC) and\nsuccessive interference cancellation (SIC) techniques are utilized for the\nintegrated navigation and communication (INAC) networks, and the composed\nnavigation and communication signals are reflected or transmitted to ground\nusers or indoor users located in urban canyons. To meet diverse application\nneeds, navigation-oriented (NO)-INAC and communication-oriented (CO)-INAC have\nbeen developed, each tailored according to distinct power allocation factors.\nWe then proposed two algorithms, namely navigation-prioritized-algorithm (NPA)\nand communication-prioritized-algorithm (CPA), to improve the navigation or\ncommunication performance by selecting the satellite with the optimized\nposition dilution of precision (PDoP) or with the best channel gain. The\neffectiveness of the proposed STAR-RIS-aided INAC network is quantified by\nanalyzing the positioning error for navigation services and by evaluating\ncommunication performance through achievable ergodic rate metrics. Our\nsatellite selection approach indicates that: the positioning services at the\nurban canyon users can be completed with the aid of STAR-RIS. 2) Additionally,\nit is observed that while a single STAR-RIS array can extend the navigational\nlink, it fails to serve users in indoor scenarios, highlighting a limitation in\nthe current system design.",
      "url": "http://arxiv.org/abs/2507.01766v1",
      "published_time_eastern_timestamp": 1751467618.0
    },
    {
      "title": "First Steps Towards Voice Anonymization for Code-Switching Speech",
      "summary": "The goal of voice anonymization is to modify an audio such that the true\nidentity of its speaker is hidden. Research on this task is typically limited\nto the same English read speech datasets, thus the efficacy of current methods\nfor other types of speech data remains unknown. In this paper, we present the\nfirst investigation of voice anonymization for the multilingual phenomenon of\ncode-switching speech. We prepare two corpora for this task and propose\nadaptations to a multilingual anonymization model to make it applicable for\ncode-switching speech. By testing the anonymization performance of this and two\nlanguage-independent methods on the datasets, we find that only the\nmultilingual system performs well in terms of privacy and utility preservation.\nFurthermore, we observe challenges in performing utility evaluations on this\ndata because of its spontaneous character and the limited code-switching\nsupport by the multilingual speech recognition model.",
      "url": "http://arxiv.org/abs/2507.01765v1",
      "published_time_eastern_timestamp": 1751467604.0
    }
  ]
}
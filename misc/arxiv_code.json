{
  "last_updated": "2025-09-11T02:17:42.816233-04:00",
  "papers": [
    {
      "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
      "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
      "url": "http://arxiv.org/abs/2509.08827v1",
      "published_time_eastern_timestamp": 1757527183.0
    },
    {
      "title": "Fluid Antenna Systems: A Geometric Approach to Error Probability and\n  Fundamental Limits",
      "summary": "The fluid antenna system (FAS) concept is an emerging paradigm that promotes\nthe utilization of the feature of shape and position reconfigurability in\nantennas to broaden the design of wireless communication systems. This also\nmeans that spatial diversity can be exploited in an unconventional way.\nHowever, a rigorous framework for error probability analysis of FAS under\nrealistic spatially correlated channels has been lacking. In this paper, we\nfill this gap by deriving a tight, closed-form asymptotic expression for the\nsymbol error rate (SER) that establishes the fundamental scaling law linking\nthe system's SER to the channel's spatial correlation structure. A key insight\nof our analysis is that the achievable diversity gain is governed not by the\nnumber of antenna ports, but by the channel's effective rank. To find this\ncritical parameter, we propose a novel dual-pronged approach. First of all, we\ndevelop a geometry-based algorithm that extracts distinct performance\nthresholds from the channel's eigenvalue spectrum. Second, we theoretically\nprove that the effective rank converges to a fundamental limit dictated solely\nby the antenna's normalized aperture width. We further establish the\nequivalence between the threshold identified by the geometric algorithm and the\nderived theoretical limit, providing rigorous validation for the proposed\nmethod. Our effective rank model achieves higher accuracy than existing\napproaches in the literature. Building on this framework, we offer a complete\ncharacterization of diversity and coding gains. The analysis leads to a\ndefinitive design insight: FAS performance improvements are fundamentally\ndriven by enlarging the antenna's explorable aperture, which increases the\neffective channel rank, whereas increasing port density within a fixed aperture\nyields diminishing returns.",
      "url": "http://arxiv.org/abs/2509.08815v1",
      "published_time_eastern_timestamp": 1757526452.0
    },
    {
      "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages",
      "summary": "Subword-based tokenization methods often fail to preserve morphological\nboundaries, a limitation especially pronounced in low-resource, morphologically\ncomplex languages such as those written in the Geez script. To address this, we\npresent MoVoC (Morpheme-aware Subword Vocabulary Construction) and train\nMoVoC-Tok, a tokenizer that integrates supervised morphological analysis into\nthe subword vocabulary. This hybrid segmentation approach combines\nmorpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological\nintegrity while maintaining lexical meaning. To tackle resource scarcity, we\ncurate and release manually annotated morpheme data for four Geez script\nlanguages and a morpheme-aware vocabulary for two of them. While the proposed\ntokenization method does not lead to significant gains in automatic translation\nquality, we observe consistent improvements in intrinsic metrics, MorphoScore,\nand Boundary Precision, highlighting the value of morphology-aware segmentation\nin enhancing linguistic fidelity and token efficiency. Our morpheme-annotated\ndatasets and tokenizer will be publicly available to support further research\nin low-resource, morphologically rich languages. Our code and data are\navailable on GitHub: https://github.com/hailaykidu/MoVoC",
      "url": "http://arxiv.org/abs/2509.08812v1",
      "published_time_eastern_timestamp": 1757526310.0
    },
    {
      "title": "Handling Open-Vocabulary Constructs in Formalizing Specifications:\n  Retrieval-Augmented Parsing with Expert Knowledge",
      "summary": "We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known\nbeforehand -- in the context of converting natural language (NL) specifications\ninto formal languages (e.g., temporal logic or code). Models fare poorly on\nOVCs due to a lack of necessary knowledge a priori. In such situations, a\ndomain expert can provide correct constructs at inference time based on their\npreferences or domain knowledge. Our goal is to effectively reuse this\ninference-time, expert-provided knowledge for future parses without retraining\nthe model. We present dynamic knowledge-augmented parsing(DKAP), where in\naddition to the input sentence, the model receives (dynamically growing) expert\nknowledge as a key-value lexicon that associates NL phrases with correct OVC\nconstructs. We propose ROLex, a retrieval-augmented parsing approach that uses\nthis lexicon. A retriever and a generator are trained to find and use the\nkey-value store to produce the correct parse. A key challenge lies in curating\ndata for this retrieval-augmented parser. We utilize synthetic data generation\nand the data augmentation techniques on annotated (NL sentence, FL statement)\npairs to train the augmented parser. To improve training effectiveness, we\npropose multiple strategies to teach models to focus on the relevant subset of\nretrieved knowledge. Finally, we introduce a new evaluation paradigm modeled\nafter the DKAP problem and simulate the scenario across three formalization\ntasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a\ndifficult challenge, and ROLex helps improve the performance of baseline models\nby using dynamic expert knowledge effectively.",
      "url": "http://arxiv.org/abs/2509.08808v1",
      "published_time_eastern_timestamp": 1757526068.0
    },
    {
      "title": "Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation",
      "summary": "Event-based cameras (EBCs) are a promising new technology for star\ntracking-based attitude determination, but prior studies have struggled to\ndetermine accurate ground truth for real data. We analyze the accuracy of an\nEBC star tracking system utilizing the Earth's motion as the ground truth for\ncomparison. The Earth rotates in a regular way with very small irregularities\nwhich are measured to the level of milli-arcseconds. By keeping an event camera\nstatic and pointing it through a ground-based telescope at the night sky, we\ncreate a system where the only camera motion in the celestial reference frame\nis that induced by the Earth's rotation. The resulting event stream is\nprocessed to generate estimates of orientation which we compare to the\nInternational Earth Rotation and Reference System (IERS) measured orientation\nof the Earth. The event camera system is able to achieve a root mean squared\nacross error of 18.47 arcseconds and an about error of 78.84 arcseconds.\nCombined with the other benefits of event cameras over framing sensors (reduced\ncomputation due to sparser data streams, higher dynamic range, lower energy\nconsumption, faster update rates), this level of accuracy suggests the utility\nof event cameras for low-cost and low-latency star tracking. We provide all\ncode and data used to generate our results:\nhttps://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.",
      "url": "http://arxiv.org/abs/2509.08794v1",
      "published_time_eastern_timestamp": 1757525050.0
    },
    {
      "title": "CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks\n  with Entropy Regularization",
      "summary": "Massive MIMO systems rely on accurate Channel State Information (CSI)\nfeedback to enable high-gain beam-forming. However, the feedback overhead\nscales linearly with the number of antennas, presenting a major bottleneck.\nWhile recent deep learning methods have improved CSI compression, most overlook\nthe impact of quantization and entropy coding, limiting their practical\ndeployability. In this work, we propose an end-to-end CSI compression framework\nthat integrates a Spatial Correlation-Guided Attention Mechanism with\nquantization and entropy-aware training. Our model effectively exploits the\nspatial correlation among the antennas, thereby learning compact,\nentropy-optimized latent representations for efficient coding. This reduces the\nrequired feedback bitrates without sacrificing reconstruction accuracy, thereby\nyielding a superior rate-distortion trade-off. Experiments show that our method\nsurpasses existing end-to-end CSI compression schemes, exceeding benchmark\nperformance by an average of 21.5% on indoor datasets and 18.9% on outdoor\ndatasets. The proposed framework results in a practical and efficient CSI\nfeedback scheme.",
      "url": "http://arxiv.org/abs/2509.08776v1",
      "published_time_eastern_timestamp": 1757523953.0
    },
    {
      "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation",
      "summary": "Robot navigation in dynamic, human-centered environments requires\nsocially-compliant decisions grounded in robust scene understanding. Recent\nVision-Language Models (VLMs) exhibit promising capabilities such as object\nrecognition, common-sense reasoning, and contextual understanding-capabilities\nthat align with the nuanced requirements of social robot navigation. However,\nit remains unclear whether VLMs can accurately understand complex social\nnavigation scenes (e.g., inferring the spatial-temporal relations among agents\nand human intentions), which is essential for safe and socially compliant robot\nnavigation. While some recent works have explored the use of VLMs in social\nrobot navigation, no existing work systematically evaluates their ability to\nmeet these necessary conditions. In this paper, we introduce the Social\nNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question\nAnswering (VQA) dataset and benchmark designed to evaluate VLMs for scene\nunderstanding in real-world social robot navigation scenarios. SocialNav-SUB\nprovides a unified framework for evaluating VLMs against human and rule-based\nbaselines across VQA tasks requiring spatial, spatiotemporal, and social\nreasoning in social robot navigation. Through experiments with state-of-the-art\nVLMs, we find that while the best-performing VLM achieves an encouraging\nprobability of agreeing with human answers, it still underperforms simpler\nrule-based approach and human consensus baselines, indicating critical gaps in\nsocial scene understanding of current VLMs. Our benchmark sets the stage for\nfurther research on foundation models for social robot navigation, offering a\nframework to explore how VLMs can be tailored to meet real-world social robot\nnavigation needs. An overview of this paper along with the code and data can be\nfound at https://larg.github.io/socialnav-sub .",
      "url": "http://arxiv.org/abs/2509.08757v1",
      "published_time_eastern_timestamp": 1757522820.0
    },
    {
      "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
      "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
      "url": "http://arxiv.org/abs/2509.08755v1",
      "published_time_eastern_timestamp": 1757522771.0
    },
    {
      "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling",
      "summary": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling",
      "url": "http://arxiv.org/abs/2509.08753v1",
      "published_time_eastern_timestamp": 1757522581.0
    },
    {
      "title": "CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection\n  in Crowded Scenes",
      "summary": "This paper introduces a novel method for end-to-end crowd detection that\nleverages object density information to enhance existing transformer-based\ndetectors. We present CrowdQuery (CQ), whose core component is our CQ module\nthat predicts and subsequently embeds an object density map. The embedded\ndensity information is then systematically integrated into the decoder.\nExisting density map definitions typically depend on head positions or\nobject-based spatial statistics. Our method extends these definitions to\ninclude individual bounding box dimensions. By incorporating density\ninformation into object queries, our method utilizes density-guided queries to\nimprove detection in crowded scenes. CQ is universally applicable to both 2D\nand 3D detection without requiring additional data. Consequently, we are the\nfirst to design a method that effectively bridges 2D and 3D detection in\ncrowded environments. We demonstrate the integration of CQ into both a general\n2D and 3D transformer-based object detector, introducing the architectures CQ2D\nand CQ3D. CQ is not limited to the specific transformer models we selected.\nExperiments on the STCrowd dataset for both 2D and 3D domains show significant\nperformance improvements compared to the base models, outperforming most\nstate-of-the-art methods. When integrated into a state-of-the-art crowd\ndetector, CQ can further improve performance on the challenging CrowdHuman\ndataset, demonstrating its generalizability. The code is released at\nhttps://github.com/mdaehl/CrowdQuery.",
      "url": "http://arxiv.org/abs/2509.08738v1",
      "published_time_eastern_timestamp": 1757521536.0
    },
    {
      "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to\n  Single-turn Jailbreak Templates",
      "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.",
      "url": "http://arxiv.org/abs/2509.08729v1",
      "published_time_eastern_timestamp": 1757521064.0
    },
    {
      "title": "Securing Cryptographic Software via Typed Assembly Language (Extended\n  Version)",
      "summary": "Authors of cryptographic software are well aware that their code should not\nleak secrets through its timing behavior, and, until 2018, they believed that\nfollowing industry-standard constant-time coding guidelines was sufficient.\nHowever, the revelation of the Spectre family of speculative execution attacks\ninjected new complexities.\n  To block speculative attacks, prior work has proposed annotating the\nprogram's source code to mark secret data, with hardware using this information\nto decide when to speculate (i.e., when only public values are involved) or not\n(when secrets are in play). While these solutions are able to track secret\ninformation stored on the heap, they suffer from limitations that prevent them\nfrom correctly tracking secrets on the stack, at a cost in performance.\n  This paper introduces SecSep, a transformation framework that rewrites\nassembly programs so that they partition secret and public data on the stack.\nBy moving from the source-code level to assembly rewriting, SecSep is able to\naddress limitations of prior work. The key challenge in performing this\nassembly rewriting stems from the loss of semantic information through the\nlengthy compilation process. The key innovation of our methodology is a new\nvariant of typed assembly language (TAL), Octal, which allows us to address\nthis challenge. Assembly rewriting is driven by compile-time inference within\nOctal. We apply our technique to cryptographic programs and demonstrate that it\nenables secure speculation efficiently, incurring a low average overhead of\n$1.2\\%$.",
      "url": "http://arxiv.org/abs/2509.08727v1",
      "published_time_eastern_timestamp": 1757521051.0
    },
    {
      "title": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across\n  Repositories",
      "summary": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach.",
      "url": "http://arxiv.org/abs/2509.08724v1",
      "published_time_eastern_timestamp": 1757520923.0
    },
    {
      "title": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated\n  Cross-Modal Fusion",
      "summary": "As multimodal large language models (MLLMs) advance, their large-scale\narchitectures pose challenges for deployment in resource-constrained\nenvironments. In the age of large models, where energy efficiency,\ncomputational scalability and environmental sustainability are paramount, the\ndevelopment of lightweight and high-performance models is critical for\nreal-world applications. As such, we propose a lightweight MLLM framework for\nend-to-end visual question answering. Our proposed approach centres on\nBreezeCLIP, a compact yet powerful vision-language encoder optimised for\nefficient multimodal understanding. With only 1.2 billion parameters overall,\nour model significantly reduces computational cost while achieving performance\ncomparable to standard-size MLLMs. Experiments conducted on multiple datasets\nfurther validate its effectiveness in balancing accuracy and efficiency. The\nmodular and extensible design enables generalisation to broader multimodal\ntasks. The proposed lightweight vision-language framework is denoted as BcQLM\n(BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising\npath toward deployable MLLMs under practical hardware constraints. The source\ncode is available at https://github.com/thico0224/BcQLM.",
      "url": "http://arxiv.org/abs/2509.08715v1",
      "published_time_eastern_timestamp": 1757520589.0
    },
    {
      "title": "The More You Automate, the Less You See: Hidden Pitfalls of AI Scientist\n  Systems",
      "summary": "AI scientist systems, capable of autonomously executing the full research\nworkflow from hypothesis generation and experimentation to paper writing, hold\nsignificant potential for accelerating scientific discovery. However, the\ninternal workflow of these systems have not been closely examined. This lack of\nscrutiny poses a risk of introducing flaws that could undermine the integrity,\nreliability, and trustworthiness of their research outputs. In this paper, we\nidentify four potential failure modes in contemporary AI scientist systems:\ninappropriate benchmark selection, data leakage, metric misuse, and post-hoc\nselection bias. To examine these risks, we design controlled experiments that\nisolate each failure mode while addressing challenges unique to evaluating AI\nscientist systems. Our assessment of two prominent open-source AI scientist\nsystems reveals the presence of several failures, across a spectrum of\nseverity, which can be easily overlooked in practice. Finally, we demonstrate\nthat access to trace logs and code from the full automated workflow enables far\nmore effective detection of such failures than examining the final paper alone.\nWe thus recommend journals and conferences evaluating AI-generated research to\nmandate submission of these artifacts alongside the paper to ensure\ntransparency, accountability, and reproducibility.",
      "url": "http://arxiv.org/abs/2509.08713v1",
      "published_time_eastern_timestamp": 1757520264.0
    },
    {
      "title": "Tokenizing Loops of Antibodies",
      "summary": "The complementarity-determining regions of antibodies are loop structures\nthat are key to their interactions with antigens, and of high importance to the\ndesign of novel biologics. Since the 1980s, categorizing the diversity of CDR\nstructures into canonical clusters has enabled the identification of key\nstructural motifs of antibodies. However, existing approaches have limited\ncoverage and cannot be readily incorporated into protein foundation models.\nHere we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibody\nloop tokenizer that encodes backbone dihedral angles and sequence. Igloo is\ntrained using a contrastive learning objective to map loops with similar\nbackbone dihedral angles closer together in latent space. Igloo can efficiently\nretrieve the closest matching loop structures from a structural antibody\ndatabase, outperforming existing methods on identifying similar H3 loops by\n5.9\\%. Igloo assigns tokens to all loops, addressing the limited coverage issue\nof canonical clusters, while retaining the ability to recover canonical loop\nconformations. To demonstrate the versatility of Igloo tokens, we show that\nthey can be incorporated into protein language models with IglooLM and\nIglooALM. On predicting binding affinity of heavy chain variants, IglooLM\noutperforms the base protein language model on 8 out of 10 antibody-antigen\ntargets. Additionally, it is on par with existing state-of-the-art\nsequence-based and multimodal protein language models, performing comparably to\nmodels with $7\\times$ more parameters. IglooALM samples antibody loops which\nare diverse in sequence and more consistent in structure than state-of-the-art\nantibody inverse folding models. Igloo demonstrates the benefit of introducing\nmultimodal tokens for antibody loops for encoding the diverse landscape of\nantibody loops, improving protein foundation models, and for antibody CDR\ndesign.",
      "url": "http://arxiv.org/abs/2509.08707v1",
      "published_time_eastern_timestamp": 1757519779.0
    },
    {
      "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals",
      "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D\nmaps or learned controllers, which can be computationally expensive and\ndifficult to generalize across diverse environments. In this work, we present a\nnovel RGB-only, object-level topometric navigation pipeline that enables\nzero-shot, long-horizon robot navigation without requiring 3D maps or\npre-trained controllers. Our approach integrates global topological path\nplanning with local metric trajectory control, allowing the robot to navigate\ntowards object-level sub-goals while avoiding obstacles. We address key\nlimitations of previous methods by continuously predicting local trajectory\nusing monocular depth and traversability estimation, and incorporating an\nauto-switching mechanism that falls back to a baseline controller when\nnecessary. The system operates using foundational models, ensuring open-set\napplicability without the need for domain-specific fine-tuning. We demonstrate\nthe effectiveness of our method in both simulated environments and real-world\ntests, highlighting its robustness and deployability. Our approach outperforms\nexisting state-of-the-art methods, offering a more adaptable and effective\nsolution for visual navigation in open-set environments. The source code is\nmade publicly available: https://github.com/podgorki/TANGO.",
      "url": "http://arxiv.org/abs/2509.08699v1",
      "published_time_eastern_timestamp": 1757519012.0
    },
    {
      "title": "Multi-Modal Robust Enhancement for Coastal Water Segmentation: A\n  Systematic HSV-Guided Framework",
      "summary": "Coastal water segmentation from satellite imagery presents unique challenges\ndue to complex spectral characteristics and irregular boundary patterns.\nTraditional RGB-based approaches often suffer from training instability and\npoor generalization in diverse maritime environments. This paper introduces a\nsystematic robust enhancement framework, referred to as Robust U-Net, that\nleverages HSV color space supervision and multi-modal constraints for improved\ncoastal water segmentation. Our approach integrates five synergistic\ncomponents: HSV-guided color supervision, gradient-based coastline\noptimization, morphological post-processing, sea area cleanup, and connectivity\ncontrol. Through comprehensive ablation studies, we demonstrate that HSV\nsupervision provides the highest impact (0.85 influence score), while the\ncomplete framework achieves superior training stability (84\\% variance\nreduction) and enhanced segmentation quality. Our method shows consistent\nimprovements across multiple evaluation metrics while maintaining computational\nefficiency. For reproducibility, our training configurations and code are\navailable here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.",
      "url": "http://arxiv.org/abs/2509.08694v1",
      "published_time_eastern_timestamp": 1757518486.0
    },
    {
      "title": "Spatial-Spectral Chromatic Coding of Interference Signatures in SAR\n  Imagery: Signal Modeling and Physical-Visual Interpretation",
      "summary": "Synthetic Aperture Radar (SAR) images are conventionally visualized as\ngrayscale amplitude representations, which often fail to explicitly reveal\ninterference characteristics caused by external radio emitters and unfocused\nsignals. This paper proposes a novel spatial-spectral chromatic coding method\nfor visual analysis of interference patterns in single-look complex (SLC) SAR\nimagery. The method first generates a series of spatial-spectral images via\nspectral subband decomposition that preserve both spatial structures and\nspectral signatures. These images are subsequently chromatically coded into a\ncolor representation using RGB/HSV dual-space coding, using a set of\nspecifically designed color palette. This method intrinsically encodes the\nspatial-spectral properties of interference into visually discernible patterns,\nenabling rapid visual interpretation without additional processing. To\nfacilitate physical interpretation, mathematical models are established to\ntheoretically analyze the physical mechanisms of responses to various\ninterference types. Experiments using real datasets demonstrate that the method\neffectively highlights interference regions and unfocused echo or signal\nresponses (e.g., blurring, ambiguities, and moving target effects), providing\nanalysts with a practical tool for visual interpretation, quality assessment,\nand data diagnosis in SAR imagery.",
      "url": "http://arxiv.org/abs/2509.08693v1",
      "published_time_eastern_timestamp": 1757518464.0
    },
    {
      "title": "Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute\n  Coding",
      "summary": "Given encoded 3D point cloud geometry available at the decoder, we study the\nproblem of lossy attribute compression in a multi-resolution B-spline\nprojection framework. A target continuous 3D attribute function is first\nprojected onto a sequence of nested subspaces $\\mathcal{F}^{(p)}_{l_0}\n\\subseteq \\cdots \\subseteq \\mathcal{F}^{(p)}_{L}$, where\n$\\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basis\nfunction of order $p$ at a chosen scale and its integer shifts. The projected\nlow-pass coefficients $F_l^*$ are computed by variable-complexity unrolling of\na rate-distortion (RD) optimization algorithm into a feed-forward network,\nwhere the rate term is the sparsity-promoting $\\ell_1$-norm. Thus, the\nprojection operation is end-to-end differentiable. For a chosen coarse-to-fine\npredictor, the coefficients are then adjusted to account for the prediction\nfrom a lower-resolution to a higher-resolution, which is also optimized in a\ndata-driven manner.",
      "url": "http://arxiv.org/abs/2509.08685v1",
      "published_time_eastern_timestamp": 1757517801.0
    }
  ]
}
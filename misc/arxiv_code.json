{
  "last_updated": "2025-10-18T05:11:13.505588-04:00",
  "papers": [
    {
      "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at\n  Scale",
      "summary": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising\ncontender to typical modular VLMs, shaped by evolving model architectures and\ntraining paradigms. Yet, two lingering clouds cast shadows over its widespread\nexploration and promotion: (-) What fundamental constraints set native VLMs\napart from modular ones, and to what extent can these barriers be overcome? (-)\nHow to make research in native VLMs more accessible and democratized, thereby\naccelerating progress in the field. In this paper, we clarify these challenges\nand outline guiding principles for constructing native VLMs. Specifically, one\nnative VLM primitive should: (i) effectively align pixel and word\nrepresentations within a shared semantic space; (ii) seamlessly integrate the\nstrengths of formerly separate vision and language modules; (iii) inherently\nembody various cross-modal properties that support unified vision-language\nencoding, aligning, and reasoning. Hence, we launch NEO, a novel family of\nnative VLMs built from first principles, capable of rivaling top-tier modular\ncounterparts across diverse real-world scenarios. With only 390M image-text\nexamples, NEO efficiently develops visual perception from scratch while\nmitigating vision-language conflicts inside a dense and monolithic model\ncrafted from our elaborate primitives. We position NEO as a cornerstone for\nscalable and powerful native VLMs, paired with a rich set of reusable\ncomponents that foster a cost-effective and extensible ecosystem. Our code and\nmodels are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
      "url": "http://arxiv.org/abs/2510.14979v1",
      "published_time_eastern_timestamp": 1760637598.0
    },
    {
      "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
      "summary": "Identity-consistent generation has become an important focus in text-to-image\nresearch, with recent models achieving notable success in producing images\naligned with a reference identity. Yet, the scarcity of large-scale paired\ndatasets containing multiple images of the same individual forces most\napproaches to adopt reconstruction-based training. This reliance often leads to\na failure mode we term copy-paste, where the model directly replicates the\nreference face rather than preserving identity across natural variations in\npose, expression, or lighting. Such over-similarity undermines controllability\nand limits the expressive power of generation. To address these limitations, we\n(1) construct a large-scale paired dataset MultiID-2M, tailored for\nmulti-person scenarios, providing diverse references for each identity; (2)\nintroduce a benchmark that quantifies both copy-paste artifacts and the\ntrade-off between identity fidelity and variation; and (3) propose a novel\ntraining paradigm with a contrastive identity loss that leverages paired data\nto balance fidelity with diversity. These contributions culminate in\nWithAnyone, a diffusion-based model that effectively mitigates copy-paste while\npreserving high identity similarity. Extensive qualitative and quantitative\nexperiments demonstrate that WithAnyone significantly reduces copy-paste\nartifacts, improves controllability over pose and expression, and maintains\nstrong perceptual quality. User studies further validate that our method\nachieves high identity fidelity while enabling expressive controllable\ngeneration.",
      "url": "http://arxiv.org/abs/2510.14975v1",
      "published_time_eastern_timestamp": 1760637594.0
    },
    {
      "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
      "summary": "Few-step diffusion or flow-based generative models typically distill a\nvelocity-predicting teacher into a student that predicts a shortcut towards\ndenoised data. This format mismatch has led to complex distillation procedures\nthat often suffer from a quality-diversity trade-off. To address this, we\npropose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output\nlayer of a student flow model to predict a network-free policy at one timestep.\nThe policy then produces dynamic flow velocities at future substeps with\nnegligible overhead, enabling fast and accurate ODE integration on these\nsubsteps without extra network evaluations. To match the policy's ODE\ntrajectory to the teacher's, we introduce a novel imitation distillation\napproach, which matches the policy's velocity to the teacher's along the\npolicy's trajectory using a standard $\\ell_2$ flow matching loss. By simply\nmimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable\ntraining and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it\nattains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT\narchitecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves\nsubstantially better diversity than state-of-the-art few-step methods, while\nmaintaining teacher-level quality.",
      "url": "http://arxiv.org/abs/2510.14974v1",
      "published_time_eastern_timestamp": 1760637591.0
    },
    {
      "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
      "summary": "This work studies how to adaptively recompute key-value (KV) caches for\ndiffusion large language models (DLMs) to maximize prediction accuracy while\nminimizing decoding latency. Prior methods' decoders recompute QKV for all\ntokens at every denoising step and layer, despite KV states changing little\nacross most steps, especially in shallow layers, leading to substantial\nredundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens\nprimarily act as a length-bias and can be cached block-wise beyond the active\nprediction window; (2) KV dynamics increase with depth, suggesting that\nselective refresh starting from deeper layers is sufficient; and (3) the\nmost-attended token exhibits the smallest KV drift, providing a conservative\nlower bound on cache change for other tokens. Building on these, we propose\n${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that\njointly decides ${when}$ to refresh (via an attention-aware drift test on the\nmost-attended token) and ${where}$ to refresh (via a depth-aware schedule that\nrecomputes from a chosen layer onward while reusing shallow-layer caches and\noff-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs\nadaptive, layer-aware cache updates for diffusion LLMs, reducing redundant\ncomputation and accelerating decoding with negligible loss in generation\nquality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across\nmathematical reasoning and code generation tasks demonstrate consistent\nspeedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences,\nand $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy\nthan the baseline. Our method achieves significantly higher throughput\n($6.8\\times$ on GSM8K) than existing confidence-based approaches while\npreserving generation quality, enabling practical deployment of diffusion LLMs.",
      "url": "http://arxiv.org/abs/2510.14973v1",
      "published_time_eastern_timestamp": 1760637588.0
    },
    {
      "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
      "summary": "Large language models (LLMs) for code rely on subword tokenizers, such as\nbyte-pair encoding (BPE), learned from mixed natural language text and\nprogramming language code but driven by statistics rather than grammar. As a\nresult, semantically identical code snippets can be tokenized differently\ndepending on superficial factors such as whitespace or identifier naming. To\nmeasure the impact of this misalignment, we introduce TokDrift, a framework\nthat applies semantic-preserving rewrite rules to create code variants\ndiffering only in tokenization. Across nine code LLMs, including large ones\nwith over 30B parameters, even minor formatting changes can cause substantial\nshifts in model behavior. Layer-wise analysis shows that the issue originates\nin early embeddings, where subword segmentation fails to capture grammar token\nboundaries. Our findings identify misaligned tokenization as a hidden obstacle\nto reliable code understanding and generation, highlighting the need for\ngrammar-aware tokenization for future code LLMs.",
      "url": "http://arxiv.org/abs/2510.14972v1",
      "published_time_eastern_timestamp": 1760637585.0
    },
    {
      "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent\n  Training",
      "summary": "Digital agents require diverse, large-scale UI trajectories to generalize\nacross real-world tasks, yet collecting such data is prohibitively expensive in\nboth human annotation, infra and engineering perspectives. To this end, we\nintroduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates\nstructured UI states and transitions to synthesize training trajectories at\nscale. Our paradigm integrates a digital world simulator for diverse UI states,\na guided rollout process for coherent exploration, and a trajectory wrapper\nthat produces high-quality and diverse trajectories for agent training. We\nfurther propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that\nenables more rapid and data-efficient scaling by prioritizing high-impact tasks\nand synthesizes informative trajectory variants. Experiments on WebArena and\nAndroidWorld show that UI-Simulator rivals or surpasses open-source agents\ntrained on real UIs with significantly better robustness, despite using weaker\nteacher models. Moreover, UI-Simulator-Grow matches the performance of\nLlama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model,\nhighlighting the potential of targeted synthesis scaling paradigm to\ncontinuously and efficiently enhance the digital agents.",
      "url": "http://arxiv.org/abs/2510.14969v1",
      "published_time_eastern_timestamp": 1760637578.0
    },
    {
      "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in\n  Long-Horizon Tasks",
      "summary": "To tackle long-horizon tasks, recent hierarchical vision-language-action\n(VLAs) frameworks employ vision-language model (VLM)-based planners to\ndecompose complex manipulation tasks into simpler sub-tasks that low-level\nvisuomotor policies can easily handle. Typically, the VLM planner is finetuned\nto learn to decompose a target task. This finetuning requires target task\ndemonstrations segmented into sub-tasks by either human annotation or heuristic\nrules. However, the heuristic subtasks can deviate significantly from the\ntraining data of the visuomotor policy, which degrades task performance. To\naddress these issues, we propose a Retrieval-based Demonstration Decomposer\n(RDD) that automatically decomposes demonstrations into sub-tasks by aligning\nthe visual features of the decomposed sub-task intervals with those from the\ntraining data of the low-level visuomotor policies. Our method outperforms the\nstate-of-the-art sub-task decomposer on both simulation and real-world tasks,\ndemonstrating robustness across diverse settings. Code and more results are\navailable at rdd-neurips.github.io.",
      "url": "http://arxiv.org/abs/2510.14968v1",
      "published_time_eastern_timestamp": 1760637577.0
    },
    {
      "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their\n  Connection to Diffusion Language Models",
      "summary": "Language models with recurrent depth, also referred to as universal or looped\nwhen considering transformers, are defined by the capacity to increase their\ncomputation through the repetition of layers. Recent efforts in pretraining\nhave demonstrated that these architectures can scale to modern language\nmodeling tasks while exhibiting advantages in reasoning tasks. In this work, we\nexamine the relationship between recurrent-depth models and diffusion language\nmodels. Building on their similarities, we develop a new diffusion forcing\nsampler for these models to accelerate generation. The sampler advances by\ndecoding new tokens at every forward pass of the model, while the latent states\nof these tokens can be further refined in parallel through recurrence.\nTheoretically, generation with our sampler is strictly more expressive than the\nbaseline autoregressive generation using the same time budget on modern\nhardware. Moreover, this sampler, based on principles from diffusion\nliterature, can be directly applied to existing 3.5B recurrent-depth\ntransformers without any tuning, leading to up to a 5x speedup. Consequently,\nour findings not only provide an efficient mechanism for parallelizing the\nextra computation in recurrent-depth models at inference, but also suggest that\nsuch models can be naturally viewed as strong continuous, though causal,\ndiffusion language models.",
      "url": "http://arxiv.org/abs/2510.14961v1",
      "published_time_eastern_timestamp": 1760637547.0
    },
    {
      "title": "RealDPO: Real or Not Real, that is the Preference",
      "summary": "Video generative models have recently achieved notable advancements in\nsynthesis quality. However, generating complex motions remains a critical\nchallenge, as existing models often struggle to produce natural, smooth, and\ncontextually consistent movements. This gap between generated and real-world\nmotions limits their practical applicability. To address this issue, we\nintroduce RealDPO, a novel alignment paradigm that leverages real-world data as\npositive samples for preference learning, enabling more accurate motion\nsynthesis. Unlike traditional supervised fine-tuning (SFT), which offers\nlimited corrective feedback, RealDPO employs Direct Preference Optimization\n(DPO) with a tailored loss function to enhance motion realism. By contrasting\nreal-world videos with erroneous model outputs, RealDPO enables iterative\nself-correction, progressively refining motion quality. To support\npost-training in complex motion synthesis, we propose RealAction-5K, a curated\ndataset of high-quality videos capturing human daily activities with rich and\nprecise motion details. Extensive experiments demonstrate that RealDPO\nsignificantly improves video quality, text alignment, and motion realism\ncompared to state-of-the-art models and existing preference optimization\ntechniques.",
      "url": "http://arxiv.org/abs/2510.14955v1",
      "published_time_eastern_timestamp": 1760637505.0
    },
    {
      "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked\n  Autoregression",
      "summary": "Whole-body multi-modal human motion generation poses two primary challenges:\ncreating an effective motion generation mechanism and integrating various\nmodalities, such as text, speech, and music, into a cohesive framework. Unlike\nprevious methods that usually employ discrete masked modeling or autoregressive\nmodeling, we develop a continuous masked autoregressive motion transformer,\nwhere a causal attention is performed considering the sequential nature within\nthe human motion. Within this transformer, we introduce a gated linear\nattention and an RMSNorm module, which drive the transformer to pay attention\nto the key actions and suppress the instability caused by either the abnormal\nmovements or the heterogeneous distributions within multi-modalities. To\nfurther enhance both the motion generation and the multimodal generalization,\nwe employ the DiT structure to diffuse the conditions from the transformer\ntowards the targets. To fuse different modalities, AdaLN and cross-attention\nare leveraged to inject the text, speech, and music signals. Experimental\nresults demonstrate that our framework outperforms previous methods across all\nmodalities, including text-to-motion, speech-to-gesture, and music-to-dance.\nThe code of our method will be made public.",
      "url": "http://arxiv.org/abs/2510.14954v1",
      "published_time_eastern_timestamp": 1760637473.0
    },
    {
      "title": "StarStream: Automatic detection algorithm for stellar streams",
      "summary": "The Gaia mission has led to the discovery of over 100 stellar streams in the\nMilky Way, most of which likely originated from globular clusters (GCs). As the\nupcoming wide-field surveys can potentially continue to increase the number of\nknown streams, there is a growing need to shift focus from manual detection of\nindividual streams to automated detection methods that prioritize both quality\nand quantity. Traditional techniques rely heavily on the visual expectation\nthat GC streams are dynamically cold and thin. This assumption does not hold\nfor all streams, whose morphologies and kinematics can vary significantly with\nthe progenitor's mass and orbit. As a result, these methods are biased toward a\nsubset of the whole stream population, with often unquantified purity and\ncompleteness. In this work, we present StarStream, an automatic stream\ndetection algorithm based on a physics-inspired model rather than visual\nexpectation. Our method provides a more accurate prediction of stream stars in\nthe multi-dimensional space of observables, while using fewer free parameters\nto account for the diversity of streams. Applied to a mock GC stream catalog\ntailored for the Gaia DR3 dataset, our algorithm achieves both purity and\ncompleteness of at least 65% at Galactic latitudes |b| > 30 degree.",
      "url": "http://arxiv.org/abs/2510.14929v1",
      "published_time_eastern_timestamp": 1760636462.0
    },
    {
      "title": "Instruction Set Migration at Warehouse Scale",
      "summary": "Migrating codebases from one instruction set architecture (ISA) to another is\na major engineering challenge. A recent example is the adoption of Arm (in\naddition to x86) across the major Cloud hyperscalers. Yet, this problem has\nseen limited attention by the academic community. Most work has focused on\nstatic and dynamic binary translation, and the traditional conventional wisdom\nhas been that this is the primary challenge.\n  In this paper, we show that this is no longer the case. Modern ISA migrations\ncan often build on a robust open-source ecosystem, making it possible to\nrecompile all relevant software from scratch. This introduces a new and\nmultifaceted set of challenges, which are different from binary translation.\n  By analyzing a large-scale migration from x86 to Arm at Google, spanning\nalmost 40,000 code commits, we derive a taxonomy of tasks involved in ISA\nmigration. We show how Google automated many of the steps involved, and\ndemonstrate how AI can play a major role in automatically addressing these\ntasks. We identify tasks that remain challenging and highlight research\nchallenges that warrant further attention.",
      "url": "http://arxiv.org/abs/2510.14928v1",
      "published_time_eastern_timestamp": 1760636461.0
    },
    {
      "title": "Predicting Task Performance with Context-aware Scaling Laws",
      "summary": "Scaling laws have transformed our understanding of large language models by\nlinking upstream metrics like cross-entropy loss to design factors such as\nmodel size, training data, and compute. However, these conventional laws fail\nto capture downstream task performance, where context plays a critical role. In\nthis work, we propose a straightforward, interpretable framework that jointly\nmodels downstream performance as a function of the training compute and the\nprovided context. We empirically validate our framework by fitting it on the\nobserved downstream performance of extended-context variants of Llama-2-7B and\nLlama-2-13B across 65,500 unique instances spanning three tasks: arithmetic\nreasoning, common sense reasoning, and machine translation. Our results\ndemonstrate that our framework accurately models in-distribution downstream\nperformance, generalizes across three orders of magnitude in training compute,\nand reliably extrapolates performance as the amount of context increases. These\nfindings offer valuable insights into the interplay between training compute\nand context utilization, providing guidance for designing more efficient\nlong-context LLMs for diverse downstream tasks. Our code is available at\nhttps://github.com/wang-research-lab/context-scaling.",
      "url": "http://arxiv.org/abs/2510.14919v1",
      "published_time_eastern_timestamp": 1760636118.0
    },
    {
      "title": "Budget-aware Test-time Scaling via Discriminative Verification",
      "summary": "Test-time scaling is a powerful strategy for boosting the performance of\nlarge language models on complex reasoning tasks. While state-of-the-art\napproaches often employ generative verifiers to select the best solution from a\npool of candidates, this method incurs prohibitive computational costs,\nlimiting its practicality. In this work, we shift the focus to a more\nbudget-aware paradigm: discriminative verification. We conduct a thorough\nempirical analysis and demonstrate that while discriminative verifiers may\nunderperform in isolation, combining them with self-consistency in a hybrid\napproach creates a powerful and efficient test-time scaling mechanism. Notably,\nunder a fixed compute budget, this hybrid approach surpasses state-of-the-art\ngenerative verification by a significant margin: achieving up to 15.3\\% higher\naccuracy on AIME2025. Our findings establish that for practical, real-world\napplications, budget-aware scaling with discriminative verifiers is not only a\n\"free\" upgrade over self-consistency, but also a more effective and efficient\nalternative to costly generative techniques. Code is available at\nhttps://github.com/wang-research-lab/verification.",
      "url": "http://arxiv.org/abs/2510.14913v1",
      "published_time_eastern_timestamp": 1760635802.0
    },
    {
      "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object\n  Trajectories in Videos",
      "summary": "Dense Video Object Captioning (DVOC) is the task of jointly detecting,\ntracking, and captioning object trajectories in a video, requiring the ability\nto understand spatio-temporal details and describe them in natural language.\nDue to the complexity of the task and the high cost associated with manual\nannotation, previous approaches resort to disjoint training strategies,\npotentially leading to suboptimal performance. To circumvent this issue, we\npropose to generate captions about spatio-temporally localized entities\nleveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets\nwith our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an\nend-to-end model capable of jointly detecting, segmenting, tracking and\ncaptioning object trajectories. Moreover, with pretraining on LVISCap and\nLV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three\nexisting benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are\navailable at https://www.gabriel.fiastre.fr/maskcaptioner/.",
      "url": "http://arxiv.org/abs/2510.14904v1",
      "published_time_eastern_timestamp": 1760635222.0
    },
    {
      "title": "Fast and fault-tolerant logical measurements: Auxiliary hypergraphs and\n  transversal surgery",
      "summary": "Quantum code surgery is a promising technique to perform fault-tolerant\ncomputation on quantum low-density parity-check codes. Recent developments have\nsignificantly reduced the space overhead of surgery. However, generic surgery\noperations still require $O(d)$ rounds of repeated syndrome extraction to be\nmade fault-tolerant. In this work, we focus on reducing the time overhead of\nsurgery. We first present a general set of conditions that ensure\nfault-tolerant surgery operations can be performed with constant time overhead.\nThis fast surgery necessarily makes use of an auxiliary complex described by a\nhypergraph rather than a graph. We then introduce a concrete scheme called\nblock reading, which performs transversal surgery across multiple code blocks.\nWe further investigate surgery operations with intermediate time overhead,\nbetween $O(1)$ and $O(d)$, which apply to quantum locally testable codes.\nFinally, we establish a circuit equivalence between homomorphic measurement and\nhypergraph surgery and derive bounds on the time overhead of generic logical\nmeasurement schemes. Overall, our results demonstrate that reducing the time\ncost of code surgery is not reliant on the quantum memory being single-shot.\nInstead it is chiefly the connectivity between a code and its measurement\nancilla system that determines the achievable measurement time overhead.",
      "url": "http://arxiv.org/abs/2510.14895v1",
      "published_time_eastern_timestamp": 1760634756.0
    },
    {
      "title": "A Comprehensive Framework for Efficient Court Case Management and\n  Prioritization",
      "summary": "The Indian judicial system faces a critical challenge with approximately 52\nmillion pending cases, causing significant delays that impact socio-economic\nstability. This study proposes a cloud-based software framework to classify and\nprioritize court cases using algorithmic methods based on parameters such as\nseverity of crime committed, responsibility of parties involved, case filing\ndates, previous hearing's data, priority level (e.g., Urgent, Medium, Ordinary)\nprovided as input, and relevant Indian Penal Code (IPC), Code of Criminal\nProcedure (CrPC), and other legal sections (e.g., Hindu Marriage Act, Indian\nContract Act). Cases are initially entered by advocates on record or court\nregistrars, followed by automated hearing date allocation that balances fresh\nand old cases while accounting for court holidays and leaves. The system\nstreamlines appellate processes by fetching data from historical case\ndatabases. Our methodology integrates algorithmic prioritization, a robust\nnotification system, and judicial interaction, with features that allow judges\nto view daily case counts and their details. Simulations demonstrate that the\nsystem can process cases efficiently, with reliable notification delivery and\npositive user satisfaction among judges and registrars. Future iterations will\nincorporate advanced machine learning for dynamic prioritization, addressing\ncritical gaps in existing court case management systems to enhance efficiency\nand reduce backlogs.",
      "url": "http://arxiv.org/abs/2510.14892v1",
      "published_time_eastern_timestamp": 1760634698.0
    },
    {
      "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with\n  Multi-Scale Reference Attention",
      "summary": "Text-to-image generation with visual autoregressive~(VAR) models has recently\nachieved impressive advances in generation fidelity and inference efficiency.\nWhile control mechanisms have been explored for diffusion models, enabling\nprecise and flexible control within VAR paradigm remains underexplored. To\nbridge this critical gap, in this paper, we introduce ScaleWeaver, a novel\nframework designed to achieve high-fidelity, controllable generation upon\nadvanced VAR models through parameter-efficient fine-tuning. The core module in\nScaleWeaver is the improved MMDiT block with the proposed Reference Attention\nmodule, which efficiently and effectively incorporates conditional information.\nDifferent from MM Attention, the proposed Reference Attention module discards\nthe unnecessary attention from image$\\rightarrow$condition, reducing\ncomputational cost while stabilizing control injection. Besides, it\nstrategically emphasizes parameter reuse, leveraging the capability of the VAR\nbackbone itself with a few introduced parameters to process control\ninformation, and equipping a zero-initialized linear projection to ensure that\ncontrol signals are incorporated effectively without disrupting the generative\ncapability of the base model. Extensive experiments show that ScaleWeaver\ndelivers high-quality generation and precise control while attaining superior\nefficiency over diffusion-based methods, making ScaleWeaver a practical and\neffective solution for controllable text-to-image generation within the visual\nautoregressive paradigm. Code and models will be released.",
      "url": "http://arxiv.org/abs/2510.14882v1",
      "published_time_eastern_timestamp": 1760634059.0
    },
    {
      "title": "BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data",
      "summary": "Existing collision prediction methods often fail to distinguish between\nego-vehicle threats and random accidents not involving the ego vehicle, leading\nto excessive false alerts in real-world deployment. We present BADAS, a family\nof collision prediction models trained on Nexar's real-world dashcam collision\ndataset -- the first benchmark designed explicitly for ego-centric evaluation.\nWe re-annotate major benchmarks to identify ego involvement, add consensus\nalert-time labels, and synthesize negatives where needed, enabling fair AP/AUC\nand temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and\ncomes in two variants: BADAS-Open (trained on our 1.5k public videos) and\nBADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and\nNexar, BADAS achieves state-of-the-art AP/AUC and outperforms a\nforward-collision ADAS baseline while producing more realistic time-to-accident\nestimates. We release our BADAS-Open model weights and code, along with\nre-annotations of all evaluation datasets to promote ego-centric collision\nprediction research.",
      "url": "http://arxiv.org/abs/2510.14876v1",
      "published_time_eastern_timestamp": 1760633730.0
    },
    {
      "title": "AREPO-RSG: Aspherical Circumstellar Material and Winds from Pulsating\n  Dusty Red Supergiants in Global 3D Radiation Hydrodynamic Simulations",
      "summary": "Recent observations have revealed a surprisingly large fraction of\nhydrogen-rich supernovae (SNe) interacting with dense confined circumstellar\nmaterial (CSM), whose origin is heavily debated. Exploiting our recent\nimplementation of a sophisticated radiation transport scheme in the moving-mesh\ncode AREPO, we perform full-sphere 3D radiation hydrodynamic simulations of red\nsupergiant envelopes. For $10\\, M_\\odot$ and $20\\, M_\\odot$ core-carbon-burning\nstars, we find that large-amplitude radial pulsations lift the surface material\nof density $10^{-14}$-$10^{-12}\\; \\mathrm{g\\; cm^{-3}}$ to the circumstellar\nenvironment up to $3\\times10^{14}$ cm, consistent with the inferred density for\nthe interacting SN 2013fs. There, radiation acts on dust to drive highly\nanisotropic outflows of $10^{-6}$-$10^{-5}\\, M_\\odot\\, \\mathrm{yr^{-1}}$. The\ntotal CSM masses for both simulations are $\\sim 0.01\\, M_\\odot$. Due to\nconvection, the CSM density structure has order-of-magnitude angular\nvariations, dominated by large-scale asymmetries. We suggest that (1) the CSM\naround the progenitor is bound material instead of a widely-assumed steady\nwind, (2) highly aspherical CSM is common and can be created by surface\nconvection rather than only from binary interactions, and (3) 3D effects need\nto be incorporated in 1D SN modeling, potentially via effective clumping. Based\non our simulations, we propose a 1D analytical CSM model to be directly used\nfor SN observable modeling. We predict that progenitor pulsations (seen in SN\n2023ixf) and highly-confined CSM (seen in SN 2013fs) should be common among\nmost hydrogen-rich SNe. This can be tested with progenitor monitoring using\nRubin Observatory and near-future high-cadence surveys such as ULTRASAT and\nUVEX.",
      "url": "http://arxiv.org/abs/2510.14875v1",
      "published_time_eastern_timestamp": 1760633669.0
    }
  ]
}
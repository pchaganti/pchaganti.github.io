{
  "last_updated": "2025-06-18T08:24:16.638620-04:00",
  "papers": [
    {
      "title": "Optimistic MEV in Ethereum Layer 2s: Why Blockspace Is Always in Demand",
      "summary": "Layer 2 rollups are rapidly absorbing DeFi activity, securing over $40\nbillion and accounting for nearly half of Ethereum's DEX volume by Q1 2025, yet\ntheir MEV dynamics remain understudied. We address this gap by defining and\nquantifying optimistic MEV, a form of speculative, on-chain cyclic arbitrage\nwhose detection and execution logic reside largely on-chain in smart contracts.\nAs a result of their speculative nature and lack of off-chain opportunity\nverification, optimistic MEV transactions frequently fail to execute a\nprofitable arbitrage.\n  Applying our multi-stage identification pipeline to Arbitrum, Base, and\nOptimism, we find that in Q1 2025, optimistic MEV accounts for over 50% of\non-chain gas on Base and Optimism and 7% on Arbitrum, driven mainly by\n\"interaction\" probes (on-chain computations searching for arbitrage). This\nspeculative probing keeps blocks on Base and Optimism persistently full.\nDespite consuming over half of on-chain gas, optimistic MEV transactions pay\nless than one quarter of total gas fees. Cross-network comparison reveals\ndivergent success rates, differing patterns of code reuse, and sensitivity to\nvarying sequencer ordering and block production times. Finally, OLS regressions\nlink optimistic MEV trade count to ETH volatility, retail trading activity, and\nDEX aggregator usage, showing how Layer 2 protocol parameters uniquely\nencourage speculative MEV.",
      "url": "http://arxiv.org/abs/2506.14768v1",
      "published_time_eastern_timestamp": 1750183108.0
    },
    {
      "title": "A Variational Framework for Improving Naturalness in Generative Spoken\n  Language Models",
      "summary": "The success of large language models in text processing has inspired their\nadaptation to speech modeling. However, since speech is continuous and complex,\nit is often discretized for autoregressive modeling. Speech tokens derived from\nself-supervised models (known as semantic tokens) typically focus on the\nlinguistic aspects of speech but neglect prosodic information. As a result,\nmodels trained on these tokens can generate speech with reduced naturalness.\nExisting approaches try to fix this by adding pitch features to the semantic\ntokens. However, pitch alone cannot fully represent the range of paralinguistic\nattributes, and selecting the right features requires careful hand-engineering.\nTo overcome this, we propose an end-to-end variational approach that\nautomatically learns to encode these continuous speech attributes to enhance\nthe semantic tokens. Our approach eliminates the need for manual extraction and\nselection of paralinguistic features. Moreover, it produces preferred speech\ncontinuations according to human raters. Code, samples and models are available\nat https://github.com/b04901014/vae-gslm.",
      "url": "http://arxiv.org/abs/2506.14767v1",
      "published_time_eastern_timestamp": 1750183097.0
    },
    {
      "title": "Reasoning with Exploration: An Entropy Perspective",
      "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.",
      "url": "http://arxiv.org/abs/2506.14758v1",
      "published_time_eastern_timestamp": 1750182843.0
    },
    {
      "title": "Joint Error Correction and Fading Channel Estimation Enhancement\n  Leveraging GRAND",
      "summary": "We present a novel method for error correction in the presence of fading\nchannel estimation errors (CEE). When such errors are significant, considerable\nperformance losses can be observed if the wireless transceiver is not adapted.\nInstead of refining the estimate by increasing the pilot sequence length or\nimproving the estimation algorithm, we propose two new approaches based on\nGuessing Random Additive Noise Decoding (GRAND) decoders. The first method\ninvolves testing multiple candidates for the channel estimate located in the\ncomplex neighborhood around the original pilot-based estimate. All these\ncandidates are employed in parallel to compute log-likelihood ratios (LLR).\nThese LLRs are used as soft input to Ordered Reliability Bits GRAND (ORBGRAND).\nPosterior likelihood formulas associated with ORBGRAND are then computed to\ndetermine which channel candidate leads to the most probable codeword. The\nsecond method is a refined version of the first approach accounting for the\npresence of residual CEE in the LLR computation. The performance of these two\ntechniques is evaluated for [128,112] 5G NR CA-Polar and CRC codes. For the\nconsidered settings, block error rate (BLER) gains of several dBs are observed\ncompared to cases where CEE is ignored.",
      "url": "http://arxiv.org/abs/2506.14756v1",
      "published_time_eastern_timestamp": 1750182644.0
    },
    {
      "title": "Optimizing Length Compression in Large Reasoning Models",
      "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
      "url": "http://arxiv.org/abs/2506.14755v1",
      "published_time_eastern_timestamp": 1750182616.0
    },
    {
      "title": "Union-Intersection Union-Find for Decoding Depolarizing Errors in\n  Topological Codes",
      "summary": "In this paper, we introduce the Union-Intersection Union-Find (UIUF)\nalgorithm for decoding depolarizing errors in topological codes, combining the\nstrengths of iterative and standard Union-Find (UF) decoding. While iterative\nUF improves performance at moderate error rates, it lacks an error correction\nguarantee. To address this, we develop UIUF, which maintains the enhanced\nperformance of iterative UF while ensuring error correction up to half the code\ndistance. Through simulations under code capacity, phenomenological, and biased\nnoise models, we show that UIUF significantly outperforms UF, reducing the\nlogical error rate by over an order of magnitude (at around $10^{-5}$).\nMoreover, UIUF achieves lower logical error rates than the Minimum Weight\nPerfect Matching (MWPM) decoder on rotated surface codes under both the code\ncapacity and phenomenological noise models, while preserving efficient\nlinear-time complexity.",
      "url": "http://arxiv.org/abs/2506.14745v1",
      "published_time_eastern_timestamp": 1750181414.0
    },
    {
      "title": "Sulphur abundances in star-forming regions from optical emission lines:\n  A new approach based on photoionization models consistent with the direct\n  method",
      "summary": "The derivation of sulphur chemical abundances in the gas-phase of\nstar-forming galaxies is explored in this work, using the emission lines\nproduced in these regions in the optical part of the spectrum and by means of\nphotoionization models. We adapted the code HII-CHI-mistry to account for these\nabundances by implementing additional grids of models that assume a variable\nsulphur-to-oxygen abundance ratio, beyond the commonly assumed solar ratio. The\naddition of these models, and their use in a new iteration of the code allows\nus to use sulphur lines to precisely estimate the sulphur abundance, even in\nthe absence of auroral lines. This approach aligns with the results from the\ndirect method, and no additional assumptions about the ionization correction\nfactor are needed, as the models directly predict the total sulphur abundance.\nWe applied this new methodology to a large sample of star-forming regions from\nthe MaNGA survey, and we explored the variation of the S/O ratio as a function\nof metallicity, making corrections for the significant contribution from\ndiffuse ionized gas, which particularly affects the [SII] emission. Our results\nindicate no significant deviations from the solar S/O value in the range 8.0 <\n12+log(O/H) < 8.7, where the bulk of the MaNGA sample stays, , but also with\npossible enhancements of sulphur production both at low and high metallicity\nregimes. The latter may be linked to the depletion of oxygen in the gas-phase\ndue to its incorporation onto dust grains",
      "url": "http://arxiv.org/abs/2506.14736v1",
      "published_time_eastern_timestamp": 1750180700.0
    },
    {
      "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
      "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
      "url": "http://arxiv.org/abs/2506.14731v1",
      "published_time_eastern_timestamp": 1750180354.0
    },
    {
      "title": "YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object\n  Detection Framework",
      "summary": "Multispectral object detection, which integrates information from multiple\nbands, can enhance detection accuracy and environmental adaptability, holding\ngreat application potential across various fields. Although existing methods\nhave made progress in cross-modal interaction, low-light conditions, and model\nlightweight, there are still challenges like the lack of a unified single-stage\nframework, difficulty in balancing performance and fusion strategy, and\nunreasonable modality weight allocation. To address these, based on the YOLOv11\nframework, we present YOLOv11-RGBT, a new comprehensive multimodal object\ndetection framework. We designed six multispectral fusion modes and\nsuccessfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After\nreevaluating the importance of the two modalities, we proposed a P3 mid-fusion\nstrategy and multispectral controllable fine-tuning (MCF) strategy for\nmultispectral models. These improvements optimize feature fusion, reduce\nredundancy and mismatches, and boost overall model performance. Experiments\nshow our framework excels on three major open-source multispectral object\ndetection datasets, like LLVIP and FLIR. Particularly, the multispectral\ncontrollable fine-tuning strategy significantly enhanced model adaptability and\nrobustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP\nby 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and\nstrategies' effectiveness. The code is available at:\nhttps://github.com/wandahangFY/YOLOv11-RGBT.",
      "url": "http://arxiv.org/abs/2506.14696v1",
      "published_time_eastern_timestamp": 1750178220.0
    },
    {
      "title": "Breaking even with magic: demonstration of a high-fidelity logical\n  non-Clifford gate",
      "summary": "Encoding quantum information to protect it from errors is essential for\nperforming large-scale quantum computations. Performing a universal set of\nquantum gates on encoded states demands a potentially large resource overhead\nand minimizing this overhead is key for the practical development of\nlarge-scale fault-tolerant quantum computers. We propose and experimentally\nimplement a magic-state preparation protocol to fault-tolerantly prepare a pair\nof logical magic states in a [[6,2,2]] quantum error-detecting code using only\neight physical qubits. Implementing this protocol on H1-1, a 20 qubit\ntrapped-ion quantum processor, we prepare magic states with experimental\ninfidelity $7^{+3}_{-1}\\times 10^{-5}$ with a $14.8^{+1}_{-1}\\%$ discard rate\nand use these to perform a fault-tolerant non-Clifford gate, the\ncontrolled-Hadamard (CH), with logical infidelity $\\leq 2.3^{+9}_{-9}\\times\n10^{-4}$. Notably, this significantly outperforms the unencoded physical CH\ninfidelity of $10^{-3}$. Through circuit-level stabilizer simulations, we show\nthat this protocol can be self-concatenated to produce extremely high-fidelity\nmagic states with low space-time overhead in a [[36,4,4]] quantum error\ncorrecting code, with logical error rates of $6\\times 10^{-10}$ ($5\\times\n10^{-14}$) at two-qubit error rate of $10^{-3}$ ($10^{-4}$) respectively.",
      "url": "http://arxiv.org/abs/2506.14688v1",
      "published_time_eastern_timestamp": 1750177427.0
    },
    {
      "title": "Unified Software Engineering agent as AI Software Engineer",
      "summary": "The growth of Large Language Model (LLM) technology has raised expectations\nfor automated coding. However, software engineering is more than coding and is\nconcerned with activities including maintenance and evolution of a project. In\nthis context, the concept of LLM agents has gained traction, which utilize LLMs\nas reasoning engines to invoke external tools autonomously. But is an LLM agent\nthe same as an AI software engineer? In this paper, we seek to understand this\nquestion by developing a Unified Software Engineering agent or USEagent. Unlike\nexisting work which builds specialized agents for specific software tasks such\nas testing, debugging, and repair, our goal is to build a unified agent which\ncan orchestrate and handle multiple capabilities. This gives the agent the\npromise of handling complex scenarios in software development such as fixing an\nincomplete patch, adding new features, or taking over code written by others.\nWe envision USEagent as the first draft of a future AI Software Engineer which\ncan be a team member in future software development teams involving both AI and\nhumans. To evaluate the efficacy of USEagent, we build a Unified Software\nEngineering bench (USEbench) comprising of myriad tasks such as coding,\ntesting, and patching. USEbench is a judicious mixture of tasks from existing\nbenchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on\nUSEbench consisting of 1,271 repository-level software engineering tasks,\nUSEagent shows improved efficacy compared to existing general agents such as\nOpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for\ncertain coding tasks, which provides hints on further developing the AI\nSoftware Engineer of the future.",
      "url": "http://arxiv.org/abs/2506.14683v1",
      "published_time_eastern_timestamp": 1750177153.0
    },
    {
      "title": "AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language\n  Models",
      "summary": "We introduce AIRTBench, an AI red teaming benchmark for evaluating language\nmodels' ability to autonomously discover and exploit Artificial Intelligence\nand Machine Learning (AI/ML) security vulnerabilities. The benchmark consists\nof 70 realistic black-box capture-the-flag (CTF) challenges from the Crucible\nchallenge environment on the Dreadnode platform, requiring models to write\npython code to interact with and compromise AI systems. Claude-3.7-Sonnet\nemerged as the clear leader, solving 43 challenges (61% of the total suite,\n46.9% overall success rate), with Gemini-2.5-Pro following at 39 challenges\n(56%, 34.3% overall), GPT-4.5-Preview at 34 challenges (49%, 36.9% overall),\nand DeepSeek R1 at 29 challenges (41%, 26.9% overall). Our evaluations show\nfrontier models excel at prompt injection attacks (averaging 49% success rates)\nbut struggle with system exploitation and model inversion challenges (below\n26%, even for the best performers). Frontier models are far outpacing\nopen-source alternatives, with the best truly open-source model (Llama-4-17B)\nsolving 7 challenges (10%, 1.0% overall), though demonstrating specialized\ncapabilities on certain hard challenges. Compared to human security\nresearchers, large language models (LLMs) solve challenges with remarkable\nefficiency completing in minutes what typically takes humans hours or days-with\nefficiency advantages of over 5,000x on hard challenges. Our contribution fills\na critical gap in the evaluation landscape, providing the first comprehensive\nbenchmark specifically designed to measure and track progress in autonomous AI\nred teaming capabilities.",
      "url": "http://arxiv.org/abs/2506.14682v1",
      "published_time_eastern_timestamp": 1750177146.0
    },
    {
      "title": "Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and\n  Training Factors Shape LLM Alignment Quality",
      "summary": "Supervised fine-tuning (SFT) is a critical step in aligning large language\nmodels (LLMs) with human instructions and values, yet many aspects of SFT\nremain poorly understood. We trained a wide range of base models on a variety\nof datasets including code generation, mathematical reasoning, and\ngeneral-domain tasks, resulting in 1,000+ SFT models under controlled\nconditions. We then identified the dataset properties that matter most and\nexamined the layer-wise modifications introduced by SFT. Our findings reveal\nthat some training-task synergies persist across all models while others vary\nsubstantially, emphasizing the importance of model-specific strategies.\nMoreover, we demonstrate that perplexity consistently predicts SFT\neffectiveness--often surpassing superficial similarity between trained data and\nbenchmark--and that mid-layer weight changes correlate most strongly with\nperformance gains. We will release these 1,000+ SFT models and benchmark\nresults to accelerate further research.",
      "url": "http://arxiv.org/abs/2506.14681v1",
      "published_time_eastern_timestamp": 1750176795.0
    },
    {
      "title": "Quantifying Diagnostic Signal Decay in Dementia: A National Study of\n  Medicare Hospitalization Data",
      "summary": "Background: Artificial intelligence (AI) models in healthcare depend on the\nfidelity of diagnostic data, yet the quality of such data is often compromised\nby variability in clinical documentation practices. In dementia, a condition\nalready prone to diagnostic ambiguity, this variability may introduce\nsystematic distortion into claims-based research and AI model development.\n  Methods: We analyzed Medicare Part A hospitalization data from 2016-2018 to\nexamine patterns of dementia-related ICD-10 code utilization across more than\n3,000 U.S. counties. Using a clinically informed classification of 17 ICD-10\ncodes grouped into five diagnostic categories, we applied the transitive\nSequential Pattern Mining (tSPM+) algorithm to model temporal usage structures.\nWe then used matrix similarity methods to compare local diagnostic patterns to\nnational norms and fit multivariable linear regressions to identify\ncounty-level demographic and structural correlates of divergence.\n  Findings: We found substantial geographic and demographic variation in\ndementia-related diagnostic code usage. Non-specific codes were dominant\nnationwide, while Alzheimer's disease and vascular dementia codes showed\npronounced variability. Temporal sequence analysis revealed consistent\ntransitions from specific to non-specific codes, which suggest degradation of\ndiagnostic specificity over time. Counties with higher proportions of rural\nresidents, Medicaid-eligible patients, and Black or Hispanic dementia patients\ndemonstrated significantly lower similarity to national usage patterns. Our\nmodel explained 38% of the variation in local-to-national diagnostic alignment.",
      "url": "http://arxiv.org/abs/2506.14669v1",
      "published_time_eastern_timestamp": 1750176131.0
    },
    {
      "title": "Issue Retrieval and Verification Enhanced Supplementary Code Comment\n  Generation",
      "summary": "Issue reports have been recognized to contain rich information for\nretrieval-augmented code comment generation. However, how to minimize\nhallucinations in the generated comments remains significant challenges. In\nthis paper, we propose IsComment, an issue-based LLM retrieval and verification\napproach for generating method's design rationale, usage directives, and so on\nas supplementary code comments. We first identify five main types of code\nsupplementary information that issue reports can provide through\ncode-comment-issue analysis. Next, we retrieve issue sentences containing these\ntypes of supplementary information and generate candidate code comments. To\nreduce hallucinations, we filter out those candidate comments that are\nirrelevant to the code or unverifiable by the issue report, making the code\ncomment generation results more reliable. Our experiments indicate that\ncompared with LLMs, IsComment increases the coverage of manual supplementary\ncomments from 33.6% to 72.2% for ChatGPT, from 35.8% to 88.4% for GPT-4o, and\nfrom 35.0% to 86.2% for DeepSeek-V3. Compared with existing work, IsComment can\ngenerate richer and more useful supplementary code comments for programming\nunderstanding, which is quantitatively evaluated through the MESIA metric on\nboth methods with and without manual code comments.",
      "url": "http://arxiv.org/abs/2506.14649v1",
      "published_time_eastern_timestamp": 1750174945.0
    },
    {
      "title": "GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel\n  Optimization with GuidedSelection Vectors",
      "summary": "Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), offer an efficient way to adapt large language models with\nreduced computational costs. However, their performance is limited by the small\nnumber of trainable parameters. Recent work combines LoRA with the\nMixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two\nlimitations remain in hindering the full exploitation of its potential: 1) the\ninfluence of downstream tasks when assigning expert numbers, and 2) the uniform\nrank assignment across all LoRA experts, which restricts representational\ndiversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained\nlayer-wise expert numbers and ranks allocation strategy with GuidedSelection\nVectors (GSVs). GSVs are learned via a prior bilevel optimization process to\ncapture both model- and task-specific needs, and are then used to allocate\noptimal expert numbers and ranks. Experiments on three backbone models across\ndiverse benchmarks show that GuiLoMo consistently achieves superior or\ncomparable performance to all baselines. Further analysis offers key insights\ninto how expert numbers and ranks vary across layers and tasks, highlighting\nthe benefits of adaptive expert configuration. Our code is available at\nhttps://github.com/Liar406/Gui-LoMo.git.",
      "url": "http://arxiv.org/abs/2506.14646v1",
      "published_time_eastern_timestamp": 1750174893.0
    },
    {
      "title": "Thermodynamic Evolution of Flaring Loops with Non-local Thermal\n  Transport",
      "summary": "Hot solar coronal loops, such as flaring loops, reach temperatures where the\nthermal transport becomes non-local. This occurs when the mean-free-path of\nelectrons can no longer be assumed to be small. Using a modified version of the\nLare2d code, we study the evolution of flare-heated coronal loops under three\nthermal transport models: classical Spitzer-Harm (SH), a flux-limited local\nmodel (FL), and the non-local Schurtz-Nicolai-Basquet (SNB) model. The SNB\nmodel is used extensively in laser-plasma studies. It has been benchmarked\nagainst accurate non-local Vlasov-Fokker-Planck models and proven to be the\nmost accurate non-local model which can be applied on fluid time-scales.\nAnalysis of the density-temperature evolution cycles near the loop apex reveals\na distinct evolutionary path for the SNB model, with higher temperatures and\nlower densities than local models. During energy deposition, the SNB model\nproduces a more localised and intense temperature peak at the apex due to heat\nflux suppression, which also reduces chromospheric evaporation and results in\nlower post-flare densities. EUV emission synthesis shows that the SNB model\nyields flare light curves with lower peak amplitudes and smoother decay phases.\nWe also find that non-local transport affects equilibrium loop conditions,\nproducing hotter and more rarefied apexes. These findings emphasise the need to\naccount for non-local conduction in dynamic solar phenomena and highlight the\npotential of the SNB model for improving the realism of flare simulations.\nFlux-limited conduction models cannot reproduce the results of non-local\ntransport covered by the SNB model.",
      "url": "http://arxiv.org/abs/2506.14644v1",
      "published_time_eastern_timestamp": 1750174852.0
    },
    {
      "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding\n  German Open-Ended Survey Responses on Survey Motivation",
      "summary": "The recent development and wider accessibility of LLMs have spurred\ndiscussions about how they can be used in survey research, including\nclassifying open-ended survey responses. Due to their linguistic capacities, it\nis possible that LLMs are an efficient alternative to time-consuming manual\ncoding and the pre-training of supervised machine learning models. As most\nexisting research on this topic has focused on English-language responses\nrelating to non-complex topics or on single LLMs, it is unclear whether its\nfindings generalize and how the quality of these classifications compares to\nestablished methods. In this study, we investigate to what extent different\nLLMs can be used to code open-ended survey responses in other contexts, using\nGerman data on reasons for survey participation as an example. We compare\nseveral state-of-the-art LLMs and several prompting approaches, and evaluate\nthe LLMs' performance by using human expert codings. Overall performance\ndiffers greatly between LLMs, and only a fine-tuned LLM achieves satisfactory\nlevels of predictive performance. Performance differences between prompting\napproaches are conditional on the LLM used. Finally, LLMs' unequal\nclassification performance across different categories of reasons for survey\nparticipation results in different categorical distributions when not using\nfine-tuning. We discuss the implications of these findings, both for\nmethodological research on coding open-ended responses and for their\nsubstantive analysis, and for practitioners processing or substantively\nanalyzing such data. Finally, we highlight the many trade-offs researchers need\nto consider when choosing automated methods for open-ended response\nclassification in the age of LLMs. In doing so, our study contributes to the\ngrowing body of research about the conditions under which LLMs can be\nefficiently, accurately, and reliably leveraged in survey research.",
      "url": "http://arxiv.org/abs/2506.14634v1",
      "published_time_eastern_timestamp": 1750174133.0
    },
    {
      "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning",
      "summary": "Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito",
      "url": "http://arxiv.org/abs/2506.14629v1",
      "published_time_eastern_timestamp": 1750173870.0
    },
    {
      "title": "Low-code to fight climate change: the Climaborough project",
      "summary": "The EU-funded Climaborough project supports European cities to achieve carbon\nneutrality by 2030. Eleven cities in nine countries will deploy in real\nconditions products and services fostering climate transition in their local\nenvironment. The Climaborough City Platform is being developed to monitor the\ncities' overall progress towards their climate goals by aggregating historic\nand real-time data and displaying the results in user-friendly dashboards that\nwill be used by non-technical experts to evaluate the effectiveness of local\nexperimental initiatives, identify those that yield significant impact, and\nassess the potential consequences of scaling them up to a broader level. In\nthis paper, we explain how we have put in place a low-code/no-code strategy in\nClimaborough in response to the project's aim to quickly deploy climate\ndashboards. A low-code strategy is used to accelerate the development of the\ndashboards. The dashboards embed a no-code philosophy that enables all types of\ncitizen profiles to configure and adapt the dashboard to their specific needs.",
      "url": "http://arxiv.org/abs/2506.14623v1",
      "published_time_eastern_timestamp": 1750173552.0
    }
  ]
}
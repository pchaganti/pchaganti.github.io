{
  "last_updated": "2025-10-09T05:13:13.448161-04:00",
  "papers": [
    {
      "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
      "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
      "url": "http://arxiv.org/abs/2510.07318v1",
      "published_time_eastern_timestamp": 1759946395.0
    },
    {
      "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
      "summary": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
      "url": "http://arxiv.org/abs/2510.07315v1",
      "published_time_eastern_timestamp": 1759946359.0
    },
    {
      "title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation",
      "summary": "Video DiTs have advanced video generation, yet they still struggle to model\nmulti-instance or subject-object interactions. This raises a key question: How\ndo these models internally represent interactions? To answer this, we curate\nMATRIX-11K, a video dataset with interaction-aware captions and multi-instance\nmask tracks. Using this dataset, we conduct a systematic analysis that\nformalizes two perspectives of video DiTs: semantic grounding, via\nvideo-to-text attention, which evaluates whether noun and verb tokens capture\ninstances and their relations; and semantic propagation, via video-to-video\nattention, which assesses whether instance bindings persist across frames. We\nfind both effects concentrate in a small subset of interaction-dominant layers.\nMotivated by this, we introduce MATRIX, a simple and effective regularization\nthat aligns attention in specific layers of video DiTs with multi-instance mask\ntracks from the MATRIX-11K dataset, enhancing both grounding and propagation.\nWe further propose InterGenEval, an evaluation protocol for interaction-aware\nvideo generation. In experiments, MATRIX improves both interaction fidelity and\nsemantic alignment while reducing drift and hallucination. Extensive ablations\nvalidate our design choices. Codes and weights will be released.",
      "url": "http://arxiv.org/abs/2510.07310v1",
      "published_time_eastern_timestamp": 1759946258.0
    },
    {
      "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the\n  Business Domain",
      "summary": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21\\% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.",
      "url": "http://arxiv.org/abs/2510.07309v1",
      "published_time_eastern_timestamp": 1759946255.0
    },
    {
      "title": "SpecGuard: Spectral Projection-based Advanced Invisible Watermarking",
      "summary": "Watermarking embeds imperceptible patterns into images for authenticity\nverification. However, existing methods often lack robustness against various\ntransformations primarily including distortions, image regeneration, and\nadversarial perturbation, creating real-world challenges. In this work, we\nintroduce SpecGuard, a novel watermarking approach for robust and invisible\nimage watermarking. Unlike prior approaches, we embed the message inside hidden\nconvolution layers by converting from the spatial domain to the frequency\ndomain using spectral projection of a higher frequency band that is decomposed\nby wavelet projection. Spectral projection employs Fast Fourier Transform\napproximation to transform spatial data into the frequency domain efficiently.\nIn the encoding phase, a strength factor enhances resilience against diverse\nattacks, including adversarial, geometric, and regeneration-based distortions,\nensuring the preservation of copyrighted information. Meanwhile, the decoder\nleverages Parseval's theorem to effectively learn and extract the watermark\npattern, enabling accurate retrieval under challenging transformations. We\nevaluate the proposed SpecGuard based on the embedded watermark's invisibility,\ncapacity, and robustness. Comprehensive experiments demonstrate the proposed\nSpecGuard outperforms the state-of-the-art models. To ensure reproducibility,\nthe full code is released on\n\\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\\textcolor{blue}{\\textbf{GitHub}}}.",
      "url": "http://arxiv.org/abs/2510.07302v1",
      "published_time_eastern_timestamp": 1759946181.0
    },
    {
      "title": "Fine-Grained Unambiguous Measurements",
      "summary": "Unambiguous measurements play an important role in quantum information, with\napplications ranging from quantum key distribution to quantum state\nreconstruction. Recently, such measurements have also been used in quantum\nalgorithms based on Regev's reduction. The key problem for these algorithms is\nthe S-LWE problem for lattice problems, or the Quantum Decoding Problem for\ncode problems. A key idea for addressing this problem is to use unambiguous\nmeasurements to recover $k$ coordinates of a code (or lattice) element $x$ from\na quantum state $|\\psi_x\\rangle$, which corresponds to a noisy word $x$ with\nerrors in quantum superposition. However, a general theoretical framework to\nanalyze this approach has been lacking.\n  In this work, we introduce the notion of fine-grained unambiguous\nmeasurements. Given a family of states\n$\\{\\,|\\psi_x\\rangle\\,\\}_{x\\in\\{0,1\\}^n}$, we ask whether there exist\nmeasurements that can return, with certainty, $k$ bits of information about\n$x$. We study this question in the setting of symmetric states, which naturally\narises in the Quantum Decoding Problem. We show that determining the maximal\nnumber of parities that a measurement can output can be formulated as a linear\nprogram, and we use its dual formulation to derive several upper bounds. In\nparticular, we establish necessary and sufficient conditions for the existence\nof fine-grained unambiguous measurements and prove impossibility results\nshowing, in particular, that such measurements cannot improve upon the approach\nof arXiv:2310.20651. Finally, we discuss the implications of these findings for\nthe Quantum Decoding Problem.",
      "url": "http://arxiv.org/abs/2510.07298v1",
      "published_time_eastern_timestamp": 1759945988.0
    },
    {
      "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
      "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
      "url": "http://arxiv.org/abs/2510.07293v1",
      "published_time_eastern_timestamp": 1759945816.0
    },
    {
      "title": "Evolutionary Profiles for Protein Fitness Prediction",
      "summary": "Predicting the fitness impact of mutations is central to protein engineering\nbut constrained by limited assays relative to the size of sequence space.\nProtein language models (pLMs) trained with masked language modeling (MLM)\nexhibit strong zero-shot fitness prediction; we provide a unifying view by\ninterpreting natural evolution as implicit reward maximization and MLM as\ninverse reinforcement learning (IRL), in which extant sequences act as expert\ndemonstrations and pLM log-odds serve as fitness estimates. Building on this\nperspective, we introduce EvoIF, a lightweight model that integrates two\ncomplementary sources of evolutionary signal: (i) within-family profiles from\nretrieved homologs and (ii) cross-family structural-evolutionary constraints\ndistilled from inverse folding logits. EvoIF fuses sequence-structure\nrepresentations with these profiles via a compact transition block, yielding\ncalibrated probabilities for log-odds scoring. On ProteinGym (217 mutational\nassays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve\nstate-of-the-art or competitive performance while using only 0.15% of the\ntraining data and fewer parameters than recent large models. Ablations confirm\nthat within-family and cross-family profiles are complementary, improving\nrobustness across function types, MSA depths, taxa, and mutation depths. The\ncodes will be made publicly available at https://github.com/aim-uofa/EvoIF.",
      "url": "http://arxiv.org/abs/2510.07286v1",
      "published_time_eastern_timestamp": 1759945562.0
    },
    {
      "title": "Content-Adaptive Inference for State-of-the-art Learned Video\n  Compression",
      "summary": "While the BD-rate performance of recent learned video codec models in both\nlow-delay and random-access modes exceed that of respective modes of\ntraditional codecs on average over common benchmarks, the performance\nimprovements for individual videos with complex/large motions is much smaller\ncompared to scenes with simple motion. This is related to the inability of a\nlearned encoder model to generalize to motion vector ranges that have not been\nseen in the training set, which causes loss of performance in both coding of\nflow fields as well as frame prediction and coding. As a remedy, we propose a\ngeneric (model-agnostic) framework to control the scale of motion vectors in a\nscene during inference (encoding) to approximately match the range of motion\nvectors in the test and training videos by adaptively downsampling frames. This\nresults in down-scaled motion vectors enabling: i) better flow estimation;\nhence, frame prediction and ii) more efficient flow compression. We show that\nthe proposed framework for content-adaptive inference improves the BD-rate\nperformance of already state-of-the-art low-delay video codec DCVC-FM by up to\n41\\% on individual videos without any model fine tuning. We present ablation\nstudies to show measures of motion and scene complexity can be used to predict\nthe effectiveness of the proposed framework.",
      "url": "http://arxiv.org/abs/2510.07283v1",
      "published_time_eastern_timestamp": 1759945416.0
    },
    {
      "title": "A Comoving Framework for Planet Migration",
      "summary": "The migration of planets within their nascent protoplanetary disks is a\nfundamental process that shapes the final architecture of planetary systems.\nHowever, studying this phenomenon through direct hydrodynamical simulations is\ncomputationally demanding, with traditional methods on fixed grids being\nill-suited for tracking planet migration over long timescales due to their high\ncost and limited domain. In this work, we present a self-consistent comoving\nframework designed to overcome these challenges. Our method employs a\ncoordinate transformation that scales with the planet's evolving semi-major\naxis, keeping the planet stationary with respect to its local computational\ngrid. This transforms the standard hydrodynamic equations by introducing a\nsource term that accounts for the inertial forces of the non-inertial reference\nframe. We implement this framework in the FARGO3D code and validate it through\na benchmark test, demonstrating excellent agreement with conventional\nfixed-grid simulations until the latter are compromised by boundary effects.\nOur analysis shows that the comoving method can be over an order of magnitude\nmore computationally efficient, dramatically reducing the cost of simulating\nmigrating planets. Furthermore, the framework's adaptability enables efficient,\nhigh-resolution studies of planets on eccentric orbits by keeping them\nstationary within the computational grid. This framework serves as both a\npowerful numerical and theoretical tool, simplifying the time-dependent flow\naround a migrating planet that offers clearer physical insight. It enables\nlong-term, self-consistent studies of planet-disk interaction, representing a\ncrucial step towards performing planet-population synthesis based on full\nhydrodynamical simulations.",
      "url": "http://arxiv.org/abs/2510.07282v1",
      "published_time_eastern_timestamp": 1759945398.0
    },
    {
      "title": "Transversal dimension jump for product qLDPC codes",
      "summary": "We introduce transversal dimension jump, a code-switching protocol for lifted\nproduct (LP) quantum low-density parity-check (qLDPC) codes across different\nchain-complex dimensions, enabling universal fault-tolerant quantum computation\nwith low overhead. The construction leverages the product structure of LP codes\nto implement one-way transversal CNOTs between a 3D code and its 2D component\ncodes, enabling teleportation-based switching. Combined with constant-depth CCZ\ngates in 3D LP codes and low-overhead transversal Clifford gates in 2D LP\ncodes, this yields universal, high-rate quantum logical computation with high\nthresholds and low space-time costs. Beyond asymptotic schemes, we identify\nexplicit 3D-2D LP code pairs supporting cup-product CCZ gates, including\nbivariate tricycle-bicycle families such as the $[[81,3,5]]$-$[[54,2,6]]$ pair,\nwhere the 3D tricycle codes admit depth-2 CCZ, weight-6 stabilizers, and\npseudo-thresholds $\\gtrsim 0.4\\%$. As a byproduct, we show that the 3D codes\nenable highly efficient magic-state preparation: a single round of stabilizer\nmeasurements followed by depth-2 CCZ and postselection produces states with\nerror $<10^{-9}$ and success probability $\\sim 35\\%$. Our results establish a\nnative integration of qLDPC codes with complementary transversal gates-covering\nnearly all practically relevant families known so far-and open a broad design\nspace for scalable, low-overhead universal quantum computation.",
      "url": "http://arxiv.org/abs/2510.07269v1",
      "published_time_eastern_timestamp": 1759944907.0
    },
    {
      "title": "ARGscape: A modular, interactive tool for manipulation of spatiotemporal\n  ancestral recombination graphs",
      "summary": "Ancestral recombination graphs (ARGs) encode the complete genealogical\nhistory of a population of recombining lineages. ARGs, and their succinct\nrepresentation, tree sequences, are increasingly central to modern population\ngenetics methods, yet building an intuition for ARGs remains challenging. This\nis particularly true when analyzing ancestry in a geographic context, as there\nis a critical lack of dedicated, interactive tools capable of visualizing ARGs\nas spatiotemporal objects. To address this gap, we introduce ARGscape, an\ninteractive platform for simulating, analyzing, and visualizing ARGs across\nspace and time. ARGscape provides a user-friendly graphical interface featuring\ndynamic 2- and 3-dimensional visualizations to explore ARGs through space and\ntime, as well as a novel \"spatial diff\" visualization for quantitative\ncomparison of geographic inference methods. ARGscape is an innovative, unified\nframework that seamlessly integrates leading command-line, Python, and R-based\ntools for ARG simulation, manipulation, and use in spatiotemporal inference\ninto both graphical and command-line interfaces. By integrating these various\nfunctionalities, ARGscape facilitates novel data exploration and hypothesis\ngeneration, while lowering the barrier to entry for spatiotemporal ARG analysis\nin both research and education use-cases. ARGscape is built with a Python\nFastAPI backend and a React/TypeScript frontend. It is freely available as a\nlive demo at https://www.argscape.com and as a Python package on PyPI (pip\ninstall argscape). The source code and documentation are available on GitHub at\nhttps://github.com/chris-a-talbot/argscape.",
      "url": "http://arxiv.org/abs/2510.07255v1",
      "published_time_eastern_timestamp": 1759944027.0
    },
    {
      "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model\n  Factuality Evaluation",
      "summary": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
      "url": "http://arxiv.org/abs/2510.07238v1",
      "published_time_eastern_timestamp": 1759943167.0
    },
    {
      "title": "A Bernstein polynomial approach for the estimation of cumulative\n  distribution functions in the presence of missing data",
      "summary": "We study nonparametric estimation of univariate cumulative distribution\nfunctions (CDFs) pertaining to data missing at random. The proposed estimators\nsmooth the inverse probability weighted (IPW) empirical CDF with the Bernstein\noperator, yielding monotone, $[0,1]$-valued curves that automatically adapt to\nbounded supports. We analyze two versions: a pseudo estimator that uses known\npropensities and a feasible estimator that uses propensities estimated\nnonparametrically from discrete auxiliary variables, the latter scenario being\nmuch more common in practice. For both, we derive pointwise bias and variance\nexpansions, establish the optimal polynomial degree $m$ with respect to the\nmean integrated squared error, and prove the asymptotic normality. A key\nfinding is that the feasible estimator has a smaller variance than the pseudo\nestimator by an explicit nonnegative correction term. We also develop an\nefficient degree selection procedure via least-squares cross-validation. Monte\nCarlo experiments demonstrate that, for moderate to large sample sizes, the\nBernstein-smoothed feasible estimator outperforms both its unsmoothed\ncounterpart and an integrated version of the IPW kernel density estimator\nproposed by Dubnicka (2009) in the same context. A real-data application to\nfasting plasma glucose from the 2017-2018 NHANES survey illustrates the method\nin a practical setting. All code needed to reproduce our analyses is readily\naccessible on GitHub.",
      "url": "http://arxiv.org/abs/2510.07235v1",
      "published_time_eastern_timestamp": 1759942983.0
    },
    {
      "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and\n  Distillation",
      "summary": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.",
      "url": "http://arxiv.org/abs/2510.07227v1",
      "published_time_eastern_timestamp": 1759942666.0
    },
    {
      "title": "GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in\n  Image Generation",
      "summary": "Text-to-image synthesis has made remarkable progress, yet accurately\ninterpreting complex and lengthy prompts remains challenging, often resulting\nin semantic inconsistencies and missing details. Existing solutions, such as\nfine-tuning, are model-specific and require training, while prior automatic\nprompt optimization (APO) approaches typically lack systematic error analysis\nand refinement strategies, resulting in limited reliability and effectiveness.\nMeanwhile, test-time scaling methods operate on fixed prompts and on noise or\nsample numbers, limiting their interpretability and adaptability. To solve\nthese, we introduce a flexible and efficient test-time prompt optimization\nstrategy that operates directly on the input text. We propose a plug-and-play\nmulti-agent system called GenPilot, integrating error analysis,\nclustering-based adaptive exploration, fine-grained verification, and a memory\nmodule for iterative optimization. Our approach is model-agnostic,\ninterpretable, and well-suited for handling long and complex prompts.\nSimultaneously, we summarize the common patterns of errors and the refinement\nstrategy, offering more experience and encouraging further exploration.\nExperiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%\ndemonstrate the strong capability of our methods in enhancing the text and\nimage consistency and structural coherence of generated images, revealing the\neffectiveness of our test-time prompt optimization strategy. The code is\navailable at https://github.com/27yw/GenPilot.",
      "url": "http://arxiv.org/abs/2510.07217v1",
      "published_time_eastern_timestamp": 1759942312.0
    },
    {
      "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient\n  Multilingual Control for Large Language Models",
      "summary": "Large language models exhibit strong multilingual capabilities despite\nlimited exposure to non-English data. Prior studies show that English-centric\nlarge language models map multilingual content into English-aligned\nrepresentations at intermediate layers and then project them back into\ntarget-language token spaces in the final layer. From this observation, we\nhypothesize that this cross-lingual transition is governed by a small and\nsparse set of dimensions, which occur at consistent indices across the\nintermediate to final layers. Building on this insight, we introduce a simple,\ntraining-free method to identify and manipulate these dimensions, requiring\nonly as few as 50 sentences of either parallel or monolingual data. Experiments\non a multilingual generation control task reveal the interpretability of these\ndimensions, demonstrating that the interventions in these dimensions can switch\nthe output language while preserving semantic content, and that it surpasses\nthe performance of prior neuron-based approaches at a substantially lower cost.",
      "url": "http://arxiv.org/abs/2510.07213v1",
      "published_time_eastern_timestamp": 1759942017.0
    },
    {
      "title": "COMPAct: Computational Optimization and Automated Modular design of\n  Planetary Actuators",
      "summary": "The optimal design of robotic actuators is a critical area of research, yet\nlimited attention has been given to optimizing gearbox parameters and\nautomating actuator CAD. This paper introduces COMPAct: Computational\nOptimization and Automated Modular Design of Planetary Actuators, a framework\nthat systematically identifies optimal gearbox parameters for a given motor\nacross four gearbox types, single-stage planetary gearbox (SSPG), compound\nplanetary gearbox (CPG), Wolfrom planetary gearbox (WPG), and double-stage\nplanetary gearbox (DSPG). The framework minimizes mass and actuator width while\nmaximizing efficiency, and further automates actuator CAD generation to enable\ndirect 3D printing without manual redesign. Using this framework, optimal\ngearbox designs are explored over a wide range of gear ratios, providing\ninsights into the suitability of different gearbox types across various gear\nratio ranges. In addition, the framework is used to generate CAD models of all\nfour gearbox types with varying gear ratios and motors. Two actuator types are\nfabricated and experimentally evaluated through power efficiency, no-load\nbacklash, and transmission stiffness tests. Experimental results indicate that\nthe SSPG actuator achieves a mechanical efficiency of 60-80 %, a no-load\nbacklash of 0.59 deg, and a transmission stiffness of 242.7 Nm/rad, while the\nCPG actuator demonstrates 60 % efficiency, 2.6 deg backlash, and a stiffness of\n201.6 Nm/rad. Code available at:\nhttps://anonymous.4open.science/r/COMPAct-SubNum-3408 Video:\nhttps://youtu.be/99zOKgxsDho",
      "url": "http://arxiv.org/abs/2510.07197v1",
      "published_time_eastern_timestamp": 1759940931.0
    },
    {
      "title": "Prompt, Synthesize, Fine-Tune: A Secure Code Generation Recipe",
      "summary": "Although Large Language Models (LLMs) show promising solutions to automated\ncode generation, they often produce insecure code that threatens software\nsecurity. Current approaches (e.g., SafeCoder) to improve secure code\ngeneration suffer from limited and imbalanced datasets, reducing their\neffectiveness and generalizability. In this work, we present Secure-Instruct, a\nnovel framework that automatically synthesizes high-quality vulnerable and\nsecure code examples, generates fine-tuning instructions, and instruction-tunes\nLLMs to align task description and secure code generation abilities. We\nevaluate Secure-Instruct on four representative LLMs using two benchmarks: our\nown CWEBench and the existing CWEval. CWEBench comprises 93 scenarios on 44\nCWEs, all without overlap with Secure-Instruct's synthetic instruction-tuning\ndataset, while CWEval covers 31 CWEs with 119 manually verified\nsecurity-critical tasks. We find that Secure-Instruct improves not only the\nsecurity but also the functional correctness of the generated code. On\nCWEBench, Secure-Instruct substantially improves secure code generation, giving\na 14.3% average increase in secure ratio over the pretrained models and\noutperforms SafeCoder by 7.6%. On CWEval, Secure-Instruct achieves a 14%\nincrease for CodeLlama-7B and 5.8% for Mistral-7B in Func-Sec@1 over pretrained\nmodels, and surpasses SafeCoder by 15.8% and 6.8% respectively.",
      "url": "http://arxiv.org/abs/2510.07189v1",
      "published_time_eastern_timestamp": 1759940649.0
    },
    {
      "title": "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for\n  Robotics",
      "summary": "Vision-Language Models (VLMs) have shown remarkable capabilities in spatial\nreasoning, yet they remain fundamentally limited to qualitative precision and\nlack the computational precision required for real-world robotics. Current\napproaches fail to leverage metric cues from depth sensors and camera\ncalibration, instead reducing geometric problems to pattern recognition tasks\nthat cannot deliver the centimeter-level accuracy essential for robotic\nmanipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel\nframework that transforms VLMs from perceptual estimators to geometric\ncomputers by enabling them to generate and execute precise geometric\ncomputations through external tools. Rather than attempting to internalize\ncomplex geometric operations within neural networks, TIGeR empowers models to\nrecognize geometric reasoning requirements, synthesize appropriate\ncomputational code, and invoke specialized libraries for exact calculations. To\nsupport this paradigm, we introduce TIGeR-300K, a comprehensive\ntool-invocation-oriented dataset covering point transformations, pose\nestimation, trajectory generation, and spatial compatibility verification,\ncomplete with tool invocation sequences and intermediate computations. Through\na two-stage training pipeline combining supervised fine-tuning (SFT) and\nreinforcement fine-tuning (RFT) with our proposed hierarchical reward design,\nTIGeR achieves SOTA performance on geometric reasoning benchmarks while\ndemonstrating centimeter-level precision in real-world robotic manipulation\ntasks.",
      "url": "http://arxiv.org/abs/2510.07181v1",
      "published_time_eastern_timestamp": 1759940423.0
    }
  ]
}
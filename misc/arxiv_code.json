{
  "last_updated": "2025-11-11T10:14:12.611768-05:00",
  "papers": [
    {
      "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with\n  Contact Fields",
      "summary": "Despite years of research, real-time diverse grasp synthesis for dexterous\nhands remains an unsolved core challenge in robotics and computer graphics. We\npresent Lightning Grasp, a novel high-performance procedural grasp synthesis\nalgorithm that achieves orders-of-magnitude speedups over state-of-the-art\napproaches, while enabling unsupervised grasp generation for irregular,\ntool-like objects. The method avoids many limitations of prior approaches, such\nas the need for carefully tuned energy functions and sensitive initialization.\nThis breakthrough is driven by a key insight: decoupling complex geometric\ncomputation from the search process via a simple, efficient data structure -\nthe Contact Field. This abstraction collapses the problem complexity, enabling\na procedural search at unprecedented speeds. We open-source our system to\npropel further innovation in robotic manipulation.",
      "url": "http://arxiv.org/abs/2511.07418v1",
      "published_time_eastern_timestamp": 1762801184.0
    },
    {
      "title": "SPOT: An Annotated French Corpus and Benchmark for Detecting Critical\n  Interventions in Online Conversations",
      "summary": "We introduce SPOT (Stopping Points in Online Threads), the first annotated\ncorpus translating the sociological concept of stopping point into a\nreproducible NLP task. Stopping points are ordinary critical interventions that\npause or redirect online discussions through a range of forms (irony, subtle\ndoubt or fragmentary arguments) that frameworks like counterspeech or social\ncorrection often overlook. We operationalize this concept as a binary\nclassification task and provide reliable annotation guidelines. The corpus\ncontains 43,305 manually annotated French Facebook comments linked to URLs\nflagged as false information by social media users, enriched with contextual\nmetadata (article, post, parent comment, page or group, and source). We\nbenchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs\nunder various prompting strategies. Results show that fine-tuned encoders\noutperform prompted LLMs in F1 score by more than 10 percentage points,\nconfirming the importance of supervised learning for emerging non-English\nsocial media tasks. Incorporating contextual metadata further improves encoder\nmodels F1 scores from 0.75 to 0.78. We release the anonymized dataset, along\nwith the annotation guidelines and code in our code repository, to foster\ntransparency and reproducible research.",
      "url": "http://arxiv.org/abs/2511.07405v1",
      "published_time_eastern_timestamp": 1762800880.0
    },
    {
      "title": "A Diffusion Model to Shrink Proteins While Maintaining Their Function",
      "summary": "Many proteins useful in modern medicine or bioengineering are challenging to\nmake in the lab, fuse with other proteins in cells, or deliver to tissues in\nthe body, because their sequences are too long. Shortening these sequences\ntypically involves costly, time-consuming experimental campaigns. Ideally, we\ncould instead use modern models of massive databases of sequences from nature\nto learn how to propose shrunken proteins that resemble sequences found in\nnature. Unfortunately, these models struggle to efficiently search the\ncombinatorial space of all deletions, and are not trained with inductive biases\nto learn how to delete. To address this gap, we propose SCISOR, a novel\ndiscrete diffusion model that deletes letters from sequences to generate\nprotein samples that resemble those found in nature. To do so, SCISOR trains a\nde-noiser to reverse a forward noising process that adds random insertions to\nnatural sequences. As a generative model, SCISOR fits evolutionary sequence\ndata competitively with previous large models. In evaluation, SCISOR achieves\nstate-of-the-art predictions of the functional effects of deletions on\nProteinGym. Finally, we use the SCISOR de-noiser to shrink long protein\nsequences, and show that its suggested deletions result in significantly more\nrealistic proteins and more often preserve functional motifs than previous\nmodels of evolutionary sequences.",
      "url": "http://arxiv.org/abs/2511.07390v1",
      "published_time_eastern_timestamp": 1762800384.0
    },
    {
      "title": "samsara: A Continuous-Time Markov Chain Monte Carlo Sampler for\n  Trans-Dimensional Bayesian Analysis",
      "summary": "Bayesian inference requires determining the posterior distribution, a task\nthat becomes particularly challenging when the dimension of the parameter space\nis large and unknown. This limitation arises in many physics problems, such as\nMixture Models (MM) with an unknown number of components or the inference of\noverlapping signals in noisy data, as in the Laser Interferometer Space Antenna\n(LISA) Global Fit problem. Traditional approaches, such as product-space\nmethods or Reversible-Jump Markov Chain Monte Carlo (RJMCMC), often face\nefficiency and convergence limitations. This paper presents samsara, a\nContinuous-Time Markov Chain Monte Carlo (CTMCMC) framework that models\nparameter evolution through Poisson-driven birth, death, and mutation\nprocesses. samsara is designed to sample models of unknown dimensionality. By\nrequiring detailed balance through adaptive rate definitions, CTMCMC achieves\nautomatic acceptance of trans-dimensional moves and high sampling efficiency.\nThe code features waiting time weighted estimators, optimized memory storage,\nand a modular design for easy customization. We validate samsara on three\nbenchmark problems: an analytic trans-dimensional distribution, joint inference\nof sine waves and Lorentzians in time series, and a Gaussian MM with an unknown\nnumber of components. In all cases, the code shows excellent agreement with\nanalytical and Nested Sampling results. All these features push samsara as a\npowerful alternative to RJMCMC for large- and variable-dimensional Bayesian\ninference problems.",
      "url": "http://arxiv.org/abs/2511.07385v1",
      "published_time_eastern_timestamp": 1762800254.0
    },
    {
      "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted\n  Recurrence",
      "summary": "Recent advances in depth-recurrent language models show that recurrence can\ndecouple train-time compute and parameter count from test-time compute. In this\nwork, we study how to convert existing pretrained non-recurrent language models\ninto depth-recurrent models. We find that using a curriculum of recurrences to\nincrease the effective depth of the model over the course of training preserves\nperformance while reducing total computational cost. In our experiments, on\nmathematics, we observe that converting pretrained models to recurrent ones\nresults in better performance at a given compute budget than simply\npost-training the original non-recurrent language model.",
      "url": "http://arxiv.org/abs/2511.07384v1",
      "published_time_eastern_timestamp": 1762800187.0
    },
    {
      "title": "Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for\n  Bangla-to-Python Code Generation",
      "summary": "Large Language Models (LLMs) have advanced the automated generation of code\nfrom natural language prompts. However, low-resource languages (LRLs) like\nBangla remain underrepresented due to the limited availability of\ninstruction-to-code datasets and evaluation benchmarks. To address this, the\nBLP Workshop at IJCNLP-AACL 2025 introduced a shared task on \"Code Generation\nin Bangla\". In this work, we propose a method that combines instruction\nprompting with a test-driven, feedback-guided iterative refinement process\nusing a fine-tuned Qwen2.5-14B model. The model generates code from Bangla\ninstructions, tests it against unit tests, and iteratively refines any failing\noutputs through three evaluation passes, using test feedback to guide each\nstep. This approach helped our team \"Retriv\" to secure 2nd place in the shared\ntask with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla\ninstruction understanding and Python code generation, emphasizing the need for\ntargeted methods in LRLs. We made experimental scripts publicly available for\nthe community.",
      "url": "http://arxiv.org/abs/2511.07382v1",
      "published_time_eastern_timestamp": 1762800104.0
    },
    {
      "title": "Enhanced GCD through ORBGRAND-AI: Exploiting Partial and Total\n  Correlation in Noise",
      "summary": "There have been significant advances in recent years in the development of\nforward error correction decoders that can decode codes of any structure,\nincluding practical realizations in synthesized circuits and taped out chips.\nWhile essentially all soft-decision decoders assume that bits have been\nimpacted independently on the channel, for one of these new approaches it has\nbeen established that channel dependencies can be exploited to achieve superior\ndecoding accuracy, resulting in Ordered Reliability Bits Guessing Random\nAdditive Noise Decoding Approximate Independence (ORBGRAND-AI). Building on\nthat capability, here we consider the integration of ORBGRAND-AI as a pattern\ngenerator for Guessing Codeword Decoding (GCD). We first establish that a\ndirect approach delivers mildly degraded block error rate (BLER) but with\nreduced number of queried patterns when compared to ORBGRAND-AI. We then show\nthat with a more nuanced approach it is possible to leverage total correlation\nto deliver an additional BLER improvement of around 0.75 dB while retaining\nreduced query numbers.",
      "url": "http://arxiv.org/abs/2511.07376v1",
      "published_time_eastern_timestamp": 1762799663.0
    },
    {
      "title": "AcousTools: A `Full-Stack', Python-Based, Acoustic Holography Library",
      "summary": "Acoustic Holography is an emerging field where mid-air ultrasound is\ncontrolled and manipulated for novel and exciting applications. These range\nfrom mid-air haptics, volumetric displays, contactless fabrication, and even\nchemical and biomedical applications such as drug delivery. To develop these\napplications, a software framework to predict acoustic behaviour and simulating\nresulting effects, such as applied forces or scattering patterns is desirable.\nThere have been various software libraries and platforms that attempt to fill\nthis role, but there is yet to be a single piece of software that acts as a\n'full-stack' solution. We define this full-stack as the process from\nabstraction to physicalisation starting with setup, modelling acoustic\npropagation, transducer phase retrieval, sound field analysis, and control of\nthe acoustic holographic hardware itself. Existing methods fail to fulfil one\nor more of these categories. To address this, we present AcousTools, a\nPython-based acoustic holography library, designed to support the full suite of\nacoustic holographic applications and we show AcousTools's ability to meet each\nstep of the full-stack's requirements. AcousTools has the potential to become\nthe standard code library for acoustic holography, with the uniquely complete\nsuite of features wrapped in a language that is known to be easy to use,\nAcousTools will increase the ability for researchers to develop novel\napplications as well as accurately review other's work. The full-stack, aside\nfrom software, will also be useful for researchers - providing a way to view\nand compare methodologies by understanding where they fit into the stack.",
      "url": "http://arxiv.org/abs/2511.07336v1",
      "published_time_eastern_timestamp": 1762796193.0
    },
    {
      "title": "FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework\n  for Equity Research Report Generation",
      "summary": "While LLMs have shown great success in financial tasks like stock prediction\nand question answering, their application in fully automating Equity Research\nReport generation remains uncharted territory. In this paper, we formulate the\nEquity Research Report (ERR) Generation task for the first time. To address the\ndata scarcity and the evaluation metrics absence, we present an open-source\nevaluation benchmark for ERR generation - FinRpt. We frame a Dataset\nConstruction Pipeline that integrates 7 financial data types and produces a\nhigh-quality ERR dataset automatically, which could be used for model training\nand evaluation. We also introduce a comprehensive evaluation system including\n11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent\nframework specifically tailored to address this task, named FinRpt-Gen, and\ntrain several LLM-based agents on the proposed datasets using Supervised\nFine-Tuning and Reinforcement Learning. Experimental results indicate the data\nquality and metrics effectiveness of the benchmark FinRpt and the strong\nperformance of FinRpt-Gen, showcasing their potential to drive innovation in\nthe ERR generation field. All code and datasets are publicly available.",
      "url": "http://arxiv.org/abs/2511.07322v1",
      "published_time_eastern_timestamp": 1762795352.0
    },
    {
      "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with\n  Adaptive Verifiable Environments",
      "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable\nEnvironments (RLVE), an approach using verifiable environments that\nprocedurally generate problems and provide algorithmically verifiable rewards,\nto scale up RL for language models (LMs). RLVE enables each verifiable\nenvironment to dynamically adapt its problem difficulty distribution to the\npolicy model's capabilities as training progresses. In contrast, static data\ndistributions often lead to vanishing learning signals when problems are either\ntoo easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a\nlarge-scale suite of 400 verifiable environments carefully developed through\nmanual environment engineering. Using RLVE-Gym, we show that environment\nscaling, i.e., expanding the collection of training environments, consistently\nimproves generalizable reasoning capabilities. RLVE with joint training across\nall 400 environments in RLVE-Gym yields a 3.37% absolute average improvement\nacross six reasoning benchmarks, starting from one of the strongest 1.5B\nreasoning LMs. By comparison, continuing this LM's original RL training yields\nonly a 0.49% average absolute gain despite using over 3x more compute. We\nrelease our code publicly.",
      "url": "http://arxiv.org/abs/2511.07317v1",
      "published_time_eastern_timestamp": 1762795115.0
    }
  ]
}
{
  "last_updated": "2025-09-25T20:53:37.892987-04:00",
  "papers": [
    {
      "title": "The HyLight model for hydrogen emission lines in simulated nebulae",
      "summary": "Hydrogen recombination lines are key diagnostics of ionized gas in the\ninterstellar medium (ISM), particularly within photoionized nebulae.\nHydrodynamical simulations, even those that include radiative transfer, do not\nusually determine the level population of hydrogen required to compute line\nintensities, but rather interpolate them from pre-computed tables. Here we\npresent the HyLight atomic model, which captures the dominant processes\ngoverning the level populations, enabling the calculation of all dipole-allowed\nhydrogen transitions as well as two-photon transitions from the 2s to 1s state\nwithout the need to pre-computed tables. We compare HyLight predictions to\nthose of other codes and published tables, finding differences between the\nvarious rates of up to factors of several per cent for common transitions,\nincluding those of the Balmer and Brackett series. However, we find sub-per\ncent agreement between HyLight and the Cloudy spectral synthesis code when\nenforcing photo-ionisation equilibrium in gas under typical nebular conditions\nof density and temperature. Importantly, HyLight can also predict emissivities\nif the gas is not in photo-ionisation equilibrium. As examples, we compute the\nratios between the total photoionization rate and line intensities in a nebula,\nand post-process a snapshot from Sparcs, a hydrodynamical code that combines\nradiative transfer with non-equilibrium physics, and compute mock hydrogen\nemission line maps which can be compared directly to observations. Implemented\nin Python, HyLight is an accurate tool for determining the level population in\nneutral hydrogen, a crucial step in bridging the gap between simulations and\nobservations in studies of photoionized regions in galaxies.",
      "url": "http://arxiv.org/abs/2509.20361v1",
      "published_time_eastern_timestamp": 1758736789.0
    },
    {
      "title": "Language Models that Think, Chat Better",
      "summary": "Reinforcement learning with verifiable rewards (RLVR) improves language model\nreasoning by using rule-based rewards in verifiable domains such as mathematics\nand code. However, RLVR leads to limited generalization for open-ended tasks --\nsuch as writing outline essays or making meal plans -- where humans reason\nroutinely. This paper shows that the RLVR paradigm is effective beyond\nverifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking\n(**RLMT**) for general-purpose chat capabilities. Using diverse real-world\nprompts, RLMT requires LMs to generate long CoT reasoning before response, and\noptimizes them with online RL against a preference-based reward model used in\nRLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and\ninstruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT\nconsistently outperforms standard RLHF pipelines. This includes substantial\ngains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and\nArenaHardV2), along with 1-3 point improvements on other tasks like creative\nwriting and general knowledge. Our best 8B model surpasses GPT-4o in chat and\ncreative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be\napplied directly to base models without an SFT stage, akin to R1-Zero training.\nRemarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT\nrecipe outperforms Llama-3.1-8B-Instruct post-trained with a complex\nmulti-staged pipeline with 25M+ examples. We close with qualitative and\nquantitative analyses of how trained models plan their responses. Our results\nrethink the post-training pipeline and call upon future work to understand and\nemploy thinking more broadly.",
      "url": "http://arxiv.org/abs/2509.20357v1",
      "published_time_eastern_timestamp": 1758736654.0
    },
    {
      "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
      "summary": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.",
      "url": "http://arxiv.org/abs/2509.20354v1",
      "published_time_eastern_timestamp": 1758736611.0
    },
    {
      "title": "xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G\n  Wireless Networks for Real-Time Digital Agriculture",
      "summary": "Advanced scientific applications require coupling distributed sensor networks\nwith centralized high-performance computing facilities. Citrus Under Protective\nScreening (CUPS) exemplifies this need in digital agriculture, where citrus\nresearch facilities are instrumented with numerous sensors monitoring\nenvironmental conditions and detecting protective screening damage. CUPS\ndemands access to computational fluid dynamics codes for modeling environmental\nconditions and guiding real-time interventions like water application or\nrobotic repairs. These computing domains have contrasting properties: sensor\nnetworks provide low-performance, limited-capacity, unreliable data access,\nwhile high-performance facilities offer enormous computing power through\nhigh-latency batch processing. Private 5G networks present novel capabilities\naddressing this challenge by providing low latency, high throughput, and\nreliability necessary for near-real-time coupling of edge sensor networks with\nHPC simulations. This work presents xGFabric, an end-to-end system coupling\nsensor networks with HPC facilities through Private 5G networks. The prototype\nconnects remote sensors via 5G network slicing to HPC systems, enabling\nreal-time digital agriculture simulation.",
      "url": "http://arxiv.org/abs/2509.20340v1",
      "published_time_eastern_timestamp": 1758735211.0
    },
    {
      "title": "A Recovery Guarantee for Sparse Neural Networks",
      "summary": "We prove the first guarantees of sparse recovery for ReLU neural networks,\nwhere the sparse network weights constitute the signal to be recovered.\nSpecifically, we study structural properties of the sparse network weights for\ntwo-layer, scalar-output networks under which a simple iterative hard\nthresholding algorithm recovers these weights exactly, using memory that grows\nlinearly in the number of nonzero weights. We validate this theoretical result\nwith simple experiments on recovery of sparse planted MLPs, MNIST\nclassification, and implicit neural representations. Experimentally, we find\nperformance that is competitive with, and often exceeds, a high-performing but\nmemory-inefficient baseline based on iterative magnitude pruning.",
      "url": "http://arxiv.org/abs/2509.20323v1",
      "published_time_eastern_timestamp": 1758733848.0
    },
    {
      "title": "A Comprehensive Evaluation of YOLO-based Deer Detection Performance on\n  Edge Devices",
      "summary": "The escalating economic losses in agriculture due to deer intrusion,\nestimated to be in the hundreds of millions of dollars annually in the U.S.,\nhighlight the inadequacy of traditional mitigation strategies since these\nmethods are often labor-intensive, costly, and ineffective for modern farming\nsystems. To overcome this, there is a critical need for intelligent, autonomous\nsolutions which require accurate and efficient deer detection. But the progress\nin this field is impeded by a significant gap in the literature, mainly the\nlack of a domain-specific, practical dataset and limited study on the on-field\ndeployability of deer detection systems. Addressing this gap, this study\npresents a comprehensive evaluation of state-of-the-art deep learning models\nfor deer detection in challenging real-world scenarios. The contributions of\nthis work are threefold. First, we introduce a curated, publicly available\ndataset of 3,095 annotated images with bounding-box annotations of deer,\nderived from the Idaho Cameratraps project. Second, we provide an extensive\ncomparative analysis of 12 model variants across four recent YOLO\narchitectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a\nhigh-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing\nplatforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the\nreal-time detection is not feasible in Raspberry Pi without hardware-specific\nmodel optimization, while NVIDIA Jetson provides greater than 30 FPS with\nGPU-accelerated inference on 's' and 'n' series models. This study also reveals\nthat smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and\nYOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and\ncomputational efficiency (FPS > 30). To support further research, both the\nsource code and datasets are publicly available at\nhttps://github.com/WinnerBishal/track-the-deer.",
      "url": "http://arxiv.org/abs/2509.20318v1",
      "published_time_eastern_timestamp": 1758733310.0
    },
    {
      "title": "mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies",
      "summary": "End-to-end learning of robot control policies, structured as neural networks,\nhas emerged as a promising approach to robotic manipulation. To complete many\ncommon tasks, relevant objects are required to pass in and out of a robot's\nfield of view. In these settings, spatial memory - the ability to remember the\nspatial composition of the scene - is an important competency. However,\nbuilding such mechanisms into robot learning systems remains an open research\nproblem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D\nAction Policies), a 3D diffusion policy that generates robot trajectories based\non a semantic 3D reconstruction of the environment. We show in simulation\nexperiments that our approach is effective at solving tasks where\nstate-of-the-art approaches without memory mechanisms struggle. We release our\nreconstruction system, training code, and evaluation tasks to spur research in\nthis direction.",
      "url": "http://arxiv.org/abs/2509.20297v1",
      "published_time_eastern_timestamp": 1758731396.0
    },
    {
      "title": "FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory\n  for Segmentation-oriented Anomaly Synthesis",
      "summary": "Industrial anomaly segmentation relies heavily on pixel-level annotations,\nyet real-world anomalies are often scarce, diverse, and costly to label.\nSegmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a\npromising alternative; however, existing methods struggle to balance sampling\nefficiency and generation quality. Moreover, most approaches treat all spatial\nregions uniformly, overlooking the distinct statistical differences between\nanomaly and background areas. This uniform treatment hinders the synthesis of\ncontrollable, structure-specific anomalies tailored for segmentation tasks. In\nthis paper, we propose FAST, a foreground-aware diffusion framework featuring\ntwo novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the\nForeground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling\nalgorithm specifically designed for segmentation-oriented industrial anomaly\nsynthesis, which accelerates the reverse process through coarse-to-fine\naggregation and enables the synthesis of state-of-the-art segmentation-oriented\nanomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the\nanomaly-aware noise within the masked foreground regions at each sampling step,\npreserving localized anomaly signals throughout the denoising trajectory.\nExtensive experiments on multiple industrial benchmarks demonstrate that FAST\nconsistently outperforms existing anomaly synthesis methods in downstream\nsegmentation tasks. We release the code at:\nhttps://anonymous.4open.science/r/NeurIPS-938.",
      "url": "http://arxiv.org/abs/2509.20295v1",
      "published_time_eastern_timestamp": 1758731295.0
    },
    {
      "title": "When Judgment Becomes Noise: How Design Failures in LLM Judge Benchmarks\n  Silently Undermine Validity",
      "summary": "LLM-judged benchmarks are increasingly used to evaluate complex model\nbehaviors, yet their design introduces failure modes absent in conventional\nground-truth based benchmarks. We argue that without tight objectives and\nverifiable constructions, benchmark rankings can produce high-confidence\nrankings that are in fact largely noise. We introduce two mechanisms to\ndiagnose these issues. Schematic adherence quantifies how much of a judge's\noverall verdict is explained by the explicit evaluation schema, revealing\nunexplained variance when judges deviate from their own rubric. Psychometric\nvalidity aggregates internal consistency and discriminant validity signals to\nquantify irreducible uncertainty in any benchmarking run. Applying these tools\nto Arena-Hard Auto, we find severe schema incoherence and factor collapse\nacross popular judges: for example, unexplained variance exceeding 90 percent\nfor DeepSeek-R1-32B and factor correlations above 0.93 for most criteria. We\nalso show that the ELO-style aggregation used by Arena-Hard Auto collapses and\nmasks genuine ranking uncertainty. Our results highlight design failures that\nundermine validity and offer actionable principles for building better-scoped,\nreliability-aware LLM-judged benchmarks. We release our code at\nhttps://anonymous.4open.science/r/judgment-to-noise-947D/README.md",
      "url": "http://arxiv.org/abs/2509.20293v1",
      "published_time_eastern_timestamp": 1758731207.0
    },
    {
      "title": "PGCLODA: Prompt-Guided Graph Contrastive Learning for\n  Oligopeptide-Infectious Disease Association Prediction",
      "summary": "Infectious diseases continue to pose a serious threat to public health,\nunderscoring the urgent need for effective computational approaches to screen\nnovel anti-infective agents. Oligopeptides have emerged as promising candidates\nin antimicrobial research due to their structural simplicity, high\nbioavailability, and low susceptibility to resistance. Despite their potential,\ncomputational models specifically designed to predict associations between\noligopeptides and infectious diseases remain scarce. This study introduces a\nprompt-guided graph-based contrastive learning framework (PGCLODA) to uncover\npotential associations. A tripartite graph is constructed with oligopeptides,\nmicrobes, and diseases as nodes, incorporating both structural and semantic\ninformation. To preserve critical regions during contrastive learning, a\nprompt-guided graph augmentation strategy is employed to generate meaningful\npaired views. A dual encoder architecture, integrating Graph Convolutional\nNetwork (GCN) and Transformer, is used to jointly capture local and global\nfeatures. The fused embeddings are subsequently input into a multilayer\nperceptron (MLP) classifier for final prediction. Experimental results on a\nbenchmark dataset indicate that PGCLODA consistently outperforms\nstate-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and\nhyperparameter studies confirm the contribution of each module. Case studies\nfurther validate the generalization ability of PGCLODA and its potential to\nuncover novel, biologically relevant associations. These findings offer\nvaluable insights for mechanism-driven discovery and oligopeptide-based drug\ndevelopment. The source code of PGCLODA is available online at\nhttps://github.com/jjnlcode/PGCLODA.",
      "url": "http://arxiv.org/abs/2509.20290v1",
      "published_time_eastern_timestamp": 1758731113.0
    },
    {
      "title": "HiPerformer: A High-Performance Global-Local Segmentation Model with\n  Modular Hierarchical Fusion Strategy",
      "summary": "Both local details and global context are crucial in medical image\nsegmentation, and effectively integrating them is essential for achieving high\naccuracy. However, existing mainstream methods based on CNN-Transformer hybrid\narchitectures typically employ simple feature fusion techniques such as serial\nstacking, endpoint concatenation, or pointwise addition, which struggle to\naddress the inconsistencies between features and are prone to information\nconflict and loss. To address the aforementioned challenges, we innovatively\npropose HiPerformer. The encoder of HiPerformer employs a novel modular\nhierarchical architecture that dynamically fuses multi-source features in\nparallel, enabling layer-wise deep integration of heterogeneous information.\nThe modular hierarchical design not only retains the independent modeling\ncapability of each branch in the encoder, but also ensures sufficient\ninformation transfer between layers, effectively avoiding the degradation of\nfeatures and information loss that come with traditional stacking methods.\nFurthermore, we design a Local-Global Feature Fusion (LGFF) module to achieve\nprecise and efficient integration of local details and global semantic\ninformation, effectively alleviating the feature inconsistency problem and\nresulting in a more comprehensive feature representation. To further enhance\nmulti-scale feature representation capabilities and suppress noise\ninterference, we also propose a Progressive Pyramid Aggregation (PPA) module to\nreplace traditional skip connections. Experiments on eleven public datasets\ndemonstrate that the proposed method outperforms existing segmentation\ntechniques, demonstrating higher segmentation accuracy and robustness. The code\nis available at https://github.com/xzphappy/HiPerformer.",
      "url": "http://arxiv.org/abs/2509.20280v1",
      "published_time_eastern_timestamp": 1758730542.0
    },
    {
      "title": "Investigating Security Implications of Automatically Generated Code on\n  the Software Supply Chain",
      "summary": "In recent years, various software supply chain (SSC) attacks have posed\nsignificant risks to the global community. Severe consequences may arise if\ndevelopers integrate insecure code snippets that are vulnerable to SSC attacks\ninto their products. Particularly, code generation techniques, such as large\nlanguage models (LLMs), have been widely utilized in the developer community.\nHowever, LLMs are known to suffer from inherent issues when generating code,\nincluding fabrication, misinformation, and reliance on outdated training data,\nall of which can result in serious software supply chain threats. In this\npaper, we investigate the security threats to the SSC that arise from these\ninherent issues. We examine three categories of threats, including eleven\npotential SSC-related threats, related to external components in source code,\nand continuous integration configuration files. We find some threats in\nLLM-generated code could enable attackers to hijack software and workflows,\nwhile some others might cause potential hidden threats that compromise the\nsecurity of the software over time. To understand these security impacts and\nseverity, we design a tool, SSCGuard, to generate 439,138 prompts based on\nSSC-related questions collected online, and analyze the responses of four\npopular LLMs from GPT and Llama. Our results show that all identified\nSSC-related threats persistently exist. To mitigate these risks, we propose a\nnovel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce\nfabrication, and a middleware-based defense that informs users of various SSC\nthreats.",
      "url": "http://arxiv.org/abs/2509.20277v1",
      "published_time_eastern_timestamp": 1758730517.0
    },
    {
      "title": "Predictive Coding-based Deep Neural Network Fine-tuning for\n  Computationally Efficient Domain Adaptation",
      "summary": "As deep neural networks are increasingly deployed in dynamic, real-world\nenvironments, relying on a single static model is often insufficient. Changes\nin input data distributions caused by sensor drift or lighting variations\nnecessitate continual model adaptation. In this paper, we propose a hybrid\ntraining methodology that enables efficient on-device domain adaptation by\ncombining the strengths of Backpropagation and Predictive Coding. The method\nbegins with a deep neural network trained offline using Backpropagation to\nachieve high initial performance. Subsequently, Predictive Coding is employed\nfor online adaptation, allowing the model to recover accuracy lost due to\nshifts in the input data distribution. This approach leverages the robustness\nof Backpropagation for initial representation learning and the computational\nefficiency of Predictive Coding for continual learning, making it particularly\nwell-suited for resource-constrained edge devices or future neuromorphic\naccelerators. Experimental results on the MNIST and CIFAR-10 datasets\ndemonstrate that this hybrid strategy enables effective adaptation with a\nreduced computational overhead, offering a promising solution for maintaining\nmodel performance in dynamic environments.",
      "url": "http://arxiv.org/abs/2509.20269v2",
      "published_time_eastern_timestamp": 1758729807.0
    },
    {
      "title": "An Educational Guide for 2D Stellar Structure Calculations of Rapidly\n  Rotating Stars using the ESTER code",
      "summary": "The Evolution STEllaire en Rotation (ESTER) code is the first 2D stellar\nstructure code to be made open-source and freely available to the astronomy and\nastrophysics community. An important and novel advancement of this code is that\nit can reproduce the distorted shape and observable signatures (e.g., gravity\ndarkening) of rapidly rotating stars. ESTER also calculates the steady-state\nlarge-scale flows within the star, namely their differential rotation and\nassociated meridional circulation. In this report, we explore and document the\nphysics implemented within version 1.1.0rc2 of the ESTER code, in a way that\ncomplements published descriptions. We illustrate this physics by plotting how\nstellar structure parameters vary through stellar interiors at a range of\nlatitudes and at different angular velocities. We investigate how the thin\nconvective envelopes of intermediate mass stars vary with latitude when rapidly\nrotating, becoming deeper and thicker near the equator. Simple comparisons of\nESTER model predictions (e.g., central temperature and density, luminosity)\nwith the output from the Modules for Experiments in Stellar Astrophysics (MESA)\ncode [Paxton et al., 2010] shows generally good agreement. Additional\ncomparisons provide important benchmarking and verification for ESTER as a\ncomparatively young code. Finally, we provide a guide for installing and\nrunning the code on our local university cluster, aimed at helping students to\nbegin work.",
      "url": "http://arxiv.org/abs/2509.20264v1",
      "published_time_eastern_timestamp": 1758729126.0
    },
    {
      "title": "An Anisotropic Cross-View Texture Transfer with Multi-Reference\n  Non-Local Attention for CT Slice Interpolation",
      "summary": "Computed tomography (CT) is one of the most widely used non-invasive imaging\nmodalities for medical diagnosis. In clinical practice, CT images are usually\nacquired with large slice thicknesses due to the high cost of memory storage\nand operation time, resulting in an anisotropic CT volume with much lower\ninter-slice resolution than in-plane resolution. Since such inconsistent\nresolution may lead to difficulties in disease diagnosis, deep learning-based\nvolumetric super-resolution methods have been developed to improve inter-slice\nresolution. Most existing methods conduct single-image super-resolution on the\nthrough-plane or synthesize intermediate slices from adjacent slices; however,\nthe anisotropic characteristic of 3D CT volume has not been well explored. In\nthis paper, we propose a novel cross-view texture transfer approach for CT\nslice interpolation by fully utilizing the anisotropic nature of 3D CT volume.\nSpecifically, we design a unique framework that takes high-resolution in-plane\ntexture details as a reference and transfers them to low-resolution\nthrough-plane images. To this end, we introduce a multi-reference non-local\nattention module that extracts meaningful features for reconstructing\nthrough-plane high-frequency details from multiple in-plane images. Through\nextensive experiments, we demonstrate that our method performs significantly\nbetter in CT slice interpolation than existing competing methods on public CT\ndatasets including a real-paired benchmark, verifying the effectiveness of the\nproposed framework. The source code of this work is available at\nhttps://github.com/khuhm/ACVTT.",
      "url": "http://arxiv.org/abs/2509.20242v1",
      "published_time_eastern_timestamp": 1758727959.0
    },
    {
      "title": "A HyperGraphMamba-Based Multichannel Adaptive Model for ncRNA\n  Classification",
      "summary": "Non-coding RNAs (ncRNAs) play pivotal roles in gene expression regulation and\nthe pathogenesis of various diseases. Accurate classification of ncRNAs is\nessential for functional annotation and disease diagnosis. To address existing\nlimitations in feature extraction depth and multimodal fusion, we propose\nHGMamba-ncRNA, a HyperGraphMamba-based multichannel adaptive model, which\nintegrates sequence, secondary structure, and optionally available expression\nfeatures of ncRNAs to enhance classification performance. Specifically, the\nsequence of ncRNA is modeled using a parallel Multi-scale Convolution and LSTM\narchitecture (MKC-L) to capture both local patterns and long-range dependencies\nof nucleotides. The structure modality employs a multi-scale graph transformer\n(MSGraphTransformer) to represent the multi-level topological characteristics\nof ncRNA secondary structures. The expression modality utilizes a Chebyshev\nPolynomial-based Kolmogorov-Arnold Network (CPKAN) to effectively model and\ninterpret high-dimensional expression profiles. Finally, by incorporating\nvirtual nodes to facilitate efficient and comprehensive multimodal interaction,\nHyperGraphMamba is proposed to adaptively align and integrate multichannel\nheterogeneous modality features. Experiments conducted on three public datasets\ndemonstrate that HGMamba-ncRNA consistently outperforms state-of-the-art\nmethods in terms of accuracy and other metrics. Extensive empirical studies\nfurther confirm the model's robustness, effectiveness, and strong\ntransferability, offering a novel and reliable strategy for complex ncRNA\nfunctional classification. Code and datasets are available at\nhttps://anonymous.4open.science/r/HGMamba-ncRNA-94D0.",
      "url": "http://arxiv.org/abs/2509.20240v1",
      "published_time_eastern_timestamp": 1758727909.0
    },
    {
      "title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature\n  reliance through controlled suppression",
      "summary": "The hypothesis that Convolutional Neural Networks (CNNs) are inherently\ntexture-biased has shaped much of the discourse on feature use in deep\nlearning. We revisit this hypothesis by examining limitations in the\ncue-conflict experiment by Geirhos et al. To address these limitations, we\npropose a domain-agnostic framework that quantifies feature reliance through\nsystematic suppression of shape, texture, and color cues, avoiding the\nconfounds of forced-choice conflicts. By evaluating humans and neural networks\nunder controlled suppression conditions, we find that CNNs are not inherently\ntexture-biased but predominantly rely on local shape features. Nonetheless,\nthis reliance can be substantially mitigated through modern training strategies\nor architectures (ConvNeXt, ViTs). We further extend the analysis across\ncomputer vision, medical imaging, and remote sensing, revealing that reliance\npatterns differ systematically: computer vision models prioritize shape,\nmedical imaging models emphasize color, and remote sensing models exhibit a\nstronger reliance towards texture. Code is available at\nhttps://github.com/tomburgert/feature-reliance.",
      "url": "http://arxiv.org/abs/2509.20234v1",
      "published_time_eastern_timestamp": 1758727483.0
    },
    {
      "title": "The Cream Rises to the Top: Efficient Reranking Method for Verilog Code\n  Generation",
      "summary": "LLMs face significant challenges in Verilog generation due to limited\ndomain-specific knowledge. While sampling techniques improve pass@k metrics,\nhardware engineers need one trustworthy solution rather than uncertain\ncandidates. To bridge this gap, we formulate it as a semantic alignment problem\nbetween requirements and Verilog implementations, and propose VCD-RNK, a\ndiscriminator model tailored for efficient Verilog code reranking.\nSpecifically, VCD-RNKincorporates Verilog-specific reasoning by distilling\nexpert knowledge across three dimensions: code semantic analysis, test case\ngeneration, and functional correctness assessment. By explicitly simulating the\nabove reasoning processes during inference, VCD-RNK effectively avoids\ncomputationally intensive test execution in existing methods.",
      "url": "http://arxiv.org/abs/2509.20215v1",
      "published_time_eastern_timestamp": 1758726741.0
    },
    {
      "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for\n  Efficient LLM Deployment",
      "summary": "We study weight-only post-training quantization (PTQ), which quantizes the\nweights of a large language model (LLM) without retraining, using little or no\ncalibration data. Weight-only PTQ is crucial for reducing the memory footprint\nand latency of LLM inference, especially in memory-bound, small-batch inference\nscenarios, such as personalized inference on edge devices. Despite its\nimportance, irregular weight distributions with heavy-tailed outliers in LLMs\ncomplicate quantization, recently motivating rotation-based methods that\ntransform weights into near-Gaussian distributions, which are more regular with\nfewer outliers, thereby reducing quantization error. In this work, we first\nderive the information-theoretically optimal bit allocation for Gaussianized\nweights under given bit budgets, revealing that fine-grained fractional-bit\nquantizers approaching the Gaussian distortion-rate bound are essential to\nachieve near-optimal quantization performance. To bridge this theoretical\ninsight and practical implementation, we introduce Q-Palette, a versatile\ncollection of fractional-bit quantizers that range from trellis-coded\nquantizers offering near-optimal distortion to simpler vector and scalar\nquantizers optimized for faster inference, all efficiently implemented with\noptimized CUDA kernels across various bitwidths. Furthermore, leveraging\nQ-Palette as a foundational component, we propose a novel mixed-scheme\nquantization framework, jointly optimizing quantizer choices and layer fusion\ndecisions given resource constraints. The code is available at\nhttps://github.com/snu-mllab/Q-Palette.",
      "url": "http://arxiv.org/abs/2509.20214v1",
      "published_time_eastern_timestamp": 1758726644.0
    },
    {
      "title": "PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation",
      "summary": "Point clouds produced by 3D sensors are often sparse and noisy, posing\nchallenges for tasks requiring dense and high-fidelity 3D representations.\nPrior work has explored both implicit feature-based upsampling and\ndistance-function learning to address this, but often at the expense of\ngeometric interpretability or robustness to input sparsity. To overcome these\nlimitations, we propose PU-Gaussian, a novel upsampling network that models the\nlocal neighborhood around each point using anisotropic 3D Gaussian\ndistributions. These Gaussians capture the underlying geometric structure,\nallowing us to perform upsampling explicitly in the local geometric domain by\ndirect point sampling. The sampling process generates a dense, but coarse,\npoint cloud. A subsequent refinement network adjusts the coarse output to\nproduce a more uniform distribution and sharper edges. We perform extensive\ntesting on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves\nstate-of-the-art performance. We make code and model weights publicly available\nat https://github.com/mvg-inatech/PU-Gaussian.git.",
      "url": "http://arxiv.org/abs/2509.20207v1",
      "published_time_eastern_timestamp": 1758726123.0
    }
  ]
}
{
  "last_updated": "2025-07-15T17:10:59.970850-04:00",
  "papers": [
    {
      "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong\n  Universal Face Embedder",
      "summary": "Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.",
      "url": "http://arxiv.org/abs/2507.10552v1",
      "published_time_eastern_timestamp": 1752515999.0
    },
    {
      "title": "Disentangling Neural Disjunctive Normal Form Models",
      "summary": "Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.",
      "url": "http://arxiv.org/abs/2507.10546v1",
      "published_time_eastern_timestamp": 1752515973.0
    },
    {
      "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation",
      "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/.",
      "url": "http://arxiv.org/abs/2507.10543v1",
      "published_time_eastern_timestamp": 1752515948.0
    },
    {
      "title": "Graph World Model",
      "summary": "World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM.",
      "url": "http://arxiv.org/abs/2507.10539v1",
      "published_time_eastern_timestamp": 1752515865.0
    },
    {
      "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks",
      "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.",
      "url": "http://arxiv.org/abs/2507.10535v1",
      "published_time_eastern_timestamp": 1752515789.0
    },
    {
      "title": "WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling",
      "summary": "Despite rapid progress in end-to-end AI music generation, AI-driven modeling\nof professional Digital Signal Processing (DSP) workflows remains challenging.\nIn particular, while there is growing interest in neural black-box modeling of\naudio effect graphs (e.g. reverb, compression, equalization), AI-based\napproaches struggle to replicate the nuanced signal flow and parameter\ninteractions used in professional workflows. Existing differentiable plugin\napproaches often diverge from real-world tools, exhibiting inferior performance\nrelative to simplified neural controllers under equivalent computational\nconstraints. We introduce WildFX, a pipeline containerized with Docker for\ngenerating multi-track audio mixing datasets with rich effect graphs, powered\nby a professional Digital Audio Workstation (DAW) backend. WildFX supports\nseamless integration of cross-platform commercial plugins or any plugins in the\nwild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,\nsidechains, crossovers) and achieving efficient parallelized processing. A\nminimalist metadata interface simplifies project/plugin configuration.\nExperiments demonstrate the pipeline's validity through blind estimation of\nmixing graphs, plugin/gain parameters, and its ability to bridge AI research\nwith practical DSP demands. The code is available on:\nhttps://github.com/IsaacYQH/WildFX.",
      "url": "http://arxiv.org/abs/2507.10534v1",
      "published_time_eastern_timestamp": 1752515738.0
    },
    {
      "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
      "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
      "url": "http://arxiv.org/abs/2507.10524v1",
      "published_time_eastern_timestamp": 1752515340.0
    },
    {
      "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex\n  Scientific Question Answering in Ecology",
      "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.",
      "url": "http://arxiv.org/abs/2507.10522v1",
      "published_time_eastern_timestamp": 1752515248.0
    },
    {
      "title": "A Classification of Transversal Clifford Gates for Qubit Stabilizer\n  Codes",
      "summary": "This work classifies stabilizer codes by the set of diagonal Clifford gates\nthat can be implemented transversally on them. We show that, for any stabilizer\ncode, its group of diagonal transversal Clifford gates on $\\ell$ code blocks\nmust be one of six distinct families of matrix groups. We further develop the\ntheory of classifying stabilizer codes by via matrix algebras of endomorphisms\nfirst introduced by Rains, and give a complete classification of the diagonal\nClifford symmetries of $\\ell$ code blocks. A number of corollaries are given in\nthe final section.",
      "url": "http://arxiv.org/abs/2507.10519v1",
      "published_time_eastern_timestamp": 1752515010.0
    },
    {
      "title": "The Power of Certainty: How Confident Models Lead to Better Segmentation",
      "summary": "Deep learning models have been proposed for automatic polyp detection and\nprecise segmentation of polyps during colonoscopy procedures. Although these\nstate-of-the-art models achieve high performance, they often require a large\nnumber of parameters. Their complexity can make them prone to overfitting,\nparticularly when trained on biased datasets, and can result in poor\ngeneralization across diverse datasets. Knowledge distillation and\nself-distillation are proposed as promising strategies to mitigate the\nlimitations of large, over-parameterized models. These approaches, however, are\nresource-intensive, often requiring multiple models and significant memory\nduring training. We propose a confidence-based self-distillation approach that\noutperforms state-of-the-art models by utilizing only previous iteration data\nstorage during training, without requiring extra computation or memory usage\nduring testing. Our approach calculates the loss between the previous and\ncurrent iterations within a batch using a dynamic confidence coefficient. To\nevaluate the effectiveness of our approach, we conduct comprehensive\nexperiments on the task of polyp segmentation. Our approach outperforms\nstate-of-the-art models and generalizes well across datasets collected from\nmultiple clinical centers. The code will be released to the public once the\npaper is accepted.",
      "url": "http://arxiv.org/abs/2507.10490v1",
      "published_time_eastern_timestamp": 1752513163.0
    },
    {
      "title": "Can You Detect the Difference?",
      "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.",
      "url": "http://arxiv.org/abs/2507.10475v1",
      "published_time_eastern_timestamp": 1752512157.0
    },
    {
      "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection,\n  Transfer, and Reconstruction",
      "summary": "Blind facial image restoration is highly challenging due to unknown complex\ndegradations and the sensitivity of humans to faces. Although existing methods\nintroduce auxiliary information from generative priors or high-quality\nreference images, they still struggle with identity preservation problems,\nmainly due to improper feature introduction on detailed textures. In this\npaper, we focus on effectively incorporating appropriate features from\nhigh-quality reference images, presenting a novel blind facial image\nrestoration method that considers reference selection, transfer, and\nreconstruction (RefSTAR). In terms of selection, we construct a reference\nselection (RefSel) module. For training the RefSel module, we construct a\nRefSel-HQ dataset through a mask generation pipeline, which contains annotating\nmasks for 10,000 ground truth-reference pairs. As for the transfer, due to the\ntrivial solution in vanilla cross-attention operations, a feature fusion\nparadigm is designed to force the features from the reference to be integrated.\nFinally, we propose a reference image reconstruction mechanism that further\nensures the presence of reference image features in the output image. The cycle\nconsistency loss is also redesigned in conjunction with the mask. Extensive\nexperiments on various backbone models demonstrate superior performance,\nshowing better identity preservation ability and reference feature transfer\nquality. Source code, dataset, and pre-trained models are available at\nhttps://github.com/yinzhicun/RefSTAR.",
      "url": "http://arxiv.org/abs/2507.10470v1",
      "published_time_eastern_timestamp": 1752511829.0
    },
    {
      "title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos",
      "summary": "Existing methods for reconstructing animatable 3D animals from videos\ntypically rely on sparse semantic keypoints to fit parametric models. However,\nobtaining such keypoints is labor-intensive, and keypoint detectors trained on\nlimited animal data are often unreliable. To address this, we propose\n4D-Animal, a novel framework that reconstructs animatable 3D animals from\nvideos without requiring sparse keypoint annotations. Our approach introduces a\ndense feature network that maps 2D representations to SMAL parameters,\nenhancing both the efficiency and stability of the fitting process.\nFurthermore, we develop a hierarchical alignment strategy that integrates\nsilhouette, part-level, pixel-level, and temporal cues from pre-trained 2D\nvisual models to produce accurate and temporally coherent reconstructions\nacross frames. Extensive experiments demonstrate that 4D-Animal outperforms\nboth model-based and model-free baselines. Moreover, the high-quality 3D assets\ngenerated by our method can benefit other 3D tasks, underscoring its potential\nfor large-scale applications. The code is released at\nhttps://github.com/zhongshsh/4D-Animal.",
      "url": "http://arxiv.org/abs/2507.10437v1",
      "published_time_eastern_timestamp": 1752510271.0
    },
    {
      "title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment",
      "summary": "With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.",
      "url": "http://arxiv.org/abs/2507.10432v1",
      "published_time_eastern_timestamp": 1752510065.0
    },
    {
      "title": "A mapping of the Min-Sum decoder to reduction operations, and its\n  implementation using CUDA kernels",
      "summary": "Decoders for Low Density Parity Check (LDPC) codes are usually tailored to an\napplication and optimized once the specific content and structure of the parity\nmatrix are known. In this work we consider the parity matrix as an argument of\nthe Min-Sum decoder, and provide a GPU implementation that is independent of\nthe content of the parity matrix, and relies only on its dimensions.",
      "url": "http://arxiv.org/abs/2507.10424v1",
      "published_time_eastern_timestamp": 1752509410.0
    },
    {
      "title": "Self-Admitted GenAI Usage in Open-Source Software",
      "summary": "The widespread adoption of generative AI (GenAI) tools such as GitHub Copilot\nand ChatGPT is transforming software development. Since generated source code\nis virtually impossible to distinguish from manually written code, their\nreal-world usage and impact on open-source software development remain poorly\nunderstood. In this paper, we introduce the concept of self-admitted GenAI\nusage, that is, developers explicitly referring to the use of GenAI tools for\ncontent creation in software artifacts. Using this concept as a lens to study\nhow GenAI tools are integrated into open-source software projects, we analyze a\ncurated sample of more than 250,000 GitHub repositories, identifying 1,292 such\nself-admissions across 156 repositories in commit messages, code comments, and\nproject documentation. Using a mixed methods approach, we derive a taxonomy of\n32 tasks, 10 content types, and 11 purposes associated with GenAI usage based\non 284 qualitatively coded mentions. We then analyze 13 documents with policies\nand usage guidelines for GenAI tools and conduct a developer survey to uncover\nthe ethical, legal, and practical concerns behind them. Our findings reveal\nthat developers actively manage how GenAI is used in their projects,\nhighlighting the need for project-level transparency, attribution, and quality\ncontrol practices in the new era of AI-assisted software development. Finally,\nwe examine the longitudinal impact of GenAI adoption on code churn in 151\nrepositories with self-admitted GenAI usage and find no general increase,\ncontradicting popular narratives on the impact of GenAI on software\ndevelopment.",
      "url": "http://arxiv.org/abs/2507.10422v1",
      "published_time_eastern_timestamp": 1752509149.0
    },
    {
      "title": "A Matrix Completion Approach for the Construction of MDP Convolutional\n  Codes",
      "summary": "Maximum Distance Profile (MDP) convolutional codes are an important class of\nchannel codes due to their maximal delay-constrained error correction\ncapabilities. The design of MDP codes has attracted significant attention from\nthe research community. However, only limited attention was given to addressing\nthe complexity of encoding and decoding operations. This paper aims to reduce\nencoding complexity by constructing partial unit-memory MDP codes with\nstructured and sparse generator matrices. In particular, we present a matrix\ncompletion framework that extends a structured superregular matrix (e.g.,\nCauchy) over a small field to a sparse sliding generator matrix of an MDP code.\nWe show that the proposed construction can reduce the encoding complexity\ncompared to the current state-of-the-art MDP code designs.",
      "url": "http://arxiv.org/abs/2507.10417v1",
      "published_time_eastern_timestamp": 1752508783.0
    },
    {
      "title": "Fault-Tolerant Quantum Error Correction for Constant-Excitation\n  Stabilizer Codes under Coherent Noise",
      "summary": "Collective coherent noise poses challenges for fault-tolerant quantum error\ncorrection (FTQEC), as it falls outside the usual stochastic noise models.\nWhile constant excitation (CE) codes can naturally avoid coherent noise, a\ncomplete fault-tolerant framework for the use of these codes under realistic\nnoise models has been elusive. Here, we introduce a complete fault-tolerant\narchitecture for CE CSS codes based on dual-rail concatenation. After showing\nthat transversal CNOT gates violate CE code constraints, we introduce\nCE-preserving logical CNOT gates and modified Shor- and Steane-type syndrome\nextraction schemes using zero-controlled NOT gates and CE-compatible ancilla.\nThis enables fault-tolerant syndrome-extraction circuits fully compatible with\nCE constraints. We also present an extended stabilizer simulation algorithm\nthat efficiently tracks both stochastic and collective coherent noise. Using\nour framework, we identify minimal CE codes, including the $[[12,1,3]]$ and\n$[[14,3,3]]$ codes, and demonstrate that the $[[12,1,3]]$ code achieves strong\nperformance under coherent noise. Our results establish the first complete\nFTQEC framework for CE codes, demonstrating their robustness to coherent noise.\nThis highlights the potential of CE codes as a possible solution for quantum\nprocessors dominated by collective coherent noise.",
      "url": "http://arxiv.org/abs/2507.10395v1",
      "published_time_eastern_timestamp": 1752507432.0
    },
    {
      "title": "Gas-phase Elemental abundances in Molecular cloudS (GEMS) XI. The\n  evolution of HCN, HNC, and N2H+ isotopic ratios in starless cores",
      "summary": "Isotopic ratios have been used as chemical diagnostics to investigate the\norigin of the material in the Solar System. We have determined the HCN, HNC,\nand N2H+ isotopic ratios and the chemical age in a large sample of 23 starless\ncores located in different environments. This work uses IRAM 30m data to\nconstrain the D/H ratio of HCN, HNC, and N2H+ as well as the 14N/15N ratio of\nHCN and HNC. The observed abundances have been modeled using the chemical code\nDNAUTILUS 2.0. Deuterated compounds are detected in all of our sample cores,\nwith average DNC/HNC, DCN/HCN, and N2D+/N2H+ values of 0.054$\\pm$0.019,\n0.036$\\pm$0.033, and 0.15$\\pm$0.11, respectively. The deuterium fractions\n(Dfrac) show a weak correlation with temperature and a large scatter that\nreflects that other factors such as core evolution could also play a\nsignificant role. Our chemical model is able to reproduce all the observed\nvalues with 0.2-0.3 Myr in Taurus and 0.3-0.5 Myr in Perseus and Orion. The\n14N/15N isotopic ratio is found to be different between HCN/HC15N (430$\\pm$120)\nand HNC/H15NC (296$\\pm$64). We find no correlation between these ratios and the\ndeuterium fractions, but we report a weak correlation with temperature. The\nDfrac of HCN, HNC, and N2H+ can be used as evolutionary tracers of starless\ncores as long as the physical parameters are well constrained. The HCN/HC15N\nand HNC/H15NC ratios are not correlated with Dfrac, suggesting that the\ndetected variations are not correlated with the core evolutionary stage. The\naverage value of the HCN/HC15N ratio in our sample is significantly higher than\nthe values measured in protostars and protoplanetary disks, possibly indicating\nthat nitrogen fractionation processes are taking place during the protostellar\nphase.",
      "url": "http://arxiv.org/abs/2507.10380v1",
      "published_time_eastern_timestamp": 1752506420.0
    },
    {
      "title": "Massive stars advanced evolution: I -- New reaction rates for carbon and\n  oxygen nuclear reactions",
      "summary": "The nuclear rates for reactions involving 12C and 16O are key to compute the\nenergy release and nucleosynthesis of massive stars during their evolution.\nThese rates shape the stellar structure and evolution, and impact the nature of\nthe final compact remnant. We explore the impact of new nuclear reaction rates\nfor 12C({\\alpha},{\\gamma})16O, 12C+12C, 12C+16O and 16O+16O reactions for\nmassive stars. We aim to investigate how the structure and nucleosynthesis\nevolve and how these processes influence the stellar fate. We computed stellar\nmodels using the GENEC code, including updated rates for\n12C({\\alpha},{\\gamma})16O and, for the three fusion reactions, new rates\nfollowing a fusion suppression scenario and new theoretical rates obtained with\nTDHF calculations. The updated 12C({\\alpha},{\\gamma})16O rates mainly impact\nthe chemical structure evolution changing the 12C/16O ratio with little effect\non the CO core mass. This variation in the 12C/16O ratio is critical for\npredicting the stellar fate, which is very sensitive to 12C abundance. The\ncombined new rates for 12C+12C and 16O+16O fusion reactions according to the\nHIN(RES) model lead to shorter C- and O-burning lifetimes, and shift the\nignition conditions to higher temperatures and densities. Theoretical TDHF\nrates primarily affect C-burning, increasing its duration and lowering the\nignition temperature. These changes alter the core chemical structure, the\ncarbon shell size and duration, and hence the compactness. They also affect\nnucleosynthesis. This work shows that accurate reaction rates for key processes\nin massive star evolution drive significant changes in stellar burning\nlifetimes, chemical evolution, and stellar fate. In addition, discrepancies\nbetween experimental and theoretical rates introduce uncertainties in model\npredictions, influencing both the internal structure and the supernova ejecta\ncomposition.",
      "url": "http://arxiv.org/abs/2507.10377v1",
      "published_time_eastern_timestamp": 1752506206.0
    }
  ]
}
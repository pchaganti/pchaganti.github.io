{
  "last_updated": "2026-02-09T10:45:08.707329-05:00",
  "papers": [
    {
      "title": "InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning",
      "summary": "Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.",
      "url": "http://arxiv.org/abs/2602.06960v1",
      "published_time_eastern_timestamp": 1770404367.0
    },
    {
      "title": "DAWN: Dependency-Aware Fast Inference for Diffusion LLMs",
      "summary": "Diffusion large language models (dLLMs) have shown advantages in text generation, particularly due to their inherent ability for parallel decoding. However, constrained by the quality--speed trade-off, existing inference solutions adopt conservative parallel strategies, leaving substantial efficiency potential underexplored. A core challenge is that parallel decoding assumes each position can be filled independently, but tokens are often semantically coupled. Thus, the correct choice at one position constrains valid choices at others. Without modeling these inter-token dependencies, parallel strategies produce deteriorated outputs. Motivated by this insight, we propose DAWN, a training-free, dependency-aware decoding method for fast dLLM inference. DAWN extracts token dependencies and leverages two key motivations: (1) positions dependent on unmasked certain positions become more reliable, (2) simultaneously unmasking strongly coupled uncertain positions induces errors. Given those findings, DAWN leverages a dependency graph to select more reliable unmasking positions at each iteration, achieving high parallelism with negligible loss in generation quality. Extensive experiments across multiple models and datasets demonstrate that DAWN speedups the inference by 1.80-8.06x over baselines while preserving the generation quality. Code is released at https://github.com/lizhuo-luo/DAWN.",
      "url": "http://arxiv.org/abs/2602.06953v1",
      "published_time_eastern_timestamp": 1770403889.0
    },
    {
      "title": "Mapping plasma properties of Cassiopeia A with XRISM/Resolve: a Bayesian analysis via UltraSPEX",
      "summary": "Mapping the physical conditions of the shocked plasma of young supernova remnants (SNR) is crucial for understanding their explosion mechanisms, ejecta structure, and large-scale asymmetries. Using $>350$ ks of XRISM/Resolve high spectral resolution observations of Cassiopeia A (Cas A), the youngest known Galactic core-collapse SNR, we present the first microcalorimeter-based plasma parameter maps of any SNR. We tessellate Cas A into $1'\\times1'$ regions and fit the broadband spectra as thermal emission from two pure-metal ejecta components -- corresponding to intermediate-mass elements (IMEs) and iron-group elements (IGEs) -- plus nonthermal synchrotron radiation. For robust inference, we introduce UltraSPEX, a Bayesian framework that couples the SPEX plasma code with the UltraNest nested-sampling algorithm, yielding full posterior distributions and exploration of parameter degeneracies. Key findings include enhanced Ar/Si and Ca/Si abundance ratios near the base of the Si-rich jets, and a high Ni/Fe mass ratio ($0.08\\pm0.015$) in the base of NE jet. IGEs ejecta exhibit systematically higher Doppler velocities and broadenings than IMEs ejecta in most regions, with maximum differences of $\\sim800$ km/s and $\\sim1200$ km/s, respectively; Ca shows distinct (faster) kinematics from other IMEs in several SE regions. The ionization timescale and electron temperature show a robust anti-correlation, particularly for IGEs. This relation and measured parameter values could be explained by semi-analytical models with significant ejecta clumping (overdensities of $\\sim10$ for IGEs and up to $\\sim100$ for IMEs) and reduced historical reverse-shock velocities. Nonthermal emission accounts for a substantial fraction, with at least 47% of the 4--6 keV continuum and dominates in the western regions, where the spectrum hardens.",
      "url": "http://arxiv.org/abs/2602.06952v1",
      "published_time_eastern_timestamp": 1770403886.0
    },
    {
      "title": "Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay",
      "summary": "Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.",
      "url": "http://arxiv.org/abs/2602.06942v1",
      "published_time_eastern_timestamp": 1770403274.0
    },
    {
      "title": "Endogenous Resistance to Activation Steering in Language Models",
      "summary": "Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.",
      "url": "http://arxiv.org/abs/2602.06941v1",
      "published_time_eastern_timestamp": 1770403272.0
    },
    {
      "title": "Reciprocal Latent Fields for Precomputed Sound Propagation",
      "summary": "Realistic sound propagation is essential for immersion in a virtual scene, yet physically accurate wave-based simulations remain computationally prohibitive for real-time applications. Wave coding methods address this limitation by precomputing and compressing impulse responses of a given scene into a set of scalar acoustic parameters, which can reach unmanageable sizes in large environments with many source-receiver pairs. We introduce Reciprocal Latent Fields (RLF), a memory-efficient framework for encoding and predicting these acoustic parameters. The RLF framework employs a volumetric grid of trainable latent embeddings decoded with a symmetric function, ensuring acoustic reciprocity. We study a variety of decoders and show that leveraging Riemannian metric learning leads to a better reproduction of acoustic phenomena in complex scenes. Experimental validation demonstrates that RLF maintains replication quality while reducing the memory footprint by several orders of magnitude. Furthermore, a MUSHRA-like subjective listening test indicates that sound rendered via RLF is perceptually indistinguishable from ground-truth simulations.",
      "url": "http://arxiv.org/abs/2602.06937v1",
      "published_time_eastern_timestamp": 1770402671.0
    },
    {
      "title": "Automatic Detection and Analysis of Singing Mistakes for Music Pedagogy",
      "summary": "The advancement of machine learning in audio analysis has opened new possibilities for technology-enhanced music education. This paper introduces a framework for automatic singing mistake detection in the context of music pedagogy, supported by a newly curated dataset. The dataset comprises synchronized teacher learner vocal recordings, with annotations marking different types of mistakes made by learners. Using this dataset, we develop different deep learning models for mistake detection and benchmark them. To compare the efficacy of mistake detection systems, a new evaluation methodology is proposed. Experiments indicate that the proposed learning-based methods are superior to rule-based methods. A systematic study of errors and a cross-teacher study reveal insights into music pedagogy that can be utilised for various music applications. This work sets out new directions of research in music pedagogy. The codes and dataset are publicly available.",
      "url": "http://arxiv.org/abs/2602.06917v1",
      "published_time_eastern_timestamp": 1770401736.0
    },
    {
      "title": "TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering",
      "summary": "As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench",
      "url": "http://arxiv.org/abs/2602.06911v1",
      "published_time_eastern_timestamp": 1770401078.0
    },
    {
      "title": "Magnetohydrodynamic instabilities in stellar radiative regions. I. Linear study of shear-driven instabilities",
      "summary": "This paper is the first in a series investigating magnetohydrodynamic instabilities that may contribute to angular-momentum transport and magnetic-field evolution in stellar radiative zones. We focus on shear-driven instabilities, specifically the Goldreich-Schubert-Fricke (GSF) instability and the magnetorotational instability (MRI), which are expected to play key roles in the internal dynamics of radiative regions. We carried out a local linear stability analysis using a numerical approach that extends beyond classical limiting cases and includes stabilizing effects such as stratification and magnetic tension, allowing the exploration of realistic flow regimes. These results were validated through a global mode analysis in a Taylor-Couette configuration. We recovered the standard MRI and azimuthal MRI stability criteria and quantified the effects of stratification, magnetic tension, and diffusion on their growth. In strongly sheared regimes, we derived a new criterion for the magnetised GSF (MGSF) instability and clarified the transition from SMRI to MGSF as stratification and magnetic effects narrow the unstable domain. We also provided approximate growth-time formulae that identify the dominant instability under given stellar conditions and can be implemented in 1D stellar evolution codes. Global Taylor-Couette calculations validate the local WKB analysis. Applied to subgiants and young red giants, our results show that shear-driven instabilities can grow rapidly for magnetic fields below 100 kG. Strong axial fields (100 kG) confined to the hydrogen-burning shell suppress instabilities unless the shear is sufficiently distant. These results support incorporating our criteria and growth estimates into stellar evolution models to assess the efficiency of shear-driven transport.",
      "url": "http://arxiv.org/abs/2602.06889v1",
      "published_time_eastern_timestamp": 1770398633.0
    },
    {
      "title": "Vision Transformer Finetuning Benefits from Non-Smooth Components",
      "summary": "The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity.",
      "url": "http://arxiv.org/abs/2602.06883v1",
      "published_time_eastern_timestamp": 1770397942.0
    },
    {
      "title": "TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code",
      "summary": "Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficient cycles. To overcome these challenges, we present TraceCoder, a collaborative multi-agent framework that emulates the observe-analyze-repair process of human experts. The framework first instruments the code with diagnostic probes to capture fine-grained runtime traces, enabling deep insight into its internal execution. It then conducts causal analysis on these traces to accurately identify the root cause of the failure. This process is further enhanced by a novel Historical Lesson Learning Mechanism (HLLM), which distills insights from prior failed repair attempts to inform subsequent correction strategies and prevent recurrence of similar mistakes. To ensure stable convergence, a Rollback Mechanism enforces that each repair iteration constitutes a strict improvement toward the correct solution. Comprehensive experiments across multiple benchmarks show that TraceCoder achieves up to a 34.43\\% relative improvement in Pass@1 accuracy over existing advanced baselines. Ablation studies verify the significance of each system component, with the iterative repair process alone contributing a 65.61\\% relative gain in accuracy. Furthermore, TraceCoder significantly outperforms leading iterative methods in terms of both accuracy and cost-efficiency.",
      "url": "http://arxiv.org/abs/2602.06875v1",
      "published_time_eastern_timestamp": 1770397188.0
    },
    {
      "title": "Asteroseismic ages for 17,000 stars in Kepler, K2 and TESS",
      "summary": "The availability of asteroseismic constraints for tens of thousands of red giant (RG) stars has opened the door to robust age estimates, enabling time-resolved studies of different populations of stars in the Milky Way. This study leverages data from Kepler, K2, and TESS, in conjunction with astrometric data from Gaia DR3 and spectroscopic constraints from APOGEE DR17 and GALAH DR3, to infer parameters for over 17,000 RGs. We use the code PARAM to homogeneously infer stellar properties considering in detail the sensitivity of our results to different choices of observational constraints. We focus on age estimation, identifying potentially unreliable age determinations, and highlight stars with unreliable $Δν$ measurements based on comparisons using Gaia luminosities. These are particularly relevant in K2 data due to the short duration of the observations of each campaign, and therefore important to characterise for Galactic archaeology studies where the spatial range of K2 is a benefit. Thanks to the combination of data from different missions we explore trends in age, mass, and orbital parameters such as $R_\\mathrm{g}$ and $Z_\\mathrm{max}$, and examine time-resolved [$α$/M]-[Fe/H] planes across different Galactic regions. Additionally, we compare age distributions in low- and high-$α$ populations and chemically selected ex situ stars. The study also extends known mass-[C/N] ratio relationships to lower masses. The catalogues resulting from this work will be instrumental in addressing key questions in Galactic archaeology and stellar evolution, and to improve training sets for machine-learning-based age estimations.",
      "url": "http://arxiv.org/abs/2602.06870v1",
      "published_time_eastern_timestamp": 1770396936.0
    },
    {
      "title": "Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing",
      "summary": "Adapting pre-trained vision models using parameter-efficient fine-tuning (PEFT) remains challenging, as it aims to achieve performance comparable to full fine-tuning using a minimal number of trainable parameters. When applied to complex dense prediction tasks, existing methods exhibit limitations, including input-agnostic modeling and redundant cross-layer representations. To this end, we propose AdaRoute, a new adapter-style method featuring a simple mixture-of-experts (MoE) architecture. Specifically, we introduce shared expert centers, where each expert is a trainable parameter matrix. During a feedforward pass, each AdaRoute module in the network dynamically generates weight matrices tailored for the current module via a simple dynamic parameter routing mechanism, which selectively aggregates parameter matrices in the corresponding expert center. Dynamic weight matrices in AdaRoute modules facilitate low-rank adaptation in an input-dependent manner, thus generating more customized and powerful feature representations. Moreover, since AdaRoute modules across multiple network layers share the same expert center, they improve feature diversity by promoting implicit cross-layer feature interaction. Extensive experiments demonstrate the superiority of AdaRoute on diverse vision tasks, including semantic segmentation, object detection and instance segmentation, and panoptic segmentation. Code will be available at: https://bit.ly/3NZcr0H.",
      "url": "http://arxiv.org/abs/2602.06862v1",
      "published_time_eastern_timestamp": 1770396638.0
    },
    {
      "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
      "summary": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
      "url": "http://arxiv.org/abs/2602.06855v1",
      "published_time_eastern_timestamp": 1770396302.0
    },
    {
      "title": "SEMA: Simple yet Effective Learning for Multi-Turn Jailbreak Attacks",
      "summary": "Multi-turn jailbreaks capture the real threat model for safety-aligned chatbots, where single-turn attacks are merely a special case. Yet existing approaches break under exploration complexity and intent drift. We propose SEMA, a simple yet effective framework that trains a multi-turn attacker without relying on any existing strategies or external data. SEMA comprises two stages. Prefilling self-tuning enables usable rollouts by fine-tuning on non-refusal, well-structured, multi-turn adversarial prompts that are self-generated with a minimal prefix, thereby stabilizing subsequent learning. Reinforcement learning with intent-drift-aware reward trains the attacker to elicit valid multi-turn adversarial prompts while maintaining the same harmful objective. We anchor harmful intent in multi-turn jailbreaks via an intent-drift-aware reward that combines intent alignment, compliance risk, and level of detail. Our open-loop attack regime avoids dependence on victim feedback, unifies single- and multi-turn settings, and reduces exploration complexity. Across multiple datasets, victim models, and jailbreak judges, our method achieves state-of-the-art (SOTA) attack success rates (ASR), outperforming all single-turn baselines, manually scripted and template-driven multi-turn baselines, as well as our SFT (Supervised Fine-Tuning) and DPO (Direct Preference Optimization) variants. For instance, SEMA performs an average $80.1\\%$ ASR@1 across three closed-source and open-source victim models on AdvBench, 33.9% over SOTA. The approach is compact, reproducible, and transfers across targets, providing a stronger and more realistic stress test for large language model (LLM) safety and enabling automatic redteaming to expose and localize failure modes. Our code is available at: https://github.com/fmmarkmq/SEMA.",
      "url": "http://arxiv.org/abs/2602.06854v1",
      "published_time_eastern_timestamp": 1770396297.0
    },
    {
      "title": "SuReNav: Superpixel Graph-based Constraint Relaxation for Navigation in Over-constrained Environments",
      "summary": "We address the over-constrained planning problem in semi-static environments. The planning objective is to find a best-effort solution that avoids all hard constraint regions while minimally traversing the least risky areas. Conventional methods often rely on pre-defined area costs, limiting generalizations. Further, the spatial continuity of navigation spaces makes it difficult to identify regions that are passable without overestimation. To overcome these challenges, we propose SuReNav, a superpixel graph-based constraint relaxation and navigation method that imitates human-like safe and efficient navigation. Our framework consists of three components: 1) superpixel graph map generation with regional constraints, 2) regional-constraint relaxation using graph neural network trained on human demonstrations for safe and efficient navigation, and 3) interleaving relaxation, planning, and execution for complete navigation. We evaluate our method against state-of-the-art baselines on 2D semantic maps and 3D maps from OpenStreetMap, achieving the highest human-likeness score of complete navigation while maintaining a balanced trade-off between efficiency and safety. We finally demonstrate its scalability and generalization performance in real-world urban navigation with a quadruped robot, Spot.",
      "url": "http://arxiv.org/abs/2602.06807v1",
      "published_time_eastern_timestamp": 1770393338.0
    },
    {
      "title": "Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling",
      "summary": "An impediment to using Large Language Models (LLMs) for reasoning output verification is that LLMs struggle to reliably identify errors in thinking traces, particularly in long outputs, domains requiring expert knowledge, and problems without verifiable rewards. We propose a data-driven approach to automatically construct highly granular reasoning error taxonomies to enhance LLM-driven error detection on unseen reasoning traces. Our findings indicate that classification approaches that leverage these error taxonomies, or \"rubrics\", demonstrate strong error identification compared to baseline methods in technical domains like coding, math, and chemical engineering. These rubrics can be used to build stronger LLM-as-judge reward functions for reasoning model training via reinforcement learning. Experimental results show that these rewards have the potential to improve models' task accuracy on difficult domains over models trained by general LLMs-as-judges by +45%, and approach performance of models trained by verifiable rewards while using as little as 20% as many gold labels. Through our approach, we extend the usage of reward rubrics from assessing qualitative model behavior to assessing quantitative model correctness on tasks typically learned via RLVR rewards. This extension opens the door for teaching models to solve complex technical problems without a full dataset of gold labels, which are often highly costly to procure.",
      "url": "http://arxiv.org/abs/2602.06795v1",
      "published_time_eastern_timestamp": 1770393112.0
    },
    {
      "title": "Towards Understanding What State Space Models Learn About Code",
      "summary": "State Space Models (SSMs) have emerged as an efficient alternative to the transformer architecture. Recent studies show that SSMs can match or surpass Transformers on code understanding tasks, such as code retrieval, when trained under similar conditions. However, their internal mechanisms remain a black box. We present the first systematic analysis of what SSM-based code models actually learn and perform the first comparative analysis of SSM and Transformer-based code models. Our analysis reveals that SSMs outperform Transformers at capturing code syntax and semantics in pretraining but forgets certain syntactic and semantic relations during fine-tuning on task, especially when the task emphasizes short-range dependencies. To diagnose this, we introduce SSM-Interpret, a frequency-domain framework that exposes a spectral shift toward short-range dependencies during fine-tuning. Guided by these findings, we propose architectural modifications that significantly improve the performance of SSM-based code model, validating that our analysis directly enables better models.",
      "url": "http://arxiv.org/abs/2602.06774v1",
      "published_time_eastern_timestamp": 1770391786.0
    },
    {
      "title": "Sparse Spike Encoding of Channel Responses for Energy Efficient Human Activity Recognition",
      "summary": "ISAC enables pervasive monitoring, but modern sensing algorithms are often too complex for energy-constrained edge devices. This motivates the development of learning techniques that balance accuracy performance and energy efficiency. Spiking Neural Networks (SNNs) are a promising alternative, processing information as sparse binary spike trains and potentially reducing energy consumption by orders of magnitude. In this work, we propose a spiking convolutional autoencoder (SCAE) that learns tailored spike-encoded representations of channel impulse responses (CIR), jointly trained with an SNN for human activity recognition (HAR), thereby eliminating the need for Doppler domain preprocessing. The results show that our SCAE-SNN achieves F1 scores comparable to a hybrid approach (almost 96%), while producing substantially sparser spike encoding (81.1% sparsity). We also show that encoding CIR data prior to classification improves both HAR accuracy and efficiency. The code is available at https://github.com/ele-ciccia/SCAE-SNN-HAR.",
      "url": "http://arxiv.org/abs/2602.06766v1",
      "published_time_eastern_timestamp": 1770391221.0
    },
    {
      "title": "R-Align: Enhancing Generative Reward Models through Rationale-Centric Meta-Judging",
      "summary": "Reinforcement Learning from Human Feedback (RLHF) remains indispensable for aligning large language models (LLMs) in subjective domains. To enhance robustness, recent work shifts toward Generative Reward Models (GenRMs) that generate rationales before predicting preferences. Yet in GenRM training and evaluation, practice remains outcome-label-only, leaving reasoning quality unchecked. We show that reasoning fidelity-the consistency between a GenRM's preference decision and reference decision rationales-is highly predictive of downstream RLHF outcomes, beyond standard label accuracy. Specifically, we repurpose existing reward-model benchmarks to compute Spurious Correctness (S-Corr)-the fraction of label-correct decisions with rationales misaligned with golden judgments. Our empirical evaluation reveals substantial S-Corr even for competitive GenRMs, and higher S-Corr is associated with policy degeneration under optimization. To improve fidelity, we propose Rationale-Centric Alignment, R-Align, which augments training with gold judgments and explicitly supervises rationale alignment. R-Align reduces S-Corr on RM benchmarks and yields consistent gains in actor performance across STEM, coding, instruction following, and general tasks.",
      "url": "http://arxiv.org/abs/2602.06763v1",
      "published_time_eastern_timestamp": 1770391031.0
    }
  ]
}
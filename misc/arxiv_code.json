{
  "last_updated": "2025-08-24T14:14:51.304348-04:00",
  "papers": [
    {
      "title": "CineScale: Free Lunch in High-Resolution Cinematic Visual Generation",
      "summary": "Visual diffusion models achieve remarkable progress, yet they are typically\ntrained at limited resolutions due to the lack of high-resolution data and\nconstrained computation resources, hampering their ability to generate\nhigh-fidelity images or videos at higher resolutions. Recent efforts have\nexplored tuning-free strategies to exhibit the untapped potential\nhigher-resolution visual generation of pre-trained models. However, these\nmethods are still prone to producing low-quality visual content with repetitive\npatterns. The key obstacle lies in the inevitable increase in high-frequency\ninformation when the model generates visual content exceeding its training\nresolution, leading to undesirable repetitive patterns deriving from the\naccumulated errors. In this work, we propose CineScale, a novel inference\nparadigm to enable higher-resolution visual generation. To tackle the various\nissues introduced by the two types of video generation architectures, we\npropose dedicated variants tailored to each. Unlike existing baseline methods\nthat are confined to high-resolution T2I and T2V generation, CineScale broadens\nthe scope by enabling high-resolution I2V and V2V synthesis, built atop\nstate-of-the-art open-source video generation frameworks. Extensive experiments\nvalidate the superiority of our paradigm in extending the capabilities of\nhigher-resolution visual generation for both image and video models.\nRemarkably, our approach enables 8k image generation without any fine-tuning,\nand achieves 4k video generation with only minimal LoRA fine-tuning. Generated\nvideo samples are available at our website:\nhttps://eyeline-labs.github.io/CineScale/.",
      "url": "http://arxiv.org/abs/2508.15774v1",
      "published_time_eastern_timestamp": 1755799197.0
    },
    {
      "title": "Visual Autoregressive Modeling for Instruction-Guided Image Editing",
      "summary": "Recent advances in diffusion models have brought remarkable visual fidelity\nto instruction-guided image editing. However, their global denoising process\ninherently entangles the edited region with the entire image context, leading\nto unintended spurious modifications and compromised adherence to editing\ninstructions. In contrast, autoregressive models offer a distinct paradigm by\nformulating image synthesis as a sequential process over discrete visual\ntokens. Their causal and compositional mechanism naturally circumvents the\nadherence challenges of diffusion-based methods. In this paper, we present\nVAREdit, a visual autoregressive (VAR) framework that reframes image editing as\na next-scale prediction problem. Conditioned on source image features and text\ninstructions, VAREdit generates multi-scale target features to achieve precise\nedits. A core challenge in this paradigm is how to effectively condition the\nsource image tokens. We observe that finest-scale source features cannot\neffectively guide the prediction of coarser target features. To bridge this\ngap, we introduce a Scale-Aligned Reference (SAR) module, which injects\nscale-matched conditioning information into the first self-attention layer.\nVAREdit demonstrates significant advancements in both editing adherence and\nefficiency. On standard benchmarks, it outperforms leading diffusion-based\nmethods by 30\\%+ higher GPT-Balance score. Moreover, it completes a\n$512\\times512$ editing in 1.2 seconds, making it 2.2$\\times$ faster than the\nsimilarly sized UltraEdit. The models are available at\nhttps://github.com/HiDream-ai/VAREdit.",
      "url": "http://arxiv.org/abs/2508.15772v1",
      "published_time_eastern_timestamp": 1755799172.0
    },
    {
      "title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass",
      "summary": "3D content generation has recently attracted significant research interest\ndue to its applications in VR/AR and embodied AI. In this work, we address the\nchallenging task of synthesizing multiple 3D assets within a single scene\nimage. Concretely, our contributions are fourfold: (i) we present SceneGen, a\nnovel framework that takes a scene image and corresponding object masks as\ninput, simultaneously producing multiple 3D assets with geometry and texture.\nNotably, SceneGen operates with no need for optimization or asset retrieval;\n(ii) we introduce a novel feature aggregation module that integrates local and\nglobal scene information from visual and geometric encoders within the feature\nextraction module. Coupled with a position head, this enables the generation of\n3D assets and their relative spatial positions in a single feedforward pass;\n(iii) we demonstrate SceneGen's direct extensibility to multi-image input\nscenarios. Despite being trained solely on single-image inputs, our\narchitectural design enables improved generation performance with multi-image\ninputs; and (iv) extensive quantitative and qualitative evaluations confirm the\nefficiency and robust generation abilities of our approach. We believe this\nparadigm offers a novel solution for high-quality 3D content generation,\npotentially advancing its practical applications in downstream tasks. The code\nand model will be publicly available at: https://mengmouxu.github.io/SceneGen.",
      "url": "http://arxiv.org/abs/2508.15769v1",
      "published_time_eastern_timestamp": 1755799156.0
    },
    {
      "title": "Colour Codes Reach Surface Code Performance using Vibe Decoding",
      "summary": "Two-dimensional quantum colour codes hold significant promise for quantum\nerror correction, offering advantages such as planar connectivity and low\noverhead logical gates. Despite their theoretical appeal, the practical\ndeployment of these codes faces challenges due to complex decoding requirements\ncompared to surface codes. This paper introduces vibe decoding which, for the\nfirst time, brings colour code performance on par with the surface code under\npractical decoding. Our approach leverages an ensemble of belief propagation\ndecoders - each executing a distinct serial message passing schedule - combined\nwith localised statistics post-processing. We refer to this combined protocol\nas VibeLSD. The VibeLSD decoder is highly versatile: our numerical results show\nit outperforms all practical existing colour code decoders across various\nsyndrome extraction schemes, noise models, and error rates. By estimating qubit\nfootprints through quantum memory simulations, we show that colour codes can\noperate with overhead that is comparable to, and in some cases lower than, that\nof the surface code. This, combined with the fact that localised statistics\ndecoding is a parallel algorithm, makes VibeLSD suitable for implementation on\nspecialised hardware for real-time decoding. Our results establish the colour\ncode as a practical architecture for near-term quantum hardware, providing\nimproved compilation efficiency for both Clifford and non-Clifford gates\nwithout incurring additional qubit overhead relative to the surface code.",
      "url": "http://arxiv.org/abs/2508.15743v1",
      "published_time_eastern_timestamp": 1755797922.0
    },
    {
      "title": "EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal\n  E-Commerce Models",
      "summary": "E-commerce platforms are rich in multimodal data, featuring a variety of\nimages that depict product details. However, this raises an important question:\ndo these images always enhance product understanding, or can they sometimes\nintroduce redundancy or degrade performance? Existing datasets are limited in\nboth scale and design, making it difficult to systematically examine this\nquestion. To this end, we introduce EcomMMMU, an e-commerce multimodal\nmultitask understanding dataset with 406,190 samples and 8,989,510 images.\nEcomMMMU is comprised of multi-image visual-language data designed with 8\nessential tasks and a specialized VSS subset to benchmark the capability of\nmultimodal large language models (MLLMs) to effectively utilize visual content.\nAnalysis on EcomMMMU reveals that product images do not consistently improve\nperformance and can, in some cases, degrade it. This indicates that MLLMs may\nstruggle to effectively leverage rich visual content for e-commerce tasks.\nBuilding on these insights, we propose SUMEI, a data-driven method that\nstrategically utilizes multiple images via predicting visual utilities before\nusing them for downstream tasks. Comprehensive experiments demonstrate the\neffectiveness and robustness of SUMEI. The data and code are available through\nhttps://anonymous.4open.science/r/submission25.",
      "url": "http://arxiv.org/abs/2508.15721v1",
      "published_time_eastern_timestamp": 1755795672.0
    },
    {
      "title": "A Grant-free Coded Random Access Scheme for Near-field Communications",
      "summary": "The industrial Internet of things (IIoT) is revolutionizing industrial\nprocesses by facilitating massive machine-type communications among countless\ninterconnected devices. To efficiently handle the resulting large-scale and\nsporadic traffic, grant-free random access protocols-especially coded random\naccess (CRA)-have emerged as scalable and reliable solutions. At the same time,\nadvancements in wireless hardware, including extremely large-scale MIMO arrays\nand high-frequency communication (e.g., mmWave, Terahertz), are pushing network\noperations into the near-field propagation regime, allowing for dense\nconnectivity and enhanced spatial multiplexing. This paper proposes an\ninnovative approach that combines near-field spatial multiplexing with the\ninterference mitigation capabilities of CRA, utilizing an extremely large\naperture array at the access point. This integration improves reliability and\nreduces access latency, offering a robust framework for IIoT connectivity in\nnext-generation 6G networks.",
      "url": "http://arxiv.org/abs/2508.15673v1",
      "published_time_eastern_timestamp": 1755791663.0
    },
    {
      "title": "Benchmarking Computer Science Survey Generation",
      "summary": "Scientific survey articles play a vital role in summarizing research\nprogress, yet their manual creation is becoming increasingly infeasible due to\nthe rapid growth of academic literature. While large language models (LLMs)\noffer promising capabilities for automating this process, progress in this area\nis hindered by the absence of standardized benchmarks and evaluation protocols.\nTo address this gap, we introduce SurGE (Survey Generation Evaluation), a new\nbenchmark for evaluating scientific survey generation in the computer science\ndomain. SurGE consists of (1) a collection of test instances, each including a\ntopic description, an expert-written survey, and its full set of cited\nreferences, and (2) a large-scale academic corpus of over one million papers\nthat serves as the retrieval pool. In addition, we propose an automated\nevaluation framework that measures generated surveys across four dimensions:\ninformation coverage, referencing accuracy, structural organization, and\ncontent quality. Our evaluation of diverse LLM-based approaches shows that\nsurvey generation remains highly challenging, even for advanced self-reflection\nframeworks. These findings highlight the complexity of the task and the\nnecessity for continued research. We have open-sourced all the code, data, and\nmodels at: https://github.com/oneal2000/SurGE",
      "url": "http://arxiv.org/abs/2508.15658v1",
      "published_time_eastern_timestamp": 1755791110.0
    },
    {
      "title": "Beyond the Nyquist frequency: Asteroseismic catalog of undersampled\n  Kepler late subgiants and early red giants",
      "summary": "Subgiants and early red giants are crucial for studying the first dredge-up,\na key evolutionary phase where the convective envelope deepens, mixing\npreviously interior-processed material and bringing it to the surface. Yet,\nvery few have been seismically characterized with Kepler because their\noscillation frequencies are close to the 30 minute sampling frequency of the\nmission. We developed a new method as part of the new PyA2Z code to identify\nsuper-Nyquist oscillators and infer their global seismic parameters,\n$\\nu_\\mathrm{max}$ and large separation, $\\Delta\\nu$. Applying PyA2Z to 2 065\nKepler targets, we seismically characterize 285 super-Nyquist and 168\nclose-to-Nyquist stars with masses from 0.8 to 1.6 M$_\\odot$. In combination\nwith APOGEE spectroscopy, Gaia spectro-photometry, and stellar models, we\nderive stellar ages for the sample. There is good agreement between the\npredicted and actual positions of stars on the HR diagram (luminosity vs.\neffective temperature) as a function of mass and composition. While the timing\nof dredge-up is consistent with predictions, the magnitude and mass dependence\nshow discrepancies with models, possibly due to uncertainties in model physics\nor calibration issues in observed abundance scales.",
      "url": "http://arxiv.org/abs/2508.15654v1",
      "published_time_eastern_timestamp": 1755790971.0
    },
    {
      "title": "MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for\n  Efficient Online HD Map Construction",
      "summary": "Online HD map construction is a fundamental task in autonomous driving\nsystems, aiming to acquire semantic information of map elements around the ego\nvehicle based on real-time sensor inputs. Recently, several approaches have\nachieved promising results by incorporating offline priors such as SD maps and\nHD maps or by fusing multi-modal data. However, these methods depend on stale\noffline maps and multi-modal sensor suites, resulting in avoidable\ncomputational overhead at inference. To address these limitations, we employ a\nknowledge distillation strategy to transfer knowledge from multimodal models\nwith prior knowledge to an efficient, low-cost, and vision-centric student\nmodel. Specifically, we propose MapKD, a novel multi-level cross-modal\nknowledge distillation framework with an innovative Teacher-Coach-Student (TCS)\nparadigm. This framework consists of: (1) a camera-LiDAR fusion model with\nSD/HD map priors serving as the teacher; (2) a vision-centric coach model with\nprior knowledge and simulated LiDAR to bridge the cross-modal knowledge\ntransfer gap; and (3) a lightweight vision-based student model. Additionally,\nwe introduce two targeted knowledge distillation strategies: Token-Guided 2D\nPatch Distillation (TGPD) for bird's eye view feature alignment and Masked\nSemantic Response Distillation (MSRD) for semantic learning guidance. Extensive\nexperiments on the challenging nuScenes dataset demonstrate that MapKD improves\nthe student model by +6.68 mIoU and +10.94 mAP while simultaneously\naccelerating inference speed. The code is available\nat:https://github.com/2004yan/MapKD2026.",
      "url": "http://arxiv.org/abs/2508.15653v1",
      "published_time_eastern_timestamp": 1755790638.0
    },
    {
      "title": "SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in\n  Large Language Models",
      "summary": "Large Language Models (LLMs) excel at various natural language processing\ntasks but remain vulnerable to jailbreaking attacks that induce harmful content\ngeneration. In this paper, we reveal a critical safety inconsistency: LLMs can\nmore effectively identify harmful requests as discriminators than defend\nagainst them as generators. This insight inspires us to explore aligning the\nmodel's inherent discrimination and generation capabilities. To this end, we\npropose SDGO (Self-Discrimination-Guided Optimization), a reinforcement\nlearning framework that leverages the model's own discrimination capabilities\nas a reward signal to enhance generation safety through iterative\nself-improvement. Our method does not require any additional annotated data or\nexternal models during the training phase. Extensive experiments demonstrate\nthat SDGO significantly improves model safety compared to both prompt-based and\ntraining-based baselines while maintaining helpfulness on general benchmarks.\nBy aligning LLMs' discrimination and generation capabilities, SDGO brings\nrobust performance against out-of-distribution (OOD) jailbreaking attacks. This\nalignment achieves tighter coupling between these two capabilities, enabling\nthe model's generation capability to be further enhanced with only a small\namount of discriminative samples. Our code and datasets are available at\nhttps://github.com/NJUNLP/SDGO.",
      "url": "http://arxiv.org/abs/2508.15648v1",
      "published_time_eastern_timestamp": 1755789969.0
    },
    {
      "title": "High-Capacity and Low-PAPR BICM-OFDM Systems Using Non-Equiprobable and\n  Non-Uniform Constellation Shaping With Clipping and Filtering",
      "summary": "We address a design of high-capacity and low-peak-to-average power ratio\n(PAPR) orthogonal frequency-division multiplexing (OFDM) systems based on\nbit-interleaved coded modulation (BICM) utilizing non-equiprobable and\nnon-uniform (NENU) constellations as well as clipping and filtering (CAF). The\nproposed constellations are generated using a truncated Gaussian distribution,\nand the merging of constellation points, where the former creates a non-uniform\nconstellation (NUC), and the latter decreases the number of signal points\nwithout compromising the achievable bit-wise mutual information (BMI). Since\nthe proposed constellations are uniquely determined by only the two parameters,\neach associated with NUC and cardinality, the complexity required for the\nnumerical optimization process can be significantly low. We focus on the\nconstellation design based on one dimension, i.e., pulse amplitude modulation\n(PAM), which facilitates the reduction of demapping complexity for the BICM\nreceiver. The use of CAF at the transmitter can efficiently reduce the PAPR of\nOFDM signals; however, it introduces clipping noise that may degrade error rate\nperformance, making the application of clipping noise cancellation (CNC) at the\nreceiver essential. Therefore, we optimize the NENU constellations in the\npresence of CAF and CNC. Simulation results demonstrate that the combination of\nconstellation shaping with CAF and CNC enables BICM-OFDM systems to\nsimultaneously achieve low PAPR and high spectral efficiency over additive\nwhite Gaussian noise (AWGN) as well as frequency-selective Rayleigh fading\nchannels. Furthermore, comparative studies confirm that the proposed system\nsignificantly outperforms the single-carrier counterpart (i.e., DFT-precoded\nBICM-OFDM) in terms of PAPR and bit error rate (BER) performance over fading\nchannels.",
      "url": "http://arxiv.org/abs/2508.15639v1",
      "published_time_eastern_timestamp": 1755788879.0
    },
    {
      "title": "ASCMamba: Multimodal Time-Frequency Mamba for Acoustic Scene\n  Classification",
      "summary": "Acoustic Scene Classification (ASC) is a fundamental problem in computational\naudition, which seeks to classify environments based on the distinctive\nacoustic features. In the ASC task of the APSIPA ASC 2025 Grand Challenge, the\norganizers introduce a multimodal ASC task. Unlike traditional ASC systems that\nrely solely on audio inputs, this challenge provides additional textual\ninformation as inputs, including the location where the audio is recorded and\nthe time of recording. In this paper, we present our proposed system for the\nASC task in the APSIPA ASC 2025 Grand Challenge. Specifically, we propose a\nmultimodal network, \\textbf{ASCMamba}, which integrates audio and textual\ninformation for fine-grained acoustic scene understanding and effective\nmultimodal ASC. The proposed ASCMamba employs a DenseEncoder to extract\nhierarchical spectral features from spectrograms, followed by a dual-path Mamba\nblocks that capture long-range temporal and frequency dependencies using\nMamba-based state space models. In addition, we present a two-step\npseudo-labeling mechanism to generate more reliable pseudo-labels. Results show\nthat the proposed system outperforms all the participating teams and achieves a\n6.2% improvement over the baseline. Code, model and pre-trained checkpoints are\navailable at https://github.com/S-Orion/ASCMamba.git.",
      "url": "http://arxiv.org/abs/2508.15632v1",
      "published_time_eastern_timestamp": 1755788218.0
    },
    {
      "title": "Formation of heavy double neutron stars I: Eddington-limited accretion\n  for a 1.4 $M_{\\odot}$ neutron star at solar metallicity",
      "summary": "More than 30 Galactic double neutron star (DNS) binaries have now been\nidentified through radio pulsar timing. The 24 DNSs in the Galactic field with\nmeasured total masses lie in the narrow range of 2.3--2.9 $M_{\\odot}$. In\ncontrast, gravitational-wave observations have detected two DNS mergers:\nGW170817, with a total mass of 2.7 $M_{\\odot}$, and GW190425, with a\nsignificantly higher mass of 3.4 $M_{\\odot}$. The unusually high mass of\nGW190425 suggests a non-standard formation channel not represented in the known\nGalactic population. To investigate the origin of such a massive DNS system, we\nmodel the late evolutionary stages of helium stars with initial masses between\n2.5 and 9.8 $M_{\\odot}$ in binaries with 1.4 $M_{\\odot}$ neutron star\ncompanions, using the 1D stellar evolution code MESA at solar metallicity. We\ntest alternative formation pathways and calibrate our models to reproduce the\nobserved Galactic DNS mass and orbital distributions. By incorporating a\nmodified natal kick prescription, our population synthesis results are broadly\nconsistent with the observed total mass distribution of known DNS systems. Only\na small fraction of DNSs of our model have total masses $\\geq$ 3 $M_{\\odot}$,\ninsufficient to explain the high rate of massive DNS mergers inferred from GW\nobservations. However, our model rules out the formation of heavy DNS systems\nlike GW190425 via the second unstable mass transfer.",
      "url": "http://arxiv.org/abs/2508.15624v1",
      "published_time_eastern_timestamp": 1755787845.0
    },
    {
      "title": "Interface on demand: Towards AI native Control interfaces for 6G",
      "summary": "Traditional standardized network interfaces face significant limitations,\nincluding vendor-specific incompatibilities, rigid design assumptions, and lack\nof adaptability for new functionalities. We propose a multi-agent framework\nleveraging large language models (LLMs) to generate control interfaces on\ndemand between network functions (NFs). This includes a matching agent, which\naligns required control functionalities with NF capabilities, and a\ncode-generation agent, which generates the necessary API server for interface\nrealization. We validate our approach using simulated multi-vendor gNB and WLAN\nAP environments. The performance evaluations highlight the trade-offs between\ncost and latency across LLMs for interface generation tasks. Our work sets the\nfoundation for AI-native dynamic control interface generation, paving the way\nfor enhanced interoperability and adaptability in future mobile networks.",
      "url": "http://arxiv.org/abs/2508.15595v1",
      "published_time_eastern_timestamp": 1755785311.0
    },
    {
      "title": "Any-to-any Speaker Attribute Perturbation for Asynchronous Voice\n  Anonymization",
      "summary": "Speaker attribute perturbation offers a feasible approach to asynchronous\nvoice anonymization by employing adversarially perturbed speech as anonymized\noutput. In order to enhance the identity unlinkability among anonymized\nutterances from the same original speaker, the targeted attack training\nstrategy is usually applied to anonymize the utterances to a common designated\nspeaker. However, this strategy may violate the privacy of the designated\nspeaker who is an actual speaker. To mitigate this risk, this paper proposes an\nany-to-any training strategy. It is accomplished by defining a batch mean loss\nto anonymize the utterances from various speakers within a training mini-batch\nto a common pseudo-speaker, which is approximated as the average speaker in the\nmini-batch. Based on this, a speaker-adversarial speech generation model is\nproposed, incorporating the supervision from both the untargeted attack and the\nany-to-any strategies. The speaker attribute perturbations are generated and\nincorporated into the original speech to produce its anonymized version. The\neffectiveness of the proposed model was justified in asynchronous voice\nanonymization through experiments conducted on the VoxCeleb datasets.\nAdditional experiments were carried out to explore the potential limitations of\nspeaker-adversarial speech in voice privacy protection. With them, we aim to\nprovide insights for future research on its protective efficacy against\nblack-box speaker extractors \\textcolor{black}{and adaptive attacks, as well\nas} generalization to out-of-domain datasets \\textcolor{black}{and stability}.\nAudio samples and open-source code are published in\nhttps://github.com/VoicePrivacy/any-to-any-speaker-attribute-perturbation.",
      "url": "http://arxiv.org/abs/2508.15565v1",
      "published_time_eastern_timestamp": 1755783668.0
    },
    {
      "title": "The CP2K Program Package Made Simple",
      "summary": "CP2K is a versatile open-source software package for simulations across a\nwide range of atomistic systems, from isolated molecules in the gas phase to\nlow-dimensional functional materials and interfaces, as well as highly\nsymmetric crystalline solids, disordered amorphous glasses, and weakly\ninteracting soft-matter systems in the liquid state and in solution. This\nreview highlights CP2K's capabilities for computing both static and dynamical\nproperties using quantum-mechanical and classical simulation methods. In\ncontrast to the accompanying theory and code paper [J. Chem. Phys. 152, 194103\n(2020)], the focus here is on the practical usage and applications of CP2K,\nwith underlying theoretical concepts introduced only as needed.",
      "url": "http://arxiv.org/abs/2508.15559v1",
      "published_time_eastern_timestamp": 1755783528.0
    },
    {
      "title": "HEAS: Hierarchical Evolutionary Agent Simulation Framework for\n  Cross-Scale Modeling and Multi-Objective Search",
      "summary": "Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that\nunifies layered agent-based modeling with evolutionary optimization and\ntournament evaluation in a single, reproducible workflow. HEAS represents\nmodels as hierarchies of lightweight processes (\"streams\") scheduled in\ndeterministic layers that read and write a shared context, making cross-scale\ncouplings explicit and auditable. A compact API and CLI-simulate, optimize,\nevaluate-expose single- and multi-objective evolution, PyTorch policy\nintegration via parameter flattening/unflattening, and general tournament\ntooling with user-defined scoring and voting rules. The framework standardizes\nevaluation through uniform per-step and episode metrics, persists seeds,\nlogbooks, and hall-of-fame archives, and provides plotting helpers for traces,\nPareto fronts, and comparative outcomes, reducing glue code and improving\ncomparability across studies. HEAS emphasizes separation of mechanism from\norchestration, allowing exogenous drivers, endogenous agents, and aggregators\nto be composed and swapped without refactoring, while the same model can be\nused for forward simulation, optimization, or systematic comparison. We\nillustrate usage with two compact examples-an ecological system and an\nenterprise decision-making setting. HEAS offers a practical foundation for\ncross-disciplinary, multi-level inquiry, yielding reliable, reproducible\nresults.",
      "url": "http://arxiv.org/abs/2508.15555v1",
      "published_time_eastern_timestamp": 1755783346.0
    },
    {
      "title": "Deep Equilibrium Convolutional Sparse Coding for Hyperspectral Image\n  Denoising",
      "summary": "Hyperspectral images (HSIs) play a crucial role in remote sensing but are\noften degraded by complex noise patterns. Ensuring the physical property of the\ndenoised HSIs is vital for robust HSI denoising, giving the rise of deep\nunfolding-based methods. However, these methods map the optimization of a\nphysical model to a learnable network with a predefined depth, which lacks\nconvergence guarantees. In contrast, Deep Equilibrium (DEQ) models treat the\nhidden layers of deep networks as the solution to a fixed-point problem and\nmodels them as infinite-depth networks, naturally consistent with the\noptimization. Under the framework of DEQ, we propose a Deep Equilibrium\nConvolutional Sparse Coding (DECSC) framework that unifies local\nspatial-spectral correlations, nonlocal spatial self-similarities, and global\nspatial consistency for robust HSI denoising. Within the convolutional sparse\ncoding (CSC) framework, we enforce shared 2D convolutional sparse\nrepresentation to ensure global spatial consistency across bands, while\nunshared 3D convolutional sparse representation captures local spatial-spectral\ndetails. To further exploit nonlocal self-similarities, a transformer block is\nembedded after the 2D CSC. Additionally, a detail enhancement module is\nintegrated with the 3D CSC to promote image detail preservation. We formulate\nthe proximal gradient descent of the CSC model as a fixed-point problem and\ntransform the iterative updates into a learnable network architecture within\nthe framework of DEQ. Experimental results demonstrate that our DECSC method\nachieves superior denoising performance compared to state-of-the-art methods.",
      "url": "http://arxiv.org/abs/2508.15553v1",
      "published_time_eastern_timestamp": 1755783311.0
    },
    {
      "title": "A Novel Mutation Based Method for Detecting FPGA Logic Synthesis Tool\n  Bugs",
      "summary": "FPGA (Field-Programmable Gate Array) logic synthesis tools are key components\nin the EDA (Electronic Design Automation) toolchain. They convert hardware\ndesigns written in description languages such as Verilog into gate-level\nrepresentations for FPGAs. However, defects in these tools may lead to\nunexpected behaviors and pose security risks. Therefore, it is crucial to\nharden these tools through testing. Although several methods have been proposed\nto automatically test FPGA logic synthesis tools, the challenge remains of\ninsufficient semantic and logical complexity in test programs. In this paper,\nwe propose VERMEI, a new method for testing FPGA logic synthesis tools. VERMEI\nconsists of three modules: preprocessing, equivalent mutation, and bug\nidentification. The preprocessing module identifies zombie logic (inactive code\nwith no impact on the circuit output) in seed programs through simulation and\ncoverage analysis. The equivalent mutation module generates equivalent variants\nof seed programs by pruning or inserting logic fragments in zombie areas. It\nuses Bayesian sampling to extract logic fragments from historical Verilog\ndesigns, making the generated variants have complex control flows and\nstructures. The bug identification module, based on differential testing,\ncompares the synthesized outputs of seed and variant programs to identify bugs.\nExperiments on Yosys, Vivado, and Quartus demonstrate that VERMEI outperforms\nthe state-of-the-art methods. Within five months, VERMEI reported 15 bugs to\nvendors, 9 of which were confirmed as new.",
      "url": "http://arxiv.org/abs/2508.15536v1",
      "published_time_eastern_timestamp": 1755781919.0
    },
    {
      "title": "Direct Solution of the Time-Dependent Covariant Radiative Transfer\n  Equation and its Coupling to General Relativistic Magnetohydrodynamics with\n  cuHARM",
      "summary": "In this paper we present a major update to the general relativistic\nmagnetohydrodynamics (GRMHD) code cuHARM, which adds fully covariant treatment\nof radiation transport and the subsequent radiation backreaction on the\ndynamics of the fluid. For the radiative calculations, we discretize and solve\nthe radiation transfer equation on a geodesic grid, in order to resolve the\nangular distribution of the radiation field everywhere in space. This allows\nfor detailed treatment of non-isotropic radiation fields, which is crucial for\naccurately resolving regions of intermediate optical depth. We present the\nequations solved, the numerical methods used, and standard tests used to verify\nthe different aspects of a radiation hydrodynamics code, in particular\nradiation transport and radiation-fluid interaction. We present an application\nof the code to the case of black hole radiative accretion. This new radiation\nmodule is fully GPU-accelerated and represents a major advance in the\ncapabilities of cuHARM.",
      "url": "http://arxiv.org/abs/2508.15532v1",
      "published_time_eastern_timestamp": 1755781770.0
    }
  ]
}
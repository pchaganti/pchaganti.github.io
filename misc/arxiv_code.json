{
  "last_updated": "2025-06-05T02:18:17.727952-04:00",
  "papers": [
    {
      "title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large\n  Multimodal Models",
      "summary": "Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for\nenabling intelligent interaction with 3D environments. While prior efforts\noften rely on explicit 3D inputs or specialized model architectures, we ask:\ncan LMMs reason about 3D space using only structured 2D representations derived\nfrom perception? We introduce Struct2D, a perception-guided prompting framework\nthat combines bird's-eye-view (BEV) images with object marks and object-centric\nmetadata, optionally incorporating egocentric keyframes when needed. Using\nStruct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs\n(e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning\nabilities when provided with structured 2D inputs, effectively handling tasks\nsuch as relative direction estimation and route planning. Building on these\ninsights, we construct Struct2D-Set, a large-scale instruction tuning dataset\nwith 200K fine-grained QA pairs across eight spatial reasoning categories,\ngenerated automatically from 3D indoor scenes. We fine-tune an open-source LMM\n(Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple\nbenchmarks, including 3D question answering, dense captioning, and object\ngrounding. Our approach demonstrates that structured 2D inputs can effectively\nbridge perception and language reasoning in LMMs-without requiring explicit 3D\nrepresentations as input. We will release both our code and dataset to support\nfuture research.",
      "url": "http://arxiv.org/abs/2506.04220v1",
      "published_time_eastern_timestamp": 1749059884.0
    },
    {
      "title": "Pseudo-Simulation for Autonomous Driving",
      "summary": "Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical\nlimitations. Real-world evaluation is often challenging due to safety concerns\nand a lack of reproducibility, whereas closed-loop simulation can face\ninsufficient realism or high computational costs. Open-loop evaluation, while\nbeing efficient and data-driven, relies on metrics that generally overlook\ncompounding errors. In this paper, we propose pseudo-simulation, a novel\nparadigm that addresses these limitations. Pseudo-simulation operates on real\ndatasets, similar to open-loop evaluation, but augments them with synthetic\nobservations generated prior to evaluation using 3D Gaussian Splatting. Our key\nidea is to approximate potential future states the AV might encounter by\ngenerating a diverse set of observations that vary in position, heading, and\nspeed. Our method then assigns a higher importance to synthetic observations\nthat best match the AV's likely behavior using a novel proximity-based\nweighting scheme. This enables evaluating error recovery and the mitigation of\ncausal confusion, as in closed-loop benchmarks, without requiring sequential\ninteractive simulation. We show that pseudo-simulation is better correlated\nwith closed-loop simulations (R^2=0.8) than the best existing open-loop\napproach (R^2=0.7). We also establish a public leaderboard for the community to\nbenchmark new methodologies with pseudo-simulation. Our code is available at\nhttps://github.com/autonomousvision/navsim.",
      "url": "http://arxiv.org/abs/2506.04218v1",
      "published_time_eastern_timestamp": 1749059873.0
    },
    {
      "title": "Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object\n  Detector",
      "summary": "Object detectors often suffer a decrease in performance due to the large\ndomain gap between the training data (source domain) and real-world data\n(target domain). Diffusion-based generative models have shown remarkable\nabilities in generating high-quality and diverse images, suggesting their\npotential for extracting valuable feature from various domains. To effectively\nleverage the cross-domain feature representation of diffusion models, in this\npaper, we train a detector with frozen-weight diffusion model on the source\ndomain, then employ it as a teacher model to generate pseudo labels on the\nunlabeled target domain, which are used to guide the supervised learning of the\nstudent model on the target domain. We refer to this approach as Diffusion\nDomain Teacher (DDT). By employing this straightforward yet potent framework,\nwe significantly improve cross-domain object detection performance without\ncompromising the inference speed. Our method achieves an average mAP\nimprovement of 21.2% compared to the baseline on 6 datasets from three common\ncross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},\nsurpassing the current state-of-the-art (SOTA) methods by an average of 5.7%\nmAP. Furthermore, extensive experiments demonstrate that our method\nconsistently brings improvements even in more powerful and complex models,\nhighlighting broadly applicable and effective domain adaptation capability of\nour DDT. The code is available at\nhttps://github.com/heboyong/Diffusion-Domain-Teacher.",
      "url": "http://arxiv.org/abs/2506.04211v1",
      "published_time_eastern_timestamp": 1749059806.0
    },
    {
      "title": "TracLLM: A Generic Framework for Attributing Long Context LLMs",
      "summary": "Long context large language models (LLMs) are deployed in many real-world\napplications such as RAG, agent, and broad LLM-integrated applications. Given\nan instruction and a long context (e.g., documents, PDF files, webpages), a\nlong context LLM can generate an output grounded in the provided context,\naiming to provide more accurate, up-to-date, and verifiable outputs while\nreducing hallucinations and unsupported claims. This raises a research\nquestion: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)\nin the context that contribute most to or are responsible for the generated\noutput by an LLM? This process, which we call context traceback, has various\nreal-world applications, such as 1) debugging LLM-based systems, 2) conducting\npost-attack forensic analysis for attacks (e.g., prompt injection attack,\nknowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources\nto enhance the trust of users towards outputs generated by LLMs. When applied\nto context traceback for long context LLMs, existing feature attribution\nmethods such as Shapley have sub-optimal performance and/or incur a large\ncomputational cost. In this work, we develop TracLLM, the first generic context\ntraceback framework tailored to long context LLMs. Our framework can improve\nthe effectiveness and efficiency of existing feature attribution methods. To\nimprove the efficiency, we develop an informed search based algorithm in\nTracLLM. We also develop contribution score ensemble/denoising techniques to\nimprove the accuracy of TracLLM. Our evaluation results show TracLLM can\neffectively identify texts in a long context that lead to the output of an LLM.\nOur code and data are at: https://github.com/Wang-Yanting/TracLLM.",
      "url": "http://arxiv.org/abs/2506.04202v1",
      "published_time_eastern_timestamp": 1749059296.0
    },
    {
      "title": "Analyzing Line-of-sight selection biases in galaxy-scale strong lensing\n  with external convergence and shear",
      "summary": "The upcoming Vera Rubin Observatory Legacy Survey of Space and Time (LSST)\nwill dramatically increase the number of strong gravitational lensing systems,\nrequiring precise modeling of line-of-sight (LOS) effects to mitigate biases in\nlensing observations and cosmological inferences. We develop a method to\nconstruct joint distributions of external convergence ($\\kappa_{\\mathrm{ext}}$)\nand shear ($\\gamma_{\\mathrm{ext}}$) for strong lensing LOS by aggregating\nlarge-scale structure simulations with high-resolution halo renderings and\nnon-linear correction. Our approach captures both smooth background matter and\nperturbations from halos, enabling accurate modeling of LOS effects. We apply\nnon-linear LOS corrections to $\\kappa_{\\mathrm{ext}}$ and\n$\\gamma_{\\mathrm{ext}}$ that address the non-additive lensing effects caused by\nobjects along the LOS in strong lensing. We find that, with a minimum image\nseparation of $1.0^{\\prime\\prime}$, non-linear LOS correction due to the\npresence of a dominant deflector slightly increases the ratio of quadruple to\ndouble lenses; this non-linear LOS correction also introduces systematic biases\nof $\\sim 0.1\\%$ for galaxy-AGN lenses in the inferred Hubble constant ($H_0$)\nif not accounted for. We also observe a $0.66\\%$ bias for galaxy-galaxy lenses\non $H_0$, and even larger biases up to $1.02\\%$ for galaxy-AGN systems if LOS\neffects are not accounted for. These results highlight the importance of LOS\nfor precision cosmology. The publicly available code and datasets provide tools\nfor incorporating LOS effects in future analyses.",
      "url": "http://arxiv.org/abs/2506.04201v1",
      "published_time_eastern_timestamp": 1749059162.0
    },
    {
      "title": "A fast and memoryless numerical method for solving fractional\n  differential equations",
      "summary": "The numerical solution of implicit and stiff differential equations by\nimplicit numerical integrators has been largely investigated and there exist\nmany excellent efficient codes available in the scientific community, as Radau5\n(based on a Runge-Kutta collocation method at Radau points) and Dassl, based on\nbackward differentiation formulas, among the others. When solving fractional\nordinary differential equations (ODEs), the derivative operator is replaced by\na non-local one and the fractional ODE is reformulated as a Volterra integral\nequation, to which these codes cannot be directly applied.\n  This article is a follow-up of the article by the authors (Guglielmi and\nHairer, SISC, 2025) for differential equations with distributed delays. The\nmain idea is to approximate the fractional kernel $t^{\\alpha -1}/ \\Gamma\n(\\alpha )$ ($\\alpha >0$) by a sum of exponential functions or by a sum of\nexponential functions multiplied by a monomial, and then to transform the\nfractional integral (of convolution type) into a set of ordinary differential\nequations. The augmented system is typically stiff and thus requires the use of\nan implicit method. It can have a very large dimension and requires a special\ntreatment of the arising linear systems.\n  The present work presents an algorithm for the construction of an\napproximation of the fractional kernel by a sum of exponential functions, and\nit shows how the arising linear systems in a stiff time integrator can be\nsolved efficiently. It is explained how the code Radau5 can be used for solving\nfractional differential equations. Numerical experiments illustrate the\naccuracy and the efficiency of the proposed method. Driver examples are\npublicly available from the homepages of the authors.",
      "url": "http://arxiv.org/abs/2506.04188v1",
      "published_time_eastern_timestamp": 1749058561.0
    },
    {
      "title": "R-Search: Empowering LLM Reasoning with Search via Multi-Reward\n  Reinforcement Learning",
      "summary": "Large language models (LLMs) have notably progressed in multi-step and\nlong-chain reasoning. However, extending their reasoning capabilities to\nencompass deep interactions with search remains a non-trivial challenge, as\nmodels often fail to identify optimal reasoning-search interaction\ntrajectories, resulting in suboptimal responses. We propose R-Search, a novel\nreinforcement learning framework for Reasoning-Search integration, designed to\nenable LLMs to autonomously execute multi-step reasoning with deep search\ninteraction, and learn optimal reasoning search interaction trajectories via\nmulti-reward signals, improving response quality in complex logic- and\nknowledge-intensive tasks. R-Search guides the LLM to dynamically decide when\nto retrieve or reason, while globally integrating key evidence to enhance deep\nknowledge interaction between reasoning and search. During RL training,\nR-Search provides multi-stage, multi-type rewards to jointly optimize the\nreasoning-search trajectory. Experiments on seven datasets show that R-Search\noutperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1%\n(out-of-domain). The code and data are available at\nhttps://github.com/QingFei1/R-Search.",
      "url": "http://arxiv.org/abs/2506.04185v1",
      "published_time_eastern_timestamp": 1749058162.0
    },
    {
      "title": "Core-collapse supernova parameter estimation with the upcoming Vera C.\n  Rubin Observatory",
      "summary": "The Vera Rubin Observatory's Legacy Survey of Space and Time (LSST) is\nexpected to revolutionize time-domain optical astronomy as we know it. With its\nunprecedented depth, the LSST will survey the southern hemisphere sky,\ngenerating nearly 32 trillion observations over its nominal 10-year operation.\nAmong these, approximately 10 million will be supernovae (SNe). These\nobservations will uniquely characterize the SN population, enabling studies of\nknown and rare SN types, detailed parameterization of their light curves, deep\nsearches for new SN progenitor populations, the discovery of strongly lensed\nSNe, and the compilation of a large, well-characterized sample of superluminous\nSNe. We analyzed a sample of 22663 simulations of LSST light curves for core\ncollapse SNe (CCSNe), modeled using the radiative transfer code STELLA. We\nanalyzed this dataset with the software CASTOR, which enables the\nreconstruction of synthetic light curves and spectra via a machine learning\ntechnique that allows one to retrieve the complete parameter map of a SN. For\neach parameter we compared the observed and the true values, determining how\nLSST light curves alone will contribute to characterize the progenitor and the\nexplosion. Our results indicate that LSST alone will not suffice for a\ncomprehensive and precise characterization of progenitor properties and\nexplosion parameters. The limited spectral coverage of LSST light curves (in\nmost cases) does not allow for the accurate estimation of bolometric\nluminosity, and consequently, of the explosion energy and nickel yield.\nAdditionally, the redshift-absorption degeneracy is difficult to resolve\nwithout supplementary information. These findings suggest that for the most\ninteresting SNe, complementary follow-up observations using spectrographs and\noptical facilities (particularly in the infrared bands) will be essential for\naccurate parameter determination.",
      "url": "http://arxiv.org/abs/2506.04184v1",
      "published_time_eastern_timestamp": 1749058144.0
    },
    {
      "title": "SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and\n  Module Decoupling",
      "summary": "Large language models (LLMs) achieve remarkable performance across tasks but\nincur substantial computational costs due to their deep, multi-layered\narchitectures. Layer pruning has emerged as a strategy to alleviate these\ninefficiencies, but conventional static pruning methods overlook two critical\ndynamics inherent to LLM inference: (1) horizontal dynamics, where token-level\nheterogeneity demands context-aware pruning decisions, and (2) vertical\ndynamics, where the distinct functional roles of MLP and self-attention layers\nnecessitate component-specific pruning policies. We introduce SkipGPT, a\ndynamic layer pruning framework designed to optimize computational resource\nallocation through two core innovations: (1) global token-aware routing to\nprioritize critical tokens, and (2) decoupled pruning policies for MLP and\nself-attention components. To mitigate training instability, we propose a\ntwo-stage optimization paradigm: first, a disentangled training phase that\nlearns routing strategies via soft parameterization to avoid premature pruning\ndecisions, followed by parameter-efficient LoRA fine-tuning to restore\nperformance impacted by layer removal. Extensive experiments demonstrate that\nSkipGPT reduces over 40% of model parameters while matching or exceeding the\nperformance of the original dense model across benchmarks. By harmonizing\ndynamic efficiency with preserved expressivity, SkipGPT advances the practical\ndeployment of scalable, resource-aware LLMs. Our code is publicly available at:\nhttps://github.com/EIT-NLP/SkipGPT.",
      "url": "http://arxiv.org/abs/2506.04179v1",
      "published_time_eastern_timestamp": 1749057991.0
    },
    {
      "title": "OpenThoughts: Data Recipes for Reasoning Models",
      "summary": "Reasoning models have made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best\ntraining recipes for reasoning since state-of-the-art models often rely on\nproprietary datasets with little to no public information available. To address\nthis, the goal of the OpenThoughts project is to create open-source datasets\nfor training reasoning models. After initial explorations, our OpenThoughts2-1M\ndataset led to OpenThinker2-32B, the first model trained on public reasoning\ndata to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as\nAIME and LiveCodeBench. We then improve our dataset further by systematically\ninvestigating each step of our data generation pipeline with 1,000+ controlled\nexperiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples\nand using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves\nstate-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,\nand 54% on GPQA Diamond. All of our datasets and models are available on\nhttps://openthoughts.ai.",
      "url": "http://arxiv.org/abs/2506.04178v1",
      "published_time_eastern_timestamp": 1749057939.0
    },
    {
      "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D\n  Gaussian Splatting",
      "summary": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene\nrepresentation and novel view synthesis due to its efficient rendering\ncapabilities. However, 3DGS demands relatively significant GPU memory, limiting\nits use on devices with restricted computational resources. Previous approaches\nhave focused on pruning less important Gaussians, effectively compressing 3DGS\nbut often requiring a fine-tuning stage and lacking adaptability for the\nspecific memory needs of different devices. In this work, we present an elastic\ninference method for 3DGS. Given an input for the desired model size, our\nmethod selects and transforms a subset of Gaussians, achieving substantial\nrendering performance without additional fine-tuning. We introduce a tiny\nlearnable module that controls Gaussian selection based on the input\npercentage, along with a transformation module that adjusts the selected\nGaussians to complement the performance of the reduced model. Comprehensive\nexperiments on ZipNeRF, MipNeRF and Tanks\\&Temples scenes demonstrate the\neffectiveness of our approach. Code is available at https://flexgs.github.io.",
      "url": "http://arxiv.org/abs/2506.04174v1",
      "published_time_eastern_timestamp": 1749057477.0
    },
    {
      "title": "A primal-dual price-optimization method for computing equilibrium prices\n  in mean-field games models",
      "summary": "We develop a simple yet efficient Lagrangian method for computing equilibrium\nprices in a mean-field game price-formation model. We prove that equilibrium\nprices are optimal in terms of a suitable criterion and derive a primal-dual\ngradient-based algorithm for computing them. One of the highlights of our\ncomputational framework is the efficient, simple, and flexible implementation\nof the algorithm using modern automatic differentiation techniques. Our\nimplementation is modular and admits a seamless extension to high-dimensional\nsettings with more complex dynamics, costs, and equilibrium conditions.\nAdditionally, automatic differentiation enables a versatile algorithm that\nrequires only coding the cost functions of agents. It automatically handles the\ngradients of the costs, thereby eliminating the need to manually form the\nadjoint equations.",
      "url": "http://arxiv.org/abs/2506.04169v1",
      "published_time_eastern_timestamp": 1749056842.0
    },
    {
      "title": "Horizon Reduction Makes RL Scalable",
      "summary": "In this work, we study the scalability of offline reinforcement learning (RL)\nalgorithms. In principle, a truly scalable offline RL algorithm should be able\nto solve any given problem, regardless of its complexity, given sufficient\ndata, compute, and model capacity. We investigate if and how current offline RL\nalgorithms match up to this promise on diverse, challenging, previously\nunsolved tasks, using datasets up to 1000x larger than typical offline RL\ndatasets. We observe that despite scaling up data, many existing offline RL\nalgorithms exhibit poor scaling behavior, saturating well below the maximum\nperformance. We hypothesize that the horizon is the main cause behind the poor\nscaling of offline RL. We empirically verify this hypothesis through several\nanalysis experiments, showing that long horizons indeed present a fundamental\nbarrier to scaling up offline RL. We then show that various horizon reduction\ntechniques substantially enhance scalability on challenging tasks. Based on our\ninsights, we also introduce a minimal yet scalable method named SHARSA that\neffectively reduces the horizon. SHARSA achieves the best asymptotic\nperformance and scaling behavior among our evaluation methods, showing that\nexplicitly reducing the horizon unlocks the scalability of offline RL. Code:\nhttps://github.com/seohongpark/horizon-reduction",
      "url": "http://arxiv.org/abs/2506.04168v1",
      "published_time_eastern_timestamp": 1749056814.0
    },
    {
      "title": "On the Synthetic Channels in Polar Codes over Binary-Input Discrete\n  Memoryless Channels",
      "summary": "Polar codes introduced by Arikan in 2009 are the first code family achieving\nthe capacity of binary-input discrete memoryless channels (BIDMCs) with\nlow-complexity encoding and decoding. Identifying unreliable synthetic channels\nin polar code construction is crucial. Currently, because of the large size of\nthe output alphabets of synthetic channels, there is no effective approach to\nevaluate their reliability, except in the case that the underlying channels are\nbinary erasure channels. This paper defines equivalence and symmetry based on\nthe likelihood ratio profile of BIDMCs and characterizes symmetric BIDMCs as\nrandom switching channels (RSCs) of binary symmetric channels. By converting\nthe generation of synthetic channels in polar code construction into algebraic\noperations on underlying channels, some compact representations of RSCs for\nthese synthetic channels are derived. Moreover, a lower bound for the average\nnumber of elements that possess the same likelihood ratio within the output\nalphabet of any synthetic channel generated in polar codes is also derived.",
      "url": "http://arxiv.org/abs/2506.04163v1",
      "published_time_eastern_timestamp": 1749056524.0
    },
    {
      "title": "Image Editing As Programs with Diffusion Models",
      "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
      "url": "http://arxiv.org/abs/2506.04158v1",
      "published_time_eastern_timestamp": 1749056244.0
    },
    {
      "title": "SLAC: Simulation-Pretrained Latent Action Space for Whole-Body\n  Real-World RL",
      "summary": "Building capable household and industrial robots requires mastering the\ncontrol of versatile, high-degree-of-freedom (DoF) systems such as mobile\nmanipulators. While reinforcement learning (RL) holds promise for autonomously\nacquiring robot control policies, scaling it to high-DoF embodiments remains\nchallenging. Direct RL in the real world demands both safe exploration and high\nsample efficiency, which are difficult to achieve in practice. Sim-to-real RL,\non the other hand, is often brittle due to the reality gap. This paper\nintroduces SLAC, a method that renders real-world RL feasible for complex\nembodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic\nlatent action space. SLAC trains this latent action space via a customized\nunsupervised skill discovery method designed to promote temporal abstraction,\ndisentanglement, and safety, thereby facilitating efficient downstream\nlearning. Once a latent action space is learned, SLAC uses it as the action\ninterface for a novel off-policy RL algorithm to autonomously learn downstream\ntasks through real-world interactions. We evaluate SLAC against existing\nmethods on a suite of bimanual mobile manipulation tasks, where it achieves\nstate-of-the-art performance. Notably, SLAC learns contact-rich whole-body\ntasks in under an hour of real-world interactions, without relying on any\ndemonstrations or hand-crafted behavior priors. More information, code, and\nvideos at robo-rl.github.io",
      "url": "http://arxiv.org/abs/2506.04147v1",
      "published_time_eastern_timestamp": 1749055315.0
    },
    {
      "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
      "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient ($\\rho$)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
      "url": "http://arxiv.org/abs/2506.04142v1",
      "published_time_eastern_timestamp": 1749054824.0
    },
    {
      "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued\n  Speech Video-to-Speech Generation",
      "summary": "Cued Speech (CS) enhances lipreading through hand coding, providing precise\nspeech perception support for the hearing-impaired. CS Video-to-Speech\ngeneration (CSV2S) task aims to convert the CS visual expressions (CS videos)\nof hearing-impaired individuals into comprehensible speech signals. Direct\ngeneration of speech from CS video (called single CSV2S) yields poor\nperformance due to insufficient CS data. Current research mostly focuses on CS\nRecognition (CSR), which convert video content into linguistic text. Based on\nthis, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech\nsystem. This combined architecture relies on text as an intermediate medium for\nstepwise cross-modal alignment, which may lead to error propagation and\ntemporal misalignment between speech and video dynamics. To address these\nchallenges, we propose a novel approach that directly generates speech from CS\nvideos without relying on intermediate text. Building upon this, we propose\nUniCUE, the first unified framework for CSV2S, whose core innovation lies in\nthe integration of the CSR task that provides fine-grained visual-semantic\ninformation to facilitate speech generation from CS videos. More precisely, (1)\na novel fine-grained semantic alignment pool to ensure precise mapping between\nvisual features and speech contents; (2) a VisioPhonetic adapter to bridge\ncross-task representations, ensuring seamless compatibility between two\ndistinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is\nintroduced to enhance fine-grained spatiotemporal correlations between lip and\nhand movements in CS video. Experiments on our new established Chinese CS\ndataset (14 cuers1: 8 hearing-impaired and 6 normal-hearing) show that our\nUniCUE significantly reduces Word Error Rate by 78.3% and improves lip-speech\nsynchronization by 32% compared to the single CSV2S.",
      "url": "http://arxiv.org/abs/2506.04134v1",
      "published_time_eastern_timestamp": 1749054409.0
    },
    {
      "title": "CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation\n  in Courtroom Dialogues",
      "summary": "Courtrooms are places where lives are determined and fates are sealed, yet\nthey are not impervious to manipulation. Strategic use of manipulation in legal\njargon can sway the opinions of judges and affect the decisions. Despite the\ngrowing advancements in NLP, its application in detecting and analyzing\nmanipulation within the legal domain remains largely unexplored. Our work\naddresses this gap by introducing LegalCon, a dataset of 1,063 annotated\ncourtroom conversations labeled for manipulation detection, identification of\nprimary manipulators, and classification of manipulative techniques, with a\nfocus on long conversations. Furthermore, we propose CLAIM, a two-stage,\nIntent-driven Multi-agent framework designed to enhance manipulation analysis\nby enabling context-aware and informed decision-making. Our results highlight\nthe potential of incorporating agentic frameworks to improve fairness and\ntransparency in judicial processes. We hope that this contributes to the\nbroader application of NLP in legal discourse analysis and the development of\nrobust tools to support fairness in legal decision-making. Our code and data\nare available at https://github.com/Disha1001/CLAIM.",
      "url": "http://arxiv.org/abs/2506.04131v1",
      "published_time_eastern_timestamp": 1749054179.0
    },
    {
      "title": "Guided Speculative Inference for Efficient Test-Time Alignment of LLMs",
      "summary": "We propose Guided Speculative Inference (GSI), a novel algorithm for\nefficient reward-guided decoding in large language models. GSI combines soft\nbest-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative\nsamples from a small auxiliary model $\\pi_S(y\\mid x)$. We provably approximate\nthe optimal tilted policy $\\pi_{\\beta,B}(y\\mid x) \\propto \\pi_B(y\\mid\nx)\\exp(\\beta\\,r(x,y))$ of soft best-of-$n$ under the primary model $\\pi_B$. We\nderive a theoretical bound on the KL divergence between our induced\ndistribution and the optimal policy. In experiments on reasoning benchmarks\n(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy\nthan standard soft best-of-$n$ with $\\pi_S$ and reward-guided speculative\ndecoding (Liao et al., 2025), and in certain settings even outperforms soft\nbest-of-$n$ with $\\pi_B$. The code is available at\nhttps://github.com/j-geuter/GSI .",
      "url": "http://arxiv.org/abs/2506.04118v1",
      "published_time_eastern_timestamp": 1749053546.0
    }
  ]
}
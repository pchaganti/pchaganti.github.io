{
  "last_updated": "2025-06-18T17:11:37.489218-04:00",
  "papers": [
    {
      "title": "JaCoText: A Pretrained Model for Java Code-Text Generation",
      "summary": "Pretrained transformer-based models have shown high performance in natural language generation task. However, a new wave of interest has surged: automatic programming language generation. This task consists of translating natural language instructions to a programming code. Despite the fact that well-known pretrained models on language generation have achieved good performance in learning programming languages, effort is still needed in automatic code generation. In this paper, we introduce JaCoText, a model based on Transformers neural network. It aims to generate java source code from natural language text. JaCoText leverages advantages of both natural language and code generation models. More specifically, we study some findings from the state of the art and use them to (1) initialize our model from powerful pretrained models, (2) explore additional pretraining on our java dataset, (3) carry out experiments combining the unimodal and bimodal data in the training, and (4) scale the input and output length during the fine-tuning of the model. Conducted experiments on CONCODE dataset show that JaCoText achieves new state-of-the-art results.",
      "url": "http://arxiv.org/abs/2303.12869v1",
      "published_time_eastern_timestamp": 1679511685.0
    },
    {
      "title": "JU_KS@SAIL_CodeMixed-2017: Sentiment Analysis for Indian Code Mixed Social Media Texts",
      "summary": "This paper reports about our work in the NLP Tool Contest @ICON-2017, shared task on Sentiment Analysis for Indian Languages (SAIL) (code mixed). To implement our system, we have used a machine learning algo-rithm called Multinomial Na√Øve Bayes trained using n-gram and SentiWordnet features. We have also used a small SentiWordnet for English and a small SentiWordnet for Bengali. But we have not used any SentiWordnet for Hindi language. We have tested our system on Hindi-English and Bengali-English code mixed social media data sets released for the contest. The performance of our system is very close to the best system participated in the contest. For both Bengali-English and Hindi-English runs, our system was ranked at the 3rd position out of all submitted runs and awarded the 3rd prize in the contest.",
      "url": "http://arxiv.org/abs/1802.05737v1",
      "published_time_eastern_timestamp": 1518724963.0
    },
    {
      "title": "pynucastro: an interface to nuclear reaction rates and code generator for reaction network equations",
      "summary": "pynucastro addresses two needs in the field of nuclear astrophysics: visual exploration of nuclear reaction rates or networks and automated code generation for integrating reaction network ODEs. pynucastro accomplishes this by interfacing with nuclear reaction rate parameterizations published by the JINA Reaclib project (Cyburt et al. 2010).",
      "url": "http://arxiv.org/abs/1803.08920v1",
      "published_time_eastern_timestamp": 1521580634.0
    },
    {
      "title": "Some punctured codes of several families of binary linear codes",
      "summary": "Two general constructions of linear codes with functions over finite fields have been extensively studied in the literature. The first one is given by $\\mathcal{C}(f)=\\left\\{ {\\rm Tr}(af(x)+bx)_{x \\in \\mathbb{F}_{q^m}^*}: a,b \\in \\mathbb{F}_{q^m} \\right\\}$, where $q$ is a prime power, $\\bF_{q^m}^*=\\bF_{q^m} \\setminus \\{0\\}$, $\\tr$ is the trace function from $\\bF_{q^m}$ to $\\bF_q$, and $f(x)$ is a function from $\\mathbb{F}_{q^m}$ to $\\mathbb{F}_{q^m}$ with $f(0)=0$. Almost bent functions, quadratic functions and some monomials on $\\bF_{2^m}$ were used in the first construction, and many families of binary linear codes with few weights were obtained in the literature. This paper studies some punctured codes of these binary codes. Several families of binary linear codes with few weights and new parameters are obtained in this paper. Several families of distance-optimal binary linear codes with new parameters are also produced in this paper.",
      "url": "http://arxiv.org/abs/2101.08425v1",
      "published_time_eastern_timestamp": 1611200798.0
    },
    {
      "title": "Understanding Code Patterns - Analysis, Interpretation & Measurement",
      "summary": "This research paper aims to find, analyze and understand code patterns in any software system and measure its quality by defining standards and proposing a formula for the same. Every code that is written can be divided into different code segments, each having its own impact on the overall system. We can analyze these code segments to get the code quality. The measures used in this paper include Lines of Code, Number of calls made by a module, Execution time, the system knowledge of user and developers, the use of generalization, inheritance, reusability and other object-oriented concepts. The entire software code is divided into code snippets, based on the logic that they implement. Each of these code snippets has an impact. This measure is called Impact Factor and is valued by the software developer and/or other system stakeholders. Efficiency = (Code Area / Execution Time) * Qr",
      "url": "http://arxiv.org/abs/1106.6159v1",
      "published_time_eastern_timestamp": 1309426398.0
    },
    {
      "title": "Robust Learning of Diverse Code Edits",
      "summary": "Software engineering activities frequently involve edits to existing code. However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements. In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm. Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity. Today's code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning. To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting. Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones. We show the generality of our approach on two model families (DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation and general problem-solving abilities post adaptation. We opensource the models, synthetic dataset, and implementation at https://aka.ms/nextcoder.",
      "url": "http://arxiv.org/abs/2503.03656v2",
      "published_time_eastern_timestamp": 1741192744.0
    },
    {
      "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs",
      "summary": "In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.",
      "url": "http://arxiv.org/abs/2502.19411v1",
      "published_time_eastern_timestamp": 1740596142.0
    },
    {
      "title": "SCC: Automatic Classification of Code Snippets",
      "summary": "Determining the programming language of a source code file has been considered in the research community; it has been shown that Machine Learning (ML) and Natural Language Processing (NLP) algorithms can be effective in identifying the programming language of source code files. However, determining the programming language of a code snippet or a few lines of source code is still a challenging task. Online forums such as Stack Overflow and code repositories such as GitHub contain a large number of code snippets. In this paper, we describe Source Code Classification (SCC), a classifier that can identify the programming language of code snippets written in 21 different programming languages. A Multinomial Naive Bayes (MNB) classifier is employed which is trained using Stack Overflow posts. It is shown to achieve an accuracy of 75% which is higher than that with Programming Languages Identification (PLI a proprietary online classifier of snippets) whose accuracy is only 55.5%. The average score for precision, recall and the F1 score with the proposed tool are 0.76, 0.75 and 0.75, respectively. In addition, it can distinguish between code snippets from a family of programming languages such as C, C++ and C#, and can also identify the programming language version such as C# 3.0, C# 4.0 and C# 5.0.",
      "url": "http://arxiv.org/abs/1809.07945v1",
      "published_time_eastern_timestamp": 1537505440.0
    },
    {
      "title": "On the largest minimum distances of [n,6] LCD codes",
      "summary": "Linear complementary dual (LCD) codes can be used to against side-channel attacks and fault noninvasive attacks. Let $d_{a}(n,6)$ and $d_{l}(n,6)$ be the minimum weights of all binary optimal linear codes and LCD codes with length $n$ and dimension 6, respectively.In this article, we aim to obtain the values of $d_{l}(n,6)$ for $n\\geq 51$ by investigating the nonexistence and constructions of LCD codes with given parameters. Suppose that $s \\ge 0$ and $0\\leq t\\leq 62$ are two integers and $n=63s+t$. Using the theories of defining vectors, generalized anti-codes, reduced codes and nested codes, we exactly determine $d_{l}(n,6)$ for $t \\notin\\{21,22,25,26,33,34,37,38,45,46\\}$, while we show that $d_{l}(n,6)\\in$$\\{d_{a}(n,6)$ $-1,d_{a}(n,6)\\}$ for $t\\in\\{21,22,26,34,37,38,46\\}$ and $ d_{l}(n,6)\\in$$ \\{d_{a}(n,6)-2,$ $d_{a}(n,6)-1\\}$ for$t\\in{25,33,45\\}$.",
      "url": "http://arxiv.org/abs/2406.02065v1",
      "published_time_eastern_timestamp": 1717487159.0
    },
    {
      "title": "Floquet codes without parent subsystem codes",
      "summary": "We propose a new class of error-correcting dynamic codes in two and three dimensions that has no explicit connection to any parent subsystem code. The two-dimensional code, which we call the CSS honeycomb code, is geometrically similar to that of the honeycomb code by Hastings and Haah, and also dynamically embeds an instantaneous toric code. However, unlike the honeycomb code it possesses an explicit CSS structure and its gauge checks do not form a subsystem code. Nevertheless, we show that our dynamic protocol conserves logical information and possesses a threshold for error correction. We generalize this construction to three dimensions and obtain a code that fault-tolerantly alternates between realizing two type-I fracton models, the checkerboard and the X-cube model. Finally, we show the compatibility of our CSS honeycomb code protocol and the honeycomb code by showing the possibility of randomly switching between the two protocols without information loss while still measuring error syndromes. We call this more general aperiodic structure `dynamic tree codes', which we also generalize to three dimensions. We construct a probabilistic finite automaton prescription that generates dynamic tree codes correcting any single-qubit Pauli errors and can be viewed as a step towards the development of practical fault-tolerant random codes.",
      "url": "http://arxiv.org/abs/2210.02468v4",
      "published_time_eastern_timestamp": 1664992802.0
    },
    {
      "title": "EVOR: Evolving Retrieval for Code Generation",
      "summary": "Recently the retrieval-augmented generation (RAG) has been successfully applied in code generation. However, existing pipelines for retrieval-augmented code generation (RACG) employ static knowledge bases with a single source, limiting the adaptation capabilities of Large Language Models (LLMs) to domains they have insufficient knowledge of. In this work, we develop a novel pipeline, EVOR, that employs the synchronous evolution of both queries and diverse knowledge bases. On two realistic settings where the external knowledge is required to solve code generation tasks, we compile four new datasets associated with frequently updated libraries and long-tail programming languages, named EVOR-BENCH. Extensive experiments demonstrate that EVOR achieves two to four times of execution accuracy compared to other methods such as Reflexion (Shinn et al., 2024), DocPrompting (Zhou et al., 2023), etc. We demonstrate that EVOR is flexible and can be easily combined with them to achieve further improvement. Further analysis reveals that EVOR benefits from the synchronous evolution of queries and documents and the diverse information sources in the knowledge base. We hope that our studies will inspire more insights into the design of advanced RACG pipelines in future research. Our model, code, and data are available at https://arks-codegen.github.io.",
      "url": "http://arxiv.org/abs/2402.12317v2",
      "published_time_eastern_timestamp": 1708364248.0
    },
    {
      "title": "A new three-dimensional general-relativistic hydrodynamics code",
      "summary": "We present a new three-dimensional general relativistic hydrodynamics code, the Whisky code. This code incorporates the expertise developed over the past years in the numerical solution of Einstein equations and of the hydrodynamics equations in a curved spacetime, and is the result of a collaboration of several European Institutes. We here discuss the ability of the code to carry out long-term accurate evolutions of the linear and nonlinear dynamics of isolated relativistic stars.",
      "url": "http://arxiv.org/abs/1004.3849v1",
      "published_time_eastern_timestamp": 1271919785.0
    },
    {
      "title": "Endomorphisms of Linear Block Codes",
      "summary": "The automorphism groups of various linear codes are extensively studied yielding insights into the respective code structure. This knowledge is used in, e.g., theoretical analysis and in improving decoding performance, motivating the analyses of endomorphisms of linear codes. In this work, we discuss the structure of the set of transformation matrices of code endomorphisms, defined as a generalization of code automorphisms, and provide an explicit construction of a bijective mapping between the image of an endomorphism and its canonical quotient space. Furthermore, we introduce a one-to-one mapping between the set of transformation matrices of endomorphisms and a larger linear block code enabling the use of well-known algorithms for the search for suitable endomorphisms. Additionally, we propose an approach to obtain unknown code endomorphisms based on automorphisms of the code. Furthermore, we consider ensemble decoding as a possible use case for endomorphisms by introducing endomorphism ensemble decoding. Interestingly, EED can improve decoding performance when other ensemble decoding schemes are not applicable.",
      "url": "http://arxiv.org/abs/2402.00562v2",
      "published_time_eastern_timestamp": 1706791757.0
    },
    {
      "title": "Code Definition Analysis for Call Graph Generation",
      "summary": "Enterprise level software is implemented using multi-layer architecture. These layers are often implemented using de-coupled solutions with millions of lines of code. Programmers often have to track and debug a function call from user interface layer to the data access layer while troubleshooting an issue. They have to inspect the code based on search results or use design documents to construct the call graph. This process is time consuming and laborious. The development environment tools are insufficient or confined to analyzing only the code in the loaded solution. This paper proposes a method to construct a call graph of the call across several layers of the code residing in different code bases to help programmers better understand the design and architecture of the software. The signatures of class, methods, and properties were evaluated and then matched against the code files. A graph of matching functions was created. The recursive search stopped when there were no matches or the data layer code was detected. The method resulted in 78.26% accuracy when compared with manual search.",
      "url": "http://arxiv.org/abs/1610.04594v1",
      "published_time_eastern_timestamp": 1469588737.0
    },
    {
      "title": "The DTFE public software: The Delaunay Tessellation Field Estimator code",
      "summary": "We present the DTFE public software, a code for reconstructing fields from a discrete set of samples/measurements using the maximum of information contained in the point distribution. The code is written in C++ using the CGAL library and is parallelized using OpenMP. The software was designed for the analysis of cosmological data but can be used in other fields where one must interpolate quantities given at a discrete point set. The software comes with a wide suite of options to facilitate the analysis of 2- and 3-dimensional data and of both numerical simulations and galaxy redshift surveys. For comparison purposes, the code also implements the TSC and SPH grid interpolation methods. The code comes with an extensive user guide detailing the program options, examples and the inner workings of the code. The DTFE public software and further information can be found at http://www.astro.rug.nl/~voronoi/DTFE/dtfe.html .",
      "url": "http://arxiv.org/abs/1105.0370v4",
      "published_time_eastern_timestamp": 1304352015.0
    },
    {
      "title": "LoRACode: LoRA Adapters for Code Embeddings",
      "summary": "Code embeddings are essential for semantic code search; however, current approaches often struggle to capture the precise syntactic and contextual nuances inherent in code. Open-source models such as CodeBERT and UniXcoder exhibit limitations in scalability and efficiency, while high-performing proprietary systems impose substantial computational costs. We introduce a parameter-efficient fine-tuning method based on Low-Rank Adaptation (LoRA) to construct task-specific adapters for code retrieval. Our approach reduces the number of trainable parameters to less than two percent of the base model, enabling rapid fine-tuning on extensive code corpora (2 million samples in 25 minutes on two H100 GPUs). Experiments demonstrate an increase of up to 9.1% in Mean Reciprocal Rank (MRR) for Code2Code search, and up to 86.69% for Text2Code search tasks across multiple programming languages. Distinction in task-wise and language-wise adaptation helps explore the sensitivity of code retrieval for syntactical and linguistic variations. To foster research in this area, we make our code and pre-trained models publicly available.",
      "url": "http://arxiv.org/abs/2503.05315v2",
      "published_time_eastern_timestamp": 1741344645.0
    },
    {
      "title": "Projective error models: Stabilizer codes, Clifford codes, and weak stabilizer codes",
      "summary": "We introduce more general notions of Clifford codes and stabilizer codes, the latter we call weak stabilizer codes. This is all formulated in the language of projective representation theory of finite groups and we give a novel description of the detectable errors for a Clifford code. We give a complete characterization of when a Clifford code is also a weak stabilizer code in the case where the considered error model is a nice error basis. We also give examples of infinite families of non-stabilizer Clifford codes as well as examples of non-Clifford weak stabilizer codes. The latter of these types of examples is a class of codes that have not been studied in the same systematic framework as Clifford codes and stabilizer codes.",
      "url": "http://arxiv.org/abs/2506.01843v2",
      "published_time_eastern_timestamp": 1748881784.0
    },
    {
      "title": "InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback",
      "summary": "Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan & Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages. Project site with code and data: https://intercode-benchmark.github.io",
      "url": "http://arxiv.org/abs/2306.14898v3",
      "published_time_eastern_timestamp": 1687802390.0
    },
    {
      "title": "On Decoding High-Order Interleaved Sum-Rank-Metric Codes",
      "summary": "We consider decoding of vertically homogeneous interleaved sum-rank-metric codes with high interleaving order $s$, that are constructed by stacking $s$ codewords of a single constituent code.\n  We propose a Metzner--Kapturowski-like decoding algorithm that can correct errors of sum-rank weight $t <= d-2$, where $d$ is the minimum distance of the code, if the interleaving order $s > t$ and the error matrix fulfills a certain rank condition.\n  The proposed decoding algorithm generalizes the Metzner--Kapturowski(-like) decoders in the Hamming metric and the rank metric and has a computational complexity of $\\tilde{O}(\\max(n^3, n^2 s))$ operations in $\\mathbb{F}_{q^m}$, where $n$ is the length of the code.\n  The scheme performs linear-algebraic operations only and thus works for any interleaved linear sum-rank-metric code.\n  We show how the decoder can be used to decode high-order interleaved codes in the skew metric.\n  Apart from error control, the proposed decoder allows to determine the security level of code-based cryptosystems based on interleaved sum-rank metric codes.",
      "url": "http://arxiv.org/abs/2303.17454v1",
      "published_time_eastern_timestamp": 1680190098.0
    },
    {
      "title": "Automorphism Ensemble Decoding of Quantum LDPC Codes",
      "summary": "We introduce AutDEC, a fast and accurate decoder for quantum error-correcting codes with large automorphism groups. Our decoder employs a set of automorphisms of the quantum code and an ensemble of belief propagation (BP) decoders. Each BP decoder is given a syndrome which is transformed by one of the automorphisms, and is run in parallel. For quantum codes, the accuracy of BP decoders is limited because short cycles occur in the Tanner graph and our approach mitigates this effect. We demonstrate decoding accuracy comparable to BP-OSD-0 with a lower time overhead for Quantum Reed-Muller (QRM) codes in the code capacity setting, and Bivariate Bicycle (BB) codes under circuit level noise. We provide a Python repository for use by the community and the results of our simulations.",
      "url": "http://arxiv.org/abs/2503.01738v1",
      "published_time_eastern_timestamp": 1741021104.0
    }
  ]
}
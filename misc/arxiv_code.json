{
  "last_updated": "2026-01-14T10:17:38.286946-05:00",
  "papers": [
    {
      "title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System",
      "summary": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.",
      "url": "http://arxiv.org/abs/2601.08829v1",
      "published_time_eastern_timestamp": 1768330757.0
    },
    {
      "title": "Breaking the Orthogonality Barrier in Quantum LDPC Codes",
      "summary": "Classical low-density parity-check (LDPC) codes are a widely deployed and well-established technology, forming the backbone of modern communication and storage systems. It is well known that, in this classical setting, increasing the girth of the Tanner graph while maintaining regular degree distributions leads simultaneously to good belief-propagation (BP) decoding performance and large minimum distance. In the quantum setting, however, this principle does not directly apply because quantum LDPC codes must satisfy additional orthogonality constraints between their parity-check matrices. When one enforces both orthogonality and regularity in a straightforward manner, the girth is typically reduced and the minimum distance becomes structurally upper bounded.\n  In this work, we overcome this limitation by using permutation matrices with controlled commutativity and by restricting the orthogonality constraints to only the necessary parts of the construction, while preserving regular check-matrix structures. This design breaks the conventional trade-off between orthogonality, regularity, girth, and minimum distance, allowing us to construct quantum LDPC codes with large girth and without the usual distance upper bounds. As a concrete demonstration, we construct a girth-8, (3,12)-regular $[[9216,4612, \\leq 48]]$ quantum LDPC code and show that, under BP decoding combined with a low-complexity post-processing algorithm, it achieves a frame error rate as low as $10^{-8}$ on the depolarizing channel with error probability $4 \\%$.",
      "url": "http://arxiv.org/abs/2601.08824v1",
      "published_time_eastern_timestamp": 1768330663.0
    },
    {
      "title": "The drastic impact of Eddington-limit induced mass ejections on massive star populations",
      "summary": "Massive stars are the key engines of the Universe. However, their evolution and thus their ionizing feedback are still not fully understood. One of the largest gaps in current stellar evolution calculations is the lack of a model for the mass ejections that occur when the stars reach the Eddington limit, such as during an Luminous Blue Variable (LBV) phase. We aim to remedy this situation by providing a physically motivated and empirically calibrated method applicable in any 1D stellar evolution code to approximate the effect of such mass loss on stellar evolution. We employ the 1D stellar evolution code MESA, in which we implement a new mass-loss prescription that is acting when stellar models inflate too much when reaching the Eddington limit. Synthetic massive-star stellar populations using calculated grids of single-star models with this mass loss prescription are compared with the observed populations in the Large and Small Magellanic Clouds. In combination with already computed grids of binary evolution models, we investigate the impact of binarity on our predictions. Our single-star models reproduce key features of the observed stellar populations, namely (i) the absence of stars located beyond the Humphreys-Davidson limit, (ii) an upper limit of RSG luminosities, (iii) the faintest observed single WR stars, (iv) the absolute number of O-stars, WRs, and RSGs, (v) WO stars in low metallicity environments, and (vi) the positions of LBV stars in the HRD. Our binary population explains at the same time the 70% binary fraction of O-stars and the 40% binary fraction of WR stars. However, our synthetic population also has caveats, such as an overproduction of bright H-free WN stars. Our results show that the effect of Eddington-limit induced mass ejections on the structure and evolution of massive stars can remove tension between predicted and observed massive star populations.",
      "url": "http://arxiv.org/abs/2601.08822v1",
      "published_time_eastern_timestamp": 1768330622.0
    },
    {
      "title": "Optimal logical Bell measurements on stabilizer codes with linear optics",
      "summary": "Bell measurements (BMs) are ubiquitous in quantum information and technology. They are basic elements for quantum commmunication, computation, and error correction. In particular, when performed on logical qubits encoded in physical photonic qubits, they allow for a read-out of stabilizer syndrome information to enhance loss tolerance in qubit-state transmission and fusion. However, even in an ideal setting without photon loss, BMs cannot be done perfectly based on the simplest experimental toolbox of linear optics. Here we demonstrate that any logical BM on stabilizer codes can always be mapped onto a single physical BM perfomed on any qubit pair from the two codes. As a necessary condition for the success of a logical BM, this provides a general upper bound on its success probability, especially ruling out the possibility that the stabilizer information obtainable from only partially succeeding, physical linear-optics BMs could be combined into the full logical stabilizer information. We formulate sufficient criteria to find schemes for which a single successful BM on the physical level will always allow to obtain the full logical information by suitably adapting the subsequent physical measurements. Our approach based on stabilizer group theory is generally applicable to any stabilizer code, which we demonstrate for quantum parity, five-qubit, standard and rotated planar surface, tree, and seven-qubit Steane codes. Our schemes attain the general upper bound for all these codes, while this bound had previously only been reached for the quantum parity code.",
      "url": "http://arxiv.org/abs/2601.08820v1",
      "published_time_eastern_timestamp": 1768330468.0
    },
    {
      "title": "Reasoning Matters for 3D Visual Grounding",
      "summary": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.",
      "url": "http://arxiv.org/abs/2601.08811v1",
      "published_time_eastern_timestamp": 1768330121.0
    },
    {
      "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
      "summary": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
      "url": "http://arxiv.org/abs/2601.08808v1",
      "published_time_eastern_timestamp": 1768330080.0
    },
    {
      "title": "APEX-SWE",
      "summary": "We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).",
      "url": "http://arxiv.org/abs/2601.08806v1",
      "published_time_eastern_timestamp": 1768329848.0
    },
    {
      "title": "DentalX: Context-Aware Dental Disease Detection with Radiographs",
      "summary": "Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.",
      "url": "http://arxiv.org/abs/2601.08797v1",
      "published_time_eastern_timestamp": 1768329148.0
    },
    {
      "title": "Single-Period Floquet Control of Bosonic Codes with Quantum Lattice Gates",
      "summary": "Bosonic codes constitute a promising route to fault-tolerant quantum computing. {Existing Floquet protocols enable analytical construction of bosonic codes but typically rely on slow adiabatic ramps with thousands of driving periods.} In this work, we circumvent this bottleneck by introducing an analytical and deterministic Floquet method that directly synthesizes arbitrary unitaries within a single period. The phase-space unitary ensembles generated by our approach reproduce the Haar-random statistics, enabling practical pseudorandom unitaries in continuous-variable systems. We prepare various prototypical bosonic codes from vacuum and implement single-qubit logical gates with high fidelities using quantum lattice gates. By harnessing the full intrinsic nonlinearity of Josephson junctions, quantum lattice gates decompose quantum circuits into primitive operations for efficient continuous-variable quantum computing.",
      "url": "http://arxiv.org/abs/2601.08782v1",
      "published_time_eastern_timestamp": 1768328051.0
    },
    {
      "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards",
      "summary": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.\n  In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
      "url": "http://arxiv.org/abs/2601.08778v1",
      "published_time_eastern_timestamp": 1768327746.0
    },
    {
      "title": "Reliable Graph-RAG for Codebases: AST-Derived Graphs vs LLM-Extracted Knowledge Graphs",
      "summary": "Retrieval-Augmented Generation for software engineering often relies on vector similarity search, which captures topical similarity but can fail on multi-hop architectural reasoning such as controller to service to repository chains, interface-driven wiring, and inheritance. This paper benchmarks three retrieval pipelines on Java codebases (Shopizer, with additional runs on ThingsBoard and OpenMRS Core): (A) vector-only No-Graph RAG, (B) an LLM-generated knowledge graph RAG (LLM-KB), and (C) a deterministic AST-derived knowledge graph RAG (DKB) built with Tree-sitter and bidirectional traversal.\n  Using 15 architecture and code-tracing queries per repository, we measure indexing time, query latency, corpus coverage, cost, and answer correctness. DKB builds its graph in seconds, while LLM-KB requires much longer graph generation. LLM-KB also shows indexing incompleteness: on Shopizer, 377 files are skipped or missed, reducing embedded chunk coverage and graph size compared to DKB. End-to-end cost is modest for DKB relative to the vector-only baseline but much higher for LLM-KB, especially as repository scale increases. Query latency is similar for No-Graph and DKB, while LLM-KB is slower and more variable. On the Shopizer question suite, DKB achieves the highest correctness, LLM-KB is close behind, and the vector-only baseline performs worst on upstream architectural queries and has the highest hallucination risk. Overall, deterministic AST-derived graphs provide more reliable coverage and multi-hop grounding than LLM-extracted graphs at substantially lower indexing cost.",
      "url": "http://arxiv.org/abs/2601.08773v1",
      "published_time_eastern_timestamp": 1768327421.0
    },
    {
      "title": "Majority-Logic Decoding of Binary Locally Recoverable Codes: A Probabilistic Analysis",
      "summary": "Locally repairable codes (LRCs) were originally introduced to enable efficient recovery from erasures in distributed storage systems by accessing only a small number of other symbols. While their structural properties-such as bounds and constructions-have been extensively studied, the performance of LRCs under random erasures and errors has remained largely unexplored. In this work, we study the error- and erasure-correction performance of binary linear LRCs under majority-logic decoding (MLD). Focusing on LRCs with fixed locality and varying availability, we derive explicit upper bounds on the probability of decoding failure over the memoryless Binary Erasure Channel (BEC) and Binary Symmetric Channel (BSC). Our analysis characterizes the behavior of the bit-error rate (BER) and block-error rate (BLER) as functions of the locality and availability parameters. We show that, under mild growth conditions on the availability, the block decoding failure probability vanishes asymptotically, and that majority-logic decoding can successfully correct virtually all of error and erasure patterns of weight linear in the blocklength. The results reveal a substantial gap between worst-case guarantees and typical performance under stochastic channel models.",
      "url": "http://arxiv.org/abs/2601.08765v1",
      "published_time_eastern_timestamp": 1768326897.0
    },
    {
      "title": "Search for Cosmic Ray Electron Boosted Dark Matter with the CDEX-10 Experiment",
      "summary": "We present new constraints on the cosmic ray electron boosted light dark matter (CReDM) using the 205.4 kg$\\cdot$day data of the CDEX-10 experiment located at the China Jinping Underground Laboratory. The cosmic ray electron spectrum and distribution in the Galaxy are generated by the $\\tt GALPROP$ code package. In the calculation process of DM-electron scattering process in the Galaxy, we consider the energy-dependency of the DM-electron scattering cross section. The constraints on CReDM are set for both heavy and light mediator scenarios using the CDEX-10 dataset. The result exceeds previous Standard Halo Model (SHM) limits for DM mass lower than 0.6 MeV in heavy mediator case and corresponds to the best sensitivity among all direct detection experiments from 1 keV to 0.5 MeV in the light mediator scenario.",
      "url": "http://arxiv.org/abs/2601.08746v1",
      "published_time_eastern_timestamp": 1768325100.0
    },
    {
      "title": "On the Algebraic Structure Underlying the Support Enumerators of Linear Codes",
      "summary": "In this paper, we have introduced the concepts of support distribution and the support enumerator as refinements of the classical weight distribution and weight enumerator respectively, capturing coordinate level activity in linear block codes. More precisely, we have established formula for counting codewords in the linear code C whose i-th coordinate is nonzero. Moreover, we derived a MacWilliam's type identity, relating the normalized support enumerators of a linear code and its dual, explaining how coordinate information transforms under duality. Using this identity we deduce a condition for self duality based on the equality of support distributions. These results provide a more detailed understanding of code structure and complement classical weight based duality theory.",
      "url": "http://arxiv.org/abs/2601.08744v1",
      "published_time_eastern_timestamp": 1768324889.0
    },
    {
      "title": "TerraFormer: Automated Infrastructure-as-Code with LLMs Fine-Tuned via Policy-Guided Verifier Feedback",
      "summary": "Automating Infrastructure-as-Code (IaC) is challenging, and large language models (LLMs) often produce incorrect configurations from natural language (NL). We present TerraFormer, a neuro-symbolic framework for IaC generation and mutation that combines supervised fine-tuning with verifier-guided reinforcement learning, using formal verification tools to provide feedback on syntax, deployability, and policy compliance. We curate two large, high-quality NL-to-IaC datasets, TF-Gen (152k instances) and TF-Mutn (52k instances), via multi-stage verification and iterative LLM self-correction. Evaluations against 17 state-of-the-art LLMs, including ~50x larger models like Sonnet 3.7, DeepSeek-R1, and GPT-4.1, show that TerraFormer improves correctness over its base LLM by 15.94% on IaC-Eval, 11.65% on TF-Gen (Test), and 19.60% on TF-Mutn (Test). It outperforms larger models on both TF-Gen (Test) and TF-Mutn (Test), ranks third on IaC-Eval, and achieves top best-practices and security compliance.",
      "url": "http://arxiv.org/abs/2601.08734v1",
      "published_time_eastern_timestamp": 1768324110.0
    },
    {
      "title": "ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning",
      "summary": "Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.",
      "url": "http://arxiv.org/abs/2601.08732v1",
      "published_time_eastern_timestamp": 1768323940.0
    },
    {
      "title": "Soft Partition-based KAPI-ELM for Multi-Scale PDEs",
      "summary": "Physics-informed machine learning holds great promise for solving differential equations, yet existing methods struggle with highly oscillatory, multiscale, or singularly perturbed PDEs due to spectral bias, costly backpropagation, and manually tuned kernel or Fourier frequencies. This work introduces a soft partition--based Kernel-Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), a deterministic low-dimensional parameterization in which smooth partition lengths jointly control collocation centers and Gaussian kernel widths, enabling continuous coarse-to-fine resolution without Fourier features, random sampling, or hard domain interfaces. A signed-distance-based weighting further stabilizes least-squares learning on irregular geometries. Across eight benchmarks--including oscillatory ODEs, high-frequency Poisson equations, irregular-shaped domains, and stiff singularly perturbed convection-diffusion problems-the proposed method matches or exceeds the accuracy of state-of-the-art Physics-Informed Neural Network (PINN) and Theory of Functional Connections (TFC) variants while using only a single linear solve. Although demonstrated on steady linear PDEs, the results show that soft-partition kernel adaptation provides a fast, architecture-free approach for multiscale PDEs with broad potential for future physics-informed modeling. For reproducibility, the reference codes are available at https://github.com/vikas-dwivedi-2022/soft_kapi",
      "url": "http://arxiv.org/abs/2601.08719v1",
      "published_time_eastern_timestamp": 1768322618.0
    },
    {
      "title": "Real-Time Localization Framework for Autonomous Basketball Robots",
      "summary": "Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.",
      "url": "http://arxiv.org/abs/2601.08713v1",
      "published_time_eastern_timestamp": 1768322418.0
    },
    {
      "title": "Multivariate Polynomial Codes for Efficient Matrix Chain Multiplication in Distributed Systems",
      "summary": "We study the problem of computing matrix chain multiplications in a distributed computing cluster. In such systems, performance is often limited by the straggler problem, where the slowest worker dominates the overall computation latency. To resolve this issue, several coded computing strategies have been proposed, primarily focusing on the simplest case: the multiplication of two matrices. These approaches successfully alleviate the straggler effect, but they do so at the expense of higher computational complexity and increased storage needs at the workers. However, in many real-world applications, computations naturally involve long chains of matrix multiplications rather than just a single two-matrix product. Extending univariate polynomial coding to this setting has been shown to amplify the costs -- both computation and storage overheads grow significantly, limiting scalability. In this work, we propose two novel multivariate polynomial coding schemes specifically designed for matrix chain multiplication in distributed environments. Our results show that while multivariate codes introduce additional computational cost at the workers, they can dramatically reduce storage overhead compared to univariate extensions. This reveals a fundamental trade-off between computation and storage efficiency, and highlights the potential of multivariate codes as a practical solution for large-scale distributed linear algebra tasks.",
      "url": "http://arxiv.org/abs/2601.08708v1",
      "published_time_eastern_timestamp": 1768322186.0
    },
    {
      "title": "RMBRec: Robust Multi-Behavior Recommendation towards Target Behaviors",
      "summary": "Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance. While existing methods attempt to fuse these heterogeneous signals, they inherently lack a principled mechanism to ensure robustness against such behavioral inconsistency.\n  In this work, we propose Robust Multi-Behavior Recommendation towards Target Behaviors (RMBRec), a robust multi-behavior recommendation framework grounded in an information-theoretic robustness principle. We interpret robustness as a joint process of maximizing predictive information while minimizing its variance across heterogeneous behavioral environments. Under this perspective, the Representation Robustness Module (RRM) enhances local semantic consistency by maximizing the mutual information between users' auxiliary and target representations, whereas the Optimization Robustness Module (ORM) enforces global stability by minimizing the variance of predictive risks across behaviors, which is an efficient approximation to invariant risk minimization. This local-global collaboration bridges representation purification and optimization invariance in a theoretically coherent way. Extensive experiments on three real-world datasets demonstrate that RMBRec not only outperforms state-of-the-art methods in accuracy but also maintains remarkable stability under various noise perturbations. For reproducibility, our code is available at https://github.com/miaomiao-cai2/RMBRec/.",
      "url": "http://arxiv.org/abs/2601.08705v1",
      "published_time_eastern_timestamp": 1768322057.0
    }
  ]
}
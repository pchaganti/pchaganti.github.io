{
  "last_updated": "2025-09-27T08:20:01.224493-04:00",
  "papers": [
    {
      "title": "SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines",
      "summary": "We present a scientific reasoning foundation model that aligns natural\nlanguage with heterogeneous scientific representations. The model is pretrained\non a 206B-token corpus spanning scientific text, pure sequences, and\nsequence-text pairs, then aligned via SFT on 40M instructions, annealed\ncold-start bootstrapping to elicit long-form chain-of-thought, and\nreinforcement learning with task-specific reward shaping, which instills\ndeliberate scientific reasoning. It supports four capability families, covering\nup to 103 tasks across workflows: (i) faithful translation between text and\nscientific formats, (ii) text/knowledge extraction, (iii) property prediction,\n(iv) property classification, (v) unconditional and conditional sequence\ngeneration and design. Compared with specialist systems, our approach broadens\ninstruction coverage, improves cross-domain generalization, and enhances\nfidelity. We detail data curation and training and show that cross-discipline\nlearning strengthens transfer and downstream reliability. The model, instruct\ntuning datasets and the evaluation code are open-sourced at\nhttps://huggingface.co/SciReason and\nhttps://github.com/open-sciencelab/SciReason.",
      "url": "http://arxiv.org/abs/2509.21320v1",
      "published_time_eastern_timestamp": 1758822726.0
    },
    {
      "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards",
      "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
      "url": "http://arxiv.org/abs/2509.21319v1",
      "published_time_eastern_timestamp": 1758817146.0
    },
    {
      "title": "NewtonGen: Physics-Consistent and Controllable Text-to-Video Generation\n  via Neural Newtonian Dynamics",
      "summary": "A primary bottleneck in large-scale text-to-video generation today is\nphysical consistency and controllability. Despite recent advances,\nstate-of-the-art models often produce unrealistic motions, such as objects\nfalling upward, or abrupt changes in velocity and direction. Moreover, these\nmodels lack precise parameter control, struggling to generate physically\nconsistent dynamics under different initial conditions. We argue that this\nfundamental limitation stems from current models learning motion distributions\nsolely from appearance, while lacking an understanding of the underlying\ndynamics. In this work, we propose NewtonGen, a framework that integrates\ndata-driven synthesis with learnable physical principles. At its core lies\ntrainable Neural Newtonian Dynamics (NND), which can model and predict a\nvariety of Newtonian motions, thereby injecting latent dynamical constraints\ninto the video generation process. By jointly leveraging data priors and\ndynamical guidance, NewtonGen enables physically consistent video synthesis\nwith precise parameter control.",
      "url": "http://arxiv.org/abs/2509.21309v1",
      "published_time_eastern_timestamp": 1758813933.0
    },
    {
      "title": "Quantized Visual Geometry Grounded Transformer",
      "summary": "Learning-based 3D reconstruction models, represented by Visual Geometry\nGrounded Transformers (VGGTs), have made remarkable progress with the use of\nlarge-scale transformers. Their prohibitive computational and memory costs\nseverely hinder real-world deployment. Post-Training Quantization (PTQ) has\nbecome a common practice for compressing and accelerating models. However, we\nempirically observe that PTQ faces unique obstacles when compressing\nbillion-scale VGGTs: the data-independent special tokens induce heavy-tailed\nactivation distributions, while the multi-view nature of 3D data makes\ncalibration sample selection highly unstable. This paper proposes the first\nQuantization framework for VGGTs, namely QuantVGGT. This mainly relies on two\ntechnical contributions: First, we introduce Dual-Smoothed Fine-Grained\nQuantization, which integrates pre-global Hadamard rotation and post-local\nchannel smoothing to mitigate heavy-tailed distributions and inter-channel\nvariance robustly. Second, we design Noise-Filtered Diverse Sampling, which\nfilters outliers via deep-layer statistics and constructs frame-aware diverse\ncalibration clusters to ensure stable quantization ranges. Comprehensive\nexperiments demonstrate that QuantVGGT achieves the state-of-the-art results\nacross different benchmarks and bit-width, surpassing the previous\nstate-of-the-art generic quantization method with a great margin. We highlight\nthat our 4-bit QuantVGGT can deliver a 3.7$\\times$ memory reduction and\n2.5$\\times$ acceleration in real-hardware inference, while maintaining\nreconstruction accuracy above 98\\% of its full-precision counterpart. This\ndemonstrates the vast advantages and practicality of QuantVGGT in\nresource-constrained scenarios. Our code is released in\nhttps://github.com/wlfeng0509/QuantVGGT.",
      "url": "http://arxiv.org/abs/2509.21302v1",
      "published_time_eastern_timestamp": 1758813431.0
    },
    {
      "title": "Fundamental Limits of Noncoherent Massive Random Access Networks",
      "summary": "This paper studies the capacity of massive random-access cellular networks,\nmodeled as a MIMO fading channel with an infinite number of interfering cells.\nTo characterize the symmetric sum rate of the network, a random-coding argument\nis invoked together with the assumption that in all cells users draw their\ncodebooks according to the same distribution. This can be viewed as a\ngeneralization of the assumption of Gaussian codebooks, often encountered in\nthe literature. The network is further assumed to be noncoherent: the\ntransmitters and receivers are cognizant of the statistics of the fading\ncoefficients, but are ignorant of their realizations. Finally, it is assumed\nthat the users access the network at random. For the considered channel model,\nrigorous bounds on the capacity are derived. The behavior of these bounds\ndepends critically on the path loss from signals transmitted in interfering\ncells to the intended cell. In particular, if the fading coefficients of the\ninterferers (ordered according to their distance to the receiver) decay\nexponentially or more slowly, then the capacity is bounded in the transmit\npower. This confirms that the saturation regime in interference-limited\nnetworks -- observed by Lozano, Heath, and Andrews (\"Fundamental limits of\ncooperation\", IEEE Trans. Inf. Theory, Sept. 2013) -- cannot be avoided by\nrandom user activity or by using channel inputs beyond the scale family. In\ncontrast, if the fading coefficients decay faster than double-exponentially,\nthen the capacity is unbounded in the transmit power. Proving an unbounded\ncapacity is nontrivial even if the number of interfering cells is finite, since\nthe condition that the users' codebooks follow the same distribution prevents\ninterference-avoiding strategies such as time- or frequency-division multiple\naccess. We obtain this result by using bursty signaling together with treating\ninterference as noise.",
      "url": "http://arxiv.org/abs/2509.21300v1",
      "published_time_eastern_timestamp": 1758813376.0
    },
    {
      "title": "Does FLUX Already Know How to Perform Physically Plausible Image\n  Composition?",
      "summary": "Image composition aims to seamlessly insert a user-specified object into a\nnew scene, but existing models struggle with complex lighting (e.g., accurate\nshadows, water reflections) and diverse, high-resolution inputs. Modern\ntext-to-image diffusion models (e.g., SD3.5, FLUX) already encode essential\nphysical and resolution priors, yet lack a framework to unleash them without\nresorting to latent inversion, which often locks object poses into contextually\ninappropriate orientations, or brittle attention surgery. We propose SHINE, a\ntraining-free framework for Seamless, High-fidelity Insertion with Neutralized\nErrors. SHINE introduces manifold-steered anchor loss, leveraging pretrained\ncustomization adapters (e.g., IP-Adapter) to guide latents for faithful subject\nrepresentation while preserving background integrity. Degradation-suppression\nguidance and adaptive background blending are proposed to further eliminate\nlow-quality outputs and visible seams. To address the lack of rigorous\nbenchmarks, we introduce ComplexCompo, featuring diverse resolutions and\nchallenging conditions such as low lighting, strong illumination, intricate\nshadows, and reflective surfaces. Experiments on ComplexCompo and\nDreamEditBench show state-of-the-art performance on standard metrics (e.g.,\nDINOv2) and human-aligned scores (e.g., DreamSim, ImageReward, VisionReward).\nCode and benchmark will be publicly available upon publication.",
      "url": "http://arxiv.org/abs/2509.21278v1",
      "published_time_eastern_timestamp": 1758812509.0
    },
    {
      "title": "MMR1: Enhancing Multimodal Reasoning with Variance-Aware Sampling and\n  Open Resources",
      "summary": "Large multimodal reasoning models have achieved rapid progress, but their\nadvancement is constrained by two major limitations: the absence of open,\nlarge-scale, high-quality long chain-of-thought (CoT) data, and the instability\nof reinforcement learning (RL) algorithms in post-training. Group Relative\nPolicy Optimization (GRPO), the standard framework for RL fine-tuning, is prone\nto gradient vanishing when reward variance is low, which weakens optimization\nsignals and impairs convergence. This work makes three contributions: (1) We\npropose Variance-Aware Sampling (VAS), a data selection strategy guided by\nVariance Promotion Score (VPS) that combines outcome variance and trajectory\ndiversity to promote reward variance and stabilize policy optimization. (2) We\nrelease large-scale, carefully curated resources containing ~1.6M long CoT\ncold-start data and ~15k RL QA pairs, designed to ensure quality, difficulty,\nand diversity, along with a fully reproducible end-to-end training codebase.\n(3) We open-source a family of multimodal reasoning models in multiple scales,\nestablishing standardized baselines for the community. Experiments across\nmathematical reasoning benchmarks demonstrate the effectiveness of both the\ncurated data and the proposed VAS. Comprehensive ablation studies and analyses\nprovide further insight into the contributions of each component. In addition,\nwe theoretically establish that reward variance lower-bounds the expected\npolicy gradient magnitude, with VAS serving as a practical mechanism to realize\nthis guarantee. Our code, data, and checkpoints are available at\nhttps://github.com/LengSicong/MMR1.",
      "url": "http://arxiv.org/abs/2509.21268v1",
      "published_time_eastern_timestamp": 1758812309.0
    },
    {
      "title": "Grounding AI Explanations in Experience: A Reflective Cognitive\n  Architecture for Clinical Decision Support",
      "summary": "Effective disease prediction in modern healthcare demands the twin goals of\nhigh accuracy and transparent, clinically meaningful explanations. Existing\nmachine learning and large language model (LLM) based approaches often struggle\nto balance these goals. Many models yield accurate but unclear statistical\noutputs, while others generate fluent but statistically unsupported narratives,\noften undermining both the validity of the explanation and the predictive\naccuracy itself. This shortcoming comes from a shallow interaction with the\ndata, preventing the development of a deep, detailed understanding similar to a\nhuman expert's. We argue that high accuracy and high-quality explanations are\nnot separate objectives but are mutually reinforcing outcomes of a model that\ndevelops a deep, direct understanding of the data. To achieve this, we propose\nthe Reflective Cognitive Architecture (RCA), a novel framework that coordinates\nmultiple LLMs to learn from direct experience. RCA features an iterative rule\nrefinement mechanism that improves its logic from prediction errors and a\ndistribution-aware rules check mechanism that bases its reasoning in the\ndataset's global statistics. By using predictive accuracy as a signal to drive\ndeeper comprehension, RCA builds a strong internal model of the data. We\nevaluated RCA on one private and two public datasets against 22 baselines. The\nresults demonstrate that RCA not only achieves state-of-the-art accuracy and\nrobustness with a relative improvement of up to 40\\% over the baseline but,\nmore importantly, leverages this deep understanding to excel in generating\nexplanations that are clear, logical, evidence-based, and balanced,\nhighlighting its potential for creating genuinely trustworthy clinical decision\nsupport systems. The code is available at \\https://github.com/ssssszj/RCA.",
      "url": "http://arxiv.org/abs/2509.21266v1",
      "published_time_eastern_timestamp": 1758812272.0
    },
    {
      "title": "MedVSR: Medical Video Super-Resolution with Cross State-Space\n  Propagation",
      "summary": "High-resolution (HR) medical videos are vital for accurate diagnosis, yet are\nhard to acquire due to hardware limitations and physiological constraints.\nClinically, the collected low-resolution (LR) medical videos present unique\nchallenges for video super-resolution (VSR) models, including camera shake,\nnoise, and abrupt frame transitions, which result in significant optical flow\nerrors and alignment difficulties. Additionally, tissues and organs exhibit\ncontinuous and nuanced structures, but current VSR models are prone to\nintroducing artifacts and distorted features that can mislead doctors. To this\nend, we propose MedVSR, a tailored framework for medical VSR. It first employs\nCross State-Space Propagation (CSSP) to address the imprecise alignment by\nprojecting distant frames as control matrices within state-space models,\nenabling the selective propagation of consistent and informative features to\nneighboring frames for effective alignment. Moreover, we design an Inner\nState-Space Reconstruction (ISSR) module that enhances tissue structures and\nreduces artifacts with joint long-range spatial feature learning and\nlarge-kernel short-range information aggregation. Experiments across four\ndatasets in diverse medical scenarios, including endoscopy and cataract\nsurgeries, show that MedVSR significantly outperforms existing VSR models in\nreconstruction performance and efficiency. Code released at\nhttps://github.com/CUHK-AIM-Group/MedVSR.",
      "url": "http://arxiv.org/abs/2509.21265v1",
      "published_time_eastern_timestamp": 1758812219.0
    },
    {
      "title": "Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication",
      "summary": "Homonyms are words with identical spelling but distinct meanings, which pose\nchallenges for many generative models. When a homonym appears in a prompt,\ndiffusion models may generate multiple senses of the word simultaneously, which\nis known as homonym duplication. This issue is further complicated by an\nAnglocentric bias, which includes an additional translation step before the\ntext-to-image model pipeline. As a result, even words that are not homonymous\nin the original language may become homonyms and lose their meaning after\ntranslation into English. In this paper, we introduce a method for measuring\nduplication rates and conduct evaluations of different diffusion models using\nboth automatic evaluation utilizing Vision-Language Models (VLM) and human\nevaluation. Additionally, we investigate methods to mitigate the homonym\nduplication problem through prompt expansion, demonstrating that this approach\nalso effectively reduces duplication related to Anglocentric bias. The code for\nthe automatic evaluation pipeline is publicly available.",
      "url": "http://arxiv.org/abs/2509.21262v1",
      "published_time_eastern_timestamp": 1758812076.0
    },
    {
      "title": "A Latent Variable Framework for Multiple Imputation with Non-ignorable\n  Missingness: Analyzing Perceptions of Social Justice in Europe",
      "summary": "This paper proposes a general multiple imputation approach for analyzing\nlarge-scale data with missing values. An imputation model is derived from a\njoint distribution induced by a latent variable model, which can flexibly\ncapture associations among variables of mixed types. The model also allows for\nmissingness which depends on the latent variables and is thus non-ignorable\nwith respect to the observed data. We develop a frequentist multiple imputation\nmethod for this framework and provide asymptotic theory that establishes valid\ninference for a broad class of analysis models. Simulation studies confirm the\nmethod's theoretical properties and robust practical performance. The procedure\nis applied to a cross-national analysis of individuals' perceptions of justice\nand fairness of income distributions in their societies, using data from the\nEuropean Social Survey which has substantial nonresponse. The analysis\ndemonstrates that failing to account for non-ignorable missingness can yield\nbiased conclusions; for instance, complete-case analysis is shown to exaggerate\nthe correlation between personal income and perceived fairness of income\ndistributions in society. Code implementing the proposed methodology is\npublicly available at\nhttps://anonymous.4open.science/r/non-ignorable-missing-data-imputation-E885.",
      "url": "http://arxiv.org/abs/2509.21225v1",
      "published_time_eastern_timestamp": 1758810597.0
    },
    {
      "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for\n  Scientific Reasoning",
      "summary": "Large language models (LLMs) have recently shown strong progress on\nscientific reasoning, yet two major bottlenecks remain. First, explicit\nretrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and\nsteps. Second, multi-agent pipelines often dilute strong solutions by averaging\nacross all candidates. We address these challenges with a unified framework\nthat combines implicit retrieval and structured collaboration. At its\nfoundation, a Monitor-based retrieval module operates at the token level,\nintegrating external knowledge with minimal disruption to reasoning. On top of\nthis substrate, Hierarchical Solution Refinement (HSR) iteratively designates\neach candidate as an anchor to be repaired by its peers, while Quality-Aware\nIterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's\nLast Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the\nhighest reported to date, surpassing the strongest agent baseline by 13.4\npoints and leading frontier LLMs by up to 18.1 points, while simultaneously\nreducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA\nand TRQA confirm robustness across domains. Error analysis shows that reasoning\nfailures and knowledge gaps co-occur in over 85\\% of cases, while diversity\nanalysis reveals a clear dichotomy: retrieval tasks benefit from solution\nvariety, whereas reasoning tasks favor consensus. Together, these findings\ndemonstrate how implicit augmentation and structured refinement overcome the\ninefficiencies of explicit tool use and uniform aggregation. Code is available\nat: https://github.com/tangxiangru/Eigen-1.",
      "url": "http://arxiv.org/abs/2509.21193v1",
      "published_time_eastern_timestamp": 1758809155.0
    },
    {
      "title": "Toric surface codes and the periodicity of polytopes",
      "summary": "Toric codes are error-correcting codes that are derived from toric varieties,\nwhich hold a unique correspondence to integral convex polytopes. In this paper,\nwe focus on integral convex polytopes $P \\subseteq \\mathbb{R}^2$ and the toric\ncodes they define. We begin by studying period-1 polytopes -- polytopes\nsatisfying the property $L(tP)$ = $tL(P)$ for all $t \\in \\mathbb{Z}^+$, where\n$tP$ is the $t$-dilate of $P$, and we prove an explicit formula for the minimum\ndistance of toric codes associated to a particular class of period-1 polytopes.\nWe also apply the methods of Little and Schwarz, using Vandermonde matrices, to\ncompute the minimum distance of another class of period-1 polytopes.",
      "url": "http://arxiv.org/abs/2509.21178v1",
      "published_time_eastern_timestamp": 1758808711.0
    },
    {
      "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A\n  Maximum Entropy Regulated Long Chain-of-Thought Approach",
      "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.",
      "url": "http://arxiv.org/abs/2509.21170v1",
      "published_time_eastern_timestamp": 1758808316.0
    },
    {
      "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just\n  What They Say",
      "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain\n(e.g., math, code, general reasoning), motivating systems that leverage\ncomplementary strengths across models. Prior multi-LLM approaches either (i)\nroute a query to one or a few experts and generate independently, (ii)\naggregate outputs from each model via costly multi-turn exchanges, or (iii)\nfuse weights into a single model-typically requiring architectural homogeneity.\nWe introduce Mixture of Thoughts (MoT), a simple method for latent-level\ncollaboration among heterogeneous experts under a global routing scheme. For\neach query, a lightweight router selects top-$K$ experts and designates a\nprimary expert; uniformly placed interaction layers project hidden states into\na shared latent space where the primary expert performs cross-attention over\nits active (selected) peers. Pre-trained experts remain frozen; only the router\nand the lightweight interaction layers are trained with a novel joint training\nobjective that improves both the expert selection and inter-expert\ncollaboration. Across five in-distribution (ID) and three out-of-distribution\n(OOD) benchmarks, MoT surpasses the current routing and aggregation-based\nstate-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further,\nMoT significantly outperforms the best-performing single model. It achieves\nthis with single-pass inference, runtime comparable to routing baselines, and\nnone of the overheads of iterative aggregation. MoT offers a simple\nlatent-space mechanism for combining heterogeneous LLMs, a practical step\ntoward broader multi-LLM collaboration. Our code is publicly available at\nhttps://github.com/jacobfa/mot.",
      "url": "http://arxiv.org/abs/2509.21164v1",
      "published_time_eastern_timestamp": 1758808209.0
    },
    {
      "title": "Sparse Representations Improve Adversarial Robustness of Neural Network\n  Classifiers",
      "summary": "Deep neural networks perform remarkably well on image classification tasks\nbut remain vulnerable to carefully crafted adversarial perturbations. This work\nrevisits linear dimensionality reduction as a simple, data-adapted defense. We\nempirically compare standard Principal Component Analysis (PCA) with its sparse\nvariant (SPCA) as front-end feature extractors for downstream classifiers, and\nwe complement these experiments with a theoretical analysis. On the theory\nside, we derive exact robustness certificates for linear heads applied to SPCA\nfeatures: for both $\\ell_\\infty$ and $\\ell_2$ threat models (binary and\nmulticlass), the certified radius grows as the dual norms of $W^\\top u$ shrink,\nwhere $W$ is the projection and $u$ the head weights. We further show that for\ngeneral (non-linear) heads, sparsity reduces operator-norm bounds through a\nLipschitz composition argument, predicting lower input sensitivity.\nEmpirically, with a small non-linear network after the projection, SPCA\nconsistently degrades more gracefully than PCA under strong white-box and\nblack-box attacks while maintaining competitive clean accuracy. Taken together,\nthe theory identifies the mechanism (sparser projections reduce adversarial\nleverage) and the experiments verify that this benefit persists beyond the\nlinear setting. Our code is available at\nhttps://github.com/killian31/SPCARobustness.",
      "url": "http://arxiv.org/abs/2509.21130v1",
      "published_time_eastern_timestamp": 1758806482.0
    },
    {
      "title": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them",
      "summary": "The adoption of Large Language Models (LLMs) as automated evaluators\n(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation\nframeworks. We identify two fundamental types of inconsistencies: (1)\nScore-Comparison Inconsistency, where lower-rated responses outperform\nhigher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity\nInconsistency, manifested through circular preference chains (A>B>C>A) and\nequivalence contradictions (A=B=C\\neq A). We argue that these issues come from\ninformation loss in discrete rating systems and ambiguous tie judgments during\npairwise evaluation. We propose TrustJudge, a probabilistic framework that\naddresses these limitations through two key innovations: 1)\ndistribution-sensitive scoring that computes continuous expectations from\ndiscrete rating probabilities, preserving information entropy for more precise\nscoring, and 2) likelihood-aware aggregation that resolves transitivity\nviolations using bidirectional preference probabilities or perplexity. We also\nformalize the theoretical limitations of current LLM-as-a-judge frameworks and\ndemonstrate how TrustJudge's components overcome them. When evaluated with\nLlama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces\nScore-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise\nTransitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining\nhigher evaluation accuracy. Our work provides the first systematic analysis of\nevaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both\ntheoretical insights and practical solutions for reliable automated assessment.\nThe framework demonstrates consistent improvements across various model\narchitectures and scales, enabling more trustworthy LLM evaluation without\nrequiring additional training or human annotations. The codes can be found at\nhttps://github.com/TrustJudge/TrustJudge.",
      "url": "http://arxiv.org/abs/2509.21117v1",
      "published_time_eastern_timestamp": 1758805469.0
    },
    {
      "title": "Path-Controlled Secure Network Coding",
      "summary": "Multicast for securely sharing confidential data among many users is becoming\nincreasingly important. Currently, it relies on duplicate-and-forward routing\nand cryptographic methods based on computational security. However, these\napproaches neither attain multicast capacity of the network, nor ensure\nlong-term security against advances in computing (information-theoretic\nsecurity: ITS). Existing ITS solutions--quantum key distribution (QKD),\nphysical layer security (PLS), and secure network coding (SNC)--still fail to\nenable scalable networks, as their underlying assumptions, such as trusted\nnodes and wiretap thresholds, gradually become invalid as the network grows.\nHere, we develop an efficient multi-tree multicast path-finding method to\naddress this issue, integrating it with universal strongly ramp SNC. This\nsystem, path-controlled universal strongly ramp SNC (PUSNEC), can be overlaid\nonto QKD/PLS networks, enabling multicast capacity, ITS, and scalability. We\nderive the maximum leakage information to an eavesdropper under the\nprobabilistic wiretap network assumption and demonstrate secure multicast in\nmulti-hop networks through numerical simulations. Our quantitative analysis of\nthe secrecyreliability tradeoff highlights a practical approach to achieving\nsecure, reliable multicast on a global scale.",
      "url": "http://arxiv.org/abs/2509.21115v1",
      "published_time_eastern_timestamp": 1758805244.0
    },
    {
      "title": "Adapt or Regress: Rate-Memory-Compatible Spatially-Coupled Codes",
      "summary": "Spatially-coupled (SC) codes are a class of low-density parity-check (LDPC)\ncodes that have excellent performance thanks to the degrees of freedom they\noffer. An SC code is designed by partitioning a base matrix into components,\nthe number of which implies the code memory, then coupling and lifting them. In\nthe same system, various error-correction coding schemes are typically needed.\nFor example, in wireless communication standards, several channel conditions\nand data rates should be supported. In storage and computing systems, stronger\ncodes should be adopted as the device ages. Adaptive code design enables\nswitching from one code to another when needed, ensuring reliability while\nreducing hardware cost. In this paper, we introduce a class of reconfigurable\nSC codes named rate-memory-compatible SC (RMC-SC) codes, which we design\nprobabilistically. In particular, rate compatibility in RMC-SC codes is\nachieved via increasing the SC code memory, which also makes the codes\nmemory-compatible and improves performance. We express the expected number of\nshort cycles in the SC code protograph as a function of the fixed probability\ndistribution characterizing the already-designed SC code as well as the unknown\ndistribution characterizing the additional components. We use the\ngradient-descent algorithm to find a locally-optimal distribution, in terms of\ncycle count, for the new components. The method can be recursively used to\ndesign any number of SC codes needed, and we show how to extend it to other\ncases. Next, we perform the finite-length optimization using a Markov chain\nMonte Carlo (MC$^2$) approach that we update to design the proposed RMC-SC\ncodes. Experimental results demonstrate significant reductions in cycle counts\nand remarkable performance gains achieved by RMC-SC codes compared with a\nliterature-based straightforward scheme.",
      "url": "http://arxiv.org/abs/2509.21112v1",
      "published_time_eastern_timestamp": 1758805074.0
    },
    {
      "title": "BESPOKE: Benchmark for Search-Augmented Large Language Model\n  Personalization via Diagnostic Feedback",
      "summary": "Search-augmented large language models (LLMs) have advanced\ninformation-seeking tasks by integrating retrieval into generation, reducing\nusers' cognitive burden compared to traditional search systems. Yet they remain\ninsufficient for fully addressing diverse user needs, which requires\nrecognizing how the same query can reflect different intents across users and\ndelivering information in preferred forms. While recent systems such as ChatGPT\nand Gemini attempt personalization by leveraging user histories, systematic\nevaluation of such personalization is under-explored. To address this gap, we\npropose BESPOKE, the realistic benchmark for evaluating personalization in\nsearch-augmented LLMs. BESPOKE is designed to be both realistic, by collecting\nauthentic chat and search histories directly from humans, and diagnostic, by\npairing responses with fine-grained preference scores and feedback. The\nbenchmark is constructed through long-term, deeply engaged human annotation,\nwhere human annotators contributed their own histories, authored queries with\ndetailed information needs, and evaluated responses with scores and diagnostic\nfeedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key\nrequirements for effective personalization in information-seeking tasks,\nproviding a foundation for fine-grained evaluation of personalized\nsearch-augmented LLMs. Our code and data are available at\nhttps://augustinlib.github.io/BESPOKE/.",
      "url": "http://arxiv.org/abs/2509.21106v1",
      "published_time_eastern_timestamp": 1758804787.0
    }
  ]
}
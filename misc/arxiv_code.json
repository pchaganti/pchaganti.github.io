{
  "last_updated": "2026-02-17T16:26:47.996140-05:00",
  "papers": [
    {
      "title": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization",
      "summary": "Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench",
      "url": "http://arxiv.org/abs/2602.15028v1",
      "published_time_eastern_timestamp": 1771268382.0
    },
    {
      "title": "Scaling Beyond Masked Diffusion Language Models",
      "summary": "Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. In this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. We also show that Masked diffusion models can be made approximately 12% more FLOPs-efficient when trained with a simple cross-entropy objective. We find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality Pareto frontier. These results challenge the view that Masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. Scaling all methods to 1.7B parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and Masked diffusion models on GSM8K, despite worse validation perplexity. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms",
      "url": "http://arxiv.org/abs/2602.15014v1",
      "published_time_eastern_timestamp": 1771268087.0
    },
    {
      "title": "DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI",
      "summary": "Moving beyond the traditional paradigm of adapting internet-pretrained models to physical tasks, we present DM0, an Embodied-Native Vision-Language-Action (VLA) framework designed for Physical AI. Unlike approaches that treat physical grounding as a fine-tuning afterthought, DM0 unifies embodied manipulation and navigation by learning from heterogeneous data sources from the onset. Our methodology follows a comprehensive three-stage pipeline: Pretraining, Mid-Training, and Post-Training. First, we conduct large-scale unified pretraining on the Vision-Language Model (VLM) using diverse corpora--seamlessly integrating web text, autonomous driving scenarios, and embodied interaction logs-to jointly acquire semantic knowledge and physical priors. Subsequently, we build a flow-matching action expert atop the VLM. To reconcile high-level reasoning with low-level control, DM0 employs a hybrid training strategy: for embodied data, gradients from the action expert are not backpropagated to the VLM to preserve generalized representations, while the VLM remains trainable on non-embodied data. Furthermore, we introduce an Embodied Spatial Scaffolding strategy to construct spatial Chain-of-Thought (CoT) reasoning, effectively constraining the action solution space. Experiments on the RoboChallenge benchmark demonstrate that DM0 achieves state-of-the-art performance in both Specialist and Generalist settings on Table30.",
      "url": "http://arxiv.org/abs/2602.14974v1",
      "published_time_eastern_timestamp": 1771264756.0
    },
    {
      "title": "Practical and improved density functionals for computational catalysis on metal surfaces",
      "summary": "Density functional theory (DFT) has been critical towards our current atomistic understanding of catalysis on transition-metal surfaces. It has opened new paradigms in rational catalyst design, predicting fundamental properties, like the adsorption energy and reaction barriers, linked to catalytic performance. However, such applications depend sensitively on the predictive accuracy of DFT and achieving experimental-level reliability, so-called transition-metal chemical accuracy (13 kJ/mol), remains challenging for present density functional approximations (DFAs) or even methods beyond DFT. We introduce a new framework for designing DFAs tailored towards studying molecules adsorbed on transition-metal surfaces, building upon recent developments in non-self-consistent DFAs. We propose two functionals within this framework, demonstrating that transition-metal chemical accuracy can be achieved across a diverse set of 39 adsorption reactions while delivering consistent performance for 17 barrier heights. Moreover, we show that these non-self-consistent DFAs address qualitative failures that challenge current self-consistent DFAs, such as CO adsorption on Pt(111) and graphene on Ni(111). The resulting functionals are computationally practical and compatible with existing DFT codes, with scripts and workflows provided to use them. Looking ahead, this framework establishes a path toward developing more accurate and sophisticated DFAs for computational heterogeneous catalysis and beyond.",
      "url": "http://arxiv.org/abs/2602.14962v1",
      "published_time_eastern_timestamp": 1771263541.0
    },
    {
      "title": "Locally Adaptive Multi-Objective Learning",
      "summary": "We consider the general problem of learning a predictor that satisfies multiple objectives of interest simultaneously, a broad framework that captures a range of specific learning goals including calibration, regret, and multiaccuracy. We work in an online setting where the data distribution can change arbitrarily over time. Existing approaches to this problem aim to minimize the set of objectives over the entire time horizon in a worst-case sense, and in practice they do not necessarily adapt to distribution shifts. Earlier work has aimed to alleviate this problem by incorporating additional objectives that target local guarantees over contiguous subintervals. Empirical evaluation of these proposals is, however, scarce. In this article, we consider an alternative procedure that achieves local adaptivity by replacing one part of the multi-objective learning method with an adaptive online algorithm. Empirical evaluations on datasets from energy forecasting and algorithmic fairness show that our proposed method improves upon existing approaches and achieves unbiased predictions over subgroups, while remaining robust under distribution shift.",
      "url": "http://arxiv.org/abs/2602.14952v1",
      "published_time_eastern_timestamp": 1771263108.0
    },
    {
      "title": "Fluidic Shaping over arbitrary domains: theory and high order finite-elements solver",
      "summary": "Fluidic Shaping is a novel method for fabrication of optical components based on the equilibrium state of liquid volumes in neutral buoyancy, subjected to geometrical constraints. The underlying physics of this method is described by a highly nonlinear partial differential equation with Dirichlet boundary conditions and an integral constraint. To date, useful solutions for such optical liquid surfaces could be obtained analytically only for the linearized equations and only on circular or elliptical domains. A numerical solution for the non-linear equation was suggested, but only for the axi-symmetric case. Such solutions are, however, insufficient as they do not capture the full range of optical surfaces. Arbitrary domains offer an important degree of freedom for creating complex optical surfaces, and the nonlinear terms are essential for high quality solutions. Moreover, in the context of optics, it is not sufficient to resolve the shape of the surface, and it is essential to obtain accurate solutions for its curvature, which governs its optical properties. We here present the theoretical foundation for the Fluidic Shaping method over arbitrary domains, and the development of a high order (quintic) finite element numerical solver, capable of accurately resolving the topography and curvature of liquid interfaces on arbitrary domains. The code is based on reduced quintic finite elements, which we have modified to capture curved boundaries. We compare the results against low order finite elements and non-deformed high order elements, demonstrating the importance of high order approximations of both the solution and the domain. We also show the usability of the code for the prediction of optical surfaces derived from complex boundary conditions.",
      "url": "http://arxiv.org/abs/2602.14856v1",
      "published_time_eastern_timestamp": 1771257089.0
    },
    {
      "title": "BEACONS: Bounded-Error, Algebraically-Composable Neural Solvers for Partial Differential Equations",
      "summary": "The traditional limitations of neural networks in reliably generalizing beyond the convex hulls of their training data present a significant problem for computational physics, in which one often wishes to solve PDEs in regimes far beyond anything which can be experimentally or analytically validated. In this paper, we show how it is possible to circumvent these limitations by constructing formally-verified neural network solvers for PDEs, with rigorous convergence, stability, and conservation properties, whose correctness can therefore be guaranteed even in extrapolatory regimes. By using the method of characteristics to predict the analytical properties of PDE solutions a priori (even in regions arbitrarily far from the training domain), we show how it is possible to construct rigorous extrapolatory bounds on the worst-case L^inf errors of shallow neural network approximations. Then, by decomposing PDE solutions into compositions of simpler functions, we show how it is possible to compose these shallow neural networks together to form deep architectures, based on ideas from compositional deep learning, in which the large L^inf errors in the approximations have been suppressed. The resulting framework, called BEACONS (Bounded-Error, Algebraically-COmposable Neural Solvers), comprises both an automatic code-generator for the neural solvers themselves, as well as a bespoke automated theorem-proving system for producing machine-checkable certificates of correctness. We apply the framework to a variety of linear and non-linear PDEs, including the linear advection and inviscid Burgers' equations, as well as the full compressible Euler equations, in both 1D and 2D, and illustrate how BEACONS architectures are able to extrapolate solutions far beyond the training data in a reliable and bounded way. Various advantages of the approach over the classical PINN approach are discussed.",
      "url": "http://arxiv.org/abs/2602.14853v1",
      "published_time_eastern_timestamp": 1771256959.0
    },
    {
      "title": "Statistical Validation and Vetting of Exoplanet Candidate TOI 7475.01",
      "summary": "We present a comprehensive validation analysis of the exoplanet candidate TOI 7475.01 (TIC 376866659), detected by the TESS mission. Using a custom pipeline combining natural flux preservation with robust BLS detection, we identified a transit signal with a period of $3.2538$ days and a depth of $\\sim 4600$ ppm. To rule out false positives, we performed centroid analysis, spatial contamination checks using Gaia DR3, and a statistical validation using triceratops. Our results show a Signal-to-Noise Ratio (SNR) of 294.13 and a False Positive Probability (FPP) of $\\approx 0$. Based on the clean spatial environment, stable centroids, and high statistical probability, we validate TOI 7475.01 as a planetary companion.",
      "url": "http://arxiv.org/abs/2602.14840v1",
      "published_time_eastern_timestamp": 1771255987.0
    },
    {
      "title": "Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation",
      "summary": "Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.",
      "url": "http://arxiv.org/abs/2602.14837v1",
      "published_time_eastern_timestamp": 1771255744.0
    },
    {
      "title": "Constructions of linear codes from vectorial plateaued functions and their subfield codes with applications to quantum CSS codes",
      "summary": "Linear codes over finite fields parameterized by functions have proven to be a powerful tool in coding theory, yielding optimal and few-weight codes with significant applications in secret sharing, authentication codes, and association schemes. In 2023, Xu et al. introduced a construction framework for 3-dimensional linear codes parameterized by two functions, which has demonstrated considerable success in generating infinite families of optimal linear codes. Motivated by this approach, we propose a construction that extends the framework to three functions, thereby enhancing the flexibility of the parameters. Additionally, we introduce a vectorial setting by allowing vector-valued functions, expanding the construction space and the set of achievable structural properties. We analyze both scalar and vectorial frameworks, employing Bent and s-Plateaued functions, including Almost Bent, to define the code generators. By exploiting the properties of the Walsh transform, we determine the explicit parameters and weight distributions of these codes and their punctured versions. A key result of this study is that the constructed codes have few weights, and their duals are distance and dimensionally optimal with respect to both the Sphere Packing and Griesmer bounds. Furthermore, we establish a theoretical connection between our vectorial approach and the classical first generic construction of linear codes, providing sufficient conditions for the resulting codes to be minimal and self-orthogonal. Finally, we investigate applications to quantum coding theory within the Calderbank-Shor-Steane framework.",
      "url": "http://arxiv.org/abs/2602.14832v1",
      "published_time_eastern_timestamp": 1771255402.0
    },
    {
      "title": "Learning State-Tracking from Code Using Linear RNNs",
      "summary": "Over the last years, state-tracking tasks, particularly permutation composition, have become a testbed to understand the limits of sequence models architectures like Transformers and RNNs (linear and non-linear). However, these are often sequence-to-sequence tasks: learning to map actions (permutations) to states, which is incompatible with the next-token prediction setting commonly used to train language models. We address this gap by converting permutation composition into code via REPL traces that interleave state-reveals through prints and variable transformations. We show that linear RNNs capable of state-tracking excel also in this setting, while Transformers still fail. Motivated by this representation, we investigate why tracking states in code is generally difficult: actions are not always fully observable. We frame this as tracking the state of a probabilistic finite-state automaton with deterministic state reveals and show that linear RNNs can be worse than non-linear RNNs at tracking states in this setup.",
      "url": "http://arxiv.org/abs/2602.14814v1",
      "published_time_eastern_timestamp": 1771254471.0
    },
    {
      "title": "Intent-Driven Dynamic Chunking: Segmenting Documents to Reflect Predicted Information Needs",
      "summary": "Breaking long documents into smaller segments is a fundamental challenge in information retrieval. Whether for search engines, question-answering systems, or retrieval-augmented generation (RAG), effective segmentation determines how well systems can locate and return relevant information. However, traditional methods, such as fixed-length or coherence-based segmentation, ignore user intent, leading to chunks that split answers or contain irrelevant noise. We introduce Intent-Driven Dynamic Chunking (IDC), a novel approach that uses predicted user queries to guide document segmentation. IDC leverages a Large Language Model to generate likely user intents for a document and then employs a dynamic programming algorithm to find the globally optimal chunk boundaries. This represents a novel application of DP to intent-aware segmentation that avoids greedy pitfalls. We evaluated IDC on six diverse question-answering datasets, including news articles, Wikipedia, academic papers, and technical documentation. IDC outperformed traditional chunking strategies on five datasets, improving top-1 retrieval accuracy by 5% to 67%, and matched the best baseline on the sixth. Additionally, IDC produced 40-60% fewer chunks than baseline methods while achieving 93-100% answer coverage. These results demonstrate that aligning document structure with anticipated information needs significantly boosts retrieval performance, particularly for long and heterogeneous documents.",
      "url": "http://arxiv.org/abs/2602.14784v1",
      "published_time_eastern_timestamp": 1771252338.0
    },
    {
      "title": "ROSA: Roundabout Optimized Speed Advisory with Multi-Agent Trajectory Prediction in Multimodal Traffic",
      "summary": "We present ROSA -- Roundabout Optimized Speed Advisory -- a system that combines multi-agent trajectory prediction with coordinated speed guidance for multimodal, mixed traffic at roundabouts. Using a Transformer-based model, ROSA jointly predicts the future trajectories of vehicles and Vulnerable Road Users (VRUs) at roundabouts. Trained for single-step prediction and deployed autoregressively, it generates deterministic outputs, enabling actionable speed advisories. Incorporating motion dynamics, the model achieves high accuracy (ADE: 1.29m, FDE: 2.99m at a five-second prediction horizon), surpassing prior work. Adding route intention further improves performance (ADE: 1.10m, FDE: 2.36m), demonstrating the value of connected vehicle data. Based on predicted conflicts with VRUs and circulating vehicles, ROSA provides real-time, proactive speed advisories for approaching and entering the roundabout. Despite prediction uncertainty, ROSA significantly improves vehicle efficiency and safety, with positive effects even on perceived safety from a VRU perspective. The source code of this work is available under: github.com/urbanAIthi/ROSA.",
      "url": "http://arxiv.org/abs/2602.14780v1",
      "published_time_eastern_timestamp": 1771252201.0
    },
    {
      "title": "Unlocking Reasoning Capability on Machine Translation in Large Language Models",
      "summary": "Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.",
      "url": "http://arxiv.org/abs/2602.14763v1",
      "published_time_eastern_timestamp": 1771250759.0
    },
    {
      "title": "Rethinking the Role of LLMs in Time Series Forecasting",
      "summary": "Large language models (LLMs) have been introduced to time series forecasting (TSF) to incorporate contextual knowledge beyond numerical signals. However, existing studies question whether LLMs provide genuine benefits, often reporting comparable performance without LLMs. We show that such conclusions stem from limited evaluation settings and do not hold at scale. We conduct a large-scale study of LLM-based TSF (LLM4TSF) across 8 billion observations, 17 forecasting scenarios, 4 horizons, multiple alignment strategies, and both in-domain and out-of-domain settings. Our results demonstrate that \\emph{LLM4TS indeed improves forecasting performance}, with especially large gains in cross-domain generalization. Pre-alignment outperforming post-alignment in over 90\\% of tasks. Both pretrained knowledge and model architecture of LLMs contribute and play complementary roles: pretraining is critical under distribution shifts, while architecture excels at modeling complex temporal dynamics. Moreover, under large-scale mixed distributions, a fully intact LLM becomes indispensable, as confirmed by token-level routing analysis and prompt-based improvements. Overall, Our findings overturn prior negative assessments, establish clear conditions under which LLMs are not only useful, and provide practical guidance for effective model design. We release our code at https://github.com/EIT-NLP/LLM4TSF.",
      "url": "http://arxiv.org/abs/2602.14744v1",
      "published_time_eastern_timestamp": 1771249149.0
    },
    {
      "title": "Faster Optimal Decoder for Graph Codes with a Single Logical Qubit",
      "summary": "In this work, we develop an efficient decoding method for graph codes, a class of stabilizer quantum error-correcting codes constructed from graph states. While optimal decoding is generally NP-hard, we propose a faster decoder exploiting the structural properties of the underlying graph states. Although distinct error patterns may yield the same syndrome, we demonstrate that the post-measurement state follows a well-defined structure determined by the projective syndrome measurement. Building on this idea, we introduce a hierarchical decoder in which each level can be solved in polynomial time. Additionally, this decoder achieves optimal decoding performance at the lower levels of the hierarchy. This strategy avoids the need for full maximum-likelihood decoding of graph codes. Numerical results illustrate the efficiency and effectiveness of the proposed approach.",
      "url": "http://arxiv.org/abs/2602.14730v1",
      "published_time_eastern_timestamp": 1771248139.0
    },
    {
      "title": "WebWorld: A Large-Scale World Model for Web Agent Training",
      "summary": "Web agents require massive trajectories to generalize, yet real-world training is constrained by network latency, rate limits, and safety risks. We introduce \\textbf{WebWorld} series, the first open-web simulator trained at scale. While existing simulators are restricted to closed environments with thousands of trajectories, WebWorld leverages a scalable data pipeline to train on 1M+ open-web interactions, supporting reasoning, multi-format data, and long-horizon simulations of 30+ steps. For intrinsic evaluation, we introduce WebWorld-Bench with dual metrics spanning nine dimensions, where WebWorld achieves simulation performance comparable to Gemini-3-Pro. For extrinsic evaluation, Qwen3-14B trained on WebWorld-synthesized trajectories improves by +9.2\\% on WebArena, reaching performance comparable to GPT-4o. WebWorld enables effective inference-time search, outperforming GPT-5 as a world model. Beyond web simulation, WebWorld exhibits cross-domain generalization to code, GUI, and game environments, providing a replicable recipe for world model construction.",
      "url": "http://arxiv.org/abs/2602.14721v1",
      "published_time_eastern_timestamp": 1771247209.0
    },
    {
      "title": "Orcheo: A Modular Full-Stack Platform for Conversational Search",
      "summary": "Conversational search (CS) requires a complex software engineering pipeline that integrates query reformulation, ranking, and response generation. CS researchers currently face two barriers: the lack of a unified framework for efficiently sharing contributions with the community, and the difficulty of deploying end-to-end prototypes needed for user evaluation. We introduce Orcheo, an open-source platform designed to bridge this gap. Orcheo offers three key advantages: (i) A modular architecture promotes component reuse through single-file node modules, facilitating sharing and reproducibility in CS research; (ii) Production-ready infrastructure bridges the prototype-to-system gap via dual execution modes, secure credential management, and execution telemetry, with built-in AI coding support that lowers the learning curve; (iii) Starter-kit assets include 50+ off-the-shelf components for query understanding, ranking, and response generation, enabling the rapid bootstrapping of complete CS pipelines. We describe the framework architecture and validate Orcheo's utility through case studies that highlight modularity and ease of use. Orcheo is released as open source under the MIT License at https://github.com/ShaojieJiang/orcheo.",
      "url": "http://arxiv.org/abs/2602.14710v1",
      "published_time_eastern_timestamp": 1771246617.0
    },
    {
      "title": "Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs",
      "summary": "Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL",
      "url": "http://arxiv.org/abs/2602.14697v1",
      "published_time_eastern_timestamp": 1771245267.0
    },
    {
      "title": "A Critical Look at Targeted Instruction Selection: Disentangling What Matters (and What Doesn't)",
      "summary": "Instruction fine-tuning of large language models (LLMs) often involves selecting a subset of instruction training data from a large candidate pool, using a small query set from the target task. Despite growing interest, the literature on targeted instruction selection remains fragmented and opaque: methods vary widely in selection budgets, often omit zero-shot baselines, and frequently entangle the contributions of key components. As a result, practitioners lack actionable guidance on selecting instructions for their target tasks. In this work, we aim to bring clarity to this landscape by disentangling and systematically analyzing the two core ingredients: data representation and selection algorithms. Our framework enables controlled comparisons across models, tasks, and budgets. We find that only gradient-based data representations choose subsets whose similarity to the query consistently predicts performance across datasets and models. While no single method dominates, gradient-based representations paired with a greedy round-robin selection algorithm tend to perform best on average at low budgets, but these benefits diminish at larger budgets. Finally, we unify several existing selection algorithms as forms of approximate distance minimization between the selected subset and the query set, and support this view with new generalization bounds. More broadly, our findings provide critical insights and a foundation for more principled data selection in LLM fine-tuning. The code is available at https://github.com/dcml-lab/targeted-instruction-selection.",
      "url": "http://arxiv.org/abs/2602.14696v1",
      "published_time_eastern_timestamp": 1771245185.0
    }
  ]
}
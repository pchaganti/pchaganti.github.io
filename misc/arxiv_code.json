{
  "last_updated": "2025-06-24T14:16:49.269417-04:00",
  "papers": [
    {
      "title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos",
      "summary": "Editing illumination in long videos with complex dynamics has significant\nvalue in various downstream tasks, including visual content creation and\nmanipulation, as well as data scaling up for embodied AI through sim2real and\nreal2real transfer. Nevertheless, existing video relighting techniques are\npredominantly limited to portrait videos or fall into the bottleneck of\ntemporal consistency and computation efficiency. In this paper, we propose\nTC-Light, a novel paradigm characterized by the proposed two-stage post\noptimization mechanism. Starting from the video preliminarily relighted by an\ninflated video relighting model, it optimizes appearance embedding in the first\nstage to align global illumination. Then it optimizes the proposed canonical\nvideo representation, i.e., Unique Video Tensor (UVT), to align fine-grained\ntexture and lighting in the second stage. To comprehensively evaluate\nperformance, we also establish a long and highly dynamic video benchmark.\nExtensive experiments show that our method enables physically plausible\nrelighting results with superior temporal coherence and low computation cost.\nThe code and video demos are available at\nhttps://dekuliutesla.github.io/tclight/.",
      "url": "http://arxiv.org/abs/2506.18904v1",
      "published_time_eastern_timestamp": 1750701598.0
    },
    {
      "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual\n  Retrieval",
      "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-based information retrieval, cross-modal semantic similarity,\nand programming code search. Comprehensive evaluations demonstrate that\njina-embeddings-v4 achieves state-of-the-art performance on both single- modal\nand cross-modal retrieval tasks, with particular strength in processing\nvisually rich content such as tables, charts, diagrams, and mixed-media\nformats. To facilitate evaluation of this capability, we also introduce\nJina-VDR, a novel benchmark specifically designed for visually rich image\nretrieval.",
      "url": "http://arxiv.org/abs/2506.18902v1",
      "published_time_eastern_timestamp": 1750701595.0
    },
    {
      "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
      "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
      "url": "http://arxiv.org/abs/2506.18898v1",
      "published_time_eastern_timestamp": 1750701554.0
    },
    {
      "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
      "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
      "url": "http://arxiv.org/abs/2506.18896v1",
      "published_time_eastern_timestamp": 1750701542.0
    },
    {
      "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation",
      "summary": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems.",
      "url": "http://arxiv.org/abs/2506.18887v1",
      "published_time_eastern_timestamp": 1750701394.0
    },
    {
      "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
      "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
      "url": "http://arxiv.org/abs/2506.18879v1",
      "published_time_eastern_timestamp": 1750701011.0
    },
    {
      "title": "Oblivious Deletion Codes",
      "summary": "We construct deletion error-correcting codes in the oblivious model, where\nerrors are adversarial but oblivious to the encoder's randomness. Oblivious\nerrors bridge the gap between the adversarial and random error models, and are\nmotivated by applications like DNA storage, where the noise is caused by\nhard-to-model physical phenomena, but not by an adversary.\n  (1) (Explicit oblivious) We construct $t$ oblivious deletion codes, with\nredundancy $\\sim 2t\\log n$, matching the existential bound for adversarial\ndeletions.\n  (2) (List decoding implies explicit oblivious) We show that explicit\nlist-decodable codes yield explicit oblivious deletion codes with essentially\nthe same parameters. By a work of Guruswami and H\\r{a}stad (IEEE TIT, 2021),\nthis gives 2 oblivious deletion codes with redundancy $\\sim 3\\log n$, beating\nthe existential redundancy for 2 adversarial deletions.\n  (3) (Randomized oblivious) We give a randomized construction of oblivious\ncodes that, with probability at least $1-2^{-n}$, produces a code correcting\n$t$ oblivious deletions with redundancy $\\sim(t+1)\\log n$, beating the\nexistential adversarial redundancy of $\\sim 2t\\log n$.\n  (4) (Randomized adversarial) Studying the oblivious model can inform better\nconstructions of adversarial codes. The same technique produces, with\nprobability at least $1-2^{-n}$, a code correcting $t$ adversarial deletions\nwith redundancy $\\sim (2t+1)\\log n$, nearly matching the existential redundancy\nof $\\sim 2t\\log n$.\n  The common idea behind these results is to reduce the hash size by modding by\na prime chosen (randomly) from a small subset, and including a small encoding\nof the prime in the hash.",
      "url": "http://arxiv.org/abs/2506.18878v1",
      "published_time_eastern_timestamp": 1750700996.0
    },
    {
      "title": "Counting elliptic curves over $\\mathbb{Q}$ with bounded naive height",
      "summary": "In this paper, we give exact and asymptotic formulas for counting elliptic\ncurves $ E_{A,B} \\colon y^2 = x^3 + Ax + B $ with $ A, B \\in \\mathbb{Z} $,\nordered by naive height. We study the family of all such curves and also\nseveral natural subfamilies, including those with fixed $ j $-invariant and\nthose with complex multiplication (CM). In particular, we provide formulas for\ntwo commonly used normalizations of the naive height appearing in the\nliterature: the calibrated naive height, defined by \\[\nH^{\\mathrm{cal}}(E_{A,B}) := \\max\\{ 4|A|^3, 27B^2 \\}, \\] and the uncalibrated\nnaive height, defined by \\[ H^{\\mathrm{ncal}}(E_{A,B}) := \\max\\{ |A|^3, B^2 \\}.\n\\] In fact, we prove our theorems with respect to the more general naive height\n$H_{\\alpha, \\beta}(E_{A,B}) := \\max\\{ \\alpha |A|^3, \\beta B^2 \\}$, defined for\narbitrary positive real numbers $\\alpha, \\beta \\in \\mathbb{R}_{> 0}$.\n  As part of our approach, we give a completely explicit parametrization of the\nset of curves $ E_{A,B} $ with fixed $ j $-invariant and bounded naive height,\ndescribing them as twists of the curve $ E_{A_j, B_j} $ of minimal naive height\nfor the given $ j $-invariant. We also include tables comparing and verifying\nour theoretical predictions with exact counts obtained via exhaustive computer\nsearches, and we compute data for CM elliptic curves of naive height up to $\n10^{30} $. Code in SageMath is provided to compute all exact and asymptotic\nformulas appearing in the paper.",
      "url": "http://arxiv.org/abs/2506.18874v1",
      "published_time_eastern_timestamp": 1750700618.0
    },
    {
      "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
      "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
      "url": "http://arxiv.org/abs/2506.18871v1",
      "published_time_eastern_timestamp": 1750700334.0
    },
    {
      "title": "Low-Noise Operation of Stepped Frequency-Comb Sources Based on\n  Phase-Code Mode-Locking",
      "summary": "The phase-code mode-locked (PCML) laser provides a configurable frequency\ncomb light source for circular-ranging optical coherence tomography systems\n(CR-OCT). However, prior implementations of PCML suffered from high relative\nintensity noise (RIN). In this work, we demonstrate that by configuring the\noutput pulsewidth and pulse separation in relation to the linewidth of the\nintracavity Fabry-Perot etalon, low-noise operation can be achieved. We\nobserved a more than ten-fold reduction in laser RIN that enabled CR-OCT\nimaging with a sensitivity of 103 dB with 35 mW delivered to the sample and\nprovided images that are qualitatively comparable to those acquired using\ntraditional swept-source methods.",
      "url": "http://arxiv.org/abs/2506.18868v1",
      "published_time_eastern_timestamp": 1750700309.0
    },
    {
      "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints\n  during Generation",
      "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.",
      "url": "http://arxiv.org/abs/2506.18810v1",
      "published_time_eastern_timestamp": 1750695644.0
    },
    {
      "title": "A Practical Introduction to Regression-based Causal Inference in\n  Meteorology (I): All confounders measured",
      "summary": "Whether a variable is the cause of another, or simply associated with it, is\noften an important scientific question. Causal Inference is the name associated\nwith the body of techniques for addressing that question in a statistical\nsetting. Although assessing causality is relatively straightforward in the\npresence of temporal information, outside of that setting - the situation\nconsidered here - it is more difficult to assess causal effects. The\ndevelopment of the field of causal inference has involved concepts from a wide\nrange of topics, thereby limiting its adoption across some fields, including\nmeteorology. However, at its core, the requisite knowledge for causal inference\ninvolves little more than basic probability theory and regression, topics\nfamiliar to most meteorologists. By focusing on these core areas, this and a\ncompanion article provide a steppingstone for the meteorology community into\nthe field of (non-temporal) causal inference. Although some theoretical\nfoundations are presented, the main goal is the application of a specific\nmethod, called matching, to a problem in meteorology. The data for the\napplication are in public domain, and R code is provided as well, forming an\neasy path for meteorology students and researchers to enter the field.",
      "url": "http://arxiv.org/abs/2506.18808v1",
      "published_time_eastern_timestamp": 1750695542.0
    },
    {
      "title": "Context-Aware CodeLLM Eviction for AI-assisted Coding",
      "summary": "AI-assisted coding tools powered by Code Large Language Models (CodeLLMs) are\nincreasingly integrated into modern software development workflows. To address\nconcerns around privacy, latency, and model customization, many enterprises opt\nto self-host these models. However, the diversity and growing number of\nCodeLLMs, coupled with limited accelerator memory, introduce practical\nchallenges in model management and serving efficiency. This paper presents\nCACE, a novel context-aware model eviction strategy designed specifically to\noptimize self-hosted CodeLLM serving under resource constraints. Unlike\ntraditional eviction strategies based solely on recency (e.g., Least Recently\nUsed), CACE leverages multiple context-aware factors, including model load\ntime, task-specific latency sensitivity, expected output length, and recent\nusage and future demand tracked through a sliding window. We evaluate CACE\nusing realistic workloads that include both latency-sensitive code completion\nand throughput-intensive code reasoning tasks. Our experiments show that CACE\nreduces Time-to-First-Token (TTFT) and end-to-end (E2E) latency, while\nsignificantly lowering the number of model evictions compared to\nstate-of-the-art systems. Ablation studies further demonstrate the importance\nof multi-factor eviction in balancing responsiveness and resource efficiency.\nThis work contributes practical strategies for deploying scalable, low-latency\nAI coding assistants in real-world software engineering environments.",
      "url": "http://arxiv.org/abs/2506.18796v1",
      "published_time_eastern_timestamp": 1750694612.0
    },
    {
      "title": "Focus Your Attention: Towards Data-Intuitive Lightweight Vision\n  Transformers",
      "summary": "The evolution of Vision Transformers has led to their widespread adaptation\nto different domains. Despite large-scale success, there remain significant\nchallenges including their reliance on extensive computational and memory\nresources for pre-training on huge datasets as well as difficulties in\ntask-specific transfer learning. These limitations coupled with energy\ninefficiencies mainly arise due to the computation-intensive self-attention\nmechanism. To address these issues, we propose a novel Super-Pixel Based Patch\nPooling (SPPP) technique that generates context-aware, semantically rich, patch\nembeddings to effectively reduce the architectural complexity and improve\nefficiency. Additionally, we introduce the Light Latent Attention (LLA) module\nin our pipeline by integrating latent tokens into the attention mechanism\nallowing cross-attention operations to significantly reduce the time and space\ncomplexity of the attention module. By leveraging the data-intuitive patch\nembeddings coupled with dynamic positional encodings, our approach adaptively\nmodulates the cross-attention process to focus on informative regions while\nmaintaining the global semantic structure. This targeted attention improves\ntraining efficiency and accelerates convergence. Notably, the SPPP module is\nlightweight and can be easily integrated into existing transformer\narchitectures. Extensive experiments demonstrate that our proposed architecture\nprovides significant improvements in terms of computational efficiency while\nachieving comparable results with the state-of-the-art approaches, highlighting\nits potential for energy-efficient transformers suitable for edge deployment.\n(The code is available on our GitHub repository:\nhttps://github.com/zser092/Focused-Attention-ViT).",
      "url": "http://arxiv.org/abs/2506.18791v1",
      "published_time_eastern_timestamp": 1750694457.0
    },
    {
      "title": "Graph theoretic properties of Speyer's matroid polynomial $g_M(t)$",
      "summary": "We prove relations between the number of $k$-connected components of a graph,\nCrapo's invariant $\\beta(M)$ of a matroid, and Speyer's polynomial $g_M(t)$.\nThese yield a simple interpretation of $g_M'(-1)$ when $M$ is graphic or\ncographic. Furthermore, we improve Ferroni's algorithm to compute $g_M(t)$ and\nprovide an implementation and an extensive data set. These calculations reveal\na large number of graph theoretic constraints on the second derivative\n$g_M''(-1)$, which we thus advertise as an intriguing new invariant of graphs.\nWe also propose a relation between the flow polynomial and $g_M''(0)$ for cubic\ngraphs.",
      "url": "http://arxiv.org/abs/2506.18788v1",
      "published_time_eastern_timestamp": 1750694292.0
    },
    {
      "title": "Flow-Aware Diffusion for Real-Time VR Restoration: Enhancing\n  Spatiotemporal Coherence and Efficiency",
      "summary": "Cybersickness remains a critical barrier to the widespread adoption of\nVirtual Reality (VR), particularly in scenarios involving intense or artificial\nmotion cues. Among the key contributors is excessive optical flow-perceived\nvisual motion that, when unmatched by vestibular input, leads to sensory\nconflict and discomfort. While previous efforts have explored geometric or\nhardware based mitigation strategies, such methods often rely on predefined\nscene structures, manual tuning, or intrusive equipment. In this work, we\npropose U-MAD, a lightweight, real-time, AI-based solution that suppresses\nperceptually disruptive optical flow directly at the image level. Unlike prior\nhandcrafted approaches, this method learns to attenuate high-intensity motion\npatterns from rendered frames without requiring mesh-level editing or scene\nspecific adaptation. Designed as a plug and play module, U-MAD integrates\nseamlessly into existing VR pipelines and generalizes well to procedurally\ngenerated environments. The experiments show that U-MAD consistently reduces\naverage optical flow and enhances temporal stability across diverse scenes. A\nuser study further confirms that reducing visual motion leads to improved\nperceptual comfort and alleviated cybersickness symptoms. These findings\ndemonstrate that perceptually guided modulation of optical flow provides an\neffective and scalable approach to creating more user-friendly immersive\nexperiences. The code will be released at https://github.com/XXXXX (upon\npublication).",
      "url": "http://arxiv.org/abs/2506.18786v1",
      "published_time_eastern_timestamp": 1750694193.0
    },
    {
      "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks",
      "summary": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.",
      "url": "http://arxiv.org/abs/2506.18781v1",
      "published_time_eastern_timestamp": 1750693821.0
    },
    {
      "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions\n  During Code Training",
      "summary": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles.",
      "url": "http://arxiv.org/abs/2506.18777v1",
      "published_time_eastern_timestamp": 1750693544.0
    },
    {
      "title": "MuseControlLite: Multifunctional Music Generation with Lightweight\n  Conditioners",
      "summary": "We propose MuseControlLite, a lightweight mechanism designed to fine-tune\ntext-to-music generation models for precise conditioning using various\ntime-varying musical attributes and reference audio signals. The key finding is\nthat positional embeddings, which have been seldom used by text-to-music\ngeneration models in the conditioner for text conditions, are critical when the\ncondition of interest is a function of time. Using melody control as an\nexample, our experiments show that simply adding rotary positional embeddings\nto the decoupled cross-attention layers increases control accuracy from 56.6%\nto 61.1%, while requiring 6.75 times fewer trainable parameters than\nstate-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion\nTransformer model of Stable Audio Open. We evaluate various forms of musical\nattribute control, audio inpainting, and audio outpainting, demonstrating\nimproved controllability over MusicGen-Large and Stable Audio Open ControlNet\nat a significantly lower fine-tuning cost, with only 85M trainble parameters.\nSource code, model checkpoints, and demo examples are available at: https:\n//MuseControlLite.github.io/web/.",
      "url": "http://arxiv.org/abs/2506.18729v1",
      "published_time_eastern_timestamp": 1750691283.0
    },
    {
      "title": "Multi-modal Anchor Gated Transformer with Knowledge Distillation for\n  Emotion Recognition in Conversation",
      "summary": "Emotion Recognition in Conversation (ERC) aims to detect the emotions of\nindividual utterances within a conversation. Generating efficient and\nmodality-specific representations for each utterance remains a significant\nchallenge. Previous studies have proposed various models to integrate features\nextracted using different modality-specific encoders. However, they neglect the\nvarying contributions of modalities to this task and introduce high complexity\nby aligning modalities at the frame level. To address these challenges, we\npropose the Multi-modal Anchor Gated Transformer with Knowledge Distillation\n(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance\ntextual modality representations, while knowledge distillation is utilized to\nstrengthen representations of weaker modalities. Furthermore, we introduce a\nmulti-modal anchor gated transformer to effectively integrate utterance-level\nrepresentations across modalities. Extensive experiments on the IEMOCAP and\nMELD datasets demonstrate the effectiveness of knowledge distillation in\nenhancing modality representations and achieve state-of-the-art performance in\nemotion recognition. Our code is available at:\nhttps://github.com/JieLi-dd/MAGTKD.",
      "url": "http://arxiv.org/abs/2506.18716v1",
      "published_time_eastern_timestamp": 1750690402.0
    }
  ]
}
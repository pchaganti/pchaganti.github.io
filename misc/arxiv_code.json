{
  "last_updated": "2025-09-14T11:09:33.896830-04:00",
  "papers": [
    {
      "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning\n  Dataset and Comprehensive Benchmark",
      "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .",
      "url": "http://arxiv.org/abs/2509.09680v1",
      "published_time_eastern_timestamp": 1757613599.0
    },
    {
      "title": "Cosmology inference with perturbative forward modeling at the field\n  level: a comparison with joint power spectrum and bispectrum analyses",
      "summary": "We extend field-level inference to jointly constrain the cosmological\nparameters $\\{A,\\omega_{\\rm cdm},H_0\\}$, in both real and redshift space. Our\nanalyses are based on mock data generated using a perturbative forward model,\nwith noise drawn from a Gaussian distribution with a constant power spectrum.\nThis idealized setting, where the field-level likelihood is exactly Gaussian,\nallows us to precisely quantify the information content in the nonlinear field\non large scales. We find that field-level inference accurately recovers all\ncosmological parameters in both real and redshift space, with uncertainties\nconsistent with perturbation theory expectations. We show that these error bars\nare comparable to those obtained from a joint power spectrum and bispectrum\nanalysis using the same perturbative model. Finally, we perform several tests\nusing the Gaussian field-level likelihood to fit the mock data where the true\nnoise model is non-Gaussian, and find significant biases in the inferred\ncosmological parameters. These results highlight that the success of\nfield-level inference critically depends on using the correct likelihood, which\nmay be the primary challenge for applying this method to smaller scales even in\nthe perturbative regime.",
      "url": "http://arxiv.org/abs/2509.09673v1",
      "published_time_eastern_timestamp": 1757613552.0
    },
    {
      "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
      "summary": "Hallucinations in multimodal large language models (MLLMs) -- where the model\ngenerates content inconsistent with the input image -- pose significant risks\nin real-world applications, from misinformation in visual question answering to\nunsafe errors in decision-making. Existing benchmarks primarily test\nrecognition accuracy, i.e., evaluating whether models can select the correct\nanswer among distractors. This overlooks an equally critical capability for\ntrustworthy AI: recognizing when none of the provided options are correct, a\nbehavior reflecting epistemic humility. We present HumbleBench, a new\nhallucination benchmark designed to evaluate MLLMs' ability to reject plausible\nbut incorrect answers across three hallucination types: object, relation, and\nattribute. Built from a panoptic scene graph dataset, we leverage fine-grained\nscene graph annotations to extract ground-truth entities and relations, and\nprompt GPT-4-Turbo to generate multiple-choice questions, followed by a\nrigorous manual filtering process. Each question includes a \"None of the above\"\noption, requiring models not only to recognize correct visual information but\nalso to identify when no provided answer is valid. We evaluate a variety of\nstate-of-the-art MLLMs -- including both general-purpose and specialized\nreasoning models -- on HumbleBench and share valuable findings and insights\nwith the community. By incorporating explicit false-option rejection,\nHumbleBench fills a key gap in current evaluation suites, providing a more\nrealistic measure of MLLM reliability in safety-critical settings. Our code and\ndataset are released publicly and can be accessed at\nhttps://github.com/maifoundations/HumbleBench.",
      "url": "http://arxiv.org/abs/2509.09658v1",
      "published_time_eastern_timestamp": 1757613240.0
    },
    {
      "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio\n  Regulations",
      "summary": "We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG.",
      "url": "http://arxiv.org/abs/2509.09651v1",
      "published_time_eastern_timestamp": 1757612622.0
    },
    {
      "title": "Orthogonal Latin Squares of Order Ten with Two Relations: A SAT\n  Investigation",
      "summary": "A $k$-net($n$) is a combinatorial design equivalent to $k-2$ mutually\northogonal Latin squares of order $n$. A relation in a net is a linear\ndependency over $\\mathbb{F}_2$ in the incidence matrix of the net. A\ncomputational enumeration of all orthogonal pairs of Latin squares of order 10\nwhose corresponding nets have at least two nontrivial relations was achieved by\nDelisle in 2010 and verified by an independent search of Myrvold. In this\npaper, we confirm the correctness of their exhaustive enumerations with a\nsatisfiability (SAT) solver approach instead of using custom-written\nbacktracking code. Performing the enumeration using a SAT solver has at least\nthree advantages. First, it reduces the amount of trust necessary, as SAT\nsolvers produce independently-verifiable certificates that their enumerations\nare complete. These certificates can be checked by formal proof verifiers that\nare relatively simple pieces of software, and therefore easier to trust.\nSecond, it is typically more straightforward and less error-prone to use a SAT\nsolver over writing search code. Third, it can be more efficient to use a\nSAT-based approach, as SAT solvers are highly optimized pieces of software\nincorporating backtracking-with-learning for improving the efficiency of the\nbacktracking search. For example, the SAT solver completely enumerates all\northogonal pairs of Latin squares of order ten with two nontrivial relations in\nunder 2 hours on a desktop machine, while Delisle's 2010 search used 11,700 CPU\nhours. Although computer hardware was slower in 2010, this alone cannot explain\nthe improvement in the efficiency of our SAT-based search.",
      "url": "http://arxiv.org/abs/2509.09633v1",
      "published_time_eastern_timestamp": 1757611112.0
    },
    {
      "title": "I Know Who Clones Your Code: Interpretable Smart Contract Similarity\n  Detection",
      "summary": "Widespread reuse of open-source code in smart contract development boosts\nprogramming efficiency but significantly amplifies bug propagation across\ncontracts, while dedicated methods for detecting similar smart contract\nfunctions remain very limited. Conventional abstract-syntax-tree (AST) based\nmethods for smart contract similarity detection face challenges in handling\nintricate tree structures, which impedes detailed semantic comparison of code.\nRecent deep-learning based approaches tend to overlook code syntax and\ndetection interpretability, resulting in suboptimal performance.\n  To fill this research gap, we introduce SmartDetector, a novel approach for\ncomputing similarity between smart contract functions, explainable at the\nfine-grained statement level. Technically, SmartDetector decomposes the AST of\na smart contract function into a series of smaller statement trees, each\nreflecting a structural element of the source code. Then, SmartDetector uses a\nclassifier to compute the similarity score of two functions by comparing each\npair of their statement trees. To address the infinite hyperparameter space of\nthe classifier, we mathematically derive a cosine-wise diffusion process to\nefficiently search optimal hyperparameters. Extensive experiments conducted on\nthree large real-world datasets demonstrate that SmartDetector outperforms\ncurrent state-of-the-art methods by an average improvement of 14.01% in\nF1-score, achieving an overall average F1-score of 95.88%.",
      "url": "http://arxiv.org/abs/2509.09630v1",
      "published_time_eastern_timestamp": 1757610951.0
    },
    {
      "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex\n  Software Engineering",
      "summary": "The emergence of long-context language models with context windows extending\nto millions of tokens has created new opportunities for sophisticated code\nunderstanding and software development evaluation. We propose LoCoBench, a\ncomprehensive benchmark specifically designed to evaluate long-context LLMs in\nrealistic, complex software development scenarios. Unlike existing code\nevaluation benchmarks that focus on single-function completion or short-context\ntasks, LoCoBench addresses the critical evaluation gap for long-context\ncapabilities that require understanding entire codebases, reasoning across\nmultiple files, and maintaining architectural consistency across large-scale\nsoftware systems. Our benchmark provides 8,000 evaluation scenarios\nsystematically generated across 10 programming languages, with context lengths\nspanning 10K to 1M tokens, a 100x variation that enables precise assessment of\nlong-context performance degradation in realistic software development\nsettings. LoCoBench introduces 8 task categories that capture essential\nlong-context capabilities: architectural understanding, cross-file refactoring,\nmulti-session development, bug investigation, feature implementation, code\ncomprehension, integration testing, and security analysis. Through a 5-phase\npipeline, we create diverse, high-quality scenarios that challenge LLMs to\nreason about complex codebases at unprecedented scale. We introduce a\ncomprehensive evaluation framework with 17 metrics across 4 dimensions,\nincluding 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our\nevaluation of state-of-the-art long-context models reveals substantial\nperformance gaps, demonstrating that long-context understanding in complex\nsoftware development represents a significant unsolved challenge that demands\nmore attention. LoCoBench is released at:\nhttps://github.com/SalesforceAIResearch/LoCoBench.",
      "url": "http://arxiv.org/abs/2509.09614v1",
      "published_time_eastern_timestamp": 1757609704.0
    },
    {
      "title": "A Multi-Scale Feature Extraction and Fusion UNet for Pathloss Prediction\n  in UAV-Assisted mmWave Radio Networks",
      "summary": "Accurate pathloss prediction is essential for the design and optimization of\nUAV-assisted millimeter-wave (mmWave) networks. While deep learning approaches\nhave shown strong potential, their generalization across diverse environments,\nrobustness to noisy inputs, and sensitivity to UAV altitude remain\nunderexplored. To address these challenges, we propose a UNet-based deep\nlearning architecture that combines multi-scale feature extraction,\nconvolution-based feature fusion, and an atrous spatial pyramid pooling (ASPP)\nbottleneck for efficient context aggregation. The model predicts pathloss maps\nfrom log-distance, line-of-sight (LOS) mask, and building mask inputs. In\naddition, we develop a fully vectorized LOS mask computation algorithm that\nsignificantly accelerates pre-processing and enables large-scale dataset\ngeneration. Extensive evaluations on both in-house ray-tracing data and the\nRadioMapSeer benchmark demonstrate that the proposed model outperforms several\nstate-of-the-art baselines in accuracy and efficiency. All source code is\npublicly released to support reproducibility and future research.",
      "url": "http://arxiv.org/abs/2509.09606v1",
      "published_time_eastern_timestamp": 1757609159.0
    },
    {
      "title": "Fault-tolerant transformations of spacetime codes",
      "summary": "Recent advances in quantum error-correction (QEC) have shown that it is often\nbeneficial to understand fault-tolerance as a dynamical process, a circuit with\nredundant measurements that help correct errors, rather than as a static code\nequipped with a syndrome extraction circuit. Spacetime codes have emerged as a\nnatural framework to understand error correction at the circuit level while\nleveraging the traditional QEC toolbox. Here, we introduce a framework based on\nchain complexes and chain maps to model spacetime codes and transformations\nbetween them. We show that stabilizer codes, quantum circuits, and decoding\nproblems can all be described using chain complexes, and that the equivalence\nof two spacetime codes can be characterized by specific maps between chain\ncomplexes, the fault-tolerant maps, that preserve the number of encoded qubits,\nfault distance, and minimum-weight decoding problem. As an application of this\nframework, we extend the foliated cluster state construction from stabilizer\ncodes to any spacetime code, showing that any Clifford circuit can be\ntransformed into a measurement-based protocol with the same fault-tolerant\nproperties. To this protocol, we associate a chain complex which encodes the\nunderlying decoding problem, generalizing previous cluster state complex\nconstructions. Our method enables the construction of cluster states from\nnon-CSS, subsystem, and Floquet codes, as well as from logical Clifford\noperations on a given code.",
      "url": "http://arxiv.org/abs/2509.09603v1",
      "published_time_eastern_timestamp": 1757608967.0
    },
    {
      "title": "ObjectReact: Learning Object-Relative Control for Visual Navigation",
      "summary": "Visual navigation using only a single camera and a topological map has\nrecently become an appealing alternative to methods that require additional\nsensors and 3D maps. This is typically achieved through an \"image-relative\"\napproach to estimating control from a given pair of current observation and\nsubgoal image. However, image-level representations of the world have\nlimitations because images are strictly tied to the agent's pose and\nembodiment. In contrast, objects, being a property of the map, offer an\nembodiment- and trajectory-invariant world representation. In this work, we\npresent a new paradigm of learning \"object-relative\" control that exhibits\nseveral desirable characteristics: a) new routes can be traversed without\nstrictly requiring to imitate prior experience, b) the control prediction\nproblem can be decoupled from solving the image matching problem, and c) high\ninvariance can be achieved in cross-embodiment deployment for variations across\nboth training-testing and mapping-execution settings. We propose a topometric\nmap representation in the form of a \"relative\" 3D scene graph, which is used to\nobtain more informative object-level global path planning costs. We train a\nlocal controller, dubbed \"ObjectReact\", conditioned directly on a high-level\n\"WayObject Costmap\" representation that eliminates the need for an explicit RGB\ninput. We demonstrate the advantages of learning object-relative control over\nits image-relative counterpart across sensor height variations and multiple\nnavigation tasks that challenge the underlying spatial understanding\ncapability, e.g., navigating a map trajectory in the reverse direction. We\nfurther show that our sim-only policy is able to generalize well to real-world\nindoor environments. Code and supplementary material are accessible via project\npage: https://object-react.github.io/",
      "url": "http://arxiv.org/abs/2509.09594v1",
      "published_time_eastern_timestamp": 1757608457.0
    },
    {
      "title": "Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset\n  Collector",
      "summary": "To combat phishing attacks -- aimed at luring web users to divulge their\nsensitive information -- various phishing detection approaches have been\nproposed. As attackers focus on devising new tactics to bypass existing\ndetection solutions, researchers have adapted by integrating machine learning\nand deep learning into phishing detection. Phishing dataset collection is vital\nto developing effective phishing detection approaches, which highly depend on\nthe diversity of the gathered datasets. The lack of diversity in the dataset\nresults in a biased model. Since phishing websites are often short-lived,\ncollecting them is also a challenge. Consequently, very few phishing webpage\ndataset repositories exist to date. No single repository comprehensively\nconsolidates all phishing elements corresponding to a phishing webpage, namely,\nURL, webpage source code, screenshot, and related webpage resources. This paper\nintroduces a resource collection tool designed to gather various resources\nassociated with a URL, such as CSS, Javascript, favicons, webpage images, and\nscreenshots. Our tool leverages PhishTank as the primary source for obtaining\nactive phishing URLs. Our tool fetches several additional webpage resources\ncompared to PyWebCopy Python library, which provides webpage content for a\ngiven URL. Additionally, we share a sample dataset generated using our tool\ncomprising 4,056 legitimate and 5,666 phishing URLs along with their associated\nresources. We also remark on the top correlated phishing features with their\nassociated class label found in our dataset. Our tool offers a comprehensive\nresource set that can aid researchers in developing effective phishing\ndetection approaches.",
      "url": "http://arxiv.org/abs/2509.09592v1",
      "published_time_eastern_timestamp": 1757608212.0
    },
    {
      "title": "Causal PDE-Control Models: A Structural Framework for Dynamic Portfolio\n  Optimization",
      "summary": "Classical portfolio models collapse under structural breaks, while modern\nmachine-learning allocators adapt flexibly but often at the cost of\ntransparency and interpretability. This paper introduces Causal PDE-Control\nModels (CPCMs), a unifying framework that integrates causal inference,\nnonlinear filtering, and forward-backward partial differential equations for\ndynamic portfolio optimization. The framework delivers three theoretical\nadvances: (i) the existence of conditional risk-neutral measures under evolving\ninformation sets; (ii) a projection-divergence duality that quantifies the\nstability cost of departing from the causal driver manifold; and (iii) causal\ncompleteness, establishing that a finite driver span can capture all systematic\npremia. Classical methods such as Markowitz, CAPM, and Black-Litterman appear\nas degenerate cases, while reinforcement learning and deep-hedging policies\nemerge as unconstrained, symmetry-breaking approximations. Empirically, CPCM\nsolvers implemented with physics-informed neural networks achieve higher Sharpe\nratios, lower turnover, and more persistent premia than both econometric and\nmachine-learning benchmarks, using a global equity panel with more than 300\ncandidate drivers. By reframing portfolio optimization around structural\ncausality and PDE control, CPCMs provide a rigorous, interpretable, and\ncomputationally tractable foundation for robust asset allocation under\nnonstationary conditions.",
      "url": "http://arxiv.org/abs/2509.09585v1",
      "published_time_eastern_timestamp": 1757607740.0
    },
    {
      "title": "PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient\n  Fine-Tuning for Remote Sensing Change Detection",
      "summary": "To tackle the prevalence of pseudo changes, the scarcity of labeled samples,\nand the difficulty of cross-domain generalization in multi-temporal and\nmulti-source remote sensing imagery, we propose PeftCD, a change detection\nframework built upon Vision Foundation Models (VFMs) with Parameter-Efficient\nFine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese\nencoder derived from a VFM, into which LoRA and Adapter modules are seamlessly\nintegrated. This design enables highly efficient task adaptation by training\nonly a minimal set of additional parameters. To fully unlock the potential of\nVFMs, we investigate two leading backbones: the Segment Anything Model v2\n(SAM2), renowned for its strong segmentation priors, and DINOv3, a\nstate-of-the-art self-supervised representation learner. The framework is\ncomplemented by a deliberately lightweight decoder, ensuring the focus remains\non the powerful feature representations from the backbones. Extensive\nexperiments demonstrate that PeftCD achieves state-of-the-art performance\nacross multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD\n(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and\nLEVIR-CD (85.62%), with notably precise boundary delineation and strong\nsuppression of pseudo-changes. In summary, PeftCD presents an optimal balance\nof accuracy, efficiency, and generalization. It offers a powerful and scalable\nparadigm for adapting large-scale VFMs to real-world remote sensing change\ndetection applications. The code and pretrained models will be released at\nhttps://github.com/dyzy41/PeftCD.",
      "url": "http://arxiv.org/abs/2509.09572v1",
      "published_time_eastern_timestamp": 1757606923.0
    },
    {
      "title": "Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in\n  MRI-based Alzheimer's Disease Classification",
      "summary": "Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep\nlearning (DL) algorithms have been proposed to aid in the diagnosis of diseases\nsuch as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can\nsuffer from shortcut learning, in which spurious features, not directly related\nto the output label, are used for prediction. When these features are related\nto protected attributes, they can lead to performance bias against\nunderrepresented protected groups, such as those defined by race and sex. In\nthis work, we explore the potential for shortcut learning and demographic bias\nin DL based AD diagnosis from MRI. We first investigate if DL algorithms can\nidentify race or sex from 3D brain MRI scans to establish the presence or\notherwise of race and sex based distributional shifts. Next, we investigate\nwhether training set imbalance by race or sex can cause a drop in model\nperformance, indicating shortcut learning and bias. Finally, we conduct a\nquantitative and qualitative analysis of feature attributions in different\nbrain regions for both the protected attribute and AD classification tasks.\nThrough these experiments, and using multiple datasets and DL models (ResNet\nand SwinTransformer), we demonstrate the existence of both race and sex based\nshortcut learning and bias in DL based AD classification. Our work lays the\nfoundation for fairer DL diagnostic tools in brain MRI. The code is provided at\nhttps://github.com/acharaakshit/ShortMR",
      "url": "http://arxiv.org/abs/2509.09558v1",
      "published_time_eastern_timestamp": 1757605710.0
    },
    {
      "title": "Fast Polarisation-Aware Decoder for Non-Binary Polar Codes",
      "summary": "The paper investigates the emerging field of low-complexity non-binary polar\ncode (NB-PC) decoders. It shows that customizing each kernel of an NB-PC\ndecoder through offline analysis can significantly reduce the overall decoding\ncomplexity. The proposed decoder, referred to as the Fast Successive\nCancellation-Polarization Aware (FSC-PA) scheme, achieves this by minimizing\nthe computational load of parity-check nodes that share the same level of input\npolarization. The NB polar decoder is developed for both BPSK and CCSK\nmodulations. Compared to the state-of-the-art extended min-sum algorithm, the\nFSC-PA algorithm achieves an overall reduction of 60 percents in field\nadditions and 30 percents in real additions, while incurring only a negligible\nperformance loss (less than 0.2 dB degradation).",
      "url": "http://arxiv.org/abs/2509.09554v1",
      "published_time_eastern_timestamp": 1757605362.0
    },
    {
      "title": "Finite Scalar Quantization Enables Redundant and Transmission-Robust\n  Neural Audio Compression at Low Bit-rates",
      "summary": "Neural Audio Codecs (NACs) have become increasingly adopted in speech\nprocessing tasks due to their excellent rate-distortion performance and\ncompatibility with Large Language Models (LLMs) as discrete feature\nrepresentations for audio generation. While most existing codecs rely on\nResidual Vector Quantization (RVQ), Finite Scalar Quantization (FSQ) has\nrecently emerged as a compelling alternative that simplifies training and\nnatively supports single codebooks. We introduce NeuCodec, an FSQ-based NAC,\nand show that FSQ encodes baked-in redundancy which produces an encoding which\nis robust when transmitted through noisy channels. First, through an encoder\ndistillation experiment, we show that two different encoders can learn to\nencode identical audio into vastly different code sequences whilst maintaining\ncomparable reconstruction quality with the same quantizer and decoder. Second,\nwe demonstrate that FSQ has vastly superior bit-level perturbation robustness\nby comparing the performance of RVQ and FSQ codecs when simulating the\ntransmission of code sequences through a noisy channel.",
      "url": "http://arxiv.org/abs/2509.09550v1",
      "published_time_eastern_timestamp": 1757605199.0
    },
    {
      "title": "Generative Diffusion Contrastive Network for Multi-View Clustering",
      "summary": "In recent years, Multi-View Clustering (MVC) has been significantly advanced\nunder the influence of deep learning. By integrating heterogeneous data from\nmultiple views, MVC enhances clustering analysis, making multi-view fusion\ncritical to clustering performance. However, there is a problem of low-quality\ndata in multi-view fusion. This problem primarily arises from two reasons: 1)\nCertain views are contaminated by noisy data. 2) Some views suffer from missing\ndata. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)\nmethod to address this problem. SGDF leverages a multiple generative mechanism\nfor the multi-view feature of each sample. It is robust to low-quality data.\nBuilding on SGDF, we further present the Generative Diffusion Contrastive\nNetwork (GDCN). Extensive experiments show that GDCN achieves the\nstate-of-the-art results in deep MVC tasks. The source code is publicly\navailable at https://github.com/HackerHyper/GDCN.",
      "url": "http://arxiv.org/abs/2509.09527v1",
      "published_time_eastern_timestamp": 1757603366.0
    },
    {
      "title": "PIPES: A Meta-dataset of Machine Learning Pipelines",
      "summary": "Solutions to the Algorithm Selection Problem (ASP) in machine learning face\nthe challenge of high computational costs associated with evaluating various\nalgorithms' performances on a given dataset. To mitigate this cost, the\nmeta-learning field can leverage previously executed experiments shared in\nonline repositories such as OpenML. OpenML provides an extensive collection of\nmachine learning experiments. However, an analysis of OpenML's records reveals\nlimitations. It lacks diversity in pipelines, specifically when exploring data\npreprocessing steps/blocks, such as scaling or imputation, resulting in limited\nrepresentation. Its experiments are often focused on a few popular techniques\nwithin each pipeline block, leading to an imbalanced sample. To overcome the\nobserved limitations of OpenML, we propose PIPES, a collection of experiments\ninvolving multiple pipelines designed to represent all combinations of the\nselected sets of techniques, aiming at diversity and completeness. PIPES stores\nthe results of experiments performed applying 9,408 pipelines to 300 datasets.\nIt includes detailed information on the pipeline blocks, training and testing\ntimes, predictions, performances, and the eventual error messages. This\ncomprehensive collection of results allows researchers to perform analyses\nacross diverse and representative pipelines and datasets. PIPES also offers\npotential for expansion, as additional data and experiments can be incorporated\nto support the meta-learning community further. The data, code, supplementary\nmaterial, and all experiments can be found at\nhttps://github.com/cynthiamaia/PIPES.git.",
      "url": "http://arxiv.org/abs/2509.09512v1",
      "published_time_eastern_timestamp": 1757602378.0
    },
    {
      "title": "SEDM: Scalable Self-Evolving Distributed Memory for Agents",
      "summary": "Long-term multi-agent systems inevitably generate vast amounts of\ntrajectories and historical interactions, which makes efficient memory\nmanagement essential for both performance and scalability. Existing methods\ntypically depend on vector retrieval and hierarchical storage, yet they are\nprone to noise accumulation, uncontrolled memory expansion, and limited\ngeneralization across domains. To address these challenges, we present SEDM,\nSelf-Evolving Distributed Memory, a verifiable and adaptive framework that\ntransforms memory from a passive repository into an active, self-optimizing\ncomponent. SEDM integrates verifiable write admission based on reproducible\nreplay, a self-scheduling memory controller that dynamically ranks and\nconsolidates entries according to empirical utility, and cross-domain knowledge\ndiffusion that abstracts reusable insights to support transfer across\nheterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM\nimproves reasoning accuracy while reducing token overhead compared with strong\nmemory baselines, and further enables knowledge distilled from fact\nverification to enhance multi-hop reasoning. The results highlight SEDM as a\nscalable and sustainable memory mechanism for open-ended multi-agent\ncollaboration. The code will be released in the later stage of this project.",
      "url": "http://arxiv.org/abs/2509.09498v1",
      "published_time_eastern_timestamp": 1757601457.0
    },
    {
      "title": "Improving Human Motion Plausibility with Body Momentum",
      "summary": "Many studies decompose human motion into local motion in a frame attached to\nthe root joint and global motion of the root joint in the world frame, treating\nthem separately. However, these two components are not independent. Global\nmovement arises from interactions with the environment, which are, in turn,\ndriven by changes in the body configuration. Motion models often fail to\nprecisely capture this physical coupling between local and global dynamics,\nwhile deriving global trajectories from joint torques and external forces is\ncomputationally expensive and complex. To address these challenges, we propose\nusing whole-body linear and angular momentum as a constraint to link local\nmotion with global movement. Since momentum reflects the aggregate effect of\njoint-level dynamics on the body's movement through space, it provides a\nphysically grounded way to relate local joint behavior to global displacement.\nBuilding on this insight, we introduce a new loss term that enforces\nconsistency between the generated momentum profiles and those observed in\nground-truth data. Incorporating our loss reduces foot sliding and jitter,\nimproves balance, and preserves the accuracy of the recovered motion. Code and\ndata are available at the project page https://hlinhn.github.io/momentum_bmvc.",
      "url": "http://arxiv.org/abs/2509.09496v1",
      "published_time_eastern_timestamp": 1757601272.0
    }
  ]
}
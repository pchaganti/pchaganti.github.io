{
  "last_updated": "2026-02-19T10:38:57.950893-05:00",
  "papers": [
    {
      "title": "Knowledge-Embedded Latent Projection for Robust Representation Learning",
      "summary": "Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.",
      "url": "http://arxiv.org/abs/2602.16709v1",
      "published_time_eastern_timestamp": 1771441096.0
    },
    {
      "title": "E-Graphs as a Persistent Compiler Abstraction",
      "summary": "Recent algorithmic advances have made equality saturation an appealing approach to program optimization because it avoids the phase-ordering problem. Existing work uses external equality saturation libraries, or custom implementations that are deeply tied to the specific application. However, these works only apply equality saturation at a single level of abstraction, or discard the discovered equalities when code is transformed by other compiler passes. We propose an alternative approach that represents an e-graph natively in the compiler's intermediate representation, facilitating the application of constructive compiler passes that maintain the e-graph state throughout the compilation flow. We build on a Python-based MLIR framework, xDSL, and introduce a new MLIR dialect, eqsat, that represents e-graphs in MLIR code. We show that this representation expands the scope of equality saturation in the compiler, allowing us to interleave pattern rewriting with other compiler transformations. The eqsat dialect provides a unified abstraction for compilers to utilize equality saturation across various levels of intermediate representations concurrently within the same MLIR flow.",
      "url": "http://arxiv.org/abs/2602.16707v1",
      "published_time_eastern_timestamp": 1771441012.0
    },
    {
      "title": "Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents",
      "summary": "LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.",
      "url": "http://arxiv.org/abs/2602.16699v1",
      "published_time_eastern_timestamp": 1771440374.0
    },
    {
      "title": "VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection",
      "summary": "Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.",
      "url": "http://arxiv.org/abs/2602.16681v1",
      "published_time_eastern_timestamp": 1771438942.0
    },
    {
      "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
      "summary": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
      "url": "http://arxiv.org/abs/2602.16671v1",
      "published_time_eastern_timestamp": 1771438143.0
    },
    {
      "title": "Evaluating Collective Behaviour of Hundreds of LLM Agents",
      "summary": "As autonomous agents powered by LLM are increasingly deployed in society, understanding their collective behaviour in social dilemmas becomes critical. We introduce an evaluation framework where LLMs generate strategies encoded as algorithms, enabling inspection prior to deployment and scaling to populations of hundreds of agents -- substantially larger than in previous work. We find that more recent models tend to produce worse societal outcomes compared to older models when agents prioritise individual gain over collective benefits. Using cultural evolution to model user selection of agents, our simulations reveal a significant risk of convergence to poor societal equilibria, particularly when the relative benefit of cooperation diminishes and population sizes increase. We release our code as an evaluation suite for developers to assess the emergent collective behaviour of their models.",
      "url": "http://arxiv.org/abs/2602.16662v1",
      "published_time_eastern_timestamp": 1771437771.0
    },
    {
      "title": "Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments",
      "summary": "Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.",
      "url": "http://arxiv.org/abs/2602.16653v1",
      "published_time_eastern_timestamp": 1771437137.0
    },
    {
      "title": "Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval",
      "summary": "The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a \"resource divide.\" State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes \"lexical density\" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.",
      "url": "http://arxiv.org/abs/2602.16640v1",
      "published_time_eastern_timestamp": 1771435783.0
    },
    {
      "title": "A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models",
      "summary": "Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.",
      "url": "http://arxiv.org/abs/2602.16626v1",
      "published_time_eastern_timestamp": 1771435262.0
    },
    {
      "title": "ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models",
      "summary": "Current state-of-the-art multi-vector models are obtained through a small Knowledge Distillation (KD) training step on top of strong single-vector models, leveraging the large-scale pre-training of these models. In this paper, we study the pre-training of multi-vector models and show that large-scale multi-vector pre-training yields much stronger multi-vector models. Notably, a fully ColBERT-pre-trained model, ColBERT-Zero, trained only on public data, outperforms GTE-ModernColBERT as well as its base model, GTE-ModernBERT, which leverages closed and much stronger data, setting new state-of-the-art for model this size. We also find that, although performing only a small KD step is not enough to achieve results close to full pre-training, adding a supervised step beforehand allows to achieve much closer performance while skipping the most costly unsupervised phase. Finally, we find that aligning the fine-tuning and pre-training setups is crucial when repurposing existing models. To enable exploration of our results, we release various checkpoints as well as code used to train them.",
      "url": "http://arxiv.org/abs/2602.16609v1",
      "published_time_eastern_timestamp": 1771434212.0
    },
    {
      "title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification",
      "summary": "Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.",
      "url": "http://arxiv.org/abs/2602.16590v1",
      "published_time_eastern_timestamp": 1771432892.0
    },
    {
      "title": "On the Coupled Cluster Doubles Truncation Variety of Four Electrons",
      "summary": "We extend recent algebro-geometric results for coupled cluster theory of quantum many-body systems to the truncation varieties arising from the doubles approximation (CCD), focusing on the first genuinely nonlinear doubles regime of four electrons. Since this doubles truncation variety does not coincide with previously studied varieties, we initiate a systematic investigation of its basic algebro-geometric invariants. Combining theoretical and numerical results, we show that for $4$ electrons on $n\\leq 12$ orbitals, the CCD truncation variety is a complete intersection of degree $2^{\\binom{n-4}{4}}$. Using representation-theoretic arguments, we uncover a Pfaffian structure governing the quadratic relations that define the truncation variety for any $n$, and show that an exact tensor product factorization holds in a distinguished limit of disconnected doubles. We connect these structural results to the computation of the beryllium insertion into molecular hydrogen ({Be$\\cdots$H$_2$ $\\to$ H--Be--H}), a small but challenging bond formation process where multiconfigurational effects become pronounced.",
      "url": "http://arxiv.org/abs/2602.16580v1",
      "published_time_eastern_timestamp": 1771432104.0
    },
    {
      "title": "Testing non-circular black hole spacetime with X-ray reflection",
      "summary": "X-ray reflection spectroscopy is a powerful tool for testing the Kerr hypothesis and probing the strong gravity regime around accreting black holes. Most tests of General Relativity (GR) assume that the spacetime around a black hole is circular, meaning the metric possesses a specific symmetry structure common to the Kerr solution. However, deviations from circularity are predicted by various modified gravity theories and non-vacuum General Relativity solutions. In this work, we test a specific non-circular metric constructed based on a locality principle, where the deviation from the Kerr spacetime is driven by the local spacetime curvature. To accurately model the reflection spectrum in this background, we implement a relativistic ray-tracing code in horizon-penetrating (ingoing Kerr) coordinates, which are favored for their ability to avoid introducing curvature singularities at the horizon in non-circular spacetimes. We apply this model to the high-quality \\textit{NuSTAR} spectrum of the Galactic black hole binary EXO 1846--031. Our spectral analysis reveals a source with a high inclination angle ($Î¹\\approx 76^{\\circ}$) and a near-extremal spin parameter ($a_* \\approx 0.98$). While we identify a global minimum in the parameter space suggesting a non-zero deformation ($\\ell_{\\mathrm{NP}} \\approx 0.12$), the 99\\% confidence interval fully encompasses the Kerr limit ($\\ell_{\\mathrm{NP}}=0$). We conclude that the current X-ray reflection data for EXO 1846--031 are consistent with the Kerr hypothesis. This work demonstrates the feasibility of using X-ray reflection spectroscopy to constrain non-circular metrics and establishes a framework for future tests.",
      "url": "http://arxiv.org/abs/2602.16562v1",
      "published_time_eastern_timestamp": 1771430588.0
    },
    {
      "title": "MerLean: An Agentic Framework for Autoformalization in Quantum Computation",
      "summary": "We introduce MerLean, a fully automated agentic framework for autoformalization in quantum computation. MerLean extracts mathematical statements from \\LaTeX{} source files, formalizes them into verified Lean~4 code built on Mathlib, and translates the result back into human-readable \\LaTeX{} for semantic review. We evaluate MerLean on three theoretical quantum computing papers producing 2,050 Lean declarations from 114 statements in total. MerLean achieves end-to-end formalization on all three papers, reducing the verification burden to only the newly introduced definitions and axioms. Our results demonstrate that agentic autoformalization can scale to frontier research, offering both a practical tool for machine-verified peer review and a scalable engine for mining high-quality synthetic data to train future reasoning models. Our approach can also be generalized to any other rigorous research in mathematics and theoretical physics.",
      "url": "http://arxiv.org/abs/2602.16554v1",
      "published_time_eastern_timestamp": 1771430072.0
    },
    {
      "title": "New self-consistent theoretical descriptions for mass-loss rates of O-type stars",
      "summary": "Massive O-type stars lose a significant fraction of their mass through radiation-driven winds, a process that critically shapes their evolution and feedback into the interstellar medium. Accurate predictions of mass-loss rates are essential for models of stellar structure and population synthesis. We computed wind parameters for O-type stars using a self-consistent approach that couples the hydrodynamics of the wind with detailed calculations of the line acceleration. This approach follows the theory of radiation-driven stellar winds and allows us to derive mass-loss rate distributions for different atomic configurations of the stellar flux. We used the TLUSTY code for stellar atmosphere models to compute non-local thermodynamic equilibrium models; these models served as input radiation fields for the calculation of the line-force parameters, for which we used the LOCUS code. These line-force parameters were then iteratively coupled with the HYDWIND code to solve the wind hydrodynamics. The procedure was applied across a grid of stellar parameters for three chemical configurations. We obtain self-consistent wind parameters for a broad set of O-type stellar models. The results show a systematic decrease in mass-loss rates with the inclusion of more elements in the radiation field, which is attributed to a strong effect on the UV region of the spectral energy distribution. As more elements are included, resulting in a larger number of spectral lines, the contribution from the UV diminishes, leading to lower mass-loss rates. We fitted three theoretical prescriptions for $\\dot{M}$ using a Bayesian approach; this yielded Pearson correlation values greater than 0.92 for all three model grids. It also allowed for the estimation of the wind momentum-luminosity relationships for each of the grids, yielding results similar to those based on observations of O-type stars.",
      "url": "http://arxiv.org/abs/2602.16526v1",
      "published_time_eastern_timestamp": 1771427691.0
    },
    {
      "title": "A Fully Discrete Nonnegativity-Preserving FEM for a Stochastic Heat Equation",
      "summary": "We consider a stochastic heat equation with nonlinear multiplicative finite-dimensional noise that admits a unique nonnegative solution when given nonnegative initial data. Inspired by existing results for fully discrete finite difference schemes and building on the convergence analysis of semi-discrete mass-lumped finite element approximations, a fully discrete numerical method is introduced that combines mass-lumped finite elements with a Lie-Trotter splitting strategy. This discretization preserves nonnegativity at the discrete level and is shown to be convergent under suitable regularity conditions. A rigorous convergence analysis is provided, highlighting the role of mass lumping in ensuring nonnegativity and of operator splitting in decoupling the deterministic and stochastic dynamics. Numerical experiments are presented to confirm the convergence rates and the preservation of nonnegativity. In addition, we examine several numerical examples outside the scope of the established theory, aiming to explore the range of applicability and potential limitations of the proposed method.",
      "url": "http://arxiv.org/abs/2602.16508v1",
      "published_time_eastern_timestamp": 1771426394.0
    },
    {
      "title": "MMA: Multimodal Memory Agent",
      "summary": "Long-horizon multimodal agents depend on external memory; however, similarity-based retrieval often surfaces stale, low-credibility, or conflicting items, which can trigger overconfident errors. We propose Multimodal Memory Agent (MMA), which assigns each retrieved memory item a dynamic reliability score by combining source credibility, temporal decay, and conflict-aware network consensus, and uses this signal to reweight evidence and abstain when support is insufficient. We also introduce MMA-Bench, a programmatically generated benchmark for belief dynamics with controlled speaker reliability and structured text-vision contradictions. Using this framework, we uncover the \"Visual Placebo Effect\", revealing how RAG-based agents inherit latent visual biases from foundation models. On FEVER, MMA matches baseline accuracy while reducing variance by 35.2% and improving selective utility; on LoCoMo, a safety-oriented configuration improves actionable accuracy and reduces wrong answers; on MMA-Bench, MMA reaches 41.18% Type-B accuracy in Vision mode, while the baseline collapses to 0.0% under the same protocol. Code: https://github.com/AIGeeksGroup/MMA.",
      "url": "http://arxiv.org/abs/2602.16493v1",
      "published_time_eastern_timestamp": 1771425035.0
    },
    {
      "title": "Learning to Learn from Language Feedback with Social Meta-Learning",
      "summary": "Large language models (LLMs) often struggle to learn from corrective feedback within a conversational context. They are rarely proactive in soliciting this feedback, even when faced with ambiguity, which can make their dialogues feel static, one-sided, and lacking the adaptive qualities of human conversation. To address these limitations, we draw inspiration from social meta-learning (SML) in humans - the process of learning how to learn from others. We formulate SML as a finetuning methodology, training LLMs to solicit and learn from language feedback in simulated pedagogical dialogues, where static tasks are converted into interactive social learning problems. SML effectively teaches models to use conversation to solve problems they are unable to solve in a single turn. This capability generalises across domains; SML on math problems produces models that better use feedback to solve coding problems and vice versa. Furthermore, despite being trained only on fully-specified problems, these models are better able to solve underspecified tasks where critical information is revealed over multiple turns. When faced with this ambiguity, SML-trained models make fewer premature answer attempts and are more likely to ask for the information they need. This work presents a scalable approach to developing AI systems that effectively learn from language feedback.",
      "url": "http://arxiv.org/abs/2602.16488v1",
      "published_time_eastern_timestamp": 1771424533.0
    },
    {
      "title": "Team of Thoughts: Efficient Test-time Scaling of Agentic Systems through Orchestrated Tool Calling",
      "summary": "Existing Multi-Agent Systems (MAS) typically rely on static, homogeneous model configurations, limiting their ability to exploit the distinct strengths of differently post-trained models. To address this, we introduce Team-of-Thoughts, a novel MAS architecture that leverages the complementary capabilities of heterogeneous agents via an orchestrator-tool paradigm. Our framework introduces two key mechanisms to optimize performance: (1) an orchestrator calibration scheme that identifies models with superior coordination capabilities, and (2) a self-assessment protocol where tool agents profile their own domain expertise to account for variations in post-training skills. During inference, the orchestrator dynamically activates the most suitable tool agents based on these proficiency profiles. Experiments on five reasoning and code generation benchmarks show that Team-of-Thoughts delivers consistently superior task performance. Notably, on AIME24 and LiveCodeBench, our approach achieves accuracies of 96.67% and 72.53%, respectively, substantially outperforming homogeneous role-play baselines, which score 80% and 65.93%.",
      "url": "http://arxiv.org/abs/2602.16485v1",
      "published_time_eastern_timestamp": 1771424341.0
    },
    {
      "title": "Bounds and Constructions of Codes for Ordered Composite DNA Sequences",
      "summary": "This paper extends the foundational work of Dollma \\emph{et al}. on codes for ordered composite DNA sequences. We consider the general setting with an alphabet of size $q$ and a resolution parameter $k$, moving beyond the binary ($q=2$) case primarily studied previously. We investigate error-correcting codes for substitution errors and deletion errors under several channel models, including $(e_1,\\ldots,e_k)$-composite error/deletion, $e$-composite error/deletion, and the newly introduced $t$-$(e_1,\\ldots,e_t)$-composite error/deletion model.\n  We first establish equivalence relations among families of composite-error correcting codes (CECCs) and among families of composite-deletion correcting codes (CDCCs). This significantly reduces the number of distinct error-parameter sets that require separate analysis. We then derive novel and general upper bounds on the sizes of CECCs using refined sphere-packing arguments and probabilistic methods. These bounds together cover all values of parameters $q$, $k$, $(e_1,\\ldots,e_k)$ and $e$. In contrast, previous bounds were only established for $q=2$ and limited choices of $k$, $(e_1,\\ldots,e_k)$ and $e$. For CDCCs, we generalize a known non-asymptotic upper bound for $(1,0,\\ldots,0)$-CDCCs and then provide a cleaner asymptotic bound.\n  On the constructive side, for any $q\\ge2$, we propose $(1,0,\\ldots,0)$-CDCCs, $1$-CDCCs and $t$-$(1,\\ldots,1)$-CDCCs with near-optimal redundancies. These codes have efficient and systematic encoders. For substitution errors, we design the first explicit encoding and decoding algorithms for the binary $(1,0,\\ldots,0)$-CECC constructed by Dollma \\emph{et al}, and extend the approach to general $q$. Furthermore, we give an improved construction of binary $1$-CECCs, a construction of nonbinary $1$-CECCs, and a construction of $t$-$(1,\\ldots,1)$-CECCs. These constructions are also systematic.",
      "url": "http://arxiv.org/abs/2602.16406v1",
      "published_time_eastern_timestamp": 1771417655.0
    }
  ]
}
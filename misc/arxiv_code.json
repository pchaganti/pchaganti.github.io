{
  "last_updated": "2025-11-10T22:41:23.778784-05:00",
  "papers": [
    {
      "title": "Lightning Grasp: High Performance Procedural Grasp Synthesis with\n  Contact Fields",
      "summary": "Despite years of research, real-time diverse grasp synthesis for dexterous\nhands remains an unsolved core challenge in robotics and computer graphics. We\npresent Lightning Grasp, a novel high-performance procedural grasp synthesis\nalgorithm that achieves orders-of-magnitude speedups over state-of-the-art\napproaches, while enabling unsupervised grasp generation for irregular,\ntool-like objects. The method avoids many limitations of prior approaches, such\nas the need for carefully tuned energy functions and sensitive initialization.\nThis breakthrough is driven by a key insight: decoupling complex geometric\ncomputation from the search process via a simple, efficient data structure -\nthe Contact Field. This abstraction collapses the problem complexity, enabling\na procedural search at unprecedented speeds. We open-source our system to\npropel further innovation in robotic manipulation.",
      "url": "http://arxiv.org/abs/2511.07418v1",
      "published_time_eastern_timestamp": 1762801184.0
    },
    {
      "title": "SPOT: An Annotated French Corpus and Benchmark for Detecting Critical\n  Interventions in Online Conversations",
      "summary": "We introduce SPOT (Stopping Points in Online Threads), the first annotated\ncorpus translating the sociological concept of stopping point into a\nreproducible NLP task. Stopping points are ordinary critical interventions that\npause or redirect online discussions through a range of forms (irony, subtle\ndoubt or fragmentary arguments) that frameworks like counterspeech or social\ncorrection often overlook. We operationalize this concept as a binary\nclassification task and provide reliable annotation guidelines. The corpus\ncontains 43,305 manually annotated French Facebook comments linked to URLs\nflagged as false information by social media users, enriched with contextual\nmetadata (article, post, parent comment, page or group, and source). We\nbenchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs\nunder various prompting strategies. Results show that fine-tuned encoders\noutperform prompted LLMs in F1 score by more than 10 percentage points,\nconfirming the importance of supervised learning for emerging non-English\nsocial media tasks. Incorporating contextual metadata further improves encoder\nmodels F1 scores from 0.75 to 0.78. We release the anonymized dataset, along\nwith the annotation guidelines and code in our code repository, to foster\ntransparency and reproducible research.",
      "url": "http://arxiv.org/abs/2511.07405v1",
      "published_time_eastern_timestamp": 1762800880.0
    },
    {
      "title": "A Diffusion Model to Shrink Proteins While Maintaining Their Function",
      "summary": "Many proteins useful in modern medicine or bioengineering are challenging to\nmake in the lab, fuse with other proteins in cells, or deliver to tissues in\nthe body, because their sequences are too long. Shortening these sequences\ntypically involves costly, time-consuming experimental campaigns. Ideally, we\ncould instead use modern models of massive databases of sequences from nature\nto learn how to propose shrunken proteins that resemble sequences found in\nnature. Unfortunately, these models struggle to efficiently search the\ncombinatorial space of all deletions, and are not trained with inductive biases\nto learn how to delete. To address this gap, we propose SCISOR, a novel\ndiscrete diffusion model that deletes letters from sequences to generate\nprotein samples that resemble those found in nature. To do so, SCISOR trains a\nde-noiser to reverse a forward noising process that adds random insertions to\nnatural sequences. As a generative model, SCISOR fits evolutionary sequence\ndata competitively with previous large models. In evaluation, SCISOR achieves\nstate-of-the-art predictions of the functional effects of deletions on\nProteinGym. Finally, we use the SCISOR de-noiser to shrink long protein\nsequences, and show that its suggested deletions result in significantly more\nrealistic proteins and more often preserve functional motifs than previous\nmodels of evolutionary sequences.",
      "url": "http://arxiv.org/abs/2511.07390v1",
      "published_time_eastern_timestamp": 1762800384.0
    },
    {
      "title": "samsara: A Continuous-Time Markov Chain Monte Carlo Sampler for\n  Trans-Dimensional Bayesian Analysis",
      "summary": "Bayesian inference requires determining the posterior distribution, a task\nthat becomes particularly challenging when the dimension of the parameter space\nis large and unknown. This limitation arises in many physics problems, such as\nMixture Models (MM) with an unknown number of components or the inference of\noverlapping signals in noisy data, as in the Laser Interferometer Space Antenna\n(LISA) Global Fit problem. Traditional approaches, such as product-space\nmethods or Reversible-Jump Markov Chain Monte Carlo (RJMCMC), often face\nefficiency and convergence limitations. This paper presents samsara, a\nContinuous-Time Markov Chain Monte Carlo (CTMCMC) framework that models\nparameter evolution through Poisson-driven birth, death, and mutation\nprocesses. samsara is designed to sample models of unknown dimensionality. By\nrequiring detailed balance through adaptive rate definitions, CTMCMC achieves\nautomatic acceptance of trans-dimensional moves and high sampling efficiency.\nThe code features waiting time weighted estimators, optimized memory storage,\nand a modular design for easy customization. We validate samsara on three\nbenchmark problems: an analytic trans-dimensional distribution, joint inference\nof sine waves and Lorentzians in time series, and a Gaussian MM with an unknown\nnumber of components. In all cases, the code shows excellent agreement with\nanalytical and Nested Sampling results. All these features push samsara as a\npowerful alternative to RJMCMC for large- and variable-dimensional Bayesian\ninference problems.",
      "url": "http://arxiv.org/abs/2511.07385v1",
      "published_time_eastern_timestamp": 1762800254.0
    },
    {
      "title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted\n  Recurrence",
      "summary": "Recent advances in depth-recurrent language models show that recurrence can\ndecouple train-time compute and parameter count from test-time compute. In this\nwork, we study how to convert existing pretrained non-recurrent language models\ninto depth-recurrent models. We find that using a curriculum of recurrences to\nincrease the effective depth of the model over the course of training preserves\nperformance while reducing total computational cost. In our experiments, on\nmathematics, we observe that converting pretrained models to recurrent ones\nresults in better performance at a given compute budget than simply\npost-training the original non-recurrent language model.",
      "url": "http://arxiv.org/abs/2511.07384v1",
      "published_time_eastern_timestamp": 1762800187.0
    },
    {
      "title": "Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for\n  Bangla-to-Python Code Generation",
      "summary": "Large Language Models (LLMs) have advanced the automated generation of code\nfrom natural language prompts. However, low-resource languages (LRLs) like\nBangla remain underrepresented due to the limited availability of\ninstruction-to-code datasets and evaluation benchmarks. To address this, the\nBLP Workshop at IJCNLP-AACL 2025 introduced a shared task on \"Code Generation\nin Bangla\". In this work, we propose a method that combines instruction\nprompting with a test-driven, feedback-guided iterative refinement process\nusing a fine-tuned Qwen2.5-14B model. The model generates code from Bangla\ninstructions, tests it against unit tests, and iteratively refines any failing\noutputs through three evaluation passes, using test feedback to guide each\nstep. This approach helped our team \"Retriv\" to secure 2nd place in the shared\ntask with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla\ninstruction understanding and Python code generation, emphasizing the need for\ntargeted methods in LRLs. We made experimental scripts publicly available for\nthe community.",
      "url": "http://arxiv.org/abs/2511.07382v1",
      "published_time_eastern_timestamp": 1762800104.0
    },
    {
      "title": "Enhanced GCD through ORBGRAND-AI: Exploiting Partial and Total\n  Correlation in Noise",
      "summary": "There have been significant advances in recent years in the development of\nforward error correction decoders that can decode codes of any structure,\nincluding practical realizations in synthesized circuits and taped out chips.\nWhile essentially all soft-decision decoders assume that bits have been\nimpacted independently on the channel, for one of these new approaches it has\nbeen established that channel dependencies can be exploited to achieve superior\ndecoding accuracy, resulting in Ordered Reliability Bits Guessing Random\nAdditive Noise Decoding Approximate Independence (ORBGRAND-AI). Building on\nthat capability, here we consider the integration of ORBGRAND-AI as a pattern\ngenerator for Guessing Codeword Decoding (GCD). We first establish that a\ndirect approach delivers mildly degraded block error rate (BLER) but with\nreduced number of queried patterns when compared to ORBGRAND-AI. We then show\nthat with a more nuanced approach it is possible to leverage total correlation\nto deliver an additional BLER improvement of around 0.75 dB while retaining\nreduced query numbers.",
      "url": "http://arxiv.org/abs/2511.07376v1",
      "published_time_eastern_timestamp": 1762799663.0
    },
    {
      "title": "AcousTools: A `Full-Stack', Python-Based, Acoustic Holography Library",
      "summary": "Acoustic Holography is an emerging field where mid-air ultrasound is\ncontrolled and manipulated for novel and exciting applications. These range\nfrom mid-air haptics, volumetric displays, contactless fabrication, and even\nchemical and biomedical applications such as drug delivery. To develop these\napplications, a software framework to predict acoustic behaviour and simulating\nresulting effects, such as applied forces or scattering patterns is desirable.\nThere have been various software libraries and platforms that attempt to fill\nthis role, but there is yet to be a single piece of software that acts as a\n'full-stack' solution. We define this full-stack as the process from\nabstraction to physicalisation starting with setup, modelling acoustic\npropagation, transducer phase retrieval, sound field analysis, and control of\nthe acoustic holographic hardware itself. Existing methods fail to fulfil one\nor more of these categories. To address this, we present AcousTools, a\nPython-based acoustic holography library, designed to support the full suite of\nacoustic holographic applications and we show AcousTools's ability to meet each\nstep of the full-stack's requirements. AcousTools has the potential to become\nthe standard code library for acoustic holography, with the uniquely complete\nsuite of features wrapped in a language that is known to be easy to use,\nAcousTools will increase the ability for researchers to develop novel\napplications as well as accurately review other's work. The full-stack, aside\nfrom software, will also be useful for researchers - providing a way to view\nand compare methodologies by understanding where they fit into the stack.",
      "url": "http://arxiv.org/abs/2511.07336v1",
      "published_time_eastern_timestamp": 1762796193.0
    },
    {
      "title": "FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework\n  for Equity Research Report Generation",
      "summary": "While LLMs have shown great success in financial tasks like stock prediction\nand question answering, their application in fully automating Equity Research\nReport generation remains uncharted territory. In this paper, we formulate the\nEquity Research Report (ERR) Generation task for the first time. To address the\ndata scarcity and the evaluation metrics absence, we present an open-source\nevaluation benchmark for ERR generation - FinRpt. We frame a Dataset\nConstruction Pipeline that integrates 7 financial data types and produces a\nhigh-quality ERR dataset automatically, which could be used for model training\nand evaluation. We also introduce a comprehensive evaluation system including\n11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent\nframework specifically tailored to address this task, named FinRpt-Gen, and\ntrain several LLM-based agents on the proposed datasets using Supervised\nFine-Tuning and Reinforcement Learning. Experimental results indicate the data\nquality and metrics effectiveness of the benchmark FinRpt and the strong\nperformance of FinRpt-Gen, showcasing their potential to drive innovation in\nthe ERR generation field. All code and datasets are publicly available.",
      "url": "http://arxiv.org/abs/2511.07322v1",
      "published_time_eastern_timestamp": 1762795352.0
    },
    {
      "title": "RLVE: Scaling Up Reinforcement Learning for Language Models with\n  Adaptive Verifiable Environments",
      "summary": "We introduce Reinforcement Learning (RL) with Adaptive Verifiable\nEnvironments (RLVE), an approach using verifiable environments that\nprocedurally generate problems and provide algorithmically verifiable rewards,\nto scale up RL for language models (LMs). RLVE enables each verifiable\nenvironment to dynamically adapt its problem difficulty distribution to the\npolicy model's capabilities as training progresses. In contrast, static data\ndistributions often lead to vanishing learning signals when problems are either\ntoo easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a\nlarge-scale suite of 400 verifiable environments carefully developed through\nmanual environment engineering. Using RLVE-Gym, we show that environment\nscaling, i.e., expanding the collection of training environments, consistently\nimproves generalizable reasoning capabilities. RLVE with joint training across\nall 400 environments in RLVE-Gym yields a 3.37% absolute average improvement\nacross six reasoning benchmarks, starting from one of the strongest 1.5B\nreasoning LMs. By comparison, continuing this LM's original RL training yields\nonly a 0.49% average absolute gain despite using over 3x more compute. We\nrelease our code publicly.",
      "url": "http://arxiv.org/abs/2511.07317v1",
      "published_time_eastern_timestamp": 1762795115.0
    },
    {
      "title": "ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding",
      "summary": "Automatic ICD coding, the task of assigning disease and procedure codes to\nelectronic medical records, is crucial for clinical documentation and billing.\nWhile existing methods primarily enhance model understanding of code\nhierarchies and synonyms, they often overlook the pervasive use of medical\nacronyms in clinical notes, a key factor in ICD code inference. To address this\ngap, we propose a novel effective data augmentation technique that leverages\nlarge language models to expand medical acronyms, allowing models to be trained\non their full form representations. Moreover, we incorporate consistency\ntraining to regularize predictions by enforcing agreement between the original\nand augmented documents. Extensive experiments on the MIMIC-III dataset\ndemonstrate that our approach, ACE-ICD establishes new state-of-the-art\nperformance across multiple settings, including common codes, rare codes, and\nfull-code assignments. Our code is publicly available.",
      "url": "http://arxiv.org/abs/2511.07311v1",
      "published_time_eastern_timestamp": 1762794680.0
    },
    {
      "title": "LMM-IQA: Image Quality Assessment for Low-Dose CT Imaging",
      "summary": "Low-dose computed tomography (CT) represents a significant improvement in\npatient safety through lower radiation doses, but increased noise, blur, and\ncontrast loss can diminish diagnostic quality. Therefore, consistency and\nrobustness in image quality assessment become essential for clinical\napplications. In this study, we propose an LLM-based quality assessment system\nthat generates both numerical scores and textual descriptions of degradations\nsuch as noise, blur, and contrast loss. Furthermore, various inference\nstrategies - from the zero-shot approach to metadata integration and error\nfeedback - are systematically examined, demonstrating the progressive\ncontribution of each method to overall performance. The resultant assessments\nyield not only highly correlated scores but also interpretable output, thereby\nadding value to clinical workflows. The source codes of our study are available\nat https://github.com/itu-biai/lmms_ldct_iqa.",
      "url": "http://arxiv.org/abs/2511.07298v1",
      "published_time_eastern_timestamp": 1762793771.0
    },
    {
      "title": "PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving",
      "summary": "Most recent work in autonomous driving has prioritized benchmark performance\nand methodological innovation over in-depth analysis of model failures, biases,\nand shortcut learning. This has led to incremental improvements without a deep\nunderstanding of the current failures. While it is straightforward to look at\nsituations where the model fails, it is hard to understand the underlying\nreason. This motivates us to conduct a systematic study, where inputs to the\nmodel are perturbed and the predictions observed. We introduce PlanT 2.0, a\nlightweight, object-centric planning transformer designed for autonomous\ndriving research in CARLA. The object-level representation enables controlled\nanalysis, as the input can be easily perturbed (e.g., by changing the location\nor adding or removing certain objects), in contrast to sensor-based models. To\ntackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0,\nwe introduce multiple upgrades to PlanT, achieving state-of-the-art performance\non Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis\nexposes insightful failures, such as a lack of scene understanding caused by\nlow obstacle diversity, rigid expert behaviors leading to exploitable\nshortcuts, and overfitting to a fixed set of expert trajectories. Based on\nthese findings, we argue for a shift toward data-centric development, with a\nfocus on richer, more robust, and less biased datasets. We open-source our code\nand model at https://github.com/autonomousvision/plant2.",
      "url": "http://arxiv.org/abs/2511.07292v1",
      "published_time_eastern_timestamp": 1762792907.0
    },
    {
      "title": "CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference\n  Quality Assessment of Compressed Video",
      "summary": "The prevalence of user-generated content (UGC) on platforms such as YouTube\nand TikTok has rendered no-reference (NR) perceptual video quality assessment\n(VQA) vital for optimizing video delivery. Nonetheless, the characteristics of\nnon-professional acquisition and the subsequent transcoding of UGC video on\nsharing platforms present significant challenges for NR-VQA. Although NR-VQA\nmodels attempt to infer mean opinion scores (MOS), their modeling of subjective\nscores for compressed content remains limited due to the absence of\nfine-grained perceptual annotations of artifact types. To address these\nchallenges, we propose CAMP-VQA, a novel NR-VQA framework that exploits the\nsemantic understanding capabilities of large vision-language models. Our\napproach introduces a quality-aware prompting mechanism that integrates video\nmetadata (e.g., resolution, frame rate, bitrate) with key fragments extracted\nfrom inter-frame variations to guide the BLIP-2 pretraining approach in\ngenerating fine-grained quality captions. A unified architecture has been\ndesigned to model perceptual quality across three dimensions: semantic\nalignment, temporal characteristics, and spatial characteristics. These\nmultimodal features are extracted and fused, then regressed to video quality\nscores. Extensive experiments on a wide variety of UGC datasets demonstrate\nthat our model consistently outperforms existing NR-VQA methods, achieving\nimproved accuracy without the need for costly manual fine-grained annotations.\nOur method achieves the best performance in terms of average rank and linear\ncorrelation (SRCC: 0.928, PLCC: 0.938) compared to state-of-the-art methods.\nThe source code and trained models, along with a user-friendly demo, are\navailable at: https://github.com/xinyiW915/CAMP-VQA.",
      "url": "http://arxiv.org/abs/2511.07290v1",
      "published_time_eastern_timestamp": 1762792667.0
    },
    {
      "title": "StreamKV: Streaming Video Question-Answering with Segment-based KV Cache\n  Retrieval and Compression",
      "summary": "Video Large Language Models (Video-LLMs) have demonstrated significant\npotential in the areas of video captioning, search, and summarization. However,\ncurrent Video-LLMs still face challenges with long real-world videos. Recent\nmethods have introduced a retrieval mechanism that retrieves query-relevant KV\ncaches for question answering, enhancing the efficiency and accuracy of long\nreal-world videos. However, the compression and retrieval of KV caches are\nstill not fully explored. In this paper, we propose \\textbf{StreamKV}, a\ntraining-free framework that seamlessly equips Video-LLMs with advanced KV\ncache retrieval and compression. Compared to previous methods that used uniform\npartitioning, StreamKV dynamically partitions video streams into semantic\nsegments, which better preserves semantic information. For KV cache retrieval,\nStreamKV calculates a summary vector for each segment to retain segment-level\ninformation essential for retrieval. For KV cache compression, StreamKV\nintroduces a guidance prompt designed to capture the key semantic elements\nwithin each segment, ensuring only the most informative KV caches are retained\nfor answering questions. Moreover, StreamKV unifies KV cache retrieval and\ncompression within a single module, performing both in a layer-adaptive manner,\nthereby further improving the effectiveness of streaming video question\nanswering. Extensive experiments on public StreamingVQA benchmarks demonstrate\nthat StreamKV significantly outperforms existing Online Video-LLMs, achieving\nsuperior accuracy while substantially improving both memory efficiency and\ncomputational latency. The code has been released at\nhttps://github.com/sou1p0wer/StreamKV.",
      "url": "http://arxiv.org/abs/2511.07278v1",
      "published_time_eastern_timestamp": 1762791903.0
    },
    {
      "title": "Alloy-Driven Verification of Object-Centric Event Data: From Temporal\n  Logic to Knowledge Graphs",
      "summary": "Object-centric process mining addresses the limitations of traditional\napproaches, which often involve the lossy flattening of event data and obscure\nvital relationships among interacting objects. This paper presents a novel\nformal framework for Object-centric Event Data (OCED) that ensures the\ncorrectness of the meta-model and preserves native object-centric semantics\nprior to the system implementation. Our approach effectively leverages Alloy\nfor precisely specifying temporal properties and structural relationships\nbetween objects and events. This guarantees thorough verification against\npredefined OCED constraints such as cross-object cardinality bounds and\ntime-aware consistency rules, hence preventing common data integrity issues. We\ndemonstrate the effectiveness of the proposed framework in discovering and\nvalidating implicit object dependencies in event logs, particularly when\nimporting data into graph databases like Neo4j. This demonstrates how formal\nverification can avoid pitfalls that lead to data invisibility and improve\nknowledge graph creation, enrichment, and querying. To bridge theory and\npractice, our verified \\emph{FOCED} is made accessible through automatically\ngenerated Python bindings, empowering industrial users without formal methods\nexpertise. The code is available on GitHub\n\\footnote{https://github.com/sabalati/FOCED}",
      "url": "http://arxiv.org/abs/2511.07263v1",
      "published_time_eastern_timestamp": 1762790855.0
    },
    {
      "title": "Bridging the Prototype-Production Gap: A Multi-Agent System for\n  Notebooks Transformation",
      "summary": "The increasing adoption of Jupyter notebooks in data science and machine\nlearning workflows has created a gap between exploratory code development and\nproduction-ready software systems. While notebooks excel at iterative\ndevelopment and visualization, they often lack proper software engineering\nprinciples, making their transition to production environments challenging.\nThis paper presents Codelevate, a novel multi-agent system that automatically\ntransforms Jupyter notebooks into well-structured, maintainable Python code\nrepositories. Our system employs three specialized agents - Architect,\nDeveloper, and Structure - working in concert through a shared dependency tree\nto ensure architectural coherence and code quality. Our experimental results\nvalidate Codelevate's capability to bridge the prototype-to-production gap\nthrough autonomous code transformation, yielding quantifiable improvements in\ncode quality metrics while preserving computational semantics.",
      "url": "http://arxiv.org/abs/2511.07257v1",
      "published_time_eastern_timestamp": 1762790710.0
    },
    {
      "title": "NoteEx: Interactive Visual Context Manipulation for LLM-Assisted\n  Exploratory Data Analysis in Computational Notebooks",
      "summary": "Computational notebooks have become popular for Exploratory Data Analysis\n(EDA), augmented by LLM-based code generation and result interpretation.\nEffective LLM assistance hinges on selecting informative context -- the minimal\nset of cells whose code, data, or outputs suffice to answer a prompt. As\nnotebooks grow long and messy, users can lose track of the mental model of\ntheir analysis. They thus fail to curate appropriate contexts for LLM tasks,\ncausing frustration and tedious prompt engineering. We conducted a formative\nstudy (n=6) that surfaced challenges in LLM context selection and mental model\nmaintenance. Therefore, we introduce NoteEx, a JupyterLab extension that\nprovides a semantic visualization of the EDA workflow, allowing analysts to\nexternalize their mental model, specify analysis dependencies, and enable\ninteractive selection of task-relevant contexts for LLMs. A user study (n=12)\nagainst a baseline shows that NoteEx improved mental model retention and\ncontext selection, leading to more accurate and relevant LLM responses.",
      "url": "http://arxiv.org/abs/2511.07223v1",
      "published_time_eastern_timestamp": 1762789495.0
    },
    {
      "title": "Effect of Misfit and Threading Dislocations on Surface Energies of\n  PbTe-PbSe Interfaces",
      "summary": "The manufacturing processes of heterostructures determine the structure and\nproperties of their interfaces. In this work, we simulate PbTe and PbSe\nheterostructures manufactured via (1) direct wave bonding and (2)\nheteroepitaxial growth. The former contains interfaces with 2D misfit\ndislocation networks while the latter contains complex 3D networks with both\nmisfit and threading dislocations. To compute the surface energy of interfaces,\nwe measure the interaction energy across surfaces using a well-verified code.\nCompared with hypothetical interfaces modeled to be coherent, a typical\nassumption in traditional slab-based methods, the surface energy of wafer\nbonded and epitaxially grown interfaces are significantly different.\nSemi-coherent interfaces exhibit up to ~27% lower surface energies than\ncoherent ones, while coherent models overestimate surface energies by up to\n~50% relative to epitaxial interfaces. The consequence of such differences can\nlead to conflicting predictions of physical phenomena such as fracture\ntoughness or growth mode.",
      "url": "http://arxiv.org/abs/2511.07182v1",
      "published_time_eastern_timestamp": 1762787622.0
    },
    {
      "title": "A Provably-Correct and Robust Convex Model for Smooth Separable NMF",
      "summary": "Nonnegative matrix factorization (NMF) is a linear dimensionality reduction\ntechnique for nonnegative data, with applications such as hyperspectral\nunmixing and topic modeling. NMF is a difficult problem in general (NP-hard),\nand its solutions are typically not unique. To address these two issues,\nadditional constraints or assumptions are often used. In particular,\nseparability assumes that the basis vectors in the NMF are equal to some\ncolumns of the input matrix. In that case, the problem is referred to as\nseparable NMF (SNMF) and can be solved in polynomial-time with robustness\nguarantees, while identifying a unique solution. However, in real-world\nscenarios, due to noise or variability, multiple data points may lie near the\nbasis vectors, which SNMF does not leverage. In this work, we rely on the\nsmooth separability assumption, which assumes that each basis vector is close\nto multiple data points. We explore the properties of the corresponding\nproblem, referred to as smooth SNMF (SSNMF), and examine how it relates to SNMF\nand orthogonal NMF. We then propose a convex model for SSNMF and show that it\nprovably recovers the sought-after factors, even in the presence of noise. We\nfinally adapt an existing fast gradient method to solve this convex model for\nSSNMF, and show that it compares favorably with state-of-the-art methods on\nboth synthetic and hyperspectral datasets.",
      "url": "http://arxiv.org/abs/2511.07109v1",
      "published_time_eastern_timestamp": 1762782867.0
    }
  ]
}
{
  "last_updated": "2025-07-17T23:56:06.631430-04:00",
  "papers": [
    {
      "title": "Hierarchical Rectified Flow Matching with Mini-Batch Couplings",
      "summary": "Flow matching has emerged as a compelling generative modeling approach that\nis widely used across domains. To generate data via a flow matching model, an\nordinary differential equation (ODE) is numerically solved via forward\nintegration of the modeled velocity field. To better capture the multi-modality\nthat is inherent in typical velocity fields, hierarchical flow matching was\nrecently introduced. It uses a hierarchy of ODEs that are numerically\nintegrated when generating data. This hierarchy of ODEs captures the\nmulti-modal velocity distribution just like vanilla flow matching is capable of\nmodeling a multi-modal data distribution. While this hierarchy enables to model\nmulti-modal velocity distributions, the complexity of the modeled distribution\nremains identical across levels of the hierarchy. In this paper, we study how\nto gradually adjust the complexity of the distributions across different levels\nof the hierarchy via mini-batch couplings. We show the benefits of mini-batch\ncouplings in hierarchical rectified flow matching via compelling results on\nsynthetic and imaging data. Code is available at\nhttps://riccizz.github.io/HRF_coupling.",
      "url": "http://arxiv.org/abs/2507.13350v1",
      "published_time_eastern_timestamp": 1752775196.0
    },
    {
      "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement\n  Learning",
      "summary": "Recent advancements in vision-language models (VLMs) have improved\nperformance by increasing the number of visual tokens, which are often\nsignificantly longer than text tokens. However, we observe that most real-world\nscenarios do not require such an extensive number of visual tokens. While the\nperformance drops significantly in a small subset of OCR-related tasks, models\nstill perform accurately in most other general VQA tasks with only 1/4\nresolution. Therefore, we propose to dynamically process distinct samples with\ndifferent resolutions, and present a new paradigm for visual token compression,\nnamely, VisionThink. It starts with a downsampled image and smartly decides\nwhether it is sufficient for problem solving. Otherwise, the model could output\na special token to request the higher-resolution image. Compared to existing\nEfficient VLM methods that compress tokens using fixed pruning ratios or\nthresholds, VisionThink autonomously decides whether to compress tokens case by\ncase. As a result, it demonstrates strong fine-grained visual understanding\ncapability on OCR-related tasks, and meanwhile saves substantial visual tokens\non simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge\nstrategy to successfully apply RL to general VQA tasks. Moreover, we carefully\ndesign a reward function and penalty mechanism to achieve a stable and\nreasonable image resize call ratio. Extensive experiments demonstrate the\nsuperiority, efficiency, and effectiveness of our method. Our code is available\nat https://github.com/dvlab-research/VisionThink.",
      "url": "http://arxiv.org/abs/2507.13348v1",
      "published_time_eastern_timestamp": 1752775195.0
    },
    {
      "title": "$Ï€^3$: Scalable Permutation-Equivariant Visual Geometry Learning",
      "summary": "We introduce $\\pi^3$, a feed-forward neural network that offers a novel\napproach to visual geometry reconstruction, breaking the reliance on a\nconventional fixed reference view. Previous methods often anchor their\nreconstructions to a designated viewpoint, an inductive bias that can lead to\ninstability and failures if the reference is suboptimal. In contrast, $\\pi^3$\nemploys a fully permutation-equivariant architecture to predict\naffine-invariant camera poses and scale-invariant local point maps without any\nreference frames. This design makes our model inherently robust to input\nordering and highly scalable. These advantages enable our simple and bias-free\napproach to achieve state-of-the-art performance on a wide range of tasks,\nincluding camera pose estimation, monocular/video depth estimation, and dense\npoint map reconstruction. Code and models are publicly available.",
      "url": "http://arxiv.org/abs/2507.13347v1",
      "published_time_eastern_timestamp": 1752775193.0
    },
    {
      "title": "Imbalance in Balance: Online Concept Balancing in Generation Models",
      "summary": "In visual generation tasks, the responses and combinations of complex\nconcepts often lack stability and are error-prone, which remains an\nunder-explored area. In this paper, we attempt to explore the causal factors\nfor poor concept responses through elaborately designed experiments. We also\ndesign a concept-wise equalization loss function (IMBA loss) to address this\nissue. Our proposed method is online, eliminating the need for offline dataset\nprocessing, and requires minimal code changes. In our newly proposed complex\nconcept benchmark Inert-CompBench and two other public test sets, our method\nsignificantly enhances the concept response capability of baseline models and\nyields highly competitive results with only a few codes.",
      "url": "http://arxiv.org/abs/2507.13345v1",
      "published_time_eastern_timestamp": 1752775187.0
    },
    {
      "title": "Expansion creates spin-glass order in finite-connectivity models: a\n  rigorous and intuitive approach from the theory of LDPC codes",
      "summary": "Complex free-energy landscapes with many local minima separated by large\nbarriers are believed to underlie glassy behavior across diverse physical\nsystems. This is the heuristic picture associated with replica symmetry\nbreaking (RSB) in spin glasses, but RSB has only been rigorously verified for\ncertain mean-field models with all-to-all connectivity. In this work, we give a\nrigorous proof of finite temperature spin glass order for a family of models\nwith local interactions on finite-connectivity, non-Euclidean expander graphs.\nTo this end, we bypass the RSB formalism entirely, and instead exploit the\nmathematical equivalence of such models to certain low-density parity check\n(LDPC) codes. We use code expansion, a property of LDPC codes which guarantees\nextensive energy barriers around ground states. Together with mild additional\nassumptions, this allows us to construct an explicit decomposition of the\nlow-temperature Gibbs state into disjoint components, each hosting an\nasymptotically long-lived state associated with a local minimum of the\nlandscape. Each component carries at most an exponentially small fraction of\nthe total weight, and almost all components do not contain ground states --\nwhich we take together to define spin-glass order. The proof is elementary, and\ntreats various expanding graph topologies on the same footing, including those\nwith short loops where existing approaches such as the cavity method fail. Our\nresults apply rigorously to diluted p-spin glasses for sufficiently large p,\nand while unproven, we also expect our assumptions to hold in a broader family\nof codes. Motivated by this, we numerically study two simple models, on random\nregular graphs and a regular tesselation of hyperbolic space. We show that both\nmodels undergo two transitions as a function of temperature, corresponding to\nthe onset of weak ergodicity breaking and spin glass order, respectively.",
      "url": "http://arxiv.org/abs/2507.13342v1",
      "published_time_eastern_timestamp": 1752775122.0
    },
    {
      "title": "A Framework of Distributed Source Encryption using Mutual Information\n  Security Criterion and the Strong Converse Theorem",
      "summary": "We reinvestigate the general distributed secure source coding based on the\ncommon key cryptosystem proposed by Oohama and Santoso (ITW 2021). They\nproposed a framework of distributed source encryption and derived the necessary\nand sufficient conditions to have reliable and secure transmission. However,\nthe bounds of the rate region, which specifies both necessary and sufficient\nconditions to have reliable and secure transmission under the proposed\ncryptosystem, were derived based on a self-tailored non-standard} security\ncriterion. In this paper we adopt the standard security criterion, i.e.,\nstandard mutual information. We successfully establish the bounds of the rate\nregion based on this security criterion. Information spectrum method and a\nvariant of Birkhoff-von Neumann theorem play an important role in deriving the\nresult.",
      "url": "http://arxiv.org/abs/2507.13294v1",
      "published_time_eastern_timestamp": 1752771532.0
    },
    {
      "title": "Towards Formal Verification of LLM-Generated Code from Natural Language\n  Prompts",
      "summary": "In the past few years LLMs have emerged as a tool that can aid programmers by\ntaking natural language descriptions and generating code based on it. However,\nLLMs often generate incorrect code that users need to fix and the literature\nsuggests users often struggle to detect these errors. In this work we seek to\noffer formal guarantees of correctness to LLM generated code; such guarantees\ncould improve the experience of using AI Code Assistants and potentially enable\nnatural language programming for users with little or no programming knowledge.\nTo address this challenge we propose to incorporate a formal query language\nthat can represent a user's intent in a formally defined but natural\nlanguage-like manner that a user can confirm matches their intent. Then, using\nsuch a query we propose to verify LLM generated code to ensure it matches the\nuser's intent. We implement these ideas in our system, Astrogator, for the\nAnsible programming language which includes such a formal query language, a\ncalculus for representing the behavior of Ansible programs, and a symbolic\ninterpreter which is used for the verification. On a benchmark suite of 21\ncode-generation tasks, our verifier is able to verify correct code in 83% of\ncases and identify incorrect code in 92%.",
      "url": "http://arxiv.org/abs/2507.13290v1",
      "published_time_eastern_timestamp": 1752771282.0
    },
    {
      "title": "Leveraging Pre-Trained Visual Models for AI-Generated Video Detection",
      "summary": "Recent advances in Generative AI (GenAI) have led to significant improvements\nin the quality of generated visual content. As AI-generated visual content\nbecomes increasingly indistinguishable from real content, the challenge of\ndetecting the generated content becomes critical in combating misinformation,\nensuring privacy, and preventing security threats. Although there has been\nsubstantial progress in detecting AI-generated images, current methods for\nvideo detection are largely focused on deepfakes, which primarily involve human\nfaces. However, the field of video generation has advanced beyond DeepFakes,\ncreating an urgent need for methods capable of detecting AI-generated videos\nwith generic content. To address this gap, we propose a novel approach that\nleverages pre-trained visual models to distinguish between real and generated\nvideos. The features extracted from these pre-trained models, which have been\ntrained on extensive real visual content, contain inherent signals that can\nhelp distinguish real from generated videos. Using these extracted features, we\nachieve high detection performance without requiring additional model training,\nand we further improve performance by training a simple linear classification\nlayer on top of the extracted features. We validated our method on a dataset we\ncompiled (VID-AID), which includes around 10,000 AI-generated videos produced\nby 9 different text-to-video models, along with 4,000 real videos, totaling\nover 7 hours of video content. Our evaluation shows that our approach achieves\nhigh detection accuracy, above 90% on average, underscoring its effectiveness.\nUpon acceptance, we plan to publicly release the code, the pre-trained models,\nand our dataset to support ongoing research in this critical area.",
      "url": "http://arxiv.org/abs/2507.13224v1",
      "published_time_eastern_timestamp": 1752766599.0
    },
    {
      "title": "Performance Portable Gradient Computations Using Source Transformation",
      "summary": "Derivative computation is a key component of optimization, sensitivity\nanalysis, uncertainty quantification, and nonlinear solvers. Automatic\ndifferentiation (AD) is a powerful technique for evaluating such derivatives,\nand in recent years, has been integrated into programming environments such as\nJax, PyTorch, and TensorFlow to support derivative computations needed for\ntraining of machine learning models, resulting in widespread use of these\ntechnologies. The C++ language has become the de facto standard for scientific\ncomputing due to numerous factors, yet language complexity has made the\nadoption of AD technologies for C++ difficult, hampering the incorporation of\npowerful differentiable programming approaches into C++ scientific simulations.\nThis is exacerbated by the increasing emergence of architectures such as GPUs,\nwhich have limited memory capabilities and require massive thread-level\nconcurrency. Portable scientific codes rely on domain specific programming\nmodels such as Kokkos making AD for such codes even more complex. In this\npaper, we will investigate source transformation-based automatic\ndifferentiation using Clad to automatically generate portable and efficient\ngradient computations of Kokkos-based code. We discuss the modifications of\nClad required to differentiate Kokkos abstractions. We will illustrate the\nfeasibility of our proposed strategy by comparing the wall-clock time of the\ngenerated gradient code with the wall-clock time of the input function on\ndifferent cutting edge GPU architectures such as NVIDIA H100, AMD MI250x, and\nIntel Ponte Vecchio GPU. For these three architectures and for the considered\nexample, evaluating up to 10 000 entries of the gradient only took up to 2.17x\nthe wall-clock time of evaluating the input function.",
      "url": "http://arxiv.org/abs/2507.13204v1",
      "published_time_eastern_timestamp": 1752765325.0
    },
    {
      "title": "Orbis: Overcoming Challenges of Long-Horizon Prediction in Driving World\n  Models",
      "summary": "Existing world models for autonomous driving struggle with long-horizon\ngeneration and generalization to challenging scenarios. In this work, we\ndevelop a model using simple design choices, and without additional supervision\nor sensors, such as maps, depth, or multiple cameras. We show that our model\nyields state-of-the-art performance, despite having only 469M parameters and\nbeing trained on 280h of video data. It particularly stands out in difficult\nscenarios like turning maneuvers and urban traffic. We test whether discrete\ntoken models possibly have advantages over continuous models based on flow\nmatching. To this end, we set up a hybrid tokenizer that is compatible with\nboth approaches and allows for a side-by-side comparison. Our study concludes\nin favor of the continuous autoregressive model, which is less brittle on\nindividual design choices and more powerful than the model built on discrete\ntokens. Code, models and qualitative results are publicly available at\nhttps://lmb-freiburg.github.io/orbis.github.io/.",
      "url": "http://arxiv.org/abs/2507.13162v1",
      "published_time_eastern_timestamp": 1752762574.0
    }
  ]
}
{
  "last_updated": "2025-05-08T12:57:37.839960-04:00",
  "papers": [
    {
      "title": "EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via\n  Reinforcement Learning",
      "summary": "Multimodal large language models (MLLMs) have advanced perception across\ntext, vision, and audio, yet they often struggle with structured cross-modal\nreasoning, particularly when integrating audio and visual signals. We introduce\nEchoInk-R1, a reinforcement learning framework that enhances such reasoning in\nMLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group\nRelative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice\nquestion answering over synchronized audio-image pairs. To enable this, we\ncurate AVQA-R1-6K, a dataset pairing such audio-image inputs with\nmultiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves\n85.77% accuracy on the validation set, outperforming the base model, which\nscores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy,\nEchoInk-R1 demonstrates reflective reasoning by revisiting initial\ninterpretations and refining responses when facing ambiguous multimodal inputs.\nThese results suggest that lightweight reinforcement learning fine-tuning\nenhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to\nunify audio, visual, and textual modalities for general open-world reasoning\nvia reinforcement learning. Code and data are publicly released to facilitate\nfurther research.",
      "url": "http://arxiv.org/abs/2505.04623v1",
      "published_time_eastern_timestamp": 1746640789.0
    },
    {
      "title": "Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation",
      "summary": "Vision is well-known for its use in manipulation, especially using visual\nservoing. To make it robust, multiple cameras are needed to expand the field of\nview. That is computationally challenging. Merging multiple views and using\nQ-learning allows the design of more effective representations and optimization\nof sample efficiency. Such a solution might be expensive to deploy. To mitigate\nthis, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently\nmerges views to increase sample efficiency while augmenting with single-view\nfeatures to allow lightweight deployment and ensure robust policies. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad",
      "url": "http://arxiv.org/abs/2505.04619v1",
      "published_time_eastern_timestamp": 1746640768.0
    },
    {
      "title": "Active Sampling for MRI-based Sequential Decision Making",
      "summary": "Despite the superior diagnostic capability of Magnetic Resonance Imaging\n(MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and\ncomplexity. To enable such a future by reducing the magnetic field strength,\none key approach will be to improve sampling strategies. Previous work has\nshown that it is possible to make diagnostic decisions directly from k-space\nwith fewer samples. Such work shows that single diagnostic decisions can be\nmade, but if we aspire to see MRI as a true PoC, multiple and sequential\ndecisions are necessary while minimizing the number of samples acquired. We\npresent a novel multi-objective reinforcement learning framework enabling\ncomprehensive, sequential, diagnostic evaluation from undersampled k-space\ndata. Our approach during inference actively adapts to sequential decisions to\noptimally sample. To achieve this, we introduce a training methodology that\nidentifies the samples that contribute the best to each diagnostic objective\nusing a step-wise weighting reward function. We evaluate our approach in two\nsequential knee pathology assessment tasks: ACL sprain detection and cartilage\nthickness loss assessment. Our framework achieves diagnostic performance\ncompetitive with various policy-based benchmarks on disease detection, severity\nquantification, and overall sequential diagnosis, while substantially saving\nk-space samples. Our approach paves the way for the future of MRI as a\ncomprehensive and affordable PoC device. Our code is publicly available at\nhttps://github.com/vios-s/MRI_Sequential_Active_Sampling",
      "url": "http://arxiv.org/abs/2505.04586v1",
      "published_time_eastern_timestamp": 1746638871.0
    },
    {
      "title": "Componential Prompt-Knowledge Alignment for Domain Incremental Learning",
      "summary": "Domain Incremental Learning (DIL) aims to learn from non-stationary data\nstreams across domains while retaining and utilizing past knowledge. Although\nprompt-based methods effectively store multi-domain knowledge in prompt\nparameters and obtain advanced performance through cross-domain prompt fusion,\nwe reveal an intrinsic limitation: component-wise misalignment between\ndomain-specific prompts leads to conflicting knowledge integration and degraded\npredictions. This arises from the random positioning of knowledge components\nwithin prompts, where irrelevant component fusion introduces interference.To\naddress this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a\nnovel prompt-based DIL method that introduces component-aware prompt-knowledge\nalignment during training, significantly improving both the learning and\ninference capacity of the model. KA-Prompt operates in two phases: (1) Initial\nComponential Structure Configuring, where a set of old prompts containing\nknowledge relevant to the new domain are mined via greedy search, which is then\nexploited to initialize new prompts to achieve reusable knowledge transfer and\nestablish intrinsic alignment between new and old prompts. (2) Online Alignment\nPreservation, which dynamically identifies the target old prompts and applies\nadaptive componential consistency constraints as new prompts evolve. Extensive\nexperiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt.\nOur source code is available at\nhttps://github.com/zhoujiahuan1991/ICML2025-KA-Prompt",
      "url": "http://arxiv.org/abs/2505.04575v1",
      "published_time_eastern_timestamp": 1746637935.0
    },
    {
      "title": "From Flowers to Fascism? The Cottagecore to Tradwife Pipeline on Tumblr",
      "summary": "In this work we collected and analyzed social media posts to investigate\naesthetic-based radicalization where users searching for Cottagecore content\nmay find Tradwife content co-opted by white supremacists, white nationalists,\nor other far-right extremist groups. Through quantitative analysis of over\n200,000 Tumblr posts and qualitative coding of about 2,500 Tumblr posts, we did\nnot find evidence of a explicit radicalization. We found that problematic\nTradwife posts found in the literature may be confined to Tradwife-only spaces,\nwhile content in the Cottagecore tag generally did not warrant extra\nmoderation. However, we did find evidence of a mainstreaming effect in the\noverlap between the Tradwife and Cottagecore communities. In our qualitative\nanalysis there was more interaction between queer and Tradwife identities than\nexpected based on the literature, and some Tradwives even explicitly included\nqueer people and disavowed racism in the Tradwife community on Tumblr. This\ncould be genuine, but more likely it was an example of extremists re-branding\ntheir content and following platform norms to spread ideologies that would\notherwise be rejected by Tumblr users. Additionally, through temporal analysis\nwe observed a change in the central tags used by Tradwives in the Cottagecore\ntag pre- and post- 2021. Initially these posts focused on aesthetics and\nhobbies like baking and gardening, but post-2021 the central tags focused more\non religion, traditional gender roles, and homesteading, all markers of\nreactionary ideals.",
      "url": "http://arxiv.org/abs/2505.04561v1",
      "published_time_eastern_timestamp": 1746636658.0
    },
    {
      "title": "ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge\n  Distillation via $α$-$β$-Divergence",
      "summary": "Knowledge Distillation (KD) transfers knowledge from a large teacher model to\na smaller student model by minimizing the divergence between their output\ndistributions, typically using forward Kullback-Leibler divergence (FKLD) or\nreverse KLD (RKLD). It has become an effective training paradigm due to the\nbroader supervision information provided by the teacher distribution compared\nto one-hot labels. We identify that the core challenge in KD lies in balancing\ntwo mode-concentration effects: the \\textbf{\\textit{Hardness-Concentration}}\neffect, which refers to focusing on modes with large errors, and the\n\\textbf{\\textit{Confidence-Concentration}} effect, which refers to focusing on\nmodes with high student confidence. Through an analysis of how probabilities\nare reassigned during gradient updates, we observe that these two effects are\nentangled in FKLD and RKLD, but in extreme forms. Specifically, both are too\nweak in FKLD, causing the student to fail to concentrate on the target class.\nIn contrast, both are too strong in RKLD, causing the student to overly\nemphasize the target class while ignoring the broader distributional\ninformation from the teacher. To address this imbalance, we propose ABKD, a\ngeneric framework with $\\alpha$-$\\beta$-divergence. Our theoretical results\nshow that ABKD offers a smooth interpolation between FKLD and RKLD, achieving\nan effective trade-off between these effects. Extensive experiments on 17\nlanguage/vision datasets with 12 teacher-student settings confirm its efficacy.\nThe code is available at https://github.com/ghwang-s/abkd.",
      "url": "http://arxiv.org/abs/2505.04560v1",
      "published_time_eastern_timestamp": 1746636529.0
    },
    {
      "title": "DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement\n  and Fusion All at Once",
      "summary": "Visible and infrared image fusion is one of the most crucial tasks in the\nfield of image fusion, aiming to generate fused images with clear structural\ninformation and high-quality texture features for high-level vision tasks.\nHowever, when faced with severe illumination degradation in visible images, the\nfusion results of existing image fusion methods often exhibit blurry and dim\nvisual effects, posing major challenges for autonomous driving. To this end, a\nDarkness-Free network is proposed to handle Visible and infrared image\ndisentanglement and fusion all at Once (DFVO), which employs a cascaded\nmulti-task approach to replace the traditional two-stage cascaded training\n(enhancement and fusion), addressing the issue of information entropy loss\ncaused by hierarchical data transmission. Specifically, we construct a\nlatent-common feature extractor (LCFE) to obtain latent features for the\ncascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised\nto acquire high-frequency semantic information. Secondly, we design a hyper\ncross-attention module (HCAM) to extract low-frequency information and preserve\ntexture features from source images. Finally, a relevant loss function is\ndesigned to guide the holistic network learning, thereby achieving better image\nfusion. Extensive experiments demonstrate that our proposed approach\noutperforms state-of-the-art alternatives in terms of qualitative and\nquantitative evaluations. Particularly, DFVO can generate clearer, more\ninformative, and more evenly illuminated fusion results in the dark\nenvironments, achieving best performance on the LLVIP dataset with 63.258 dB\nPSNR and 0.724 CC, providing more effective information for high-level vision\ntasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.",
      "url": "http://arxiv.org/abs/2505.04526v1",
      "published_time_eastern_timestamp": 1746633585.0
    },
    {
      "title": "Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code\n  Development",
      "summary": "Large Language Models (LLM) have significantly transformed various domains,\nincluding software development. These models assist programmers in generating\ncode, potentially increasing productivity and efficiency. However, the\nenvironmental impact of utilising these AI models is substantial, given their\nhigh energy consumption during both training and inference stages. This\nresearch aims to compare the energy consumption of manual software development\nversus an LLM-assisted approach, using Codeforces as a simulation platform for\nsoftware development. The goal is to quantify the environmental impact and\npropose strategies for minimising the carbon footprint of using LLM in software\ndevelopment. Our results show that the LLM-assisted code generation leads on\naverage to 32.72 higher carbon footprint than the manual one. Moreover, there\nis a significant correlation between task complexity and the difference in the\ncarbon footprint of the two approaches.",
      "url": "http://arxiv.org/abs/2505.04521v1",
      "published_time_eastern_timestamp": 1746633126.0
    },
    {
      "title": "HunyuanCustom: A Multimodal-Driven Architecture for Customized Video\n  Generation",
      "summary": "Customized video generation aims to produce videos featuring specific\nsubjects under flexible user-defined conditions, yet existing methods often\nstruggle with identity consistency and limited input modalities. In this paper,\nwe propose HunyuanCustom, a multi-modal customized video generation framework\nthat emphasizes subject consistency while supporting image, audio, video, and\ntext conditions. Built upon HunyuanVideo, our model first addresses the\nimage-text conditioned generation task by introducing a text-image fusion\nmodule based on LLaVA for enhanced multi-modal understanding, along with an\nimage ID enhancement module that leverages temporal concatenation to reinforce\nidentity features across frames. To enable audio- and video-conditioned\ngeneration, we further propose modality-specific condition injection\nmechanisms: an AudioNet module that achieves hierarchical alignment via spatial\ncross-attention, and a video-driven injection module that integrates\nlatent-compressed conditional video through a patchify-based feature-alignment\nnetwork. Extensive experiments on single- and multi-subject scenarios\ndemonstrate that HunyuanCustom significantly outperforms state-of-the-art open-\nand closed-source methods in terms of ID consistency, realism, and text-video\nalignment. Moreover, we validate its robustness across downstream tasks,\nincluding audio and video-driven customized video generation. Our results\nhighlight the effectiveness of multi-modal conditioning and identity-preserving\nstrategies in advancing controllable video generation. All the code and models\nare available at https://hunyuancustom.github.io.",
      "url": "http://arxiv.org/abs/2505.04512v1",
      "published_time_eastern_timestamp": 1746631998.0
    },
    {
      "title": "Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts",
      "summary": "The quality of natural language texts in fine-tuning datasets plays a\ncritical role in the performance of generative models, particularly in\ncomputational creativity tasks such as poem or song lyric generation. Fluency\ndefects in generated poems significantly reduce their value. However, training\ntexts are often sourced from internet-based platforms without stringent quality\ncontrol, posing a challenge for data engineers to manage defect levels\neffectively.\n  To address this issue, we propose the use of automated linguistic anomaly\ndetection to identify and filter out low-quality texts from training datasets\nfor creative models. In this paper, we present a comprehensive comparison of\nunsupervised and supervised text anomaly detection approaches, utilizing both\nsynthetic and human-labeled datasets. We also introduce the RUPOR dataset, a\ncollection of Russian-language human-labeled poems designed for cross-sentence\ngrammatical error detection, and provide the full evaluation code. Our work\naims to empower the community with tools and insights to improve the quality of\ntraining datasets for generative models in creative domains.",
      "url": "http://arxiv.org/abs/2505.04507v1",
      "published_time_eastern_timestamp": 1746631679.0
    },
    {
      "title": "A disturbance in the force. How force fluctuations hinder dynamical\n  friction and induce core stalling",
      "summary": "Dynamical friction is an important phenomena in stellar dynamics resulting in\nthe slowing down of a test particle upon many two-body scatters with background\nparticles. Chandrasekhar's original formulation, developed for idealized\ninfinite and homogeneous systems, has been found to be sufficiently accurate\neven in models of finite extent and radially dependent density profiles.\nHowever, in some cases $N-$body simulations evidenced a breakdown of\nChandrasekhar's formalism. In particular, in the case of cored stellar systems,\nthe analytical predictions underestimate the rate of in-fall of the test\nparticle. Several explanations for such discrepancy have been proposed so far,\nin spite of this it remains unclear whether the origin is a finite N effect or\nan effect arising from the resonance of the orbits of the test and field\nparticles, which is independent on $N$, such as dynamical buoyancy. Here we aim\nat shedding some light on this issue with tailored numerical experiments. We\nperform ad hoc simulations of a massive tracer initially placed on a low\neccentricity orbit in spherical equilibrium models with increasing resolution.\nWe use an $N-$body code where the self-consistent interaction among the\nbackground particles can be substituted with the effect of the static smooth\npotential of the system's continuum limit, so that the higher order\ncontributions to the dynamical friction arising from the formation of a wake\ncan be neglected if needed. We find that, contrary to what reported in the\nprevious literature, a suppression of dynamical friction happens in both cuspy\nand cored models. When neglecting the interaction among field particles we\nobserve in both cases a clear $N^{-1/2}$ scaling of the radius at which\ndynamical friction ceases to be effective. This hints towards a\ngranularity-induced origin of the so-called core-stalling of the massive tracer\nin cored models.",
      "url": "http://arxiv.org/abs/2505.04505v1",
      "published_time_eastern_timestamp": 1746631559.0
    },
    {
      "title": "VeriFast's separation logic: a higher-order(ish) logic without laters\n  for modular verification of fine-grained concurrent programs",
      "summary": "VeriFast is one of the leading tools for semi-automated modular formal\nprogram verification. A central feature of VeriFast is its support for\nhigher-order ghost code, which enables its support for expressively specifying\nfine-grained concurrent modules, without the need for a later modality. We\npresent the first formalization and soundness proof for this aspect of\nVeriFast's logic.",
      "url": "http://arxiv.org/abs/2505.04500v1",
      "published_time_eastern_timestamp": 1746631262.0
    },
    {
      "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design\n  Parametric 3D Model Generation",
      "summary": "Recently, Large Language Models (LLMs) have achieved significant success,\nprompting increased interest in expanding their generative capabilities beyond\ngeneral text into domain-specific areas. This study investigates the generation\nof parametric sequences for computer-aided design (CAD) models using LLMs. This\nendeavor represents an initial step towards creating parametric 3D shapes with\nLLMs, as CAD model parameters directly correlate with shapes in\nthree-dimensional space. Despite the formidable generative capacities of LLMs,\nthis task remains challenging, as these models neither encounter parametric\nsequences during their pretraining phase nor possess direct awareness of 3D\nstructures. To address this, we present CAD-Llama, a framework designed to\nenhance pretrained LLMs for generating parametric 3D CAD models. Specifically,\nwe develop a hierarchical annotation pipeline and a code-like format to\ntranslate parametric 3D CAD command sequences into Structured Parametric CAD\nCode (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we\npropose an adaptive pretraining approach utilizing SPCC, followed by an\ninstruction tuning process aligned with CAD-specific guidelines. This\nmethodology aims to equip LLMs with the spatial knowledge inherent in\nparametric sequences. Experimental results demonstrate that our framework\nsignificantly outperforms prior autoregressive methods and existing LLM\nbaselines.",
      "url": "http://arxiv.org/abs/2505.04481v1",
      "published_time_eastern_timestamp": 1746629522.0
    },
    {
      "title": "TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven\n  Evolution",
      "summary": "Trajectory prediction is a crucial task in modeling human behavior,\nespecially in fields as social robotics and autonomous vehicle navigation.\nTraditional heuristics based on handcrafted rules often lack accuracy, while\nrecently proposed deep learning approaches suffer from computational cost, lack\nof explainability, and generalization issues that limit their practical\nadoption. In this paper, we introduce TrajEvo, a framework that leverages Large\nLanguage Models (LLMs) to automatically design trajectory prediction\nheuristics. TrajEvo employs an evolutionary algorithm to generate and refine\nprediction heuristics from past trajectory data. We introduce a\nCross-Generation Elite Sampling to promote population diversity and a\nStatistics Feedback Loop allowing the LLM to analyze alternative predictions.\nOur evaluations show TrajEvo outperforms previous heuristic methods on the\nETH-UCY datasets, and remarkably outperforms both heuristics and deep learning\nmethods when generalizing to the unseen SDD dataset. TrajEvo represents a first\nstep toward automated design of fast, explainable, and generalizable trajectory\nprediction heuristics. We make our source code publicly available to foster\nfuture research at https://github.com/ai4co/trajevo.",
      "url": "http://arxiv.org/abs/2505.04480v1",
      "published_time_eastern_timestamp": 1746629503.0
    },
    {
      "title": "Image Steganography For Securing Intellicise Wireless Networks:\n  \"Invisible Encryption\" Against Eavesdroppers",
      "summary": "As one of the most promising technologies for intellicise (intelligent and\nconsice) wireless networks, Semantic Communication (SemCom) significantly\nimproves communication efficiency by extracting, transmitting, and recovering\nsemantic information, while reducing transmission delay. However, an\nintegration of communication and artificial intelligence (AI) also exposes\nSemCom to security and privacy threats posed by intelligent eavesdroppers. To\naddress this challenge, image steganography in SemCom embeds secret semantic\nfeatures within cover semantic features, allowing intelligent eavesdroppers to\ndecode only the cover image. This technique offers a form of \"invisible\nencryption\" for SemCom. Motivated by these advancements, this paper conducts a\ncomprehensive exploration of integrating image steganography into SemCom.\nFirstly, we review existing encryption techniques in SemCom and assess the\npotential of image steganography in enhancing its security. Secondly, we delve\ninto various image steganographic paradigms designed to secure SemCom,\nencompassing three categories of joint source-channel coding (JSCC) models\ntailored for image steganography SemCom, along with multiple training\nstrategies. Thirdly, we present a case study to illustrate the effectiveness of\ncoverless steganography SemCom. Finally, we propose future research directions\nfor image steganography SemCom.",
      "url": "http://arxiv.org/abs/2505.04467v1",
      "published_time_eastern_timestamp": 1746628695.0
    },
    {
      "title": "M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation",
      "summary": "Sequential recommendation systems aim to predict users' next preferences\nbased on their interaction histories, but existing approaches face critical\nlimitations in efficiency and multi-scale pattern recognition. While\nTransformer-based methods struggle with quadratic computational complexity,\nrecent Mamba-based models improve efficiency but fail to capture periodic user\nbehaviors, leverage rich semantic information, or effectively fuse multimodal\nfeatures. To address these challenges, we propose \\model, a novel sequential\nrecommendation framework that integrates multi-scale Mamba with Fourier\nanalysis, Large Language Models (LLMs), and adaptive gating. First, we enhance\nMamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns\nin the frequency domain, separating meaningful trends from noise. Second, we\nincorporate LLM-based text embeddings to enrich sparse interaction data with\nsemantic context from item descriptions. Finally, we introduce a learnable gate\nmechanism to dynamically balance temporal (Mamba), frequency (FFT), and\nsemantic (LLM) features, ensuring harmonious multimodal fusion. Extensive\nexperiments demonstrate that \\model\\ achieves state-of-the-art performance,\nimproving Hit Rate@10 by 3.2\\% over existing Mamba-based models while\nmaintaining 20\\% faster inference than Transformer baselines. Our results\nhighlight the effectiveness of combining frequency analysis, semantic\nunderstanding, and adaptive fusion for sequential recommendation. Code and\ndatasets are available at: https://anonymous.4open.science/r/M2Rec.",
      "url": "http://arxiv.org/abs/2505.04445v1",
      "published_time_eastern_timestamp": 1746627269.0
    },
    {
      "title": "Towards Effectively Leveraging Execution Traces for Program Repair with\n  Code LLMs",
      "summary": "Large Language Models (LLMs) show promising performance on various\nprogramming tasks, including Automatic Program Repair (APR). However, most\napproaches to LLM-based APR are limited to the static analysis of the programs,\nwhile disregarding their runtime behavior. Inspired by knowledge-augmented NLP,\nin this work, we aim to remedy this potential blind spot by augmenting standard\nAPR prompts with program execution traces. We evaluate our approach using the\nGPT family of models on three popular APR datasets. Our findings suggest that\nsimply incorporating execution traces into the prompt provides a limited\nperformance improvement over trace-free baselines, in only 2 out of 6 tested\ndataset / model configurations. We further find that the effectiveness of\nexecution traces for APR diminishes as their complexity increases. We explore\nseveral strategies for leveraging traces in prompts and demonstrate that\nLLM-optimized prompts help outperform trace-free prompts more consistently.\nAdditionally, we show trace-based prompting to be superior to finetuning a\nsmaller LLM on a small-scale dataset; and conduct probing studies reinforcing\nthe notion that execution traces can complement the reasoning abilities of the\nLLMs.",
      "url": "http://arxiv.org/abs/2505.04441v1",
      "published_time_eastern_timestamp": 1746627161.0
    },
    {
      "title": "RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential\n  Neural Style Generation",
      "summary": "Arbitrary style transfer aims to apply the style of any given artistic image\nto another content image. Still, existing deep learning-based methods often\nrequire significant computational costs to generate diverse stylized results.\nMotivated by this, we propose a novel reinforcement learning-based framework\nfor arbitrary style transfer RLMiniStyler. This framework leverages a unified\nreinforcement learning policy to iteratively guide the style transfer process\nby exploring and exploiting stylization feedback, generating smooth sequences\nof stylized results while achieving model lightweight. Furthermore, we\nintroduce an uncertainty-aware multi-task learning strategy that automatically\nadjusts loss weights to adapt to the content and style balance requirements at\ndifferent training stages, thereby accelerating model convergence. Through a\nseries of experiments across image various resolutions, we have validated the\nadvantages of RLMiniStyler over other state-of-the-art methods in generating\nhigh-quality, diverse artistic image sequences at a lower cost. Codes are\navailable at https://github.com/fengxiaoming520/RLMiniStyler.",
      "url": "http://arxiv.org/abs/2505.04424v1",
      "published_time_eastern_timestamp": 1746626262.0
    },
    {
      "title": "Latent Manifold Reconstruction and Representation with Topological and\n  Geometrical Regularization",
      "summary": "Manifold learning aims to discover and represent low-dimensional structures\nunderlying high-dimensional data while preserving critical topological and\ngeometric properties. Existing methods often fail to capture local details with\nglobal topological integrity from noisy data or construct a balanced\ndimensionality reduction, resulting in distorted or fractured embeddings. We\npresent an AutoEncoder-based method that integrates a manifold reconstruction\nlayer, which uncovers latent manifold structures from noisy point clouds, and\nfurther provides regularizations on topological and geometric properties during\ndimensionality reduction, whereas the two components promote each other during\ntraining. Experiments on point cloud datasets demonstrate that our method\noutperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in\ndiscovering manifold structures from noisy data and preserving them through\ndimensionality reduction, as validated by visualization and quantitative\nmetrics. This work demonstrates the significance of combining manifold\nreconstruction with manifold learning to achieve reliable representation of the\nlatent manifold, particularly when dealing with noisy real-world data. Code\nrepository: https://github.com/Thanatorika/mrtg.",
      "url": "http://arxiv.org/abs/2505.04412v1",
      "published_time_eastern_timestamp": 1746625642.0
    },
    {
      "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
      "summary": "Dense visual prediction tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense prediction often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. The ``content'' features are aligned with image crop\nrepresentations to improve local discriminability, while ``context'' features\nlearn to retain the spatial correlations under the guidance of vision\nfoundation models, such as DINO. Extensive experiments demonstrate that DeCLIP\nsignificantly outperforms existing methods across multiple open-vocabulary\ndense prediction tasks, including object detection and semantic segmentation.\nCode is available at \\textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.",
      "url": "http://arxiv.org/abs/2505.04410v1",
      "published_time_eastern_timestamp": 1746625594.0
    }
  ]
}
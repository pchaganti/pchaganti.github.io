{
  "last_updated": "2025-05-12T14:16:26.874103-04:00",
  "papers": [
    {
      "title": "Anymate: A Dataset and Baselines for Learning 3D Object Rigging",
      "summary": "Rigging and skinning are essential steps to create realistic 3D animations,\noften requiring significant expertise and manual effort. Traditional attempts\nat automating these processes rely heavily on geometric heuristics and often\nstruggle with objects of complex geometry. Recent data-driven approaches show\npotential for better generality, but are often constrained by limited training\ndata. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets\npaired with expert-crafted rigging and skinning information -- 70 times larger\nthan existing datasets. Using this dataset, we propose a learning-based\nauto-rigging framework with three sequential modules for joint, connectivity,\nand skinning weight prediction. We systematically design and experiment with\nvarious architectures as baselines for each module and conduct comprehensive\nevaluations on our dataset to compare their performance. Our models\nsignificantly outperform existing methods, providing a foundation for comparing\nfuture methods in automated rigging and skinning. Code and dataset can be found\nat https://anymate3d.github.io/.",
      "url": "http://arxiv.org/abs/2505.06227v1",
      "published_time_eastern_timestamp": 1746813573.0
    },
    {
      "title": "From Fermions to Qubits: A ZX-Calculus Perspective",
      "summary": "Mapping fermionic systems to qubits on a quantum computer is often the first\nstep for algorithms in quantum chemistry and condensed matter physics. However,\nit is difficult to reconcile the many different approaches that have been\nproposed, such as those based on binary matrices, ternary trees, and stabilizer\ncodes. This challenge is further exacerbated by the many ways to describe them\n-- transformation of Majorana operators, action on Fock states, encoder\ncircuits, and stabilizers of local encodings -- making it challenging to know\nwhen the mappings are equivalent. In this work, we present a graphical\nframework for fermion-to-qubit mappings that streamlines and unifies various\nrepresentations through the ZX-calculus.\n  To start, we present the correspondence between linear encodings of the Fock\nbasis and phase-free ZX-diagrams. The commutation rules of scalable ZX-calculus\nallows us to convert the fermionic operators to Pauli operators under any\nlinear encoding. Next, we give a translation from ternary tree mappings to\nscalable ZX-diagrams, which not only directly represents the encoder map as a\nCNOT circuit, but also retains the same structure as the tree. Consequently, we\ngraphically prove that ternary tree transformations are equivalent to linear\nencodings, a recent result by Chiew et al. The scalable ZX representation\nmoreover enables us to construct an algorithm to directly compute the binary\nmatrix for any ternary tree mapping. Lastly, we present the graphical\nrepresentation of local fermion-to-qubit encodings. Its encoder ZX-diagram has\nthe same connectivity as the interaction graph of the fermionic Hamiltonian and\nalso allows us to easily identify stabilizers of the encoding.",
      "url": "http://arxiv.org/abs/2505.06212v1",
      "published_time_eastern_timestamp": 1746812521.0
    },
    {
      "title": "Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp\n  Segmentation",
      "summary": "Convolutional neural network (CNN) and Transformer-based architectures are\ntwo dominant deep learning models for polyp segmentation. However, CNNs have\nlimited capability for modeling long-range dependencies, while Transformers\nincur quadratic computational complexity. Recently, State Space Models such as\nMamba have been recognized as a promising approach for polyp segmentation\nbecause they not only model long-range interactions effectively but also\nmaintain linear computational complexity. However, Mamba-based architectures\nstill struggle to capture topological features (e.g., connected components,\nloops, voids), leading to inaccurate boundary delineation and polyp\nsegmentation. To address these limitations, we propose a new approach called\nTopo-VM-UNetV2, which encodes topological features into the Mamba-based\nstate-of-the-art polyp segmentation model, VM-UNetV2. Our method consists of\ntwo stages: Stage 1: VM-UNetV2 is used to generate probability maps (PMs) for\nthe training and test images, which are then used to compute topology attention\nmaps. Specifically, we first compute persistence diagrams of the PMs, then we\ngenerate persistence score maps by assigning persistence values (i.e., the\ndifference between death and birth times) of each topological feature to its\nbirth location, finally we transform persistence scores into attention weights\nusing the sigmoid function. Stage 2: These topology attention maps are\nintegrated into the semantics and detail infusion (SDI) module of VM-UNetV2 to\nform a topology-guided semantics and detail infusion (Topo-SDI) module for\nenhancing the segmentation results. Extensive experiments on five public polyp\nsegmentation datasets demonstrate the effectiveness of our proposed method. The\ncode will be made publicly available.",
      "url": "http://arxiv.org/abs/2505.06210v1",
      "published_time_eastern_timestamp": 1746812473.0
    },
    {
      "title": "Constructing All Birthday 3 Games as Digraphs",
      "summary": "Recently, Clow and McKay proved that the Digraph Placement ruleset is\nuniversal for normal play: for all normal play combinatorial games $X$, there\nis a Digraph Placement game $G$ with $G=X$. Clow and McKay also showed that the\n22 game values born by day 2 correspond to Digraph Placement games with at most\n4 vertices. This bound is best possible. We extend this work using a\ncombination of exhaustive and random searches to demonstrate all 1474 values\nborn by day 3 correspond to Digraph Placement games on at most 8 vertices. We\nprovide a combinatorial proof that this bound is best possible. We conclude by\ngiving improved bounds on the number of vertices required to construct all game\nvalues born by days 4 and 5.",
      "url": "http://arxiv.org/abs/2505.06206v1",
      "published_time_eastern_timestamp": 1746811956.0
    },
    {
      "title": "Decoding Algorithms for Two-dimensional Constacyclic Codes over\n  $\\mathbb{F}_q$",
      "summary": "We derive the spectral domain properties of two-dimensional (2-D)\n$(\\lambda_1, \\lambda_2)$-constacyclic codes over $\\mathbb{F}_q$ using the 2-D\nfinite field Fourier transform (FFFT). Based on the spectral nulls of 2-D\n$(\\lambda_1, \\lambda_2)$-constacyclic codes, we characterize the structure of\n2-D constacyclic coded arrays. The proposed 2-D construction has flexible code\nrates and works for any code areas, be it odd or even area. We present an\nalgorithm to detect the location of 2-D errors. Further, we also propose\ndecoding algorithms for extracting the error values using both time and\nfrequency domain properties by exploiting the sparsity that arises due to\nduality in the time and frequency domains. Through several illustrative\nexamples, we demonstrate the working of the proposed decoding algorithms.",
      "url": "http://arxiv.org/abs/2505.06201v1",
      "published_time_eastern_timestamp": 1746811731.0
    },
    {
      "title": "On Optimal Batch Size in Coded Computing",
      "summary": "We consider computing systems that partition jobs into tasks, add redundancy\nthrough coding, and assign the encoded tasks to different computing nodes for\nparallel execution. The expected execution time depends on the level of\nredundancy. The computing nodes execute large jobs in batches of tasks. We show\nthat the expected execution time depends on the batch size as well. The optimal\nbatch size that minimizes the execution time depends on the level of redundancy\nunder a fixed number of parallel servers and other system parameters.\nFurthermore, we show how to (jointly) optimize the redundancy level and batch\nsize to reduce the expected job completion time for two service-time\ndistributions. The simulation presented helps us appreciate the claims.",
      "url": "http://arxiv.org/abs/2505.06199v1",
      "published_time_eastern_timestamp": 1746811538.0
    },
    {
      "title": "An Empirical Study of Fuzz Harness Degradation",
      "summary": "The purpose of continuous fuzzing platforms is to enable fuzzing for software\nprojects via \\emph{fuzz harnesses} -- but as the projects continue to evolve,\nare these harnesses updated in lockstep, or do they run out of date? If these\nharnesses remain unmaintained, will they \\emph{degrade} over time in terms of\ncoverage achieved or number of bugs found? This is the subject of our study.\n  We study Google's OSS-Fuzz continuous fuzzing platform containing harnesses\nfor 510 open-source C/C++ projects, many of which are security-critical. A\nharness is the glue code between the fuzzer and the project, so it needs to\nadapt to changes in the project. It is often added by a project maintainer or\nas part of a, sometimes short-lived, testing effort.\n  Our analysis shows a consistent overall fuzzer coverage percentage for\nprojects in OSS-Fuzz and a surprising longevity of the bug-finding capability\nof harnesses even without explicit updates, as long as they still build.\nHowever, we also identify and manually examine individual cases of harness\ncoverage degradation and categorize their root causes. Furthermore, we\ncontribute to OSS-Fuzz and Fuzz Introspector to support metrics to detect\nharness degradation in OSS-Fuzz projects guided by this research.",
      "url": "http://arxiv.org/abs/2505.06177v1",
      "published_time_eastern_timestamp": 1746808760.0
    },
    {
      "title": "MonetGPT: Solving Puzzles Enhances MLLMs' Image Retouching Skills",
      "summary": "Retouching is an essential task in post-manipulation of raw photographs.\nGenerative editing, guided by text or strokes, provides a new tool accessible\nto users but can easily change the identity of the original objects in\nunacceptable and unpredictable ways. In contrast, although traditional\nprocedural edits, as commonly supported by photoediting tools (e.g., Gimp,\nLightroom), are conservative, they are still preferred by professionals.\nUnfortunately, professional quality retouching involves many individual\nprocedural editing operations that is challenging to plan for most novices. In\nthis paper, we ask if a multimodal large language model (MLLM) can be taught to\ncritique raw photographs, suggest suitable remedies, and finally realize them\nwith a given set of pre-authored procedural image operations. We demonstrate\nthat MLLMs can be first made aware of the underlying image processing\noperations, by training them to solve specially designed visual puzzles.\nSubsequently, such an operation-aware MLLM can both plan and propose edit\nsequences. To facilitate training, given a set of expert-edited photos, we\nsynthesize a reasoning dataset by procedurally manipulating the expert edits\nand then grounding a pretrained LLM on the visual adjustments, to synthesize\nreasoning for finetuning. The proposed retouching operations are, by\nconstruction, understandable by the users, preserve object details and\nresolution, and can be optionally overridden. We evaluate our setup on a\nvariety of test examples and show advantages, in terms of explainability and\nidentity preservation, over existing generative and other procedural\nalternatives. Code, data, models, and supplementary results can be found via\nour project website at https://monetgpt.github.io.",
      "url": "http://arxiv.org/abs/2505.06176v1",
      "published_time_eastern_timestamp": 1746808707.0
    },
    {
      "title": "Turbo-ICL: In-Context Learning-Based Turbo Equalization",
      "summary": "This paper introduces a novel in-context learning (ICL) framework, inspired\nby large language models (LLMs), for soft-input soft-output channel\nequalization in coded multiple-input multiple-output (MIMO) systems. The\nproposed approach learns to infer posterior symbol distributions directly from\na prompt of pilot signals and decoder feedback. A key innovation is the use of\nprompt augmentation to incorporate extrinsic information from the decoder\noutput as additional context, enabling the ICL model to refine its symbol\nestimates iteratively across turbo decoding iterations. Two model variants,\nbased on Transformer and state-space architectures, are developed and\nevaluated. Extensive simulations demonstrate that, when traditional linear\nassumptions break down, e.g., in the presence of low-resolution quantization,\nICL equalizers consistently outperform conventional model-based baselines, even\nwhen the latter are provided with perfect channel state information. Results\nalso highlight the advantage of Transformer-based models under limited training\ndiversity, as well as the efficiency of state-space models in\nresource-constrained scenarios.",
      "url": "http://arxiv.org/abs/2505.06175v1",
      "published_time_eastern_timestamp": 1746808169.0
    },
    {
      "title": "Leakage-resilient Algebraic Manipulation Detection Codes with Optimal\n  Parameters",
      "summary": "Algebraic Manipulation Detection (AMD) codes is a cryptographic primitive\nthat was introduced by Cramer, Dodis, Fehr, Padro and Wichs. They are keyless\nmessage authentication codes that protect messages against additive tampering\nby the adversary assuming that the adversary cannot \"see\" the codeword. For\ncertain applications, it is unreasonable to assume that the adversary computes\nthe added offset without any knowledge of the codeword c. Recently, Ahmadi and\nSafavi-Naini, and then Lin, Safavi-Naini, and Wang gave a construction of\nleakage-resilient AMD codes where the adversary has some partial information\nabout the codeword before choosing added offset, and the scheme is secure even\nconditioned on this partial information. In this paper we establish bounds on\nthe leakage rate r and the code rate k for leakage-resilient AMD codes. In\nparticular we prove that 2r + k < 1 and for the weak case (security is averaged\nover a uniformly random message) r + k < 1. These bounds hold even if adversary\nis polynomial-time bounded, as long as we allow leakage function to be\narbitrary. We present constructions of AMD codes that (asymptotically) fulfill\nthe above bounds for almost full range of parameters r and k. This shows that\nthe above bounds and constructions are in-fact optimal. In the last section we\nshow that if a leakage function is computationally bounded (we use the Ideal\nCipher Model) then it is possible to break these bounds.",
      "url": "http://arxiv.org/abs/2505.06174v1",
      "published_time_eastern_timestamp": 1746808117.0
    },
    {
      "title": "DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models",
      "summary": "We address the task of generating 3D hair geometry from a single image, which\nis challenging due to the diversity of hairstyles and the lack of paired\nimage-to-3D hair data. Previous methods are primarily trained on synthetic data\nand cope with the limited amount of such data by using low-dimensional\nintermediate representations, such as guide strands and scalp-level embeddings,\nthat require post-processing to decode, upsample, and add realism. These\napproaches fail to reconstruct detailed hair, struggle with curly hair, or are\nlimited to handling only a few hairstyles. To overcome these limitations, we\npropose DiffLocks, a novel framework that enables detailed reconstruction of a\nwide variety of hairstyles directly from a single image. First, we address the\nlack of 3D hair data by automating the creation of the largest synthetic hair\ndataset to date, containing 40K hairstyles. Second, we leverage the synthetic\nhair dataset to learn an image-conditioned diffusion-transfomer model that\ngenerates accurate 3D strands from a single frontal image. By using a\npretrained image backbone, our method generalizes to in-the-wild images despite\nbeing trained only on synthetic data. Our diffusion model predicts a scalp\ntexture map in which any point in the map contains the latent code for an\nindividual hair strand. These codes are directly decoded to 3D strands without\npost-processing techniques. Representing individual strands, instead of guide\nstrands, enables the transformer to model the detailed spatial structure of\ncomplex hairstyles. With this, DiffLocks can recover highly curled hair, like\nafro hairstyles, from a single image for the first time. Data and code is\navailable at https://radualexandru.github.io/difflocks/",
      "url": "http://arxiv.org/abs/2505.06166v1",
      "published_time_eastern_timestamp": 1746807402.0
    },
    {
      "title": "Optimization of Quantum Error Correcting Code under Temporal Variation\n  of Qubit Quality",
      "summary": "Error rates in current noisy quantum hardware are not static; they vary over\ntime and across qubits. This temporal and spatial variation challenges the\neffectiveness of fixed-distance quantum error correction (QEC) codes. In this\npaper, we analyze 12 days of calibration data from IBM's 127-qubit device\n(ibm_kyiv), showing the fluctuation of Pauli-X and CNOT gate error rates. We\ndemonstrate that fixed-distance QEC can either underperform or lead to\nexcessive overhead, depending on the selected qubit and the error rate of the\nday. We then propose a simple adaptive QEC approach that selects an appropriate\ncode distance per qubit, based on daily error rates. Using logical error rate\nmodeling, we identify qubits that cannot be used and qubits that can be\nrecovered with minimal resources. Our method avoids unnecessary resource\noverhead by excluding outlier qubits and tailoring code distances. Across 12\ncalibration days on ibm_kyiv, our adaptive strategy reduces physical qubit\noverhead by over 50% per logical qubit while maintaining access to 85-100% of\nusable qubits. To further validate the method, we repeat the experiment on two\nadditional 127-qubit devices, ibm_brisbane and ibm_sherbrooke, where the\noverhead savings reach up to 71% while still preserving over 80% qubit\nusability. This approach offers a practical and efficient path forward for\nNoisy Intermediate-Scale Quantum (NISQ)-era QEC strategies.",
      "url": "http://arxiv.org/abs/2505.06165v1",
      "published_time_eastern_timestamp": 1746807317.0
    },
    {
      "title": "Advancing Finite-Length Quantum Error Correction Using Generalized\n  Bicycle Codes",
      "summary": "Generalized bicycle (GB) codes have emerged as a promising class of quantum\nerror-correcting codes with practical decoding capabilities. While numerous\nasymptotically good quantum codes and quantum low-density parity-check code\nconstructions have been proposed, their finite block-length performance often\nremains unquantified. In this work, we demonstrate that GB codes exhibit\ncomparable or superior error correction performance in finite-length settings,\nparticularly when designed with higher or unrestricted row weights. Leveraging\ntheir flexible construction, GB codes can be tailored to achieve high rates\nwhile maintaining efficient decoding. We evaluate GB codes against other\nleading quantum code families, such as quantum Tanner codes and\nsingle-parity-check product codes, highlighting their versatility in practical\nfinite-length applications.",
      "url": "http://arxiv.org/abs/2505.06157v1",
      "published_time_eastern_timestamp": 1746806882.0
    },
    {
      "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
      "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
      "url": "http://arxiv.org/abs/2505.06111v1",
      "published_time_eastern_timestamp": 1746803473.0
    },
    {
      "title": "Parameter-Free Segmentation of Robot Movements with Cross-Correlation\n  Using Different Similarity Metrics",
      "summary": "Often, robots are asked to execute primitive movements, whether as a single\naction or in a series of actions representing a larger, more complex task.\nThese movements can be learned in many ways, but a common one is from\ndemonstrations presented to the robot by a teacher. However, these\ndemonstrations are not always simple movements themselves, and complex\ndemonstrations must be broken down, or segmented, into primitive movements. In\nthis work, we present a parameter-free approach to segmentation using\ntechniques inspired by autocorrelation and cross-correlation from signal\nprocessing. In cross-correlation, a representative signal is found in some\nlarger, more complex signal by correlating the representative signal with the\nlarger signal. This same idea can be applied to segmenting robot motion and\ndemonstrations, provided with a representative motion primitive. This results\nin a fast and accurate segmentation, which does not take any parameters. One of\nthe main contributions of this paper is the modification of the\ncross-correlation process by employing similarity metrics that can capture\nfeatures specific to robot movements. To validate our framework, we conduct\nseveral experiments of complex tasks both in simulation and in real-world. We\nalso evaluate the effectiveness of our segmentation framework by comparing\nvarious similarity metrics.",
      "url": "http://arxiv.org/abs/2505.06100v1",
      "published_time_eastern_timestamp": 1746802062.0
    },
    {
      "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog\n  Generation using LLMs",
      "summary": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%.",
      "url": "http://arxiv.org/abs/2505.06096v1",
      "published_time_eastern_timestamp": 1746801847.0
    },
    {
      "title": "Robot Learning Using Multi-Coordinate Elastic Maps",
      "summary": "To learn manipulation skills, robots need to understand the features of those\nskills. An easy way for robots to learn is through Learning from Demonstration\n(LfD), where the robot learns a skill from an expert demonstrator. While the\nmain features of a skill might be captured in one differential coordinate\n(i.e., Cartesian), they could have meaning in other coordinates. For example,\nan important feature of a skill may be its shape or velocity profile, which are\ndifficult to discover in Cartesian differential coordinate. In this work, we\npresent a method which enables robots to learn skills from human demonstrations\nvia encoding these skills into various differential coordinates, then\ndetermines the importance of each coordinate to reproduce the skill. We also\nintroduce a modified form of Elastic Maps that includes multiple differential\ncoordinates, combining statistical modeling of skills in these differential\ncoordinate spaces. Elastic Maps, which are flexible and fast to compute, allow\nfor the incorporation of several different types of constraints and the use of\nany number of demonstrations. Additionally, we propose methods for auto-tuning\nseveral parameters associated with the modified Elastic Map formulation. We\nvalidate our approach in several simulated experiments and a real-world writing\ntask with a UR5e manipulator arm.",
      "url": "http://arxiv.org/abs/2505.06092v1",
      "published_time_eastern_timestamp": 1746801526.0
    },
    {
      "title": "Algebraic Topology Principles behind Topological Quantum Error\n  Correction",
      "summary": "Quantum error correction (QEC) is crucial for numerous quantum applications,\nincluding fault-tolerant quantum computation, which is of great scientific and\nindustrial interest. Among various QEC paradigms, topological quantum error\ncorrection (TQEC) has attained the most experimental successes by far. In this\npaper, we build upon existing knowledge of TQEC by developing a generalized\ntheoretical framework of TQEC. We begin by formally defining TQEC codes and\nexploring the algebraic topological principles underlying these quantum codes,\nincluding deriving the conditions for any topological manifold to serve as a\nquantum memory. We show that TQEC for qubits works for both orientable and\nnon-orientable manifolds. Moreover, we extend the construction of TQEC to\nhigher-dimensional manifolds and provide examples for higher-dimensional TQEC\ncodes. Finally, we apply these principles to construct new codes on\n2-dimensional manifolds that have received limited attention in prior\nliterature. As a case study, we simulate the performance of TQEC codes on the\nKlein bottle $K$ and evaluate their efficacy for quantum error correction. This\nwork contributes to the advancement of TQEC by proposing a broader class of\ncodes and demonstrating their theoretical and practical potential. By\naddressing previously unexplored topological structures, our findings represent\na step forward in achieving fault-tolerant quantum computation and\ncommunication.",
      "url": "http://arxiv.org/abs/2505.06082v1",
      "published_time_eastern_timestamp": 1746800797.0
    },
    {
      "title": "Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and\n  Segmentation",
      "summary": "Deep learning has revolutionized medical image segmentation, yet its full\npotential remains constrained by the paucity of annotated datasets. While\ndiffusion models have emerged as a promising approach for generating synthetic\nimage-mask pairs to augment these datasets, they paradoxically suffer from the\nsame data scarcity challenges they aim to mitigate. Traditional mask-only\nmodels frequently yield low-fidelity images due to their inability to\nadequately capture morphological intricacies, which can critically compromise\nthe robustness and reliability of segmentation models. To alleviate this\nlimitation, we introduce Siamese-Diffusion, a novel dual-component model\ncomprising Mask-Diffusion and Image-Diffusion. During training, a Noise\nConsistency Loss is introduced between these components to enhance the\nmorphological fidelity of Mask-Diffusion in the parameter space. During\nsampling, only Mask-Diffusion is used, ensuring diversity and scalability.\nComprehensive experiments demonstrate the superiority of our method.\nSiamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps,\nwhile UNet improves by 1.52% and 1.64% on the ISIC2018. Code is available at\nGitHub.",
      "url": "http://arxiv.org/abs/2505.06068v1",
      "published_time_eastern_timestamp": 1746799647.0
    },
    {
      "title": "Context Informed Incremental Learning Improves Myoelectric Control\n  Performance in Virtual Reality Object Manipulation Tasks",
      "summary": "Electromyography (EMG)-based gesture recognition is a promising approach for\ndesigning intuitive human-computer interfaces. However, while these systems\ntypically perform well in controlled laboratory settings, their usability in\nreal-world applications is compromised by declining performance during\nreal-time control. This decline is largely due to goal-directed behaviors that\nare not captured in static, offline scenarios. To address this issue, we use\n\\textit{Context Informed Incremental Learning} (CIIL) - marking its first\ndeployment in an object-manipulation scenario - to continuously adapt the\nclassifier using contextual cues. Nine participants without upper limb\ndifferences completed a functional task in a virtual reality (VR) environment\ninvolving transporting objects with life-like grips. We compared two scenarios:\none where the classifier was adapted in real-time using contextual information,\nand the other using a traditional open-loop approach without adaptation. The\nCIIL-based approach not only enhanced task success rates and efficiency, but\nalso reduced the perceived workload by 7.1 %, despite causing a 5.8 % reduction\nin offline classification accuracy. This study highlights the potential of\nreal-time contextualized adaptation to enhance user experience and usability of\nEMG-based systems for practical, goal-oriented applications, crucial elements\ntowards their long-term adoption. The source code for this study is available\nat: https://github.com/BiomedicalITS/ciil-emg-vr.",
      "url": "http://arxiv.org/abs/2505.06064v1",
      "published_time_eastern_timestamp": 1746799205.0
    }
  ]
}
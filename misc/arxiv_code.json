{
  "last_updated": "2025-09-17T17:09:54.587119-04:00",
  "papers": [
    {
      "title": "Temporally Smooth Mesh Extraction for Procedural Scenes with Long-Range\n  Camera Trajectories using Spacetime Octrees",
      "summary": "The procedural occupancy function is a flexible and compact representation\nfor creating 3D scenes. For rasterization and other tasks, it is often\nnecessary to extract a mesh that represents the shape. Unbounded scenes with\nlong-range camera trajectories, such as flying through a forest, pose a unique\nchallenge for mesh extraction. A single static mesh representing all the\ngeometric detail necessary for the full camera path can be prohibitively large.\nTherefore, independent meshes can be extracted for different camera views, but\nthis approach may lead to popping artifacts during transitions. We propose a\ntemporally coherent method for extracting meshes suitable for long-range camera\ntrajectories in unbounded scenes represented by an occupancy function. The key\nidea is to perform 4D mesh extraction using a new spacetime tree structure\ncalled a binary-octree. Experiments show that, compared to existing baseline\nmethods, our method offers superior visual consistency at a comparable cost.\nThe code and the supplementary video for this paper are available at\nhttps://github.com/princeton-vl/BinocMesher.",
      "url": "http://arxiv.org/abs/2509.13306v1",
      "published_time_eastern_timestamp": 1758045424.0
    },
    {
      "title": "Towards an Embodied Composition Framework for Organizing Immersive\n  Computational Notebooks",
      "summary": "As immersive technologies evolve, immersive computational notebooks offer new\nopportunities for interacting with code, data, and outputs. However, scaling\nthese environments remains a challenge, particularly when analysts manually\narrange large numbers of cells to maintain both execution logic and visual\ncoherence. To address this, we introduce an embodied composition framework,\nfacilitating organizational processes in the context of immersive computational\nnotebooks. To evaluate the effectiveness of the embodied composition framework,\nwe conducted a controlled user study comparing manual and embodied composition\nframeworks in an organizational process. The results show that embodied\ncomposition frameworks significantly reduced user effort and decreased\ncompletion time. However, the design of the triggering mechanism requires\nfurther refinement. Our findings highlight the potential of embodied\ncomposition frameworks to enhance the scalability of the organizational process\nin immersive computational notebooks.",
      "url": "http://arxiv.org/abs/2509.13291v1",
      "published_time_eastern_timestamp": 1758044654.0
    },
    {
      "title": "Uchimata: a toolkit for visualization of 3D genome structures on the web\n  and in computational notebooks",
      "summary": "Summary: Uchimata is a toolkit for visualization of 3D structures of genomes.\nIt consists of two packages: a Javascript library facilitating the rendering of\n3D models of genomes, and a Python widget for visualization in Jupyter\nNotebooks. Main features include an expressive way to specify visual encodings,\nand filtering of 3D genome structures based on genomic semantics and spatial\naspects. Uchimata is designed to be highly integratable with biological tooling\navailable in Python. Availability and Implementation: Uchimata is released\nunder the MIT License. The Javascript library is available on NPM, while the\nwidget is available as a Python package hosted on PyPI. The source code for\nboth is available publicly on Github (https://github.com/hms-dbmi/uchimata and\nhttps://github.com/hms-dbmi/uchimata-py). The documentation with examples is\nhosted at https://hms-dbmi.github.io/uchimata/ Contact:\ndavid_kouril@hms.harvard.edu or nils@hms.harvard.edu.",
      "url": "http://arxiv.org/abs/2509.13290v1",
      "published_time_eastern_timestamp": 1758044637.0
    },
    {
      "title": "Runaway electron interactions with whistler waves in tokamak plasmas:\n  energy-dependent transport scaling",
      "summary": "Resonant interactions between high energy runaway electrons (REs) and\nwhistler waves are a promising mechanism for RE mitigation in tokamak plasmas.\nWhile prior studies have largely relied on quasi-linear diffusion models in\nsimplified geometries, we present a first-principles-informed framework that\nmodels RE-whistler interactions in a 3D tokamak equilibrium. This is achieved\nby coupling AORSA, which computes whistler eigenmodes for a given tokamak\nplasma equilibrium, and KORC, a kinetic orbit code that tracks full orbit RE\ntrajectories in prescribed wave fields. Our results demonstrate that REs\nundergo scattering to large pitch angles and exhibit anomalous diffusion in\nboth pitch-angle and kinetic energy space. Crucially, we observe a transition\nbetween diffusive, sub-diffusive, and super-diffusive transport regimes as a\nfunction of initial RE energy - an effect not captured by existing quasi-linear\nmodels. This anomalous transport behavior represents a significant advancement\nin understanding RE dynamics in the presence of wave - particle interactions.\nBy identifying the conditions under which anomalous diffusion arises, this work\nlays the theoretical foundation for designing targeted, wave-based mitigation\nstrategies in future tokamak experiments.",
      "url": "http://arxiv.org/abs/2509.13271v1",
      "published_time_eastern_timestamp": 1758043741.0
    },
    {
      "title": "Post-Hoc Split-Point Self-Consistency Verification for Efficient,\n  Unified Quantification of Aleatoric and Epistemic Uncertainty in Deep\n  Learning",
      "summary": "Uncertainty quantification (UQ) is vital for trustworthy deep learning, yet\nexisting methods are either computationally intensive, such as Bayesian or\nensemble methods, or provide only partial, task-specific estimates, such as\nsingle-forward-pass techniques. In this paper, we propose a post-hoc\nsingle-forward-pass framework that jointly captures aleatoric and epistemic\nuncertainty without modifying or retraining pretrained models. Our method\napplies \\emph{Split-Point Analysis} (SPA) to decompose predictive residuals\ninto upper and lower subsets, computing \\emph{Mean Absolute Residuals} (MARs)\non each side. We prove that, under ideal conditions, the total MAR equals the\nharmonic mean of subset MARs; deviations define a novel \\emph{Self-consistency\nDiscrepancy Score} (SDS) for fine-grained epistemic estimation across\nregression and classification. For regression, side-specific quantile\nregression yields prediction intervals with improved empirical coverage, which\nare further calibrated via SDS. For classification, when calibration data are\navailable, we apply SPA-based calibration identities to adjust the softmax\noutputs and then compute predictive entropy on these calibrated probabilities.\nExtensive experiments on diverse regression and classification benchmarks\ndemonstrate that our framework matches or exceeds several state-of-the-art UQ\nmethods while incurring minimal overhead.\n  Our source code is available at https://github.com/zzz0527/SPC-UQ.",
      "url": "http://arxiv.org/abs/2509.13262v1",
      "published_time_eastern_timestamp": 1758042961.0
    },
    {
      "title": "Rebound: Efficient, Expressive, and Well-Scoped Binding",
      "summary": "We introduce the Rebound library that supports well-scoped term\nrepresentations in Haskell and automates the definition of substitution,\nalpha-equivalence, and other operations that work with binding structures. The\nkey idea of our design is the use of first-class environments that map\nvariables to expressions in some new scope. By statically tracking scopes,\nusers of this library gain confidence that they have correctly maintained the\nsubtle invariants that stem from using de Bruijn indices. Behind the scenes,\nRebound uses environments to optimize the application of substitutions, while\nproviding explicit access to these data structures when desired. We demonstrate\nthat this library is expressive by using it to implement a wide range of\nlanguage features with sophisticated uses of binding and several different\noperations that use this abstract syntax. Our examples include pi-forall, a\ntutorial implementation of a type checker for a dependently-typed programming\nlanguage. Finally, we benchmark Rebound to understand its performance\ncharacteristics and find that it produces faster code than competing libraries.",
      "url": "http://arxiv.org/abs/2509.13261v1",
      "published_time_eastern_timestamp": 1758042928.0
    },
    {
      "title": "Evolution of Programmers' Trust in Generative AI Programming Assistants",
      "summary": "Motivation. Trust in generative AI programming assistants is a vital attitude\nthat impacts how programmers use those programming assistants. Programmers that\nare over-trusting may be too reliant on their tools, leading to incorrect or\nvulnerable code; programmers that are under-trusting may avoid using tools that\ncan improve their productivity and well-being.\n  Methods. Since trust is a dynamic attitude that may change over time, this\nstudy aims to understand programmers' evolution of trust after immediate (one\nhour) and extended (10 days) use of GitHub Copilot. We collected survey data\nfrom 71 upper-division computer science students working on a legacy code base,\nrepresenting a population that is about to enter the workforce. In this study,\nwe quantitatively measure student trust levels and qualitatively uncover why\nstudent trust changes.\n  Findings. Student trust, on average, increased over time. After completing a\nproject with Copilot, however, students felt that Copilot requires a competent\nprogrammer to complete some tasks manually. Students mentioned that seeing\nCopilot's correctness, understanding how Copilot uses context from the code\nbase, and learning some basics of natural language processing contributed to\ntheir elevated trust.\n  Implications. Our study helps instructors and industry managers understand\nthe factors that influence how students calibrate their trust with AI\nassistants. We make four pedagogical recommendations, which are that CS\neducators should 1) provide opportunities for students to work with Copilot on\nchallenging software engineering tasks to calibrate their trust, 2) teach\ntraditional skills of comprehending, debugging, and testing so students can\nverify output, 3) teach students about the basics of natural language\nprocessing, and 4) explicitly introduce and demonstrate the range of features\navailable in Copilot.",
      "url": "http://arxiv.org/abs/2509.13253v1",
      "published_time_eastern_timestamp": 1758042407.0
    },
    {
      "title": "Demonstration of a Logical Architecture Uniting Motion and In-Place\n  Entanglement: Shor's Algorithm, Constant-Depth CNOT Ladder, and\n  Many-Hypercube Code",
      "summary": "Logical qubits are considered an essential component for achieving\nutility-scale quantum computation. Multiple recent demonstrations of logical\nqubits on neutral atoms have relied on coherent qubit motion into entangling\nzones. However, this architecture requires motion prior to every entangling\ngate, incurring significant cost in wall clock runtime and motion-related error\naccumulation. We propose and experimentally realize an alternative architecture\nwhich unites qubit motion and in-place entanglement via nearest-neighbor gates.\nOur approach maintains all-to-all connectivity, while minimizing qubit motion\noverheads. We demonstrate three key results on Infleqtion's Sqale QPU, which\nhosts an array of 114 neutral atom qubits. First, we perform a logical qubit\nrealization of a pre-compiled variant of Shor's Algorithm. We find better\nlogical-than-physical performance over a range of settings including with loss\ncorrection and leakage detection. Second, we introduce a technique for\nperforming CNOT ladders with depth independent of both the number of logical\nqubits N and the code distance d. In proof-of-principle experiments with 8 and\n12 logical qubits, we find ~4x reduction in error via the logical encodings.\nThird, we experimentally realize initialization of the [[16, 4, 4]]\nmany-hypercube QEC code. All three results benefit from optimized compilation\nvia Superstaq, as well as our underlying architecture uniting motion and\nin-place entanglement. This architecture offers a path to reducing the overhead\nof utility-scale quantum applications relative to architectures based on\nentangling zones.",
      "url": "http://arxiv.org/abs/2509.13247v1",
      "published_time_eastern_timestamp": 1758041742.0
    },
    {
      "title": "Curriculum Multi-Task Self-Supervision Improves Lightweight\n  Architectures for Onboard Satellite Hyperspectral Image Segmentation",
      "summary": "Hyperspectral imaging (HSI) captures detailed spectral signatures across\nhundreds of contiguous bands per pixel, being indispensable for remote sensing\napplications such as land-cover classification, change detection, and\nenvironmental monitoring. Due to the high dimensionality of HSI data and the\nslow rate of data transfer in satellite-based systems, compact and efficient\nmodels are required to support onboard processing and minimize the transmission\nof redundant or low-value data, e.g. cloud-covered areas. To this end, we\nintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)\nframework designed for lightweight architectures for HSI analysis. CMTSSL\nintegrates masked image modeling with decoupled spatial and spectral jigsaw\npuzzle solving, guided by a curriculum learning strategy that progressively\nincreases data complexity during self-supervision. This enables the encoder to\njointly capture fine-grained spectral continuity, spatial structure, and global\nsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously\naddresses spatial and spectral reasoning within a unified and computationally\nefficient design, being particularly suitable for training lightweight models\nfor onboard satellite deployment. We validate our approach on four public\nbenchmark datasets, demonstrating consistent gains in downstream segmentation\ntasks, using architectures that are over 16,000x lighter than some\nstate-of-the-art models. These results highlight the potential of CMTSSL in\ngeneralizable representation learning with lightweight architectures for\nreal-world HSI applications. Our code is publicly available at\nhttps://github.com/hugocarlesso/CMTSSL.",
      "url": "http://arxiv.org/abs/2509.13229v1",
      "published_time_eastern_timestamp": 1758040679.0
    },
    {
      "title": "On the Out-of-Distribution Backdoor Attack for Federated Learning",
      "summary": "Traditional backdoor attacks in federated learning (FL) operate within\nconstrained attack scenarios, as they depend on visible triggers and require\nphysical modifications to the target object, which limits their practicality.\nTo address this limitation, we introduce a novel backdoor attack prototype for\nFL called the out-of-distribution (OOD) backdoor attack ($\\mathtt{OBA}$), which\nuses OOD data as both poisoned samples and triggers simultaneously. Our\napproach significantly broadens the scope of backdoor attack scenarios in FL.\nTo improve the stealthiness of $\\mathtt{OBA}$, we propose $\\mathtt{SoDa}$,\nwhich regularizes both the magnitude and direction of malicious local models\nduring local training, aligning them closely with their benign versions to\nevade detection. Empirical results demonstrate that $\\mathtt{OBA}$ effectively\ncircumvents state-of-the-art defenses while maintaining high accuracy on the\nmain task.\n  To address this security vulnerability in the FL system, we introduce\n$\\mathtt{BNGuard}$, a new server-side defense method tailored against\n$\\mathtt{SoDa}$. $\\mathtt{BNGuard}$ leverages the observation that OOD data\ncauses significant deviations in the running statistics of batch normalization\nlayers. This allows $\\mathtt{BNGuard}$ to identify malicious model updates and\nexclude them from aggregation, thereby enhancing the backdoor robustness of FL.\nExtensive experiments across various settings show the effectiveness of\n$\\mathtt{BNGuard}$ on defending against $\\mathtt{SoDa}$. The code is available\nat https://github.com/JiiahaoXU/SoDa-BNGuard.",
      "url": "http://arxiv.org/abs/2509.13219v1",
      "published_time_eastern_timestamp": 1758039819.0
    },
    {
      "title": "End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting\n  Detection",
      "summary": "The powerful generative capabilities of diffusion models have significantly\nadvanced the field of image synthesis, enhancing both full image generation and\ninpainting-based image editing. Despite their remarkable advancements,\ndiffusion models also raise concerns about potential misuse for malicious\npurposes. However, existing approaches struggle to identify images generated by\ndiffusion-based inpainting models, even when similar inpainted images are\nincluded in their training data. To address this challenge, we propose a novel\ndetection method based on End-to-end denoising diffusion (End4). Specifically,\nEnd4 designs a denoising reconstruction model to improve the alignment degree\nbetween the latent spaces of the reconstruction and detection processes, thus\nreconstructing features that are more conducive to detection. Meanwhile, it\nleverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local\nimage features under the guidance of attention pyramid layers at different\nscales, enhancing feature discriminability. Additionally, to evaluate detection\nperformance on inpainted images, we establish a comprehensive benchmark\ncomprising images generated from five distinct masked regions. Extensive\nexperiments demonstrate that our End4 effectively generalizes to unseen masking\npatterns and remains robust under various perturbations. Our code and dataset\nwill be released soon.",
      "url": "http://arxiv.org/abs/2509.13214v1",
      "published_time_eastern_timestamp": 1758039593.0
    },
    {
      "title": "Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection\n  in Public Surveillance",
      "summary": "Violence detection in public surveillance is critical for public safety. This\nstudy addresses challenges such as small-scale targets, complex environments,\nand real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal\nframework that integrates an enhanced YOLOv8 with a Temporal Segment Network\n(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as\na lightweight backbone, an exponential moving average (EMA) attention\nmechanism, and pruning to reduce computational cost while maintaining accuracy.\nYOLOv8 and TSN are trained separately on pedestrian and violence datasets,\nwhere YOLOv8 extracts human regions and TSN performs binary classification of\nviolent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE\nachieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming\nexisting methods in both accuracy and efficiency, demonstrating its\neffectiveness for public safety surveillance. Code is available at\nhttps://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.",
      "url": "http://arxiv.org/abs/2509.13210v1",
      "published_time_eastern_timestamp": 1758039377.0
    },
    {
      "title": "ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic\n  Medical Datasets Generation",
      "summary": "Continuum robots are advancing bronchoscopy procedures by accessing complex\nlung airways and enabling targeted interventions. However, their development is\nlimited by the lack of realistic training and test environments: Real data is\ndifficult to collect due to ethical constraints and patient safety concerns,\nand developing autonomy algorithms requires realistic imaging and physical\nfeedback. We present ROOM (Realistic Optical Observation in Medicine), a\ncomprehensive simulation framework designed for generating photorealistic\nbronchoscopy training data. By leveraging patient CT scans, our pipeline\nrenders multi-modal sensor data including RGB images with realistic noise and\nlight specularities, metric depth maps, surface normals, optical flow and point\nclouds at medically relevant scales. We validate the data generated by ROOM in\ntwo canonical tasks for medical robotics -- multi-view pose estimation and\nmonocular depth estimation, demonstrating diverse challenges that\nstate-of-the-art methods must overcome to transfer to these medical settings.\nFurthermore, we show that the data produced by ROOM can be used to fine-tune\nexisting depth estimation models to overcome these challenges, also enabling\nother downstream applications such as navigation. We expect that ROOM will\nenable large-scale data generation across diverse patient anatomies and\nprocedural scenarios that are challenging to capture in clinical settings. Code\nand data: https://github.com/iamsalvatore/room.",
      "url": "http://arxiv.org/abs/2509.13177v1",
      "published_time_eastern_timestamp": 1758036602.0
    },
    {
      "title": "More performant and scalable: Rethinking contrastive vision-language\n  pre-training of radiology in the LLM era",
      "summary": "The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable.",
      "url": "http://arxiv.org/abs/2509.13175v1",
      "published_time_eastern_timestamp": 1758036434.0
    },
    {
      "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End\n  Autonomous Driving",
      "summary": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical~data synthesis framework for training and evaluation of E2E autonomous\ndriving systems.",
      "url": "http://arxiv.org/abs/2509.13164v1",
      "published_time_eastern_timestamp": 1758035684.0
    },
    {
      "title": "Designing the Hybrid Cooperative: A Socio-Technical Architecture for\n  Scalable, Global Coordination Using Blockchain",
      "summary": "Blockchain has been promoted as a remedy for coordination in fragmented,\nmulti-stakeholder ecosystems, yet many projects stall at pilot stage. Using a\ndesign-science approach, we develop the Hybrid Cooperative (HC), a digitally\nnative governance architecture that combines smart-contract coordination with a\nminimal, code-deferent legal interface and jurisdictional modules. This\nselective decentralization decentralizes rules where programmability lowers\nagency and verification costs, and centralizes only what is needed for\nenforceability. A post-case evaluation against two traceability initiatives in\nsupply chains illustrates how the HC improves distributed task management,\nverifiable information, incentive alignment, institutional interoperability,\nand scalable, contestable governance. The paper contributes to Information\nSystems by specifying a socio-technical model for scalable, multi-stakeholder\ncoordination across regulatory and organizational boundaries.",
      "url": "http://arxiv.org/abs/2509.13156v1",
      "published_time_eastern_timestamp": 1758035371.0
    },
    {
      "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation",
      "summary": "4D radar super-resolution, which aims to reconstruct sparse and noisy point\nclouds into dense and geometrically consistent representations, is a\nfoundational problem in autonomous perception. However, existing methods often\nsuffer from high training cost or rely on complex diffusion-based sampling,\nresulting in high inference latency and poor generalization, making it\ndifficult to balance accuracy and efficiency. To address these limitations, we\npropose MSDNet, a multi-stage distillation framework that efficiently transfers\ndense LiDAR priors to 4D radar features to achieve both high reconstruction\nquality and computational efficiency. The first stage performs\nreconstruction-guided feature distillation, aligning and densifying the\nstudent's features through feature reconstruction. In the second stage, we\npropose diffusion-guided feature distillation, which treats the stage-one\ndistilled features as a noisy version of the teacher's representations and\nrefines them via a lightweight diffusion network. Furthermore, we introduce a\nnoise adapter that adaptively aligns the noise level of the feature with a\npredefined diffusion timestep, enabling a more precise denoising. Extensive\nexperiments on the VoD and in-house datasets demonstrate that MSDNet achieves\nboth high-fidelity reconstruction and low-latency inference in the task of 4D\nradar point cloud super-resolution, and consistently improves performance on\ndownstream tasks. The code will be publicly available upon publication.",
      "url": "http://arxiv.org/abs/2509.13149v1",
      "published_time_eastern_timestamp": 1758035111.0
    },
    {
      "title": "Measurement of neutron total cross-section of Mg and MgF$_{2}$ at MONNET\n  (JRC-Geel) from transmission with fine multi-scattering corrections",
      "summary": "Neutron-transmission measurements through samples of magnesium fluoride\n(MgF$_2$) and pure magnesium were performed to obtain the (n, tot) cross\nsection for all isotopes involved, $^{19}$F and $^{24-26}$Mg. Lithium-glass\ndetectors were used in conjunction with the neutron time-of-flight technique.\nThe measurement campaign was performed at the MONNET fast-neutron source of the\nEuropean Commission Joint Research Centre (JRC-Geel, Belgium). Highly precise\ncorrections for multiple scattering were calculated using a sophisticated\niterative method based on Monte Carlo simulations with the MCNP6.3 code,\naccounting for the effects of the experimental setup. With the SAMMY code, an\nR-Matrix analysis of the experimental data was performed. The extracted\ncross-sections, resonance spin and parity as well as the limitations of the\nmethod are carefully discussed.",
      "url": "http://arxiv.org/abs/2509.13135v1",
      "published_time_eastern_timestamp": 1758034401.0
    },
    {
      "title": "Optimizing Code Embeddings and ML Classifiers for Python Source Code\n  Vulnerability Detection",
      "summary": "In recent years, the growing complexity and scale of source code have\nrendered manual software vulnerability detection increasingly impractical. To\naddress this challenge, automated approaches leveraging machine learning and\ncode embeddings have gained substantial attention. This study investigates the\noptimal combination of code embedding techniques and machine learning\nclassifiers for vulnerability detection in Python source code. We evaluate\nthree embedding techniques, i.e., Word2Vec, CodeBERT, and GraphCodeBERT\nalongside two deep learning classifiers, i.e., Bidirectional Long Short-Term\nMemory (BiLSTM) networks and Convolutional Neural Networks (CNN). While CNN\npaired with GraphCodeBERT exhibits strong performance, the BiLSTM model using\nWord2Vec consistently achieves superior overall results. These findings suggest\nthat, despite the advanced architectures of recent models like CodeBERT and\nGraphCodeBERT, classical embeddings such as Word2Vec, when used with\nsequence-based models like BiLSTM, can offer a slight yet consistent\nperformance advantage. The study underscores the critical importance of\nselecting appropriate combinations of embeddings and classifiers to enhance the\neffectiveness of automated vulnerability detection systems, particularly for\nPython source code.",
      "url": "http://arxiv.org/abs/2509.13134v1",
      "published_time_eastern_timestamp": 1758034322.0
    },
    {
      "title": "Advancing Real-World Parking Slot Detection with Large-Scale Dataset and\n  Semi-Supervised Baseline",
      "summary": "As automatic parking systems evolve, the accurate detection of parking slots\nhas become increasingly critical. This study focuses on parking slot detection\nusing surround-view cameras, which offer a comprehensive bird's-eye view of the\nparking environment. However, the current datasets are limited in scale, and\nthe scenes they contain are seldom disrupted by real-world noise (e.g., light,\nocclusion, etc.). Moreover, manual data annotation is prone to errors and\nomissions due to the complexity of real-world conditions, significantly\nincreasing the cost of annotating large-scale datasets. To address these\nissues, we first construct a large-scale parking slot detection dataset (named\nCRPS-D), which includes various lighting distributions, diverse weather\nconditions, and challenging parking slot variants. Compared with existing\ndatasets, the proposed dataset boasts the largest data scale and consists of a\nhigher density of parking slots, particularly featuring more slanted parking\nslots. Additionally, we develop a semi-supervised baseline for parking slot\ndetection, termed SS-PSD, to further improve performance by exploiting\nunlabeled data. To our knowledge, this is the first semi-supervised approach in\nparking slot detection, which is built on the teacher-student model with\nconfidence-guided mask consistency and adaptive feature perturbation.\nExperimental results demonstrate the superiority of SS-PSD over the existing\nstate-of-the-art (SoTA) solutions on both the proposed dataset and the existing\ndataset. Particularly, the more unlabeled data there is, the more significant\nthe gains brought by our semi-supervised scheme. The relevant source codes and\nthe dataset have been made publicly available at\nhttps://github.com/zzh362/CRPS-D.",
      "url": "http://arxiv.org/abs/2509.13133v1",
      "published_time_eastern_timestamp": 1758034219.0
    }
  ]
}
{
  "last_updated": "2025-06-16T05:15:00.016461-04:00",
  "papers": [
    {
      "title": "code_transformed: The Influence of Large Language Models on Code",
      "summary": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.",
      "url": "http://arxiv.org/abs/2506.12014v1",
      "published_time_eastern_timestamp": 1749837579.0
    },
    {
      "title": "An Efficient Compression of Deep Neural Network Checkpoints Based on\n  Prediction and Context Modeling",
      "summary": "This paper is dedicated to an efficient compression of weights and optimizer\nstates (called checkpoints) obtained at different stages during a neural\nnetwork training process. First, we propose a prediction-based compression\napproach, where values from the previously saved checkpoint are used for\ncontext modeling in arithmetic coding. Second, in order to enhance the\ncompression performance, we also propose to apply pruning and quantization of\nthe checkpoint values. Experimental results show that our approach achieves\nsubstantial bit size reduction, while enabling near-lossless training recovery\nfrom restored checkpoints, preserving the model's performance and making it\nsuitable for storage-limited environments.",
      "url": "http://arxiv.org/abs/2506.12000v1",
      "published_time_eastern_timestamp": 1749837282.0
    },
    {
      "title": "pLSTM: parallelizable Linear Source Transition Mark networks",
      "summary": "Modern recurrent architectures, such as xLSTM and Mamba, have recently\nchallenged the Transformer in language modeling. However, their structure\nconstrains their applicability to sequences only or requires processing\nmulti-dimensional data structures, such as images or molecular graphs, in a\npre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are\nwell suited for data with a higher level structure, like 2D grids, trees, and\ndirected acyclic graphs (DAGs). In this work, we extend the notion of\nmulti-dimensionality to linear RNNs. We introduce parallelizable Linear Source\nTransition Mark networks (pLSTMs) using Source, Transition, and Mark gates that\nact on the line graph of a general DAG. This enables parallelization in analogy\nto parallel associative scans and the chunkwise-recurrent form of sequential\nlinear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this\nscheme can be efficiently implemented using einsum operations, concatenations,\nand padding in logarithmic time. pLSTMs tackle the vanishing/exploding\nactivation/gradient problem for long distances in DAGs via two distinct modes:\na directed propagation mode (P-mode) and a diffusive distribution mode\n(D-mode). To showcase the long-range capabilities of pLSTM, we introduce\narrow-pointing extrapolation as a synthetic computer vision task that contains\nlong-distance directional information. We demonstrate that pLSTMs generalize\nwell to larger image sizes, whereas Transformers struggle to extrapolate. On\nestablished molecular graph and computer vision benchmarks, pLSTMs also show\nstrong performance. Code and Datasets are available at:\nhttps://github.com/ml-jku/plstm_experiments.",
      "url": "http://arxiv.org/abs/2506.11997v1",
      "published_time_eastern_timestamp": 1749837097.0
    },
    {
      "title": "Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal",
      "summary": "Test-time scaling offers a promising way to improve the reasoning performance\nof vision-language large models (VLLMs) without additional training. In this\npaper, we explore a simple but effective approach for applying test-time\nscaling to radiology report generation. Specifically, we introduce a\nlightweight Thought Graph Traversal (TGT) framework that guides the model to\nreason through organ-specific findings in a medically coherent order. This\nframework integrates structured medical priors into the prompt, enabling deeper\nand more logical analysis with no changes to the underlying model. To further\nenhance reasoning depth, we apply a reasoning budget forcing strategy that\nadjusts the model's inference depth at test time by dynamically extending its\ngeneration process. This simple yet powerful combination allows a frozen\nradiology VLLM to self-correct and generate more accurate, consistent chest\nX-ray reports. Our method outperforms baseline prompting approaches on standard\nbenchmarks, and also reveals dataset biases through traceable reasoning paths.\nCode and prompts are open-sourced for reproducibility at\nhttps://github.com/glerium/Thought-Graph-Traversal.",
      "url": "http://arxiv.org/abs/2506.11989v1",
      "published_time_eastern_timestamp": 1749836774.0
    },
    {
      "title": "Schema-R1: A reasoning training approach for schema linking in\n  Text-to-SQL Task",
      "summary": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.",
      "url": "http://arxiv.org/abs/2506.11986v1",
      "published_time_eastern_timestamp": 1749836762.0
    },
    {
      "title": "Accurate Reduced Floating-Point Precision Implicit Monte Carlo",
      "summary": "This work describes methodologies to successfully implement the Implicit\nMonte Carlo (IMC) scheme for thermal radiative transfer in reduced-precision\nfloating-point arithmetic. The methods used can be broadly categorized into\nscaling approaches and floating-point arithmetic manipulations. Scaling\napproaches entail re-scaling values to ensure computations stay within a\nrepresentable range. Floating-point arithmetic manipulations involve changes to\norder of operations and alternative summation algorithms to minimize errors in\ncalculations. The Implicit Monte Carlo method has nonlinear dependencies,\nquantities spanning many orders of magnitude, and a sensitive coupling between\nradiation and material energy that provide significant difficulties to accurate\nreduced-precision implementations. Results from reduced and higher-precision\nimplementations of IMC solving the Su & Olson volume source benchmark problem\nare compared to demonstrate the accuracy of a correctly implemented\nreduced-precision IMC code. We show that the scaling approaches and\nfloating-point manipulations used in this work can produce solutions with\nsimilar accuracy using half-precision data types as compared to a standard\ndouble-precision implementation.",
      "url": "http://arxiv.org/abs/2506.11962v1",
      "published_time_eastern_timestamp": 1749835091.0
    },
    {
      "title": "Intractable Cookie Crumbs: Unveiling the Nexus of Stateful Banner\n  Interaction and Tracking Cookies",
      "summary": "In response to the ePrivacy Directive and the consent requirements introduced\nby the GDPR, websites began deploying consent banners to obtain user permission\nfor data collection and processing. However, due to shared third-party services\nand technical loopholes, non-consensual cross-site tracking can still occur. In\nfact, contrary to user expectations of seemingly isolated consent, a user's\ndecision on one website may affect tracking behavior on others. In this study,\nwe investigate the technical and behavioral mechanisms behind these\ndiscrepancies. Specifically, we disclose a persistent tracking mechanism\nexploiting web cookies. These cookies, which we refer to as intractable, are\ninitially set on websites with accepted banners, persist in the browser, and\nare subsequently sent to trackers before the user provides explicit consent on\nother websites. To meticulously analyze this covert tracking behavior, we\nconduct an extensive measurement study performing stateful crawls on over 20k\ndomains from the Tranco top list, strategically accepting banners in the first\nhalf of domains and measuring intractable cookies in the second half. Our\nfindings reveal that around 50% of websites send at least one intractable\ncookie, with the majority set to expire after more than 10 days. In addition,\nenabling the Global Privacy Control (GPC) signal initially reduces the number\nof intractable cookies by 30% on average, with a further 32% reduction possible\non subsequent visits by rejecting the banners. Moreover, websites with Consent\nManagement Platform (CMP) banners, on average, send 6.9 times more intractable\ncookies compared to those with native banners. Our research further reveals\nthat even if users reject all other banners, they still receive a large number\nof intractable cookies set by websites with cookie paywalls.",
      "url": "http://arxiv.org/abs/2506.11947v1",
      "published_time_eastern_timestamp": 1749833723.0
    },
    {
      "title": "Improving Large Language Model Safety with Contrastive Representation\n  Learning",
      "summary": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense",
      "url": "http://arxiv.org/abs/2506.11938v1",
      "published_time_eastern_timestamp": 1749832929.0
    },
    {
      "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
      "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
      "url": "http://arxiv.org/abs/2506.11928v1",
      "published_time_eastern_timestamp": 1749832149.0
    },
    {
      "title": "Dynamic Grid Trading Strategy: From Zero Expectation to Market\n  Outperformance",
      "summary": "We propose a profitable trading strategy for the cryptocurrency market based\non grid trading. Starting with an analysis of the expected value of the\ntraditional grid strategy, we show that under simple assumptions, its expected\nreturn is essentially zero. We then introduce a novel Dynamic Grid-based\nTrading (DGT) strategy that adapts to market conditions by dynamically\nresetting grid positions. Our backtesting results using minute-level data from\nBitcoin and Ethereum between January 2021 and July 2024 demonstrate that the\nDGT strategy significantly outperforms both the traditional grid and\nbuy-and-hold strategies in terms of internal rate of return and risk control.",
      "url": "http://arxiv.org/abs/2506.11921v1",
      "published_time_eastern_timestamp": 1749831104.0
    },
    {
      "title": "Spectra-to-Structure and Structure-to-Spectra Inference Across the\n  Periodic Table",
      "summary": "X-ray Absorption Spectroscopy (XAS) is a powerful technique for probing local\natomic environments, yet its interpretation remains limited by the need for\nexpert-driven analysis, computationally expensive simulations, and\nelement-specific heuristics. Recent advances in machine learning have shown\npromise for accelerating XAS interpretation, but many existing models are\nnarrowly focused on specific elements, edge types, or spectral regimes. In this\nwork, we present XAStruct, a learning framework capable of both predicting XAS\nspectra from crystal structures and inferring local structural descriptors from\nXAS input. XAStruct is trained on a large-scale dataset spanning over 70\nelements across the periodic table, enabling generalization to a wide variety\nof chemistries and bonding environments. The model includes the first machine\nlearning approach for predicting neighbor atom types directly from XAS spectra,\nas well as a unified regression model for mean nearest-neighbor distance that\nrequires no element-specific tuning. While we explored integrating the two\npipelines into a single end-to-end model, empirical results showed performance\ndegradation. As a result, the two tasks were trained independently to ensure\noptimal accuracy and task-specific performance. By combining deep neural\nnetworks for complex structure-property mappings with efficient baseline models\nfor simpler tasks, XAStruct offers a scalable and extensible solution for\ndata-driven XAS analysis and local structure inference. The source code will be\nreleased upon paper acceptance.",
      "url": "http://arxiv.org/abs/2506.11908v1",
      "published_time_eastern_timestamp": 1749830285.0
    },
    {
      "title": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search",
      "summary": "Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL.",
      "url": "http://arxiv.org/abs/2506.11902v1",
      "published_time_eastern_timestamp": 1749829957.0
    },
    {
      "title": "DMRS-Based Uplink Channel Estimation for MU-MIMO Systems with\n  Location-Specific SCSI Acquisition",
      "summary": "With the growing number of users in multi-user multiple-input multiple-output\n(MU-MIMO) systems, demodulation reference signals (DMRSs) are efficiently\nmultiplexed in the code domain via orthogonal cover codes (OCC) to ensure\northogonality and minimize pilot interference. In this paper, we investigate\nuplink DMRS-based channel estimation for MU-MIMO systems with Type II OCC\npattern standardized in 3GPP Release 18, leveraging location-specific\nstatistical channel state information (SCSI) to enhance performance.\nSpecifically, we propose a SCSI-assisted Bayesian channel estimator (SA-BCE)\nbased on the minimum mean square error criterion to suppress the pilot\ninterference and noise, albeit at the cost of cubic computational complexity\ndue to matrix inversions. To reduce this complexity while maintaining\nperformance, we extend the scheme to a windowed version (SA-WBCE), which\nincorporates antenna-frequency domain windowing and beam-delay domain\nprocessing to exploit asymptotic sparsity and mitigate energy leakage in\npractical systems. To avoid the frequent real-time SCSI acquisition, we\nconstruct a grid-based location-specific SCSI database based on the principle\nof spatial consistency, and subsequently leverage the uplink received signals\nwithin each grid to extract the SCSI. Facilitated by the multilinear structure\nof wireless channels, we formulate the SCSI acquisition problem within each\ngrid as a tensor decomposition problem, where the factor matrices are\nparameterized by the multi-path powers, delays, and angles. The computational\ncomplexity of SCSI acquisition can be significantly reduced by exploiting the\nVandermonde structure of the factor matrices. Simulation results demonstrate\nthat the proposed location-specific SCSI database construction method achieves\nhigh accuracy, while the SA-BCE and SA-WBCE significantly outperform\nstate-of-the-art benchmarks in MU-MIMO systems.",
      "url": "http://arxiv.org/abs/2506.11899v1",
      "published_time_eastern_timestamp": 1749829700.0
    },
    {
      "title": "Confidence-Based Self-Training for EMG-to-Speech: Leveraging Synthetic\n  EMG for Robust Modeling",
      "summary": "Voiced Electromyography (EMG)-to-Speech (V-ETS) models reconstruct speech\nfrom muscle activity signals, facilitating applications such as\nneurolaryngologic diagnostics. Despite its potential, the advancement of V-ETS\nis hindered by a scarcity of paired EMG-speech data. To address this, we\npropose a novel Confidence-based Multi-Speaker Self-training (CoM2S) approach,\nalong with a newly curated Libri-EMG dataset. This approach leverages synthetic\nEMG data generated by a pre-trained model, followed by a proposed filtering\nmechanism based on phoneme-level confidence to enhance the ETS model through\nthe proposed self-training techniques. Experiments demonstrate our method\nimproves phoneme accuracy, reduces phonological confusion, and lowers word\nerror rate, confirming the effectiveness of our CoM2S approach for V-ETS. In\nsupport of future research, we will release the codes and the proposed\nLibri-EMG dataset-an open-access, time-aligned, multi-speaker voiced EMG and\nspeech recordings.",
      "url": "http://arxiv.org/abs/2506.11862v1",
      "published_time_eastern_timestamp": 1749827584.0
    },
    {
      "title": "Structural Similarity-Inspired Unfolding for Lightweight Image\n  Super-Resolution",
      "summary": "Major efforts in data-driven image super-resolution (SR) primarily focus on\nexpanding the receptive field of the model to better capture contextual\ninformation. However, these methods are typically implemented by stacking\ndeeper networks or leveraging transformer-based attention mechanisms, which\nconsequently increases model complexity. In contrast, model-driven methods\nbased on the unfolding paradigm show promise in improving performance while\neffectively maintaining model compactness through sophisticated module design.\nBased on these insights, we propose a Structural Similarity-Inspired Unfolding\n(SSIU) method for efficient image SR. This method is designed through unfolding\nan SR optimization function constrained by structural similarity, aiming to\ncombine the strengths of both data-driven and model-driven approaches. Our\nmodel operates progressively following the unfolding paradigm. Each iteration\nconsists of multiple Mixed-Scale Gating Modules (MSGM) and an Efficient Sparse\nAttention Module (ESAM). The former implements comprehensive constraints on\nfeatures, including a structural similarity constraint, while the latter aims\nto achieve sparse activation. In addition, we design a Mixture-of-Experts-based\nFeature Selector (MoE-FS) that fully utilizes multi-level feature information\nby combining features from different steps. Extensive experiments validate the\nefficacy and efficiency of our unfolding-inspired network. Our model\noutperforms current state-of-the-art models, boasting lower parameter counts\nand reduced memory consumption. Our code will be available at:\nhttps://github.com/eezkni/SSIU",
      "url": "http://arxiv.org/abs/2506.11823v1",
      "published_time_eastern_timestamp": 1749824980.0
    },
    {
      "title": "Learning to Integrate",
      "summary": "This work deals with uncertainty quantification for a generic input\ndistribution to some resource-intensive simulation, e.g., requiring the\nsolution of a partial differential equation. While efficient numerical methods\nexist to compute integrals for high-dimensional Gaussian and other separable\ndistributions based on sparse grids (SG), input data arising in practice often\ndoes not fall into this class. We therefore employ transport maps to transform\ncomplex distributions to multivatiate standard normals. In generative learning,\na number of neural network architectures have been introduced that accomplish\nthis task approximately. Examples are affine coupling flows (ACF) and ordinary\ndifferential equation-based networks such as conditional flow matching (CFM).\nTo compute the expectation of a quantity of interest, we numerically integrate\nthe composition of the inverse of the learned transport map with the simulation\ncode output. As this map is integrated over a multivariate Gaussian\ndistribution, SG techniques can be applied. Viewing the images of the SG\nquadrature nodes as learned quadrature nodes for a given complex distribution\nmotivates our title. We demonstrate our method for monomials of total degrees\nfor which the unmapped SG rules are exact. We also apply our approach to the\nstationary diffusion equation with coefficients modeled by exponentiated L\\'evy\nrandom fields, using a Karhunen-Lo\\`eve-like modal expansions with 9 and 25\nmodes. In a series of numerical experiments, we investigate errors due to\nlearning accuracy, quadrature, statistical estimation, truncation of the modal\nseries of the input random field, and training data size for three normalizing\nflows (ACF, conditional Flow Matching and Optimal transport Flow Matching) We\ndiscuss the mathematical assumptions on which our approach is based and\ndemonstrate its shortcomings when these are violated.",
      "url": "http://arxiv.org/abs/2506.11801v1",
      "published_time_eastern_timestamp": 1749823506.0
    },
    {
      "title": "Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models",
      "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.",
      "url": "http://arxiv.org/abs/2506.11798v1",
      "published_time_eastern_timestamp": 1749823341.0
    },
    {
      "title": "InflationEasy: A C++ Lattice Code for Inflation",
      "summary": "InflationEasy is the first lattice code specifically developed for\ncosmological inflation. It simulates the nonlinear dynamics of a scalar field\non a three-dimensional lattice in an expanding FLRW universe, using\nfinite-difference spatial derivatives and a staggered leapfrog integrator for\ntime evolution. Based in part on the well-known LATTICEEASY, it incorporates\nseveral improvements tailored to inflationary applications. Among other\nfeatures, InflationEasy implements a nonperturbative $\\delta N$ method to\ncompute the curvature perturbation at the end of inflation $\\zeta$ directly\nfrom the lattice, enabling fully nonlinear studies of scenarios with large\nfluctuations or nonperturbative non-Gaussianities. The code supports a broad\nrange of inflationary models, including those relevant for primordial black\nhole formation, gravitational wave production, and large-scale structure. This\nmakes InflationEasy a versatile and lightweight tool for studying inflationary\ndynamics beyond the reach of perturbation theory.",
      "url": "http://arxiv.org/abs/2506.11797v1",
      "published_time_eastern_timestamp": 1749823301.0
    },
    {
      "title": "ALEA IACTA EST: A Declarative Domain-Specific Language for Manually\n  Performable Random Experiments",
      "summary": "Random experiments that are simple and clear enough to be performed by human\nagents feature prominently in the teaching of elementary stochastics as well as\nin games. We present Alea, a domain-specific language for the specification of\nrandom experiments. Alea code can either be analyzed statically to obtain and\ninspect probability distributions of outcomes, or be executed with a source\npseudo-randomness for simulation or as a game assistant. The language is\nintended for ease of use by non-expert programmers, in particular students of\nelementary stochastics, and players and designers of games of chance, by\nfocusing on concepts common to functional programming and basic mathematics.\nBoth the design of the language and the implementation of runtime environments\nare work in progress.",
      "url": "http://arxiv.org/abs/2506.11794v1",
      "published_time_eastern_timestamp": 1749823106.0
    },
    {
      "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks",
      "summary": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering.",
      "url": "http://arxiv.org/abs/2506.11791v1",
      "published_time_eastern_timestamp": 1749822870.0
    }
  ]
}
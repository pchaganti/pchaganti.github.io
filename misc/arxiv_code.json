{
  "last_updated": "2025-08-18T08:25:26.771189-04:00",
  "papers": [
    {
      "title": "Thyme: Think Beyond Images",
      "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.",
      "url": "http://arxiv.org/abs/2508.11630v1",
      "published_time_eastern_timestamp": 1755280789.0
    },
    {
      "title": "Pretrained Conformers for Audio Fingerprinting and Retrieval",
      "summary": "Conformers have shown great results in speech processing due to their ability\nto capture both local and global interactions. In this work, we utilize a\nself-supervised contrastive learning framework to train conformer-based\nencoders that are capable of generating unique embeddings for small segments of\naudio, generalizing well to previously unseen data. We achieve state-of-the-art\nresults for audio retrieval tasks while using only 3 seconds of audio to\ngenerate embeddings. Our models are almost completely immune to temporal\nmisalignments and achieve state-of-the-art results in cases of other audio\ndistortions such as noise, reverb or extreme temporal stretching. Code and\nmodels are made publicly available and the results are easy to reproduce as we\ntrain and test using popular and freely available datasets of different sizes.",
      "url": "http://arxiv.org/abs/2508.11609v1",
      "published_time_eastern_timestamp": 1755278349.0
    },
    {
      "title": "TinyTim: A Family of Language Models for Divergent Generation",
      "summary": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.",
      "url": "http://arxiv.org/abs/2508.11607v1",
      "published_time_eastern_timestamp": 1755278069.0
    },
    {
      "title": "Accelerated Solvers for Neutral Particle Dynamics in Plasma Simulation",
      "summary": "The simulation of turbulence in the boundary region of a tokamak is crucial\nfor understanding and optimizing the performance of fusion reactors. In this\nwork, the use of low-rank linear algebra techniques is shown to enhance the\nefficiency of boundary simulations, specifically by accelerating the solution\nof a kinetic model for the neutral particles. Solving the kinetic model\ndeterministically using the method of characteristics requires the solution of\nintegral equations, which typically result in dense linear systems upon\ndiscretization. We employ hierarchical matrix approximations to significantly\nreduce the computational cost of assembling and solving the linear systems,\nleading to substantial savings in both time and memory. The hierarchical matrix\nmethod is implemented and tested within the GBS simulation code for boundary\nsimulations, achieving over 90\\% reduction in computation time and memory, and\nenabling simulations with unprecedented spatial resolution for neutral\nparticles.",
      "url": "http://arxiv.org/abs/2508.11564v1",
      "published_time_eastern_timestamp": 1755274627.0
    },
    {
      "title": "Ultrafast X-ray interaction with photovoltaic materials: Thermal and\n  nonthermal responses",
      "summary": "Cadmium telluride (CdTe), lead sulfide (PbS), and indium tin oxide (ITO) are\nimportant in various electronic technologies, for which laser irradiation is\nused to selectively modify and design their unique semiconductor properties. We\nemploy the hybrid multiscale code XTANT-3 to simulate the kinetics of material\nresponse to ultrafast X-ray irradiation. The code accounts for nonequilibrium\nelectronic and atomic dynamics, nonadiabatic coupling, nonthermal melting, and\nbond breaking due to electronic excitation. Among the materials studied, CdTe\nexhibits the highest radiation resistance, similar to CdS. At the respective\nthreshold doses, the melting is primarily thermal, driven by electron-phonon\ncoupling, which is accompanied by the band gap closure. Additionally, all\nmaterials show nonthermal melting at higher doses. Threshold doses increase\nfurther if energy sinks and recrystallization are included. In CdTe and PbS,\nbelow 1.5 eV/atom, the band gap returns to its original value upon\nrecrystallization. As the dose increases, the cooled state becomes more\namorphous, reducing the band gap until it stabilizes. Curiously, in a narrow\nwindow of deposited doses, ITO exhibit transient superionic behavior, with the\nliquid oxygen but solid In and Sn sublattices. At 0.6 eV/atom in CdTe and 0.4\neV/atom in PbS and ITO, material ablation from the surface occurs. The results\nsuggest that femtosecond lasers may be used for tuning the band gap of\nphotovoltaic semiconductors.",
      "url": "http://arxiv.org/abs/2508.11549v1",
      "published_time_eastern_timestamp": 1755273031.0
    },
    {
      "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments",
      "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.",
      "url": "http://arxiv.org/abs/2508.11538v1",
      "published_time_eastern_timestamp": 1755272096.0
    },
    {
      "title": "Language models align with brain regions that represent concepts across\n  modalities",
      "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.",
      "url": "http://arxiv.org/abs/2508.11536v1",
      "published_time_eastern_timestamp": 1755271939.0
    },
    {
      "title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State\n  Specialization and Interaction",
      "summary": "Efficient trackers achieve faster runtime by reducing computational\ncomplexity and model parameters. However, this efficiency often compromises the\nexpense of weakened feature representation capacity, thus limiting their\nability to accurately capture target states using single-layer features. To\novercome this limitation, we propose Multi-State Tracker (MST), which utilizes\nhighly lightweight state-specific enhancement (SSE) to perform specialized\nenhancement on multi-state features produced by multi-state generation (MSG)\nand aggregates them in an interactive and adaptive manner using cross-state\ninteraction (CSI). This design greatly enhances feature representation while\nincurring minimal computational overhead, leading to improved tracking\nrobustness in complex environments. Specifically, the MSG generates multiple\nstate representations at multiple stages during feature extraction, while SSE\nrefines them to highlight target-specific features. The CSI module facilitates\ninformation exchange between these states and ensures the integration of\ncomplementary features. Notably, the introduced SSE and CSI modules adopt a\nhighly lightweight hidden state adaptation-based state space duality (HSA-SSD)\ndesign, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.\nExperimental results demonstrate that MST outperforms all previous efficient\ntrackers across multiple datasets, significantly improving tracking accuracy\nand robustness. In particular, it shows excellent runtime performance, with an\nAO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on\nthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.",
      "url": "http://arxiv.org/abs/2508.11531v1",
      "published_time_eastern_timestamp": 1755271179.0
    },
    {
      "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media",
      "summary": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.",
      "url": "http://arxiv.org/abs/2508.11503v1",
      "published_time_eastern_timestamp": 1755268207.0
    },
    {
      "title": "Swarm-in-Blocks: Simplifying Drone Swarm Programming with Block-Based\n  Language",
      "summary": "Swarm in Blocks, originally developed for CopterHack 2022, is a high-level\ninterface that simplifies drone swarm programming using a block-based language.\nBuilding on the Clover platform, this tool enables users to create\nfunctionalities like loops and conditional structures by assembling code\nblocks. In 2023, we introduced Swarm in Blocks 2.0, further refining the\nplatform to address the complexities of swarm management in a user-friendly\nway. As drone swarm applications grow in areas like delivery, agriculture, and\nsurveillance, the challenge of managing them, especially for beginners, has\nalso increased. The Atena team developed this interface to make swarm handling\naccessible without requiring extensive knowledge of ROS or programming. The\nblock-based approach not only simplifies swarm control but also expands\neducational opportunities in programming.",
      "url": "http://arxiv.org/abs/2508.11498v1",
      "published_time_eastern_timestamp": 1755267609.0
    },
    {
      "title": "OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal\n  Navigation",
      "summary": "Open-vocabulary Object Goal Navigation requires an embodied agent to reach\nobjects described by free-form language, including categories never seen during\ntraining. Existing end-to-end policies overfit small simulator datasets,\nachieving high success on training scenes but failing to generalize and\nexhibiting unsafe behaviour (frequent collisions). We introduce OVSegDT, a\nlightweight transformer policy that tackles these issues with two synergistic\ncomponents. The first component is the semantic branch, which includes an\nencoder for the target binary mask and an auxiliary segmentation loss function,\ngrounding the textual goal and providing precise spatial cues. The second\ncomponent consists of a proposed Entropy-Adaptive Loss Modulation, a per-sample\nscheduler that continuously balances imitation and reinforcement signals\naccording to the policy entropy, eliminating brittle manual phase switches.\nThese additions cut the sample complexity of training by 33%, and reduce\ncollision count in two times while keeping inference cost low (130M parameters,\nRGB-only input). On HM3D-OVON, our model matches the performance on unseen\ncategories to that on seen ones and establishes state-of-the-art results (40.1%\nSR, 20.9% SPL on val unseen) without depth, odometry, or large vision-language\nmodels. Code is available at https://github.com/CognitiveAISystems/OVSegDT.",
      "url": "http://arxiv.org/abs/2508.11479v1",
      "published_time_eastern_timestamp": 1755265695.0
    },
    {
      "title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation",
      "summary": "Although recent text-to-image (T2I) diffusion models excel at aligning\ngenerated images with textual prompts, controlling the visual style of the\noutput remains a challenging task. In this work, we propose Style-Prompting\nGuidance (SPG), a novel sampling strategy for style-specific image generation.\nSPG constructs a style noise vector and leverages its directional deviation\nfrom unconditional noise to guide the diffusion process toward the target style\ndistribution. By integrating SPG with Classifier-Free Guidance (CFG), our\nmethod achieves both semantic fidelity and style consistency. SPG is simple,\nrobust, and compatible with controllable frameworks like ControlNet and\nIPAdapter, making it practical and widely applicable. Extensive experiments\ndemonstrate the effectiveness and generality of our approach compared to\nstate-of-the-art methods. Code is available at\nhttps://github.com/Rumbling281441/SPG.",
      "url": "http://arxiv.org/abs/2508.11476v1",
      "published_time_eastern_timestamp": 1755265496.0
    },
    {
      "title": "TRACY: Benchmarking Execution Efficiency of LLM-Based Code Translation",
      "summary": "Automatic code translation is a fundamental task in modern software\ndevelopment. While the advent of Large Language Models (LLMs) has significantly\nimproved the correctness of code translation, the critical dimension of\nexecution efficiency remains overlooked. To address this gap, we introduce\nTRACY, the first comprehensive benchmark designed to evaluate the execution\nefficiency of LLM-translated code. TRACY is constructed through an LLM-driven\ntwo-stage pipeline: an initial stage generates a suite of stress tests to\namplify performance differences, followed by an efficiency-oriented task\npruning stage that isolates the efficiency-distinguishing tasks. The resulting\nbenchmark comprises 1,011 code translation tasks across C++, Java, and Python,\neach accompanied by an average of 22.1 verified reference translations and 10\ncomputationally demanding tests. Our extensive evaluation of 26 representative\nLLMs reveals that even top-tier LLMs struggle to consistently produce efficient\ncode translations. For instance, Claude-4-think, the leading model for\ncorrectness, ranks eighth overall when time efficiency is taken into account,\nsurpassed by several smaller open-source models. We further pinpoint that\nalgorithmic flaws and improper resource handling are the most detrimental,\ncausing a median time slowdown of 5.6$\\times$ and memory increase of\n12.0$\\times$, respectively. Our work underscores the necessity of jointly\noptimizing for correctness and efficiency in future LLM-based code translation.",
      "url": "http://arxiv.org/abs/2508.11468v1",
      "published_time_eastern_timestamp": 1755264832.0
    },
    {
      "title": "Importance-Aware Robust Semantic Transmission for LEO Satellite-Ground\n  Communication",
      "summary": "Satellite-ground semantic communication is anticipated to serve a critical\nrole in the forthcoming 6G era. Nonetheless, task-oriented data transmission in\nsuch systems remains a formidable challenge, primarily due to the dynamic\nnature of signal-to-noise ratio (SNR) fluctuations and the stringent bandwidth\nlimitations inherent to low Earth orbit (LEO) satellite channels. In response\nto these constraints, we propose an importance-aware robust semantic\ntransmission (IRST) framework, specifically designed for scenarios\ncharacterized by bandwidth scarcity and channel variability. The IRST scheme\nbegins by applying a segmentation model enhancement algorithm to improve the\ngranularity and accuracy of semantic segmentation. Subsequently, a task-driven\nsemantic selection method is employed to prioritize the transmission of\nsemantically vital content based on real-time channel state information.\nFurthermore, the framework incorporates a stack-based, SNR-aware channel codec\ncapable of executing adaptive channel coding in alignment with SNR variations.\nComparative evaluations across diverse operating conditions demonstrate the\nsuperior performance and resilience of the IRST model relative to existing\nbenchmarks.",
      "url": "http://arxiv.org/abs/2508.11457v1",
      "published_time_eastern_timestamp": 1755263698.0
    },
    {
      "title": "Subcortical Masks Generation in CT Images via Ensemble-Based\n  Cross-Domain Label Transfer",
      "summary": "Subcortical segmentation in neuroimages plays an important role in\nunderstanding brain anatomy and facilitating computer-aided diagnosis of\ntraumatic brain injuries and neurodegenerative disorders. However, training\naccurate automatic models requires large amounts of labelled data. Despite the\navailability of publicly available subcortical segmentation datasets for\nMagnetic Resonance Imaging (MRI), a significant gap exists for Computed\nTomography (CT). This paper proposes an automatic ensemble framework to\ngenerate high-quality subcortical segmentation labels for CT scans by\nleveraging existing MRI-based models. We introduce a robust ensembling pipeline\nto integrate them and apply it to unannotated paired MRI-CT data, resulting in\na comprehensive CT subcortical segmentation dataset. Extensive experiments on\nmultiple public datasets demonstrate the superior performance of our proposed\nframework. Furthermore, using our generated CT dataset, we train segmentation\nmodels that achieve improved performance on related segmentation tasks. To\nfacilitate future research, we make our source code, generated dataset, and\ntrained models publicly available at\nhttps://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation,\nmarking the first open-source release for CT subcortical segmentation to the\nbest of our knowledge.",
      "url": "http://arxiv.org/abs/2508.11450v1",
      "published_time_eastern_timestamp": 1755262655.0
    },
    {
      "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data\n  Augmentation and Curriculum Learning for Visual Indoor Navigation",
      "summary": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site",
      "url": "http://arxiv.org/abs/2508.11446v1",
      "published_time_eastern_timestamp": 1755262453.0
    },
    {
      "title": "Towards Efficient Hash Maps in Functional Array Languages",
      "summary": "We present a systematic derivation of a data-parallel implementation of\ntwo-level, static and collision-free hash maps, by giving a functional\nformulation of the Fredman et al. construction, and then flattening it. We\ndiscuss the challenges of providing a flexible, polymorphic, and abstract\ninterface to hash maps in a functional array language, with particular\nattention paid to the problem of dynamically sized keys, which we address by\nassociating each hash map with an arbitrary context. The algorithm is\nimplemented in Futhark, and the achieved GPU execution performance is compared\non simple benchmark problems. We find that our hash maps outperform\nconventional tree/search-based approaches. Furthermore, our implementation is\ncompared against the state-of-the-art cuCollections library, which is\nsignificantly faster for hash map construction, and to a lesser degree for\nlookups. We explain to which extent the performance difference is due to\nlow-level code generation limitation in the Futhark compiler, and to which\nextent it can be attributed to the data-parallel programming vocabulary not\nproviding the constructs necessary to express the equivalent of the algorithms\nused by cuCollections. We end by reflecting to which extent the functional\narray language programming model could, or should, be extended to address these\nweaknesses.",
      "url": "http://arxiv.org/abs/2508.11443v1",
      "published_time_eastern_timestamp": 1755262112.0
    },
    {
      "title": "Multi-Sensory Cognitive Computing for Learning Population-level Brain\n  Connectivity",
      "summary": "The generation of connectional brain templates (CBTs) has recently garnered\nsignificant attention for its potential to identify unique connectivity\npatterns shared across individuals. However, existing methods for CBT learning\nsuch as conventional machine learning and graph neural networks (GNNs) are\nhindered by several limitations. These include: (i) poor interpretability due\nto their black-box nature, (ii) high computational cost, and (iii) an exclusive\nfocus on structure and topology, overlooking the cognitive capacity of the\ngenerated CBT. To address these challenges, we introduce mCOCO (multi-sensory\nCOgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)\nto learn population-level functional CBT from BOLD\n(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow\nfor tracking state changes over time, enhancing interpretability and enabling\nthe modeling of brain-like dynamics, as demonstrated in prior literature. By\nintegrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO\ncaptures not only structure and topology but also how brain regions process\ninformation and adapt to cognitive tasks such as sensory processing, all in a\ncomputationally efficient manner. Our mCOCO framework consists of two phases:\n(1) mapping BOLD signals into the reservoir to derive individual functional\nconnectomes, which are then aggregated into a group-level CBT - an approach, to\nthe best of our knowledge, not previously explored in functional connectivity\nstudies - and (2) incorporating multi-sensory inputs through a cognitive\nreservoir, endowing the CBT with cognitive traits. Extensive evaluations show\nthat our mCOCO-based template significantly outperforms GNN-based CBT in terms\nof centeredness, discriminativeness, topological soundness, and multi-sensory\nmemory retention. Our source code is available at\nhttps://github.com/basiralab/mCOCO.",
      "url": "http://arxiv.org/abs/2508.11436v1",
      "published_time_eastern_timestamp": 1755261519.0
    },
    {
      "title": "Generative Co-Design of Antibody Sequences and Structures via Black-Box\n  Guidance in a Shared Latent Space",
      "summary": "Advancements in deep generative models have enabled the joint modeling of\nantibody sequence and structure, given the antigen-antibody complex as context.\nHowever, existing approaches for optimizing complementarity-determining regions\n(CDRs) to improve developability properties operate in the raw data space,\nleading to excessively costly evaluations due to the inefficient search\nprocess. To address this, we propose LatEnt blAck-box Design (LEAD), a\nsequence-structure co-design framework that optimizes both sequence and\nstructure within their shared latent space. Optimizing shared latent codes can\nnot only break through the limitations of existing methods, but also ensure\nsynchronization of different modality designs. Particularly, we design a\nblack-box guidance strategy to accommodate real-world scenarios where many\nproperty evaluators are non-differentiable. Experimental results demonstrate\nthat our LEAD achieves superior optimization performance for both single and\nmulti-property objectives. Notably, LEAD reduces query consumption by a half\nwhile surpassing baseline methods in property optimization. The code is\navailable at https://github.com/EvaFlower/LatEnt-blAck-box-Design.",
      "url": "http://arxiv.org/abs/2508.11424v1",
      "published_time_eastern_timestamp": 1755259153.0
    },
    {
      "title": "SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models",
      "summary": "Deep neural networks have become the go-to method for biomedical instance\nsegmentation. Generalist models like Cellpose demonstrate state-of-the-art\nperformance across diverse cellular data, though their effectiveness often\ndegrades on domains that differ from their training data. While supervised\nfine-tuning can address this limitation, it requires annotated data that may\nnot be readily available. We propose SelfAdapt, a method that enables the\nadaptation of pre-trained cell segmentation models without the need for labels.\nOur approach builds upon student-teacher augmentation consistency training,\nintroducing L2-SP regularization and label-free stopping criteria. We evaluate\nour method on the LiveCell and TissueNet datasets, demonstrating relative\nimprovements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we\nshow that our unsupervised adaptation can further improve models that were\npreviously fine-tuned with supervision. We release SelfAdapt as an easy-to-use\nextension of the Cellpose framework. The code for our method is publicly\navailable at https: //github.com/Kainmueller-Lab/self_adapt.",
      "url": "http://arxiv.org/abs/2508.11411v1",
      "published_time_eastern_timestamp": 1755257508.0
    }
  ]
}
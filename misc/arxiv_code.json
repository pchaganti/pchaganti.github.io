{
  "last_updated": "2025-09-10T20:54:22.438093-04:00",
  "papers": [
    {
      "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
      "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose \\textbf{Parallel-R1}, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a \\textbf{mid-training exploration scaffold}, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.",
      "url": "http://arxiv.org/abs/2509.07980v1",
      "published_time_eastern_timestamp": 1757440775.0
    },
    {
      "title": "Towards an application of fourth-order shear statistics II: Efficient\n  estimation of fourth-order shear correlation functions and an application to\n  the DES Y3 data",
      "summary": "Higher-order lensing statistics contain a wealth of cosmological information\nthat is not captured by second-order statistics. Stage-III lensing surveys have\nsufficient statistical power to significantly detect cumulant-based statistics\nup to fourth order. We derive and validate an efficient estimation procedure\nfor the four-point correlation function (4PCF) of polar fields such as weak\nlensing shear. We then use our approach to measure the shear 4PCF and the\nfourth-order aperture mass statistics in the DES Y3 survey. We construct an\nefficient estimator for fourth-order shear statistics which builds on the\nmultipole decomposition of the shear 4PCF. We then validate our estimator on\nmock ellipticity catalogues obtained from Gaussian random fields and on\nrealistic $N$-body simulations. Finally, we apply our estimator to the DES Y3\ndata and present a measurement of the fourth-order aperture statistics in a\nnon-tomographic setup. Due to its quadratic scaling, our estimator provides a\nsignificant speed-up over hypothetical brute force or tree-based estimation\nmethods of the shear 4PCF. We report a significant detection of the connected\npart of the fourth-order aperture mass in the DES Y3 data. We find the sampling\ndistribution of the fourth-order aperture mass to be significantly skewed. We\nmake our estimator code available on GitHub as part of the orpheus package.",
      "url": "http://arxiv.org/abs/2509.07974v1",
      "published_time_eastern_timestamp": 1757440637.0
    },
    {
      "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
      "summary": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.",
      "url": "http://arxiv.org/abs/2509.07969v1",
      "published_time_eastern_timestamp": 1757440461.0
    },
    {
      "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
      "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
      "url": "http://arxiv.org/abs/2509.07968v1",
      "published_time_eastern_timestamp": 1757440438.0
    },
    {
      "title": "Customizing the Inductive Biases of Softmax Attention using Structured\n  Matrices",
      "summary": "The core component of attention is the scoring function, which transforms the\ninputs into low-dimensional queries and keys and takes the dot product of each\npair. While the low-dimensional projection improves efficiency, it causes\ninformation loss for certain tasks that have intrinsically high-dimensional\ninputs. Additionally, attention uses the same scoring function for all input\npairs, without imposing a distance-dependent compute bias for neighboring\ntokens in the sequence. In this work, we address these shortcomings by\nproposing new scoring functions based on computationally efficient structured\nmatrices with high ranks, including Block Tensor-Train (BTT) and Multi-Level\nLow Rank (MLR) matrices. On in-context regression tasks with high-dimensional\ninputs, our proposed scoring functions outperform standard attention for any\nfixed compute budget. On language modeling, a task that exhibits locality\npatterns, our MLR-based attention method achieves improved scaling laws\ncompared to both standard attention and variants of sliding window attention.\nAdditionally, we show that both BTT and MLR fall under a broader family of\nefficient structured matrices capable of encoding either full-rank or\ndistance-dependent compute biases, thereby addressing significant shortcomings\nof standard attention. Finally, we show that MLR attention has promising\nresults for long-range time-series forecasting.",
      "url": "http://arxiv.org/abs/2509.07963v1",
      "published_time_eastern_timestamp": 1757440258.0
    },
    {
      "title": "One Model for All Tasks: Leveraging Efficient World Models in Multi-Task\n  Planning",
      "summary": "In heterogeneous multi-task learning, tasks not only exhibit diverse\nobservation and action spaces but also vary substantially in intrinsic\ndifficulty. While conventional multi-task world models like UniZero excel in\nsingle-task settings, we find that when handling large-scale heterogeneous\nenvironments, gradient conflicts and the loss of model plasticity often\nconstrain their sample and computational efficiency. In this work, we address\nthese challenges from two perspectives: the single learning iteration and the\noverall learning process. First, we investigate the impact of key design spaces\non extending UniZero to multi-task planning. We find that a Mixture-of-Experts\n(MoE) architecture provides the most substantial performance gains by\nmitigating gradient conflicts, leading to our proposed model,\n\\textit{ScaleZero}. Second, to dynamically balance the computational load\nacross the learning process, we introduce an online, LoRA-based \\textit{dynamic\nparameter scaling} (DPS) strategy. This strategy progressively integrates LoRA\nadapters in response to task-specific progress, enabling adaptive knowledge\nretention and parameter expansion. Empirical evaluations on standard benchmarks\nsuch as Atari, DMControl (DMC), and Jericho demonstrate that ScaleZero, relying\nexclusively on online reinforcement learning with one model, attains\nperformance on par with specialized single-task baselines. Furthermore, when\naugmented with our dynamic parameter scaling strategy, our method achieves\ncompetitive performance while requiring only 80\\% of the single-task\nenvironment interaction steps. These findings underscore the potential of\nScaleZero for effective large-scale multi-task learning. Our code is available\nat \\textcolor{magenta}{https://github.com/opendilab/LightZero}.",
      "url": "http://arxiv.org/abs/2509.07945v1",
      "published_time_eastern_timestamp": 1757438873.0
    },
    {
      "title": "ImportSnare: Directed \"Code Manual\" Hijacking in Retrieval-Augmented\n  Code Generation",
      "summary": "Code generation has emerged as a pivotal capability of Large Language\nModels(LLMs), revolutionizing development efficiency for programmers of all\nskill levels. However, the complexity of data structures and algorithmic logic\noften results in functional deficiencies and security vulnerabilities in\ngenerated code, reducing it to a prototype requiring extensive manual\ndebugging. While Retrieval-Augmented Generation (RAG) can enhance correctness\nand security by leveraging external code manuals, it simultaneously introduces\nnew attack surfaces.\n  In this paper, we pioneer the exploration of attack surfaces in\nRetrieval-Augmented Code Generation (RACG), focusing on malicious dependency\nhijacking. We demonstrate how poisoned documentation containing hidden\nmalicious dependencies (e.g., matplotlib_safe) can subvert RACG, exploiting\ndual trust chains: LLM reliance on RAG and developers' blind trust in LLM\nsuggestions. To construct poisoned documents, we propose ImportSnare, a novel\nattack framework employing two synergistic strategies: 1)Position-aware beam\nsearch optimizes hidden ranking sequences to elevate poisoned documents in\nretrieval results, and 2)Multilingual inductive suggestions generate\njailbreaking sequences to manipulate LLMs into recommending malicious\ndependencies. Through extensive experiments across Python, Rust, and\nJavaScript, ImportSnare achieves significant attack success rates (over 50% for\npopular libraries such as matplotlib and seaborn) in general, and is also able\nto succeed even when the poisoning ratio is as low as 0.01%, targeting both\ncustom and real-world malicious packages. Our findings reveal critical supply\nchain risks in LLM-powered development, highlighting inadequate security\nalignment for code generation tasks. To support future research, we will\nrelease the multilingual benchmark suite and datasets. The project homepage is\nhttps://importsnare.github.io.",
      "url": "http://arxiv.org/abs/2509.07941v1",
      "published_time_eastern_timestamp": 1757438480.0
    },
    {
      "title": "The JADE code. II. Modeling the coupled orbital and atmospheric\n  evolution of GJ 436 b to constrain its migration and companion",
      "summary": "The observed architecture and modeled evolution of close-in exoplanets\nprovide crucial insights into their formation pathways and survival mechanisms.\nTo investigate these fundamental questions, we employed JADE, a comprehensive\nnumerical code that models the coupled evolution of atmospheres and dynamics\nover secular timescales, rooted in present-day observations. JADE integrates\nphotoevaporation with migration driven by von Zeipel-Lidov-Kozai (ZLK) cycles\nfrom an external perturber, allowing us to explore evolutionary scenarios where\ndynamical and atmospheric processes influence each other. Here, we specifically\nconsidered GJ 436 b, a warm Neptune with an eccentric orbit and polar\nspin-orbit angle that has survived within the \"hot Neptune desert\" despite\nongoing atmospheric escape. Our extensive exploration included over 500 000\nsimulations in a framework that combines precomputed grids with Bayesian\ninference. This allowed us to constrain GJ 436 b's initial conditions and the\nproperties of its putative companion within a ZLK hypothesis. Our results\nsuggest that GJ 436 b formed at ~ 0.3 AU and, despite its current substantial\natmospheric erosion, has experienced minimal cumulative mass loss throughout\nits history, thanks to a late inward migration triggered by a distant companion\ninducing ZLK oscillations. We find that initial mutual inclinations of 80{\\deg}\n- 100{\\deg} with this companion best reproduce the observed polar orbit. By\ncombining our explored constraints with radial velocity detection limits, we\nidentified the viable parameter space for the hypothetical GJ 436 c. We found\nthat it strongly disfavors stellar and brown dwarf masses, which offers a\nuseful guide for future observational searches. This work demonstrates how\ncoupled modeling can shed light on the interplay shaping close-in exoplanets\nand explain the survival of volatile-rich worlds near the edges of the desert.",
      "url": "http://arxiv.org/abs/2509.07938v1",
      "published_time_eastern_timestamp": 1757438349.0
    },
    {
      "title": "Feature Space Analysis by Guided Diffusion Model",
      "summary": "One of the key issues in Deep Neural Networks (DNNs) is the black-box nature\nof their internal feature extraction process. Targeting vision-related domains,\nthis paper focuses on analysing the feature space of a DNN by proposing a\ndecoder that can generate images whose features are guaranteed to closely match\na user-specified feature. Owing to this guarantee that is missed in past\nstudies, our decoder allows us to evidence which of various attributes in an\nimage are encoded into a feature by the DNN, by generating images whose\nfeatures are in proximity to that feature. Our decoder is implemented as a\nguided diffusion model that guides the reverse image generation of a\npre-trained diffusion model to minimise the Euclidean distance between the\nfeature of a clean image estimated at each step and the user-specified feature.\nOne practical advantage of our decoder is that it can analyse feature spaces of\ndifferent DNNs with no additional training and run on a single COTS GPU. The\nexperimental results targeting CLIP's image encoder, ResNet-50 and vision\ntransformer demonstrate that images generated by our decoder have features\nremarkably similar to the user-specified ones and reveal valuable insights into\nthese DNNs' feature spaces.",
      "url": "http://arxiv.org/abs/2509.07936v1",
      "published_time_eastern_timestamp": 1757438319.0
    },
    {
      "title": "GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large\n  Language Models",
      "summary": "Uncertainty estimation is essential for enhancing the reliability of Large\nLanguage Models (LLMs), particularly in high-stakes applications. Existing\nmethods often overlook semantic dependencies, relying on token-level\nprobability measures that fail to capture structural relationships within the\ngenerated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty\nEstimation for Large Language Models, a structure-aware framework that\nleverages dependency parse trees and hierarchical graph pooling to refine\nuncertainty quantification. By incorporating supervised learning, GENUINE\neffectively models semantic and structural relationships, improving confidence\nassessments. Extensive experiments across NLP tasks show that GENUINE achieves\nup to 29% higher AUROC than semantic entropy-based approaches and reduces\ncalibration errors by over 15%, demonstrating the effectiveness of graph-based\nuncertainty modeling. The code is available at\nhttps://github.com/ODYSSEYWT/GUQ.",
      "url": "http://arxiv.org/abs/2509.07925v1",
      "published_time_eastern_timestamp": 1757437664.0
    },
    {
      "title": "CP-Model-Zoo: A Natural Language Query System for Constraint Programming\n  Models",
      "summary": "Constraint Programming and its high-level modeling languages have long been\nrecognized for their potential to achieve the holy grail of problem-solving.\nHowever, the complexity of modeling languages, the large number of global\nconstraints, and the art of creating good models have often hindered\nnon-experts from choosing CP to solve their combinatorial problems. While\ngenerating an expert-level model from a natural-language description of a\nproblem would be the dream, we are not yet there. We propose a tutoring system\ncalled CP-Model-Zoo, exploiting expert-written models accumulated through the\nyears. CP-Model-Zoo retrieves the closest source code model from a database\nbased on a user's natural language description of a combinatorial problem. It\nensures that expert-validated models are presented to the user while\neliminating the need for human data labeling. Our experiments show excellent\naccuracy in retrieving the correct model based on a user-input description of a\nproblem simulated with different levels of expertise.",
      "url": "http://arxiv.org/abs/2509.07867v1",
      "published_time_eastern_timestamp": 1757433315.0
    },
    {
      "title": "SCoder: Iterative Self-Distillation for Bootstrapping Small-Scale Data\n  Synthesizers to Empower Code LLMs",
      "summary": "Existing code large language models (LLMs) often rely on large-scale\ninstruction data distilled from proprietary LLMs for fine-tuning, which\ntypically incurs high costs. In this paper, we explore the potential of\nsmall-scale open-source LLMs (e.g., 7B) as synthesizers for high-quality code\ninstruction data construction. We first observe that the data synthesis\ncapability of small-scale LLMs can be enhanced by training on a few superior\ndata synthesis samples from proprietary LLMs. Building on this, we propose a\nnovel iterative self-distillation approach to bootstrap small-scale LLMs,\ntransforming them into powerful synthesizers that reduce reliance on\nproprietary LLMs and minimize costs. Concretely, in each iteration, to obtain\ndiverse and high-quality self-distilled data, we design multi-checkpoint\nsampling and multi-aspect scoring strategies for initial data selection.\nFurthermore, to identify the most influential samples, we introduce a\ngradient-based influence estimation method for final data filtering. Based on\nthe code instruction datasets from the small-scale synthesizers, we develop\nSCoder, a family of code generation models fine-tuned from DeepSeek-Coder.\nSCoder models achieve state-of-the-art code generation capabilities,\ndemonstrating the effectiveness of our method.",
      "url": "http://arxiv.org/abs/2509.07858v1",
      "published_time_eastern_timestamp": 1757432324.0
    },
    {
      "title": "Observing Double White Dwarfs with the Lunar GW Antenna",
      "summary": "The Lunar Gravitational Wave Antenna (LGWA) is a proposed gravitational-wave\ndetector that will observe in the decihertz (dHz) frequency region. In this\nband, binary white dwarf systems are expected to merge, emitting gravitational\nwaves. Detecting this emission opens new perspectives for understanding the\nType Ia supernova progenitors and for investigating dense matter physics. In\nthis work, we present the capabilities of LGWA to detect and localize\nshort-period double white dwarfs in terms of sky locations and distances. The\nanalysis is performed using a realistic spatial distribution of sources, merger\nrates, and binary-mass distributions derived from current population synthesis\nmodels. The simulated population of double white dwarfs is generated using the\nSeBa stellar-evolution code, coupled with dedicated sampling algorithms. The\nperformance of the LGWA detector, both in terms of signal detectability and\nparameter estimation, is assessed using standard gravitational-wave data\nanalysis techniques, including Fisher matrix methods, as implemented in the\nGWFish and Legwork codes. The analysis indicates that LGWA could detect\napproximately O(30) monochromatic galactic sources and O(10) extragalactic\nmergers, demonstrating the unique potential of decihertz gravitational-wave\ndetectors to access and characterize extragalactic DWD populations. This will\nopen new avenues for understanding Type Ia supernova progenitors and the\nphysics of DWDs.",
      "url": "http://arxiv.org/abs/2509.07849v1",
      "published_time_eastern_timestamp": 1757431481.0
    },
    {
      "title": "RAQ-MIMO: MIMO for Multi-Band Rydberg Atomic Quantum Receiver",
      "summary": "Rydberg atomic quantum receivers (RAQRs) are capable of receiving multi-band\nradio-frequency (RF) signals simultaneously, which are expected to break Chu's\nlimit for classical electronic antennas. However, signals from different users\nwill interfere with each other in the optical intermediate frequency (IF)\ndomain of the multi-band quantum receiver, which is termed the IF interference\n(IFI) problem. To address this problem, in this paper, we propose a multi-input\nmulti-output (MIMO) architecture for Rydberg atomic quantum receiver (RAQ-MIMO)\nby exploiting the additional spatial diversity of MIMO receivers. Specifically,\nby applying the dynamic signal model of RAQRs, we clarify the physical\nrelationship between the quantum local oscillator (LO) configurations and the\nmulti-band gains with the concept of quantum transconductance. Then, with the\nquantum transconductance-based signal model, we formulate the spectral\nefficiency (SE) maximization problem and further propose the quantum weighted\nminimum mean square error (qWMMSE) algorithm, which jointly optimizes the\nquantum LO configurations and the classical precoder/combiner matrices.\nFurthermore, we test the qWMMSE algorithm within the standard space division\nmultiple access (SDMA) scheme and the frequency division multiple access (FDMA)\nscheme. Simulation results demonstrate that the qWMMSE optimization framework\ncan significantly improve the SE of RAQ-MIMO systems for both multiple access\nschemes, and that RAQ-MIMO systems can outperform classical electronic\nreceiver-based multi-user MIMO systems by eliminating the mutual coupling\neffect between classical antennas.",
      "url": "http://arxiv.org/abs/2509.07832v1",
      "published_time_eastern_timestamp": 1757430612.0
    },
    {
      "title": "Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems",
      "summary": "Textual response generation is pivotal for multimodal \\mbox{task-oriented}\ndialog systems, which aims to generate proper textual responses based on the\nmultimodal context. While existing efforts have demonstrated remarkable\nprogress, there still exist the following limitations: 1) \\textit{neglect of\nunstructured review knowledge} and 2) \\textit{underutilization of large\nlanguage models (LLMs)}. Inspired by this, we aim to fully utilize dual\nknowledge (\\textit{i.e., } structured attribute and unstructured review\nknowledge) with LLMs to promote textual response generation in multimodal\ntask-oriented dialog systems. However, this task is non-trivial due to two key\nchallenges: 1) \\textit{dynamic knowledge type selection} and 2)\n\\textit{intention-response decoupling}. To address these challenges, we propose\na novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for\nmultimodal dialog systems (named DK2R). To be specific, DK2R first extracts\nboth structured attribute and unstructured review knowledge from external\nknowledge base given the dialog context. Thereafter, DK2R uses an LLM to\nevaluate each knowledge type's utility by analyzing LLM-generated provisional\nprobe responses. Moreover, DK2R separately summarizes the intention-oriented\nkey clues via dedicated reasoning, which are further used as auxiliary signals\nto enhance LLM-based textual response generation. Extensive experiments\nconducted on a public dataset verify the superiority of DK2R. We have released\nthe codes and parameters.",
      "url": "http://arxiv.org/abs/2509.07817v1",
      "published_time_eastern_timestamp": 1757429728.0
    },
    {
      "title": "Faster, Self-Supervised Super-Resolution for Anisotropic Multi-View MRI\n  Using a Sparse Coordinate Loss",
      "summary": "Acquiring images in high resolution is often a challenging task. Especially\nin the medical sector, image quality has to be balanced with acquisition time\nand patient comfort. To strike a compromise between scan time and quality for\nMagnetic Resonance (MR) imaging, two anisotropic scans with different\nlow-resolution (LR) orientations can be acquired. Typically, LR scans are\nanalyzed individually by radiologists, which is time consuming and can lead to\ninaccurate interpretation. To tackle this, we propose a novel approach for\nfusing two orthogonal anisotropic LR MR images to reconstruct anatomical\ndetails in a unified representation. Our multi-view neural network is trained\nin a self-supervised manner, without requiring corresponding high-resolution\n(HR) data. To optimize the model, we introduce a sparse coordinate-based loss,\nenabling the integration of LR images with arbitrary scaling. We evaluate our\nmethod on MR images from two independent cohorts. Our results demonstrate\ncomparable or even improved super-resolution (SR) performance compared to\nstate-of-the-art (SOTA) self-supervised SR methods for different upsampling\nscales. By combining a patient-agnostic offline and a patient-specific online\nphase, we achieve a substantial speed-up of up to ten times for\npatient-specific reconstruction while achieving similar or better SR quality.\nCode is available at https://github.com/MajaSchle/tripleSR.",
      "url": "http://arxiv.org/abs/2509.07798v1",
      "published_time_eastern_timestamp": 1757428710.0
    },
    {
      "title": "Query Expansion in the Age of Pre-trained and Large Language Models: A\n  Comprehensive Survey",
      "summary": "Modern information retrieval (IR) must bridge short, ambiguous queries and\never more diverse, rapidly evolving corpora. Query Expansion (QE) remains a key\nmechanism for mitigating vocabulary mismatch, but the design space has shifted\nmarkedly with pre-trained language models (PLMs) and large language models\n(LLMs). This survey synthesizes the field from three angles: (i) a\nfour-dimensional framework of query expansion - from the point of injection\n(explicit vs. implicit QE), through grounding and interaction (knowledge bases,\nmodel-internal capabilities, multi-turn retrieval) and learning alignment, to\nknowledge graph-based argumentation; (ii) a model-centric taxonomy spanning\nencoder-only, encoder-decoder, decoder-only, instruction-tuned, and\ndomain/multilingual variants, highlighting their characteristic affordances for\nQE (contextual disambiguation, controllable generation, zero-/few-shot\nreasoning); and (iii) practice-oriented guidance on where and how neural QE\nhelps in first-stage retrieval, multi-query fusion, re-ranking, and\nretrieval-augmented generation (RAG). We compare traditional query expansion\nwith PLM/LLM-based methods across seven key aspects, and we map applications\nacross web search, biomedicine, e-commerce, open-domain QA/RAG, conversational\nand code search, and cross-lingual settings. The review distills design\ngrounding and interaction, alignment/distillation (SFT/PEFT/DPO), and KG\nconstraints - as robust remedies to topic drift and hallucination. We conclude\nwith an agenda on quality control, cost-aware invocation, domain/temporal\nadaptation, evaluation beyond end-task metrics, and fairness/privacy.\nCollectively, these insights provide a principled blueprint for selecting and\ncombining QE techniques under real-world constraints.",
      "url": "http://arxiv.org/abs/2509.07794v1",
      "published_time_eastern_timestamp": 1757428271.0
    },
    {
      "title": "RayGaussX: Accelerating Gaussian-Based Ray Marching for Real-Time and\n  High-Quality Novel View Synthesis",
      "summary": "RayGauss has achieved state-of-the-art rendering quality for novel-view\nsynthesis on synthetic and indoor scenes by representing radiance and density\nfields with irregularly distributed elliptical basis functions, rendered via\nvolume ray casting using a Bounding Volume Hierarchy (BVH). However, its\ncomputational cost prevents real-time rendering on real-world scenes. Our\napproach, RayGaussX, builds on RayGauss by introducing key contributions that\naccelerate both training and inference. Specifically, we incorporate volumetric\nrendering acceleration strategies such as empty-space skipping and adaptive\nsampling, enhance ray coherence, and introduce scale regularization to reduce\nfalse-positive intersections. Additionally, we propose a new densification\ncriterion that improves density distribution in distant regions, leading to\nenhanced graphical quality on larger scenes. As a result, RayGaussX achieves 5x\nto 12x faster training and 50x to 80x higher rendering speeds (FPS) on\nreal-world datasets while improving visual quality by up to +0.56 dB in PSNR.\nProject page with videos and code: https://raygaussx.github.io/.",
      "url": "http://arxiv.org/abs/2509.07782v1",
      "published_time_eastern_timestamp": 1757427559.0
    },
    {
      "title": "What Were You Thinking? An LLM-Driven Large-Scale Study of Refactoring\n  Motivations in Open-Source Projects",
      "summary": "Context. Code refactoring improves software quality without changing external\nbehavior. Despite its advantages, its benefits are hindered by the considerable\ncost of time, resources, and continuous effort it demands. Aim. Understanding\nwhy developers refactor, and which metrics capture these motivations, may\nsupport wider and more effective use of refactoring in practice. Method. We\nperformed a large-scale empirical study to analyze developers refactoring\nactivity, leveraging Large Language Models (LLMs) to identify underlying\nmotivations from version control data, comparing our findings with previous\nmotivations reported in the literature. Results. LLMs matched human judgment in\n80% of cases, but aligned with literature-based motivations in only 47%. They\nenriched 22% of motivations with more detailed rationale, often highlighting\nreadability, clarity, and structural improvements. Most motivations were\npragmatic, focused on simplification and maintainability. While metrics related\nto developer experience and code readability ranked highest, their correlation\nwith motivation categories was weak. Conclusions. We conclude that LLMs\neffectively capture surface-level motivations but struggle with architectural\nreasoning. Their value lies in providing localized explanations, which, when\ncombined with software metrics, can form hybrid approaches. Such integration\noffers a promising path toward prioritizing refactoring more systematically and\nbalancing short-term improvements with long-term architectural goals.",
      "url": "http://arxiv.org/abs/2509.07763v1",
      "published_time_eastern_timestamp": 1757426326.0
    },
    {
      "title": "Dual of Algebraic Geometry codes from Hirzebruch surfaces",
      "summary": "In this paper, we give an explicit form for the dual of the algebraic\ngeometry code $C_e(a,b)$ defined on an Hirzebruch surface $\\mathcal{H}_e$ and\nparametrized by the divisor $aS_e + bF_e$, where $a,b\\in\\mathbb{N}$ and $S_e$\nand $F_e$ generate the Picard group $\\mathrm{Pic}( \\mathcal{H}_e)$. Notably, we\ncompute a lower bound for the minimum distance of $C_e(a,b)^\\perp$. One of the\nmain ingredient for our study is a new explicit form of the code $C_e(a,b)$\nwhich we provide at the beginning of the paper. We also investigate some\npuncturing of $C_e(a,b)$, recovering other previously studied AG codes from\ntoric surfaces. Finally, we provide a sufficient condition for orthogonal\ninclusions between the codes $C_e(a,b)$, and construct CSS quantum codes from\nthem.",
      "url": "http://arxiv.org/abs/2509.07761v1",
      "published_time_eastern_timestamp": 1757426287.0
    }
  ]
}
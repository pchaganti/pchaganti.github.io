{
  "last_updated": "2026-01-24T16:11:11.105419-05:00",
  "papers": [
    {
      "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
      "summary": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.",
      "url": "http://arxiv.org/abs/2601.16211v1",
      "published_time_eastern_timestamp": 1769108353.0
    },
    {
      "title": "IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance",
      "summary": "Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model's built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA",
      "url": "http://arxiv.org/abs/2601.16207v1",
      "published_time_eastern_timestamp": 1769108233.0
    },
    {
      "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
      "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.",
      "url": "http://arxiv.org/abs/2601.16206v1",
      "published_time_eastern_timestamp": 1769108229.0
    },
    {
      "title": "Learning to Discover at Test Time",
      "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
      "url": "http://arxiv.org/abs/2601.16175v1",
      "published_time_eastern_timestamp": 1769106240.0
    },
    {
      "title": "Scaling Sample-Based Quantum Diagonalization on GPU-Accelerated Systems using OpenMP Offload",
      "summary": "Hybrid quantum-HPC algorithms advance research by delegating complex tasks to quantum processors and using HPC systems to orchestrate workflows and complementary computations. Sample-based quantum diagonalization (SQD) is a hybrid quantum-HPC method in which information from a molecular Hamiltonian is encoded into a quantum circuit for evaluation on a quantum computer. A set of measurements on the quantum computer yields electronic configurations that are filtered on the classical computer, which also performs diagonalization on the selected subspace and identifies configurations to be carried over to the next step in an iterative process. Diagonalization is the most demanding task for the classical computer. Previous studies used the Fugaku supercomputer and a highly scalable diagonalization code designed for CPUs. In this work, we describe our efforts to enable efficient scalable and portable diagonalization on heterogeneous systems using GPUs as the main compute engines based on the previous work.\n  GPUs provide massive on-device thread-level parallelism that is well aligned with the algorithms used for diagonalization. We focus on the computation of ground-state energies and wavefunctions using the Davidson algorithm with a selected set of electron configurations. We describe the offload strategy, code transformations, and data-movement, with examples of measurements on the Frontier supercomputer and five other GPU accelerated systems. Our measurements show that GPUs provide an outstanding performance boost of order 100x on a per-node basis. This dramatically expedites the diagonalization step-essential for extracting ground and excited state energies-bringing the classical processing time down from hours to minutes.",
      "url": "http://arxiv.org/abs/2601.16169v1",
      "published_time_eastern_timestamp": 1769105629.0
    },
    {
      "title": "Tensor Reed-Muller Codes: Achieving Capacity with Quasilinear Decoding Time",
      "summary": "Define the codewords of the Tensor Reed-Muller code $\\mathsf{TRM}(r_1,m_1;r_2,m_2;\\dots;r_t,m_t)$ to be the evaluation vectors of all multivariate polynomials in the variables $\\left\\{x_{ij}\\right\\}_{i=1,\\dots,t}^{j=1,\\dots m_i}$ with degree at most $r_i$ in the variables $x_{i1},x_{i2},\\dots,x_{im_i}$. The generator matrix of $\\mathsf{TRM}(r_1,m_1;\\dots;r_t,m_t)$ is thus the tensor product of the generator matrices of the Reed-Muller codes $\\mathsf{RM}(r_1,m_1),\\dots, \\mathsf{RM}(r_t,m_t)$.\n  We show that for any constant rate $R$ below capacity, one can construct a Tensor Reed-Muller code $\\mathsf{TRM}(r_1,m_1;\\dotsc;r_t,m_t)$ of rate $R$ that is decodable in quasilinear time. For any blocklength $n$, we provide two constructions of such codes:\n  1) Our first construction (with $t=3$) has error probability $n^{-ω(\\log n)}$ and decoding time $O(n\\log\\log n)$.\n  2) Our second construction, for any $t\\geq 4$, has error probability $2^{-n^{\\frac{1}{2}-\\frac{1}{2(t-2)}-o(1)}}$ and decoding time $O(n\\log n)$.\n  One of our main tools is a polynomial-time algorithm for decoding an arbitrary tensor code $C=C_1\\otimes\\dotsc\\otimes C_t$ from $\\frac{d_{\\min}(C)}{2\\max\\{d_{\\min}(C_1),\\dotsc,d_{\\min}(C_t) \\}}-1$ adversarial errors. Crucially, this algorithm does not require the codes $C_1,\\dotsc,C_t$ to themselves be decodable in polynomial time.",
      "url": "http://arxiv.org/abs/2601.16164v1",
      "published_time_eastern_timestamp": 1769105373.0
    },
    {
      "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
      "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model's latent diffusion process, harnessing the model's pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/",
      "url": "http://arxiv.org/abs/2601.16163v1",
      "published_time_eastern_timestamp": 1769105370.0
    },
    {
      "title": "Learning to Watermark in the Latent Space of Generative Models",
      "summary": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.",
      "url": "http://arxiv.org/abs/2601.16140v1",
      "published_time_eastern_timestamp": 1769103270.0
    },
    {
      "title": "Calibration-Conditioned FiLM Decoders for Low-Latency Decoding of Quantum Error Correction Evaluated on IBM Repetition-Code Experiments",
      "summary": "Real-time decoding of quantum error correction (QEC) is essential for enabling fault-tolerant quantum computation. A practical decoder must operate with high accuracy at low latency, while remaining robust to spatial and temporal variations in hardware noise. We introduce a hardware-conditioned neural decoder framework designed to exploit the natural separation of timescales in superconducting processors, where calibration drifts occur over hours while error correction requires microsecond-scale responses. By processing calibration data through a graph-based encoder and conditioning a lightweight convolutional backbone via feature-wise linear modulation (FiLM), we decouple the heavy processing of device statistics from the low-latency syndrome decoding.\n  We evaluate this approach using the 1D repetition code as a testbed on IBM Fez, Kingston, and Pittsburgh processors, collecting over 2.7 million experimental shots spanning distances up to d = 11. We demonstrate that a single trained model generalizes to unseen qubit chains and new calibration data acquired days later without retraining. On these unseen experiments, the FiLM-conditioned decoder achieves up to an 11.1x reduction in logical error rate relative to modified minimum-weight perfect matching. We observe that by employing a network architecture that exploits the highly asynchronous nature of system calibration and decoding, hardware-conditioned neural decoding demonstrates promising, adaptive performance with negligible latency overhead relative to unconditioned baselines.",
      "url": "http://arxiv.org/abs/2601.16123v1",
      "published_time_eastern_timestamp": 1769102612.0
    },
    {
      "title": "SAMTok: Representing Any Mask with Two Words",
      "summary": "Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.",
      "url": "http://arxiv.org/abs/2601.16093v1",
      "published_time_eastern_timestamp": 1769100249.0
    },
    {
      "title": "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models",
      "summary": "Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.",
      "url": "http://arxiv.org/abs/2601.16065v1",
      "published_time_eastern_timestamp": 1769097776.0
    },
    {
      "title": "Towards a Modern Theory of Chiralization",
      "summary": "The Modern Theory of Polarization, which rigorously defines the spontaneous electric polarization of a periodic solid and provides a recipe for its computation in electronic structure codes, transformed our understanding of ferroelectricity and related dielectric properties. Here we call for the development of an analogous Modern Theory of Chiralization. We review earlier attempts to quantify chirality, highlight the fundamental and practical developments that a modern theory would facilitate, and suggest possible promising routes to its establishment.",
      "url": "http://arxiv.org/abs/2601.16042v1",
      "published_time_eastern_timestamp": 1769095120.0
    },
    {
      "title": "Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval",
      "summary": "Large Language Models (LLMs) can aid synthesis planning in chemistry, but standard prompting methods often yield hallucinated or outdated suggestions. We study LLM interactions with a reaction knowledge graph by casting reaction path retrieval as a Text2Cypher (natural language to graph query) generation problem, and define single- and multi-step retrieval tasks. We compare zero-shot prompting to one-shot variants using static, random, and embedding-based exemplar selection, and assess a checklist-driven validator/corrector loop. To evaluate our framework, we consider query validity and retrieval accuracy. We find that one-shot prompting with aligned exemplars consistently performs best. Our checklist-style self-correction loop mainly improves executability in zero-shot settings and offers limited additional retrieval gains once a good exemplar is present. We provide a reproducible Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for synthesis planning. Code is available at https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.",
      "url": "http://arxiv.org/abs/2601.16038v1",
      "published_time_eastern_timestamp": 1769094662.0
    },
    {
      "title": "Collision-Free Humanoid Traversal in Cluttered Indoor Scenes",
      "summary": "We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.",
      "url": "http://arxiv.org/abs/2601.16035v1",
      "published_time_eastern_timestamp": 1769094533.0
    },
    {
      "title": "Low-Complexity Sparse Superimposed Coding for Ultra Reliable Low Latency Communications",
      "summary": "Sparse superimposed coding (SSC) has emerged as a promising technique for short-packet transmission in ultra-reliable low-latency communication scenarios. However, conventional SSC schemes often suffer from high encoding and decoding complexity due to the use of dense codebook matrices. In this paper, we propose a low-complexity SSC scheme by designing a sparse codebook structure, where each codeword contains only a small number of non-zero elements. The decoding is performed using the traditional multipath matching pursuit algorithm, and the overall complexity is significantly reduced by exploiting the sparsity of the codebook. Simulation results show that the proposed scheme achieves a favorable trade-off between BLER performance and computational complexity, and exhibits strong robustness across different transmission block lengths.",
      "url": "http://arxiv.org/abs/2601.16012v1",
      "published_time_eastern_timestamp": 1769092700.0
    },
    {
      "title": "Prioritizing Configuration Relevance via Compiler-Based Refined Feature Ranking",
      "summary": "Modern programming languages, most notably Rust, offer advanced linguistic constructs for building highly configurable software systems as aggregation of features -- identified by a configuration. However, they pose substantial challenges for program analysis, optimization, and testing, as the combinatorial explosion of configurations often makes exhaustive exploration infeasible. In this manuscript, we present the first compiler-based method for prioritizing configurations. Our approach consists of four main steps: 1. extracting a tailored intermediate representation from the Rust compiler, 2. constructing two complementary graph-based data structures, 3. using centrality measures to rank features, and 4. refining the ranking by considering the extent of code they impact. A fixed number of most relevant configurations are generated based on the achieved feature ranking. The validity of the generated configurations is guaranteed by using a SAT solver that takes a representation of this graph in conjunctive normal form. We formalized this approach and implemented it in a prototype, RustyEx, by instrumenting the Rust compiler. An empirical evaluation on higher-ranked open source Rust projects shows that RustyEx efficiently generates user-specified sets of configurations within bounded resources, while ensuring soundness by construction. The results demonstrate that centrality-guided configuration prioritization enables effective and practical exploration of large configuration spaces, paving the way for future research in configuration-aware analysis and optimization.",
      "url": "http://arxiv.org/abs/2601.16008v1",
      "published_time_eastern_timestamp": 1769092449.0
    },
    {
      "title": "Wigner's Friend as a Circuit: Inter-Branch Communication Witness Benchmarks on Superconducting Quantum Hardware",
      "summary": "We implement and benchmark on IBM Quantum hardware the circuit family proposed by Violaris for estimating operational inter-branch communication witnesses, defined as correlations in classical measurement records produced by compiled Wigner's-friend-style circuits. We realize a five-qubit instance of the protocol as an inter-register message-transfer pattern within a single circuit, rather than physical signaling, and evaluate its behavior under realistic device noise and compilation constraints. The circuit encodes branch-conditioned evolution of an observer subsystem whose dynamics depend on a control qubit, followed by a controlled transfer operation that probes correlations between conditional measurement contexts.\n  Executing on the ibm_fez backend with 20000 shots, we observe population-based visibility of 0.877, coherence witnesses of 0.840 and -0.811 along orthogonal axes, and a phase-sensitive magnitude of approximately 1.17. While the visibility metric is insensitive to some classes of dephasing, the coherence witnesses provide complementary sensitivity to off-diagonal noise.\n  This work does not test or discriminate among interpretations of quantum mechanics. Instead, it provides a reproducible operational constraint pipeline for evaluating detectability of non-ideal channels relative to calibrated device noise.",
      "url": "http://arxiv.org/abs/2601.16004v1",
      "published_time_eastern_timestamp": 1769092209.0
    },
    {
      "title": "Efficient Cloud-edge Collaborative Approaches to SPARQL Queries over Large RDF graphs",
      "summary": "With the increasing use of RDF graphs, storing and querying such data using SPARQL remains a critical problem. Current mainstream solutions rely on cloud-based data management architectures, but often suffer from performance bottle- necks in environments with limited bandwidth or high system load. To address this issue, this paper explores for the first time the integration of edge computing to move graph data storage and processing to edge environments, thereby improving query performance. This approach requires offloading query processing to edge servers, which involves addressing two challenges: data localization and network scheduling. First, the data localization challenge lies in computing the subgraphs maintained on edge servers to quickly identify the servers that can handle specific queries. To address this challenge, we introduce a new concept of pattern-induced subgraphs. Second, the network scheduling challenge involves efficiently assigning queries to edge and cloud servers to optimize overall system performance. We tackle this by constructing a overall system model that jointly captures data distribution, query characteristics, network communication, and computational resources. Accordingly, we further propose a joint formulation of query assignment and computational resource allocation, modeling it as a Mixed Integer Nonlinear Programming (MINLP) problem and solve this problem using a modified branch-and-bound algorithm. Experimental results on real datasets under a real cloud platform demonstrate that our proposed method outperforms the state-of-the-art baseline methods in terms of efficiency. The codes are available on GitHub",
      "url": "http://arxiv.org/abs/2601.15992v1",
      "published_time_eastern_timestamp": 1769091277.0
    },
    {
      "title": "Real-Time Inviscid Fluid Dynamics and Aero-acoustics on a Sphere",
      "summary": "Real-time fluid and aeroacoustic simulation on complex surfaces can have interactive applications - from globe-based weather visualizations to immersive computer games with physically accurate wind and sound. However, conventional grid-based solvers struggle with numerical instability near surface singularities, and mesh-based approaches lack a straightforward path to solving partial differential equations (PDEs) with stable, high-order accuracy.\n  Our model presents a unified framework for real-time inviscid fluid simulation and aeroacoustics on spherical surfaces with embedded obstacles, combining the Closest Point Method (CPM), projection-based Navier-Stokes solvers, and the Ffowcs Williams-Hawkings (FWH) analogy. CPM enables surface PDEs to be solved in a Cartesian embedding without parametrization by restricting computation to a narrow band around the sphere. Each band point is mapped to its nearest surface location, where band operators project results onto the local tangent space. Surface obstacles are modelled with signed distance functions (SDFs), enforcing no-slip velocity constraints and Bernoulli-based pressure adjustments for consistent real-world boundary interactions. Aeroacoustic sources are computed directly from surface pressure force derivatives and mapped to real-time audio via frequency and amplitude modulation with artifact-suppressing hysteresis smoothing.\n  Our findings from this model simulate the behaviour of inviscid fluid on spherical surfaces while generating sound using the pressure of the fluid flowing on the surface. This approach gives results that offer stability, geometric consistency, and support applications in scientific visualization, virtual reality, and educational tools.",
      "url": "http://arxiv.org/abs/2601.15982v1",
      "published_time_eastern_timestamp": 1769090643.0
    },
    {
      "title": "Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems",
      "summary": "Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \\textcolor{blue}{\\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\\_calibr}}. The video is available at \\textcolor{blue}{\\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}",
      "url": "http://arxiv.org/abs/2601.15946v1",
      "published_time_eastern_timestamp": 1769088489.0
    }
  ]
}
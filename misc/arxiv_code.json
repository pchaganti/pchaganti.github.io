{
  "last_updated": "2025-07-08T21:01:04.944956-04:00",
  "papers": [
    {
      "title": "Beyond One Shot, Beyond One Perspective: Cross-View and Long-Horizon\n  Distillation for Better LiDAR Representations",
      "summary": "LiDAR representation learning aims to extract rich structural and semantic\ninformation from large-scale, readily available datasets, reducing reliance on\ncostly human annotations. However, existing LiDAR representation strategies\noften overlook the inherent spatiotemporal cues in LiDAR sequences, limiting\ntheir effectiveness. In this work, we propose LiMA, a novel long-term\nimage-to-LiDAR Memory Aggregation framework that explicitly captures longer\nrange temporal correlations to enhance LiDAR representation learning. LiMA\ncomprises three key components: 1) a Cross-View Aggregation module that aligns\nand fuses overlapping regions across neighboring camera views, constructing a\nmore unified and redundancy-free memory bank; 2) a Long-Term Feature\nPropagation mechanism that efficiently aligns and integrates multi-frame image\nfeatures, reinforcing temporal coherence during LiDAR representation learning;\nand 3) a Cross-Sequence Memory Alignment strategy that enforces consistency\nacross driving sequences, improving generalization to unseen environments. LiMA\nmaintains high pretraining efficiency and incurs no additional computational\noverhead during downstream tasks. Extensive experiments on mainstream\nLiDAR-based perception benchmarks demonstrate that LiMA significantly improves\nboth LiDAR semantic segmentation and 3D object detection. We hope this work\ninspires more effective pretraining paradigms for autonomous driving. The code\nhas be made publicly accessible for future research.",
      "url": "http://arxiv.org/abs/2507.05260v1",
      "published_time_eastern_timestamp": 1751911198.0
    },
    {
      "title": "Spatio-Temporal LLM: Reasoning about Environments and Actions",
      "summary": "Despite the significant recent progress of Multimodal Large Language Models\n(MLLMs), MLLMs still struggle to correctly answer prompts that require a\nholistic spatio-temporal understanding. Specifically, it is challenging to\naddress prompts that refer to 1) the entirety of an environment that an agent\nequipped with an MLLM can operate in; and simultaneously also refer to 2)\nrecent actions that just happened and are encoded in a video clip. However,\nsuch a holistic spatio-temporal understanding is important for agents operating\nin the real world. To address this issue, we first develop a framework to\ncollect a large-scale dataset. Using the collected \"Reasoning about\nEnvironments and Actions\" (REA) dataset, we show that recent methods indeed\nstruggle to correctly answer the prompts. To improve, we develop a\n\"spatio-temporal LLM\" (ST-LLM), a model equipped with projectors to improve\nboth spatial understanding of an environment and temporal understanding of\nrecent observations. On the collected REA data, we show that the proposed\nmethod significantly improves results compared to prior work. Code and data are\navailable at https://zoezheng126.github.io/STLLM-website/.",
      "url": "http://arxiv.org/abs/2507.05258v1",
      "published_time_eastern_timestamp": 1751911195.0
    },
    {
      "title": "Response Attack: Exploiting Contextual Priming to Jailbreak Large\n  Language Models",
      "summary": "Contextual priming, where earlier stimuli covertly bias later judgments,\noffers an unexplored attack surface for large language models (LLMs). We\nuncover a contextual priming vulnerability in which the previous response in\nthe dialogue can steer its subsequent behavior toward policy-violating content.\nBuilding on this insight, we propose Response Attack, which uses an auxiliary\nLLM to generate a mildly harmful response to a paraphrased version of the\noriginal malicious query. They are then formatted into the dialogue and\nfollowed by a succinct trigger prompt, thereby priming the target model to\ngenerate harmful content. Across eight open-source and proprietary LLMs, RA\nconsistently outperforms seven state-of-the-art jailbreak techniques, achieving\nhigher attack success rates. To mitigate this threat, we construct and release\na context-aware safety fine-tuning dataset, which significantly reduces the\nattack success rate while preserving model capabilities. The code and data are\navailable at https://github.com/Dtc7w3PQ/Response-Attack.",
      "url": "http://arxiv.org/abs/2507.05248v1",
      "published_time_eastern_timestamp": 1751910965.0
    },
    {
      "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
      "summary": "The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.",
      "url": "http://arxiv.org/abs/2507.05241v2",
      "published_time_eastern_timestamp": 1751910652.0
    },
    {
      "title": "Cooperative Gradient Coding",
      "summary": "This work studies gradient coding (GC) in the context of distributed training\nproblems with unreliable communication. We propose cooperative GC (CoGC), a\nnovel gradient-sharing-based GC framework that leverages cooperative\ncommunication among clients. This approach ultimately eliminates the need for\ndataset replication, making it both communication- and computation-efficient\nand suitable for federated learning (FL). By employing the standard GC decoding\nmechanism, CoGC yields strictly binary outcomes: either the global model is\nexactly recovered, or the decoding fails entirely, with no intermediate\nresults. This characteristic ensures the optimality of the training and\ndemonstrates strong resilience to client-to-server communication failures when\nthe communication channels among clients are in good condition. However, it may\nalso result in communication inefficiency and hinder convergence due to its\nlack of flexibility, especially when communication channels among clients are\nin poor condition. To overcome this limitation and further harness the\npotential of GC matrices, we propose a complementary decoding mechanism, termed\nGC$^+$, which leverages information that would otherwise be discarded during GC\ndecoding failures. This approach significantly improves system reliability\nunder unreliable communication, as the full recovery of the global model\ntypically dominates in GC$^+$. To conclude, this work establishes solid\ntheoretical frameworks for both CoGC and GC$^+$. We provide complete outage\nanalyses for each decoding mechanism, along with a rigorous investigation of\nhow outages affect the structure and performance of GC matrices. Building on\nthese analyses, we derive convergence bounds for both decoding mechanisms.\nFinally, the effectiveness of CoGC and GC$^+$ is validated through extensive\nsimulations.",
      "url": "http://arxiv.org/abs/2507.05230v1",
      "published_time_eastern_timestamp": 1751909998.0
    },
    {
      "title": "Real-Time AI-Driven Pipeline for Automated Medical Study Content\n  Generation in Low-Resource Settings: A Kenyan Case Study",
      "summary": "Juvenotes is a real-time AI-driven pipeline that automates the transformation\nof academic documents into structured exam-style question banks, optimized for\nlow-resource medical education settings in Kenya. The system combines Azure\nDocument Intelligence for OCR and Azure AI Foundry (OpenAI o3-mini) for\nquestion and answer generation in a microservices architecture, with a\nVue/TypeScript frontend and AdonisJS backend. Mobile-first design,\nbandwidth-sensitive interfaces, institutional tagging, and offline features\naddress local challenges. Piloted over seven months at Kenyan medical\ninstitutions, Juvenotes reduced content curation time from days to minutes and\nincreased daily active users by 40%. Ninety percent of students reported\nimproved study experiences. Key challenges included intermittent connectivity\nand AI-generated errors, highlighting the need for offline sync and human\nvalidation. Juvenotes shows that AI automation with contextual UX can enhance\naccess to quality study materials in low-resource settings.",
      "url": "http://arxiv.org/abs/2507.05212v1",
      "published_time_eastern_timestamp": 1751908937.0
    },
    {
      "title": "All in One: Visual-Description-Guided Unified Point Cloud Segmentation",
      "summary": "Unified segmentation of 3D point clouds is crucial for scene understanding,\nbut is hindered by its sparse structure, limited annotations, and the challenge\nof distinguishing fine-grained object classes in complex environments. Existing\nmethods often struggle to capture rich semantic and contextual information due\nto limited supervision and a lack of diverse multimodal cues, leading to\nsuboptimal differentiation of classes and instances. To address these\nchallenges, we propose VDG-Uni3DSeg, a novel framework that integrates\npre-trained vision-language models (e.g., CLIP) and large language models\n(LLMs) to enhance 3D segmentation. By leveraging LLM-generated textual\ndescriptions and reference images from the internet, our method incorporates\nrich multimodal cues, facilitating fine-grained class and instance separation.\nWe further design a Semantic-Visual Contrastive Loss to align point features\nwith multimodal queries and a Spatial Enhanced Module to model scene-wide\nrelationships efficiently. Operating within a closed-set paradigm that utilizes\nmultimodal knowledge generated offline, VDG-Uni3DSeg achieves state-of-the-art\nresults in semantic, instance, and panoptic segmentation, offering a scalable\nand practical solution for 3D understanding. Our code is available at\nhttps://github.com/Hanzy1996/VDG-Uni3DSeg.",
      "url": "http://arxiv.org/abs/2507.05211v1",
      "published_time_eastern_timestamp": 1751908920.0
    },
    {
      "title": "In-Context Learning as an Effective Estimator of Functional Correctness\n  of LLM-Generated Code",
      "summary": "When applying LLM-based code generation to software development projects that\nfollow a feature-driven or rapid application development approach, it becomes\nnecessary to estimate the functional correctness of the generated code in the\nabsence of test cases. Just as a user selects a relevant document from a ranked\nlist of retrieved ones, a software generation workflow requires a developer to\nchoose (and potentially refine) a generated solution from a ranked list of\nalternative solutions, ordered by their posterior likelihoods. This implies\nthat estimating the quality of a ranked list -- akin to estimating \"relevance\"\nfor query performance prediction (QPP) in IR -- is also crucial for generative\nsoftware development, where quality is defined in terms of \"functional\ncorrectness\". In this paper, we propose an in-context learning (ICL) based\napproach for code quality estimation. Our findings demonstrate that providing\nfew-shot examples of functionally correct code from a training set enhances the\nperformance of existing QPP approaches as well as a zero-shot-based approach\nfor code quality estimation.",
      "url": "http://arxiv.org/abs/2507.05200v1",
      "published_time_eastern_timestamp": 1751907677.0
    },
    {
      "title": "EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via\n  Embodied World Modeling",
      "summary": "The rapid advancement of Embodied AI has led to an increasing demand for\nlarge-scale, high-quality real-world data. However, collecting such embodied\ndata remains costly and inefficient. As a result, simulation environments have\nbecome a crucial surrogate for training robot policies. Yet, the significant\nReal2Sim2Real gap remains a critical bottleneck, particularly in terms of\nphysical dynamics and visual appearance. To address this challenge, we propose\nEmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both\nthe physics and appearance perspectives. Specifically, we propose PhysAligner,\na differentiable physics module designed to reduce the Real2Sim physical gap.\nIt jointly optimizes robot-specific parameters such as control gains and\nfriction coefficients to better align simulated dynamics with real-world\nobservations. In addition, we introduce VisAligner, which incorporates a\nconditional video diffusion model to bridge the Sim2Real appearance gap by\ntranslating low-fidelity simulated renderings into photorealistic videos\nconditioned on simulation states, enabling high-fidelity visual transfer.\nExtensive experiments validate the effectiveness of EmbodieDreamer. The\nproposed PhysAligner reduces physical parameter estimation error by 3.74%\ncompared to simulated annealing methods while improving optimization speed by\n89.91\\%. Moreover, training robot policies in the generated photorealistic\nenvironment leads to a 29.17% improvement in the average task success rate\nacross real-world tasks after reinforcement learning. Code, model and data will\nbe publicly available.",
      "url": "http://arxiv.org/abs/2507.05198v1",
      "published_time_eastern_timestamp": 1751907497.0
    },
    {
      "title": "CREW-WILDFIRE: Benchmarking Agentic Multi-Agent Collaborations at Scale",
      "summary": "Despite rapid progress in large language model (LLM)-based multi-agent\nsystems, current benchmarks fall short in evaluating their scalability,\nrobustness, and coordination capabilities in complex, dynamic, real-world\ntasks. Existing environments typically focus on small-scale, fully observable,\nor low-complexity domains, limiting their utility for developing and assessing\nnext-generation multi-agent Agentic AI frameworks. We introduce CREW-Wildfire,\nan open-source benchmark designed to close this gap. Built atop the human-AI\nteaming CREW simulation platform, CREW-Wildfire offers procedurally generated\nwildfire response scenarios featuring large maps, heterogeneous agents, partial\nobservability, stochastic dynamics, and long-horizon planning objectives. The\nenvironment supports both low-level control and high-level natural language\ninteractions through modular Perception and Execution modules. We implement and\nevaluate several state-of-the-art LLM-based multi-agent Agentic AI frameworks,\nuncovering significant performance gaps that highlight the unsolved challenges\nin large-scale coordination, communication, spatial reasoning, and long-horizon\nplanning under uncertainty. By providing more realistic complexity, scalable\narchitecture, and behavioral evaluation metrics, CREW-Wildfire establishes a\ncritical foundation for advancing research in scalable multi-agent Agentic\nintelligence. All code, environments, data, and baselines will be released to\nsupport future research in this emerging domain.",
      "url": "http://arxiv.org/abs/2507.05178v1",
      "published_time_eastern_timestamp": 1751906022.0
    },
    {
      "title": "OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech\n  Language Model",
      "summary": "Empathetic interaction is a cornerstone of human-machine communication, due\nto the need for understanding speech enriched with paralinguistic cues and\ngenerating emotional and expressive responses. However, the most powerful\nempathetic LSLMs are increasingly closed off, leaving the crucial details about\nthe architecture, data and development opaque to researchers. Given the\ncritical need for transparent research into the LSLMs and empathetic behavior,\nwe present OpenS2S, a fully open-source, transparent and end-to-end LSLM\ndesigned to enable empathetic speech interactions. Based on our empathetic\nspeech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved\ndecoding architecture to achieve low-latency speech generation. To facilitate\nend-to-end training, OpenS2S incorporates an automated data construction\npipeline that synthesizes diverse, high-quality empathetic speech dialogues at\nlow cost. By leveraging large language models to generate empathetic content\nand controllable text-to-speech systems to introduce speaker and emotional\nvariation, we construct a scalable training corpus with rich paralinguistic\ndiversity and minimal human supervision. We release the fully open-source\nOpenS2S model, including the dataset, model weights, pre-training and\nfine-tuning codes, to empower the broader research community and accelerate\ninnovation in empathetic speech systems. The project webpage can be accessed at\nhttps://casia-lm.github.io/OpenS2S",
      "url": "http://arxiv.org/abs/2507.05177v2",
      "published_time_eastern_timestamp": 1751905897.0
    },
    {
      "title": "Vector Cost Bimatrix Games with Applications to Autonomous Racing",
      "summary": "We formulate a vector cost alternative to the scalarization method for\nweighting and combining multi-objective costs. The algorithm produces solutions\nto bimatrix games that are simultaneously pure, unique Nash equilibria and\nPareto optimal with guarantees for avoiding worst case outcomes. We achieve\nthis by enforcing exact potential game constraints to guide cost adjustments\ntowards equilibrium, while minimizing the deviation from the original cost\nstructure. The magnitude of this adjustment serves as a metric for\ndifferentiating between Pareto optimal solutions. We implement this approach in\na racing competition between agents with heterogeneous cost structures,\nresulting in fewer collision incidents with a minimal decrease in performance.\nCode is available at https://github.com/toazbenj/race_simulation.",
      "url": "http://arxiv.org/abs/2507.05171v1",
      "published_time_eastern_timestamp": 1751905463.0
    },
    {
      "title": "Bootstrap Current Modeling in M3D-C1",
      "summary": "Bootstrap current plays a crucial role in the equilibrium of magnetically\nconfined plasmas, particularly in quasisymmetric (QS) stellarators and in\ntokamaks, where it can represent bulk of the electric current density. Accurate\nmodeling of this current is essential for understanding the magnetohydrodynamic\n(MHD) equilibrium and stability of these configurations. This study expands the\nmodeling capabilities of M3D-C1, an extended-MHD code, by implementing\nself-consistent physics models for bootstrap current. It employs two analytical\nframeworks: a generalized Sauter model (Sauter et al. (1999)), and a revised\nSauter-like model (Redl et al. (2021)). The isomorphism described by Landreman\net al. (2022) is employed to apply these models to quasisymmetric stellarators.\nThe implementation in M3D-C1 is benchmarked against neoclassical codes,\nincluding NEO, XGCa, and SFINCS, showing excellent agreement. These\nimprovements allow M3D-C1 to self-consistently calculate the neoclassical\ncontributions to plasma current in axisymmetric and quasisymmetric\nconfigurations, providing a more accurate representation of the plasma behavior\nin these configurations. A workflow for evaluating the neoclassical transport\nusing SFINCS with arbitrary toroidal equilibria calculated using M3D-C1 is also\npresented. This workflow enables a quantitative evaluation of the error in the\nSauter-like model in cases that deviate from axi- or quasi-symmetry (e.g.,\nthrough the development of an MHD instability).",
      "url": "http://arxiv.org/abs/2507.05166v1",
      "published_time_eastern_timestamp": 1751905319.0
    },
    {
      "title": "Differential Attention for Multimodal Crisis Event Analysis",
      "summary": "Social networks can be a valuable source of information during crisis events.\nIn particular, users can post a stream of multimodal data that can be critical\nfor real-time humanitarian response. However, effectively extracting meaningful\ninformation from this large and noisy data stream and effectively integrating\nheterogeneous data remains a formidable challenge. In this work, we explore\nvision language models (VLMs) and advanced fusion strategies to enhance the\nclassification of crisis data in three different tasks. We incorporate\nLLaVA-generated text to improve text-image alignment. Additionally, we leverage\nContrastive Language-Image Pretraining (CLIP)-based vision and text embeddings,\nwhich, without task-specific fine-tuning, outperform traditional models. To\nfurther refine multimodal fusion, we employ Guided Cross Attention (Guided CA)\nand combine it with the Differential Attention mechanism to enhance feature\nalignment by emphasizing critical information while filtering out irrelevant\ncontent. Our results show that while Differential Attention improves\nclassification performance, Guided CA remains highly effective in aligning\nmultimodal features. Extensive experiments on the CrisisMMD benchmark data set\ndemonstrate that the combination of pretrained VLMs, enriched textual\ndescriptions, and adaptive fusion strategies consistently outperforms\nstate-of-the-art models in classification accuracy, contributing to more\nreliable and interpretable models for three different tasks that are crucial\nfor disaster response. Our code is available at\nhttps://github.com/Munia03/Multimodal_Crisis_Event.",
      "url": "http://arxiv.org/abs/2507.05165v1",
      "published_time_eastern_timestamp": 1751905235.0
    },
    {
      "title": "LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral\n  Domains",
      "summary": "The recent proliferation of photorealistic AI-generated images (AIGI) has\nraised urgent concerns about their potential misuse, particularly on social\nmedia platforms. Current state-of-the-art AIGI detection methods typically rely\non large, deep neural architectures, creating significant computational\nbarriers to real-time, large-scale deployment on platforms like social media.\nTo challenge this reliance on computationally intensive models, we introduce\nLAID, the first framework -- to our knowledge -- that benchmarks and evaluates\nthe detection performance and efficiency of off-the-shelf lightweight neural\nnetworks. In this framework, we comprehensively train and evaluate selected\nmodels on a representative subset of the GenImage dataset across spatial,\nspectral, and fusion image domains. Our results demonstrate that lightweight\nmodels can achieve competitive accuracy, even under adversarial conditions,\nwhile incurring substantially lower memory and computation costs compared to\ncurrent state-of-the-art methods. This study offers valuable insight into the\ntrade-off between efficiency and performance in AIGI detection and lays a\nfoundation for the development of practical, scalable, and trustworthy\ndetection systems. The source code of LAID can be found at:\nhttps://github.com/nchivar/LAID.",
      "url": "http://arxiv.org/abs/2507.05162v1",
      "published_time_eastern_timestamp": 1751905099.0
    },
    {
      "title": "AI Generated Text Detection Using Instruction Fine-tuned Large Language\n  and Transformer-Based Models",
      "summary": "Large Language Models (LLMs) possess an extraordinary capability to produce\ntext that is not only coherent and contextually relevant but also strikingly\nsimilar to human writing. They adapt to various styles and genres, producing\ncontent that is both grammatically correct and semantically meaningful.\nRecently, LLMs have been misused to create highly realistic phishing emails,\nspread fake news, generate code to automate cyber crime, and write fraudulent\nscientific articles. Additionally, in many real-world applications, the\ngenerated content including style and topic and the generator model are not\nknown beforehand. The increasing prevalence and sophistication of artificial\nintelligence (AI)-generated texts have made their detection progressively more\nchallenging. Various attempts have been made to distinguish machine-generated\ntext from human-authored content using linguistic, statistical, machine\nlearning, and ensemble-based approaches. This work focuses on two primary\nobjectives Task-A, which involves distinguishing human-written text from\nmachine-generated text, and Task-B, which attempts to identify the specific LLM\nmodel responsible for the generation. Both of these tasks are based on fine\ntuning of Generative Pre-trained Transformer (GPT_4o-mini), Large Language\nModel Meta AI (LLaMA) 3 8B, and Bidirectional Encoder Representations from\nTransformers (BERT). The fine-tuned version of GPT_4o-mini and the BERT model\nhas achieved accuracies of 0.9547 for Task-A and 0.4698 for Task-B.",
      "url": "http://arxiv.org/abs/2507.05157v1",
      "published_time_eastern_timestamp": 1751904793.0
    },
    {
      "title": "Latent Motion Profiling for Annotation-free Cardiac Phase Detection in\n  Adult and Fetal Echocardiography Videos",
      "summary": "The identification of cardiac phase is an essential step for analysis and\ndiagnosis of cardiac function. Automatic methods, especially data-driven\nmethods for cardiac phase detection, typically require extensive annotations,\nwhich is time-consuming and labor-intensive. In this paper, we present an\nunsupervised framework for end-diastole (ED) and end-systole (ES) detection\nthrough self-supervised learning of latent cardiac motion trajectories from\n4-chamber-view echocardiography videos. Our method eliminates the need for\nmanual annotations, including ED and ES indices, segmentation, or volumetric\nmeasurements, by training a reconstruction model to encode interpretable\nspatiotemporal motion patterns. Evaluated on the EchoNet-Dynamic benchmark, the\napproach achieves mean absolute error (MAE) of 3 frames (58.3 ms) for ED and 2\nframes (38.8 ms) for ES detection, matching state-of-the-art supervised\nmethods. Extended to fetal echocardiography, the model demonstrates robust\nperformance with MAE 1.46 frames (20.7 ms) for ED and 1.74 frames (25.3 ms) for\nES, despite the fact that the fetal heart model is built using non-standardized\nheart views due to fetal heart positioning variability. Our results demonstrate\nthe potential of the proposed latent motion trajectory strategy for cardiac\nphase detection in adult and fetal echocardiography. This work advances\nunsupervised cardiac motion analysis, offering a scalable solution for clinical\npopulations lacking annotated data. Code will be released at\nhttps://github.com/YingyuYyy/CardiacPhase.",
      "url": "http://arxiv.org/abs/2507.05154v1",
      "published_time_eastern_timestamp": 1751904646.0
    },
    {
      "title": "SV-DRR: High-Fidelity Novel View X-Ray Synthesis Using Diffusion Model",
      "summary": "X-ray imaging is a rapid and cost-effective tool for visualizing internal\nhuman anatomy. While multi-view X-ray imaging provides complementary\ninformation that enhances diagnosis, intervention, and education, acquiring\nimages from multiple angles increases radiation exposure and complicates\nclinical workflows. To address these challenges, we propose a novel\nview-conditioned diffusion model for synthesizing multi-view X-ray images from\na single view. Unlike prior methods, which are limited in angular range,\nresolution, and image quality, our approach leverages the Diffusion Transformer\nto preserve fine details and employs a weak-to-strong training strategy for\nstable high-resolution image generation. Experimental results demonstrate that\nour method generates higher-resolution outputs with improved control over\nviewing angles. This capability has significant implications not only for\nclinical applications but also for medical education and data extension,\nenabling the creation of diverse, high-quality datasets for training and\nanalysis. Our code is available at GitHub.",
      "url": "http://arxiv.org/abs/2507.05148v1",
      "published_time_eastern_timestamp": 1751903891.0
    },
    {
      "title": "VERITAS: Verification and Explanation of Realness in Images for\n  Transparency in AI Systems",
      "summary": "The widespread and rapid adoption of AI-generated content, created by models\nsuch as Generative Adversarial Networks (GANs) and Diffusion Models, has\nrevolutionized the digital media landscape by allowing efficient and creative\ncontent generation. However, these models also blur the difference between real\nimages and AI-generated synthetic images, raising concerns regarding content\nauthenticity and integrity. While many existing solutions to detect fake images\nfocus solely on classification and higher-resolution images, they often lack\ntransparency in their decision-making, making it difficult for users to\nunderstand why an image is classified as fake. In this paper, we present\nVERITAS, a comprehensive framework that not only accurately detects whether a\nsmall (32x32) image is AI-generated but also explains why it was classified\nthat way through artifact localization and semantic reasoning. VERITAS produces\nhuman-readable explanations that describe key artifacts in synthetic images. We\nshow that this architecture offers clear explanations of the basis of zero-shot\nsynthetic image detection tasks. Code and relevant prompts can be found at\nhttps://github.com/V-i-g-n-e-s-h-N/VERITAS .",
      "url": "http://arxiv.org/abs/2507.05146v1",
      "published_time_eastern_timestamp": 1751903825.0
    },
    {
      "title": "Clinical test cases for model-based dose calculation algorithm\n  commissioning, QA and benchmarking, for 192Ir HDR brachytherapy of\n  gynecologic cancers",
      "summary": "Purpose: To develop clinically relevant test cases for commissioning\nModel-Based Dose Calculation Algorithms (MBDCAs) for 192Ir High Dose Rate (HDR)\ngynecologic brachytherapy following the workflow proposed by the TG-186 report\nand the WGDCAB report 372. Acquisition and Validation Methods: Two cervical\ncancer intracavitary HDR brachytherapy patient models were created, using\neither uniformly structured regions or realistic segmentation. The computed\ntomography (CT) images of the models were converted to DICOM CT images via\nMATLAB and imported into two Treatment Planning Systems (TPSs) with MBDCA\ncapability. The clinical segmentation was expanded to include additional organs\nat risk. The actual clinical treatment plan was generally maintained, with the\nsource replaced by a generic 192Ir HDR source. Dose to medium in medium\ncalculations were performed using the MBDCA option of each TPS, and three\ndifferent Monte Carlo (MC) simulation codes. MC results agreed within\nstatistical uncertainty, while comparisons between MBDCA and MC dose\ndistributions highlighted both strengths and limitations of the studied MBDCAs,\nsuggesting potential approaches to overcome the challenges. Data Format and\nUsage Notes: The datasets for the developed cases are available online at\nhttp://doi.org/ 10.5281/zenodo.15720996. The DICOM files include the treatment\nplan for each case, TPS, and the corresponding reference MC dose data. The\npackage also contains a TPS- and case-specific user guide for commissioning the\nMBDCAs, and files needed to replicate the MC simulations. Potential\nApplications: The provided datasets and proposed methodology offer a\ncommissioning framework for TPSs using MBDCAs, and serve as a benchmark for\nbrachytherapy researchers using MC methods. They also facilitate\nintercomparisons of MBDCA performance and provide a quality assurance resource\nfor evaluating future TPS software updates.",
      "url": "http://arxiv.org/abs/2507.05144v1",
      "published_time_eastern_timestamp": 1751903754.0
    }
  ]
}
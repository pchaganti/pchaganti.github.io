{
  "last_updated": "2025-07-24T08:26:08.433359-04:00",
  "papers": [
    {
      "title": "Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for\n  Non-Overlapping Users",
      "summary": "Cross-domain recommendation (CDR) methods predominantly leverage overlapping\nusers to transfer knowledge from a source domain to a target domain. However,\nthrough empirical studies, we uncover a critical bias inherent in these\napproaches: while overlapping users experience significant enhancements in\nrecommendation quality, non-overlapping users benefit minimally and even face\nperformance degradation. This unfairness may erode user trust, and,\nconsequently, negatively impact business engagement and revenue. To address\nthis issue, we propose a novel solution that generates virtual source-domain\nusers for non-overlapping target-domain users. Our method utilizes a dual\nattention mechanism to discern similarities between overlapping and\nnon-overlapping users, thereby synthesizing realistic virtual user embeddings.\nWe further introduce a limiter component that ensures the generated virtual\nusers align with real-data distributions while preserving each user's unique\ncharacteristics. Notably, our method is model-agnostic and can be seamlessly\nintegrated into any CDR model. Comprehensive experiments conducted on three\npublic datasets with five CDR baselines demonstrate that our method effectively\nmitigates the CDR non-overlapping user bias, without loss of overall accuracy.\nOur code is publicly available at https://github.com/WeixinChen98/VUG.",
      "url": "http://arxiv.org/abs/2507.17749v1",
      "published_time_eastern_timestamp": 1753293548.0
    },
    {
      "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven\n  Approach to QA Benchmarks",
      "summary": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.",
      "url": "http://arxiv.org/abs/2507.17747v1",
      "published_time_eastern_timestamp": 1753293494.0
    },
    {
      "title": "Educational Insights from Code: Mapping Learning Challenges in\n  Object-Oriented Programming through Code-Based Evidence",
      "summary": "Object-Oriented programming is frequently challenging for undergraduate\nComputer Science students, particularly in understanding abstract concepts such\nas encapsulation, inheritance, and polymorphism. Although the literature\noutlines various methods to identify potential design and coding issues in\nobject-oriented programming through source code analysis, such as code smells\nand SOLID principles, few studies explore how these code-level issues relate to\nlearning difficulties in Object-Oriented Programming. In this study, we explore\nthe relationship of the code issue indicators with common challenges\nencountered during the learning of object-oriented programming. Using\nqualitative analysis, we identified the main categories of learning\ndifficulties and, through a literature review, established connections between\nthese difficulties, code smells, and violations of the SOLID principles. As a\nresult, we developed a conceptual map that links code-related issues to\nspecific learning challenges in Object-Oriented Programming. The model was then\nevaluated by an expert who applied it in the analysis of the student code to\nassess its relevance and applicability in educational contexts.",
      "url": "http://arxiv.org/abs/2507.17743v1",
      "published_time_eastern_timestamp": 1753293376.0
    },
    {
      "title": "Megrez2 Technical Report",
      "summary": "We present Megrez2, a novel lightweight and high-performance language model\narchitecture optimized for device native deployment. Megrez2 introduces a novel\ncross-layer expert sharing mechanism, which significantly reduces total\nparameter count by reusing expert modules across adjacent transformer layers\nwhile maintaining most of the model's capacity. It also incorporates pre-gated\nrouting, enabling memory-efficient expert loading and faster inference. As the\nfirst instantiation of the Megrez2 architecture, we introduce the\nMegrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and\nfurther enhanced through supervised fine-tuning and reinforcement learning with\nverifiable rewards. With only 3B activated and 7.5B stored parameters,\nMegrez2-Preview demonstrates competitive or superior performance compared to\nlarger models on a wide range of tasks, including language understanding,\ninstruction following, mathematical reasoning, and code generation. These\nresults highlight the effectiveness of the Megrez2 architecture to achieve a\nbalance between accuracy, efficiency, and deployability, making it a strong\ncandidate for real-world, resource-constrained applications.",
      "url": "http://arxiv.org/abs/2507.17728v1",
      "published_time_eastern_timestamp": 1753292587.0
    },
    {
      "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in\n  Languages of West Asia and North Africa",
      "summary": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.",
      "url": "http://arxiv.org/abs/2507.17709v1",
      "published_time_eastern_timestamp": 1753291228.0
    },
    {
      "title": "Joint Asymmetric Loss for Learning with Noisy Labels",
      "summary": "Learning with noisy labels is a crucial task for training accurate deep\nneural networks. To mitigate label noise, prior studies have proposed various\nrobust loss functions, particularly symmetric losses. Nevertheless, symmetric\nlosses usually suffer from the underfitting issue due to the overly strict\nconstraint. To address this problem, the Active Passive Loss (APL) jointly\noptimizes an active and a passive loss to mutually enhance the overall fitting\nability. Within APL, symmetric losses have been successfully extended, yielding\nadvanced robust loss functions. Despite these advancements, emerging\ntheoretical analyses indicate that asymmetric losses, a new class of robust\nloss functions, possess superior properties compared to symmetric losses.\nHowever, existing asymmetric losses are not compatible with advanced\noptimization frameworks such as APL, limiting their potential and\napplicability. Motivated by this theoretical gap and the prospect of asymmetric\nlosses, we extend the asymmetric loss to the more complex passive loss scenario\nand propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We\nrigorously establish the necessary and sufficient condition under which AMSE\nsatisfies the asymmetric condition. By substituting the traditional symmetric\npassive loss in APL with our proposed AMSE, we introduce a novel robust loss\nframework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate\nthe effectiveness of our method in mitigating label noise. Code available at:\nhttps://github.com/cswjl/joint-asymmetric-loss",
      "url": "http://arxiv.org/abs/2507.17692v1",
      "published_time_eastern_timestamp": 1753289863.0
    },
    {
      "title": "CASCADE: LLM-Powered JavaScript Deobfuscator at Google",
      "summary": "Software obfuscation, particularly prevalent in JavaScript, hinders code\ncomprehension and analysis, posing significant challenges to software testing,\nstatic analysis, and malware detection. This paper introduces CASCADE, a novel\nhybrid approach that integrates the advanced coding capabilities of Gemini with\nthe deterministic transformation capabilities of a compiler Intermediate\nRepresentation (IR), specifically JavaScript IR (JSIR). By employing Gemini to\nidentify critical prelude functions, the foundational components underlying the\nmost prevalent obfuscation techniques, and leveraging JSIR for subsequent code\ntransformations, CASCADE effectively recovers semantic elements like original\nstrings and API names, and reveals original program behaviors. This method\novercomes limitations of existing static and dynamic deobfuscation techniques,\neliminating hundreds to thousands of hardcoded rules while achieving\nreliability and flexibility. CASCADE is already deployed in Google's production\nenvironment, demonstrating substantial improvements in JavaScript deobfuscation\nefficiency and reducing reverse engineering efforts.",
      "url": "http://arxiv.org/abs/2507.17691v1",
      "published_time_eastern_timestamp": 1753289852.0
    },
    {
      "title": "Contextual Code Retrieval for Commit Message Generation: A Preliminary\n  Study",
      "summary": "A commit message describes the main code changes in a commit and plays a\ncrucial role in software maintenance. Existing commit message generation (CMG)\napproaches typically frame it as a direct mapping which inputs a code diff and\nproduces a brief descriptive sentence as output. However, we argue that relying\nsolely on the code diff is insufficient, as raw code diff fails to capture the\nfull context needed for generating high-quality and informative commit\nmessages. In this paper, we propose a contextual code retrieval-based method\ncalled C3Gen to enhance CMG by retrieving commit-relevant code snippets from\nthe repository and incorporating them into the model input to provide richer\ncontextual information at the repository scope. In the experiments, we\nevaluated the effectiveness of C3Gen across various models using four objective\nand three subjective metrics. Meanwhile, we design and conduct a human\nevaluation to investigate how C3Gen-generated commit messages are perceived by\nhuman developers. The results show that by incorporating contextual code into\nthe input, C3Gen enables models to effectively leverage additional information\nto generate more comprehensive and informative commit messages with greater\npractical value in real-world development scenarios. Further analysis\nunderscores concerns about the reliability of similaritybased metrics and\nprovides empirical insights for CMG.",
      "url": "http://arxiv.org/abs/2507.17690v1",
      "published_time_eastern_timestamp": 1753289697.0
    },
    {
      "title": "Audio-Vision Contrastive Learning for Phonological Class Recognition",
      "summary": "Accurate classification of articulatory-phonological features plays a vital\nrole in understanding human speech production and developing robust speech\ntechnologies, particularly in clinical contexts where targeted phonemic\nanalysis and therapy can improve disease diagnosis accuracy and personalized\nrehabilitation. In this work, we propose a multimodal deep learning framework\nthat combines real-time magnetic resonance imaging (rtMRI) and speech signals\nto classify three key articulatory dimensions: manner of articulation, place of\narticulation, and voicing. We perform classification on 15 phonological classes\nderived from the aforementioned articulatory dimensions and evaluate the system\nwith four audio/vision configurations: unimodal rtMRI, unimodal audio signals,\nmultimodal middle fusion, and contrastive learning-based audio-vision fusion.\nExperimental results on the USC-TIMIT dataset show that our contrastive\nlearning-based approach achieves state-of-the-art performance, with an average\nF1-score of 0.81, representing an absolute increase of 0.23 over the unimodal\nbaseline. The results confirm the effectiveness of contrastive representation\nlearning for multimodal articulatory analysis. Our code and processed dataset\nwill be made publicly available at\nhttps://github.com/DaE-plz/AC_Contrastive_Phonology to support future research.",
      "url": "http://arxiv.org/abs/2507.17682v1",
      "published_time_eastern_timestamp": 1753289062.0
    },
    {
      "title": "MCM: Mamba-based Cardiac Motion Tracking using Sequential Images in MRI",
      "summary": "Myocardial motion tracking is important for assessing cardiac function and\ndiagnosing cardiovascular diseases, for which cine cardiac magnetic resonance\n(CMR) has been established as the gold standard imaging modality. Many existing\nmethods learn motion from single image pairs consisting of a reference frame\nand a randomly selected target frame from the cardiac cycle. However, these\nmethods overlook the continuous nature of cardiac motion and often yield\ninconsistent and non-smooth motion estimations. In this work, we propose a\nnovel Mamba-based cardiac motion tracking network (MCM) that explicitly\nincorporates target image sequence from the cardiac cycle to achieve smooth and\ntemporally consistent motion tracking. By developing a bi-directional Mamba\nblock equipped with a bi-directional scanning mechanism, our method facilitates\nthe estimation of plausible deformation fields. With our proposed motion\ndecoder that integrates motion information from frames adjacent to the target\nframe, our method further enhances temporal coherence. Moreover, by taking\nadvantage of Mamba's structured state-space formulation, the proposed method\nlearns the continuous dynamics of the myocardium from sequential images without\nincreasing computational complexity. We evaluate the proposed method on two\npublic datasets. The experimental results demonstrate that the proposed method\nquantitatively and qualitatively outperforms both conventional and\nstate-of-the-art learning-based cardiac motion tracking methods. The code is\navailable at https://github.com/yjh-0104/MCM.",
      "url": "http://arxiv.org/abs/2507.17678v1",
      "published_time_eastern_timestamp": 1753288843.0
    },
    {
      "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?",
      "summary": "The process of meta-learning algorithms from data, instead of relying on\nmanual design, is growing in popularity as a paradigm for improving the\nperformance of machine learning systems. Meta-learning shows particular promise\nfor reinforcement learning (RL), where algorithms are often adapted from\nsupervised or unsupervised learning despite their suboptimality for RL.\nHowever, until now there has been a severe lack of comparison between different\nmeta-learning algorithms, such as using evolution to optimise over black-box\nfunctions or LLMs to propose code. In this paper, we carry out this empirical\ncomparison of the different approaches when applied to a range of meta-learned\nalgorithms which target different parts of the RL pipeline. In addition to\nmeta-train and meta-test performance, we also investigate factors including the\ninterpretability, sample cost and train time for each meta-learning algorithm.\nBased on these findings, we propose several guidelines for meta-learning new RL\nalgorithms which will help ensure that future learned algorithms are as\nperformant as possible.",
      "url": "http://arxiv.org/abs/2507.17668v1",
      "published_time_eastern_timestamp": 1753288298.0
    },
    {
      "title": "Monocular Semantic Scene Completion via Masked Recurrent Networks",
      "summary": "Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise\noccupancy and semantic category from a single-view RGB image. Existing methods\nadopt a single-stage framework that aims to simultaneously achieve visible\nregion segmentation and occluded region hallucination, while also being\naffected by inaccurate depth estimation. Such methods often achieve suboptimal\nperformance, especially in complex scenes. We propose a novel two-stage\nframework that decomposes MSSC into coarse MSSC followed by the Masked\nRecurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent\nUnit (MS-GRU) which concentrates on the occupied regions by the proposed mask\nupdating mechanism, and a sparse GRU design is proposed to reduce the\ncomputation cost. Additionally, we propose the distance attention projection to\nreduce projection errors by assigning different attention scores according to\nthe distance to the observed surface. Experimental results demonstrate that our\nproposed unified framework, MonoMRN, effectively supports both indoor and\noutdoor scenes and achieves state-of-the-art performance on the NYUv2 and\nSemanticKITTI datasets. Furthermore, we conduct robustness analysis under\nvarious disturbances, highlighting the role of the Masked Recurrent Network in\nenhancing the model's resilience to such challenges. The source code is\npublicly available.",
      "url": "http://arxiv.org/abs/2507.17661v1",
      "published_time_eastern_timestamp": 1753288185.0
    },
    {
      "title": "On Function-Correcting Codes in the Lee Metric",
      "summary": "Function-correcting codes are a coding framework designed to minimize\nredundancy while ensuring that specific functions or computations of encoded\ndata can be reliably recovered, even in the presence of errors. The choice of\nmetric is crucial in designing such codes, as it determines which computations\nmust be protected and how errors are measured and corrected. Previous work by\nLiu and Liu [6] studied function-correcting codes over $\\mathbb{Z}_{2^l},\\\nl\\geq 2$ using the homogeneous metric, which coincides with the Lee metric over\n$\\mathbb{Z}_4$. In this paper, we extend the study to codes over\n$\\mathbb{Z}_m,$ for any positive integer $m\\geq 2$ under the Lee metric and aim\nto determine their optimal redundancy. To achieve this, we introduce irregular\nLee distance codes and derive upper and lower bounds on the optimal redundancy\nby characterizing the shortest possible length of such codes. These general\nbounds are then simplified and applied to specific classes of functions,\nincluding Lee-local functions, Lee weight functions, and Lee weight\ndistribution functions, leading to improved some bounds compared to those of\nLiu and Liu [6] over $\\mathbb{Z}_4$ and generalize the other bounds over\n$\\mathbb{Z}_m$ in the Lee metric.",
      "url": "http://arxiv.org/abs/2507.17654v1",
      "published_time_eastern_timestamp": 1753287466.0
    },
    {
      "title": "CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous\n  Nuisance Shifts",
      "summary": "An important challenge when using computer vision models in the real world is\nto evaluate their performance in potential out-of-distribution (OOD) scenarios.\nWhile simple synthetic corruptions are commonly applied to test OOD robustness,\nthey often fail to capture nuisance shifts that occur in the real world.\nRecently, diffusion models have been applied to generate realistic images for\nbenchmarking, but they are restricted to binary nuisance shifts. In this work,\nwe introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD\nrobustness of image classifiers for continuous and realistic generative\nnuisance shifts. CNS-Bench allows generating a wide range of individual\nnuisance shifts in continuous severities by applying LoRA adapters to diffusion\nmodels. To address failure cases, we propose a filtering mechanism that\noutperforms previous methods, thereby enabling reliable benchmarking with\ngenerative models. With the proposed benchmark, we perform a large-scale study\nto evaluate the robustness of more than 40 classifiers under various nuisance\nshifts. Through carefully designed comparisons and analyses, we find that model\nrankings can change for varying shifts and shift scales, which cannot be\ncaptured when applying common binary shifts. Additionally, we show that\nevaluating the model performance on a continuous scale allows the\nidentification of model failure points, providing a more nuanced understanding\nof model robustness. Project page including code and data:\nhttps://genintel.github.io/CNS.",
      "url": "http://arxiv.org/abs/2507.17651v1",
      "published_time_eastern_timestamp": 1753287348.0
    },
    {
      "title": "Reusing Attention for One-stage Lane Topology Understanding",
      "summary": "Understanding lane toplogy relationships accurately is critical for safe\nautonomous driving. However, existing two-stage methods suffer from\ninefficiencies due to error propagations and increased computational overheads.\nTo address these challenges, we propose a one-stage architecture that\nsimultaneously predicts traffic elements, lane centerlines and topology\nrelationship, improving both the accuracy and inference speed of lane topology\nunderstanding for autonomous driving. Our key innovation lies in reusing\nintermediate attention resources within distinct transformer decoders. This\napproach effectively leverages the inherent relational knowledge within the\nelement detection module to enable the modeling of topology relationships among\ntraffic elements and lanes without requiring additional computationally\nexpensive graph networks. Furthermore, we are the first to demonstrate that\nknowledge can be distilled from models that utilize standard definition (SD)\nmaps to those operates without using SD maps, enabling superior performance\neven in the absence of SD maps. Extensive experiments on the OpenLane-V2\ndataset show that our approach outperforms baseline methods in both accuracy\nand efficiency, achieving superior results in lane detection, traffic element\nidentification, and topology reasoning. Our code is available at\nhttps://github.com/Yang-Li-2000/one-stage.git.",
      "url": "http://arxiv.org/abs/2507.17617v1",
      "published_time_eastern_timestamp": 1753285696.0
    },
    {
      "title": "Comparing performance of variational quantum algorithm simulations on\n  HPC systems",
      "summary": "Variational quantum algorithms are of special importance in the research on\nquantum computing applications because of their applicability to current Noisy\nIntermediate-Scale Quantum (NISQ) devices. The main building blocks of these\nalgorithms (among them, the definition of the Hamiltonian and of the ansatz,\nthe optimizer) define a relatively large parameter space, making the comparison\nof results and performance between different approaches and software simulators\ncumbersome and prone to errors. In this paper, we employ a generic description\nof the problem, in terms of both Hamiltonian and ansatz, to port a problem\ndefinition consistently among different simulators. Three use cases of\nrelevance for current quantum hardware (ground state calculation for the\nHydrogen molecule, MaxCut, Travelling Salesman Problem) have been run on a set\nof HPC systems and software simulators to study the dependence of performance\non the runtime environment, the scalability of the simulation codes and the\nmutual agreement of the physical results, respectively. The results show that\nour toolchain can successfully translate a problem definition between different\nsimulators. On the other hand, variational algorithms are limited in their\nscaling by the long runtimes with respect to their memory footprint, so they\nexpose limited parallelism to computation. This shortcoming is partially\nmitigated by using techniques like job arrays. The potential of the parser tool\nfor exploring HPC performance and comparisons of results of variational\nalgorithm simulations is highlighted.",
      "url": "http://arxiv.org/abs/2507.17614v1",
      "published_time_eastern_timestamp": 1753285614.0
    },
    {
      "title": "CSS-$T$ codes over Binary Extension Fields and their Physical\n  Foundations",
      "summary": "We investigate the class of CSS-$T$ codes, a family of quantum\nerror-correcting codes that allows for a transversal $T$-gate. We extend the\ndefinition of a pair of linear codes $(C_1,C_2)$, $C_i\\subseteq\\mathbb{F}_q^n$,\nforming a $q$-ary CSS-$T$ code over binary extension fields, and demonstrate\nthe existence of asymptotically good sequences of LDPC CSS-$T$ codes over any\nsuch field.",
      "url": "http://arxiv.org/abs/2507.17611v1",
      "published_time_eastern_timestamp": 1753285307.0
    },
    {
      "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
      "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.",
      "url": "http://arxiv.org/abs/2507.17596v1",
      "published_time_eastern_timestamp": 1753284503.0
    },
    {
      "title": "Bounds and Equivalence of Skew Polycyclic Codes over Finite Fields",
      "summary": "We study skew polycyclic codes over a finite field $\\mathbb{F}_q$, associated\nwith a skew polynomial $f(x) \\in \\mathbb{F}_q[x;\\sigma]$, where $\\sigma$ is an\nautomorphism of $\\mathbb{F}_q$. We start by proving the Roos-like bound for\nboth the Hamming and the rank metric for this class of codes. Next, we focus on\nthe Hamming and rank equivalence between two classes of polycyclic codes by\nintroducing an equivalence relation and describing its equivalence classes.\nFinally, we present examples that illustrate applications of the theory\ndeveloped in this paper.",
      "url": "http://arxiv.org/abs/2507.17571v1",
      "published_time_eastern_timestamp": 1753283004.0
    },
    {
      "title": "CodeReasoner: Enhancing the Code Reasoning Ability with Reinforcement\n  Learning",
      "summary": "Code reasoning is a fundamental capability for large language models (LLMs)\nin the code domain. It involves understanding and predicting a program's\nexecution behavior, such as determining the output for a given input or whether\na specific statement will be executed. This capability is essential for\ndownstream tasks like debugging, code generation, and program repair. Prior\napproaches mainly rely on supervised fine-tuning to improve performance in code\nreasoning tasks. However, they often show limited gains and fail to generalize\nacross diverse scenarios. We argue this is due to two core issues: the low\nquality of training data and the limitations of supervised fine-tuning, which\nstruggles to teach general reasoning skills. To address these challenges, we\npropose CodeReasoner, a framework that spans both dataset construction and a\ntwo-stage training process. First, we introduce a method to construct datasets\nthat focus on the core execution logic of Python programs. Next, we apply\ninstruction tuning to inject execution-specific knowledge distilled from a\npowerful teacher model. We then enhance reasoning and generalization through\nGRPO reinforcement learning on top of the fine-tuned model. Experiments on\nthree widely-used code reasoning benchmarks show that CodeReasoner improves\nperformance by 27.1% to 40.2% over prior methods using a 7B model. Notably, the\n7B model matches GPT-4o on key tasks like input/output and coverage prediction.\nWhen scaled to 14B, CodeReasoner outperforms GPT-4o across all benchmarks.\nAblation studies confirm the effectiveness of each training stage and highlight\nthe importance of reasoning chains.",
      "url": "http://arxiv.org/abs/2507.17548v1",
      "published_time_eastern_timestamp": 1753280818.0
    }
  ]
}
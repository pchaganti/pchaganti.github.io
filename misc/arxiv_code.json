{
  "last_updated": "2025-08-09T14:16:30.236446-04:00",
  "papers": [
    {
      "title": "FaceAnonyMixer: Cancelable Faces via Identity Consistent Latent Space\n  Mixing",
      "summary": "Advancements in face recognition (FR) technologies have amplified privacy\nconcerns, necessitating methods that protect identity while maintaining\nrecognition utility. Existing face anonymization methods typically focus on\nobscuring identity but fail to meet the requirements of biometric template\nprotection, including revocability, unlinkability, and irreversibility. We\npropose FaceAnonyMixer, a cancelable face generation framework that leverages\nthe latent space of a pre-trained generative model to synthesize\nprivacy-preserving face images. The core idea of FaceAnonyMixer is to\nirreversibly mix the latent code of a real face image with a synthetic code\nderived from a revocable key. The mixed latent code is further refined through\na carefully designed multi-objective loss to satisfy all cancelable biometric\nrequirements. FaceAnonyMixer is capable of generating high-quality cancelable\nfaces that can be directly matched using existing FR systems without requiring\nany modifications. Extensive experiments on benchmark datasets demonstrate that\nFaceAnonyMixer delivers superior recognition accuracy while providing\nsignificantly stronger privacy protection, achieving over an 11% gain on\ncommercial API compared to recent cancelable biometric methods. Code is\navailable at: https://github.com/talha-alam/faceanonymixer.",
      "url": "http://arxiv.org/abs/2508.05636v1",
      "published_time_eastern_timestamp": 1754589599.0
    },
    {
      "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation",
      "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly.",
      "url": "http://arxiv.org/abs/2508.05635v1",
      "published_time_eastern_timestamp": 1754589584.0
    },
    {
      "title": "Towards Generalizable Safety in Crowd Navigation via Conformal\n  Uncertainty Handling",
      "summary": "Mobile robots navigating in crowds trained using reinforcement learning are\nknown to suffer performance degradation when faced with out-of-distribution\nscenarios. We propose that by properly accounting for the uncertainties of\npedestrians, a robot can learn safe navigation policies that are robust to\ndistribution shifts. Our method augments agent observations with prediction\nuncertainty estimates generated by adaptive conformal inference, and it uses\nthese estimates to guide the agent's behavior through constrained reinforcement\nlearning. The system helps regulate the agent's actions and enables it to adapt\nto distribution shifts. In the in-distribution setting, our approach achieves a\n96.93% success rate, which is over 8.80% higher than the previous\nstate-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times\nfewer intrusions into ground-truth human future trajectories. In three\nout-of-distribution scenarios, our method shows much stronger robustness when\nfacing distribution shifts in velocity variations, policy changes, and\ntransitions from individual to group dynamics. We deploy our method on a real\nrobot, and experiments show that the robot makes safe and robust decisions when\ninteracting with both sparse and dense crowds. Our code and videos are\navailable on https://gen-safe-nav.github.io/.",
      "url": "http://arxiv.org/abs/2508.05634v1",
      "published_time_eastern_timestamp": 1754589583.0
    },
    {
      "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification",
      "summary": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT.",
      "url": "http://arxiv.org/abs/2508.05629v1",
      "published_time_eastern_timestamp": 1754589544.0
    },
    {
      "title": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven\n  Evolution",
      "summary": "Trajectory prediction is a critical task in modeling human behavior,\nespecially in safety-critical domains such as social robotics and autonomous\nvehicle navigation. Traditional heuristics based on handcrafted rules often\nlack accuracy and generalizability. Although deep learning approaches offer\nimproved performance, they typically suffer from high computational cost,\nlimited explainability, and, importantly, poor generalization to\nout-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a\nframework that leverages Large Language Models (LLMs) to automatically design\ntrajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to\ngenerate and refine prediction heuristics from past trajectory data. We propose\ntwo key innovations: Cross-Generation Elite Sampling to encourage population\ndiversity, and a Statistics Feedback Loop that enables the LLM to analyze and\nimprove alternative predictions. Our evaluations demonstrate that TrajEvo\noutperforms existing heuristic methods across multiple real-world datasets, and\nnotably surpasses both heuristic and deep learning methods in generalizing to\nan unseen OOD real-world dataset. TrajEvo marks a promising step toward the\nautomated design of fast, explainable, and generalizable trajectory prediction\nheuristics. We release our source code to facilitate future research at\nhttps://github.com/ai4co/trajevo.",
      "url": "http://arxiv.org/abs/2508.05616v1",
      "published_time_eastern_timestamp": 1754589310.0
    },
    {
      "title": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency",
      "summary": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents.",
      "url": "http://arxiv.org/abs/2508.05615v1",
      "published_time_eastern_timestamp": 1754589267.0
    },
    {
      "title": "OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks",
      "summary": "Large language models excel at abstract reasoning but their capacity for\nembodied agent reasoning remains largely unexplored. We present OmniEAR, a\ncomprehensive framework for evaluating how language models reason about\nphysical interactions, tool usage, and multi-agent coordination in embodied\ntasks. Unlike existing benchmarks that provide predefined tool sets or explicit\ncollaboration directives, OmniEAR requires agents to dynamically acquire\ncapabilities and autonomously determine coordination strategies based on task\ndemands. Through text-based environment representation, we model continuous\nphysical properties and complex spatial relationships across 1,500 scenarios\nspanning household and industrial domains. Our systematic evaluation reveals\nsevere performance degradation when models must reason from constraints: while\nachieving 85-96% success with explicit instructions, performance drops to\n56-85% for tool reasoning and 63-85% for implicit collaboration, with compound\ntasks showing over 50% failure rates. Surprisingly, complete environmental\ninformation degrades coordination performance, indicating models cannot filter\ntask-relevant constraints. Fine-tuning improves single-agent tasks dramatically\n(0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing\nfundamental architectural limitations. These findings demonstrate that embodied\nreasoning poses fundamentally different challenges than current models can\naddress, establishing OmniEAR as a rigorous benchmark for evaluating and\nadvancing embodied AI systems. Our code and data are included in the\nsupplementary materials and will be open-sourced upon acceptance.",
      "url": "http://arxiv.org/abs/2508.05614v1",
      "published_time_eastern_timestamp": 1754589255.0
    },
    {
      "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models",
      "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
      "url": "http://arxiv.org/abs/2508.05613v1",
      "published_time_eastern_timestamp": 1754589236.0
    },
    {
      "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision",
      "summary": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/",
      "url": "http://arxiv.org/abs/2508.05606v1",
      "published_time_eastern_timestamp": 1754588717.0
    },
    {
      "title": "WeTok: Powerful Discrete Tokenization for High-Fidelity Visual\n  Reconstruction",
      "summary": "Visual tokenizer is a critical component for vision generation. However, the\nexisting tokenizers often face unsatisfactory trade-off between compression\nratios and reconstruction fidelity. To fill this gap, we introduce a powerful\nand concise WeTok tokenizer, which surpasses the previous leading tokenizers\nvia two core innovations. (1) Group-wise lookup-free Quantization (GQ). We\npartition the latent features into groups, and perform lookup-free quantization\nfor each group. As a result, GQ can efficiently overcome memory and computation\nlimitations of prior tokenizers, while achieving a reconstruction breakthrough\nwith more scalable codebooks. (2) Generative Decoding (GD). Different from\nprior tokenizers, we introduce a generative decoder with a prior of extra noise\nvariable. In this case, GD can probabilistically model the distribution of\nvisual data conditioned on discrete tokens, allowing WeTok to reconstruct\nvisual details, especially at high compression ratios. Extensive experiments on\nmainstream benchmarks show superior performance of our WeTok. On the ImageNet\n50k validation set, WeTok achieves a record-low zero-shot rFID (WeTok: 0.12 vs.\nFLUX-VAE: 0.18 vs. SD-VAE 3.5: 0.19). Furthermore, our highest compression\nmodel achieves a zero-shot rFID of 3.49 with a compression ratio of 768,\noutperforming Cosmos (384) 4.57 which has only 50% compression rate of ours.\nCode and models are available: https://github.com/zhuangshaobin/WeTok.",
      "url": "http://arxiv.org/abs/2508.05599v1",
      "published_time_eastern_timestamp": 1754588486.0
    },
    {
      "title": "Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis",
      "summary": "In medical time series disease diagnosis, two key challenges are identified.\nFirst, the high annotation cost of medical data leads to overfitting in models\ntrained on label-limited, single-center datasets. To address this, we propose\nincorporating external data from related tasks and leveraging AE-GAN to extract\nprior knowledge, providing valuable references for downstream tasks. Second,\nmany existing studies employ contrastive learning to derive more generalized\nmedical sequence representations for diagnostic tasks, usually relying on\nmanually designed diverse positive and negative sample pairs. However, these\napproaches are complex, lack generalizability, and fail to adaptively capture\ndisease-specific features across different conditions. To overcome this, we\nintroduce LMCF (Learnable Multi-views Contrastive Framework), a framework that\nintegrates a multi-head attention mechanism and adaptively learns\nrepresentations from different views through inter-view and intra-view\ncontrastive learning strategies. Additionally, the pre-trained AE-GAN is used\nto reconstruct discrepancies in the target data as disease probabilities, which\nare then integrated into the contrastive learning process. Experiments on three\ntarget datasets demonstrate that our method consistently outperforms other\nseven baselines, highlighting its significant impact on healthcare applications\nsuch as the diagnosis of myocardial infarction, Alzheimer's disease, and\nParkinson's disease. We release the source code at xxxxx.",
      "url": "http://arxiv.org/abs/2508.05572v1",
      "published_time_eastern_timestamp": 1754586343.0
    },
    {
      "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation",
      "summary": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/.",
      "url": "http://arxiv.org/abs/2508.05535v1",
      "published_time_eastern_timestamp": 1754582952.0
    },
    {
      "title": "The World According to LLMs: How Geographic Origin Influences LLMs'\n  Entity Deduction Capabilities",
      "summary": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home.",
      "url": "http://arxiv.org/abs/2508.05525v1",
      "published_time_eastern_timestamp": 1754582010.0
    },
    {
      "title": "Logical accreditation: a framework for efficient certification of\n  fault-tolerant computations",
      "summary": "As fault-tolerant quantum computers scale, certifying the accuracy of\ncomputations performed with encoded logical qubits will soon become classically\nintractable. This creates a critical need for scalable, device-independent\ncertification methods. In this work, we introduce logical accreditation, a\nframework for efficiently certifying quantum computations performed on logical\nqubits. Our protocol is robust against general noise models, far beyond those\ntypically considered in performance analyses of quantum error-correcting codes.\nThrough numerical simulations, we demonstrate that logical accreditation can\nscalably certify quantum advantage experiments and indicate the crossover point\nwhere encoded computations begin to outperform physical computations. Logical\naccreditation can also find application in evaluating whether logical circuit\nerror rates are sufficiently low that error mitigation can be efficiently\nperformed, extending the entropy benchmarking method to the regime of\nfault-tolerant computation, and upper-bounding the infidelity of the logical\noutput state. Underpinning the framework is a novel randomised compiling scheme\nthat converts arbitrary logical circuit noise into stochastic Pauli noise. This\nscheme includes a method for twirling non-transversal logical gates beyond the\nstandard T-gate, resolving an open problem posed by Piveteau et al. [Piveteau\net al. PRL 127, 200505 (2001)]. By bridging fault-tolerant computation and\ncomputational certification, logical accreditation offers a practical tool to\nassess the quality of computations performed on quantum hardware using encoded\nlogical qubits.",
      "url": "http://arxiv.org/abs/2508.05523v1",
      "published_time_eastern_timestamp": 1754581985.0
    },
    {
      "title": "Optimal Brain Connection: Towards Efficient Structural Pruning",
      "summary": "Structural pruning has been widely studied for its effectiveness in\ncompressing neural networks. However, existing methods often neglect the\ninterconnections among parameters. To address this limitation, this paper\nproposes a structural pruning framework termed Optimal Brain Connection. First,\nwe introduce the Jacobian Criterion, a first-order metric for evaluating the\nsaliency of structural parameters. Unlike existing first-order methods that\nassess parameters in isolation, our criterion explicitly captures both\nintra-component interactions and inter-layer dependencies. Second, we propose\nthe Equivalent Pruning mechanism, which utilizes autoencoders to retain the\ncontributions of all original connection--including pruned ones--during\nfine-tuning. Experimental results demonstrate that the Jacobian Criterion\noutperforms several popular metrics in preserving model performance, while the\nEquivalent Pruning mechanism effectively mitigates performance degradation\nafter fine-tuning. Code: https://github.com/ShaowuChen/Optimal_Brain_Connection",
      "url": "http://arxiv.org/abs/2508.05521v1",
      "published_time_eastern_timestamp": 1754581865.0
    },
    {
      "title": "Revealing Latent Information: A Physics-inspired Self-supervised\n  Pre-training Framework for Noisy and Sparse Events",
      "summary": "Event camera, a novel neuromorphic vision sensor, records data with high\ntemporal resolution and wide dynamic range, offering new possibilities for\naccurate visual representation in challenging scenarios. However, event data is\ninherently sparse and noisy, mainly reflecting brightness changes, which\ncomplicates effective feature extraction. To address this, we propose a\nself-supervised pre-training framework to fully reveal latent information in\nevent data, including edge information and texture cues. Our framework consists\nof three stages: Difference-guided Masked Modeling, inspired by the event\nphysical sampling process, reconstructs temporal intensity difference maps to\nextract enhanced information from raw event data. Backbone-fixed Feature\nTransition contrasts event and image features without updating the backbone to\npreserve representations learned from masked modeling and stabilizing their\neffect on contrastive learning. Focus-aimed Contrastive Learning updates the\nentire model to improve semantic discrimination by focusing on high-value\nregions. Extensive experiments show our framework is robust and consistently\noutperforms state-of-the-art methods on various downstream tasks, including\nobject recognition, semantic segmentation, and optical flow estimation. The\ncode and dataset are available at https://github.com/BIT-Vision/EventPretrain.",
      "url": "http://arxiv.org/abs/2508.05507v1",
      "published_time_eastern_timestamp": 1754581116.0
    },
    {
      "title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval\n  Augmented Reasoning",
      "summary": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL.",
      "url": "http://arxiv.org/abs/2508.05498v1",
      "published_time_eastern_timestamp": 1754580881.0
    },
    {
      "title": "Long Polar vs. LDPC Codes under Complexity-Constrained Decoding",
      "summary": "The prevailing opinion in industry and academia is that polar codes are\ncompetitive for short code lengths, but can no longer keep up with low-density\nparity-check (LDPC) codes as block length increases. This view is typically\nbased on the assumption that LDPC codes can be decoded with a large number of\nbelief propagation (BP) iterations. However, in practice, the number of\niterations may be rather limited due to latency and complexity constraints. In\nthis paper, we show that for a similar number of fixed-point log-likelihood\nratio (LLR) operations, long polar codes under successive cancellation (SC)\ndecoding outperform their LDPC counterparts. In particular, simplified\nsuccessive cancellation (SSC) decoding of polar codes exhibits a better\ncomplexity scaling than $N \\log{N}$ and requires fewer operations than a single\nBP iteration of an LDPC code with the same parameters.",
      "url": "http://arxiv.org/abs/2508.05485v1",
      "published_time_eastern_timestamp": 1754580230.0
    },
    {
      "title": "MM2CT: MR-to-CT translation for multi-modal image fusion with mamba",
      "summary": "Magnetic resonance (MR)-to-computed tomography (CT) translation offers\nsignificant advantages, including the elimination of radiation exposure\nassociated with CT scans and the mitigation of imaging artifacts caused by\npatient motion. The existing approaches are based on single-modality MR-to-CT\ntranslation, with limited research exploring multimodal fusion. To address this\nlimitation, we introduce Multi-modal MR to CT (MM2CT) translation method by\nleveraging multimodal T1- and T2-weighted MRI data, an innovative Mamba-based\nframework for multi-modal medical image synthesis. Mamba effectively overcomes\nthe limited local receptive field in CNNs and the high computational complexity\nissues in Transformers. MM2CT leverages this advantage to maintain long-range\ndependencies modeling capabilities while achieving multi-modal MR feature\nintegration. Additionally, we incorporate a dynamic local convolution module\nand a dynamic enhancement module to improve MRI-to-CT synthesis. The\nexperiments on a public pelvis dataset demonstrate that MM2CT achieves\nstate-of-the-art performance in terms of Structural Similarity Index Measure\n(SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our code is publicly available at\nhttps://github.com/Gots-ch/MM2CT.",
      "url": "http://arxiv.org/abs/2508.05476v1",
      "published_time_eastern_timestamp": 1754579750.0
    },
    {
      "title": "Embedding Alignment in Code Generation for Audio",
      "summary": "LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map.",
      "url": "http://arxiv.org/abs/2508.05473v1",
      "published_time_eastern_timestamp": 1754579622.0
    }
  ]
}
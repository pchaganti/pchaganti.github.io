{
  "last_updated": "2025-12-13T04:12:18.219758-05:00",
  "papers": [
    {
      "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
      "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
      "url": "http://arxiv.org/abs/2512.10957v1",
      "published_time_eastern_timestamp": 1765479596.0
    },
    {
      "title": "Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation",
      "summary": "Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.",
      "url": "http://arxiv.org/abs/2512.10949v1",
      "published_time_eastern_timestamp": 1765479592.0
    },
    {
      "title": "ImplicitRDP: An End-to-End Visual-Force Diffusion Policy with Structural Slow-Fast Learning",
      "summary": "Human-level contact-rich manipulation relies on the distinct roles of two key modalities: vision provides spatially rich but temporally slow global context, while force sensing captures rapid, high-frequency local contact dynamics. Integrating these signals is challenging due to their fundamental frequency and informational disparities. In this work, we propose ImplicitRDP, a unified end-to-end visual-force diffusion policy that integrates visual planning and reactive force control within a single network. We introduce Structural Slow-Fast Learning, a mechanism utilizing causal attention to simultaneously process asynchronous visual and force tokens, allowing the policy to perform closed-loop adjustments at the force frequency while maintaining the temporal coherence of action chunks. Furthermore, to mitigate modality collapse where end-to-end models fail to adjust the weights across different modalities, we propose Virtual-target-based Representation Regularization. This auxiliary objective maps force feedback into the same space as the action, providing a stronger, physics-grounded learning signal than raw force prediction. Extensive experiments on contact-rich tasks demonstrate that ImplicitRDP significantly outperforms both vision-only and hierarchical baselines, achieving superior reactivity and success rates with a streamlined training pipeline. Code and videos will be publicly available at https://implicit-rdp.github.io.",
      "url": "http://arxiv.org/abs/2512.10946v1",
      "published_time_eastern_timestamp": 1765479586.0
    },
    {
      "title": "MeViS: A Multi-Modal Dataset for Referring Motion Expression Video Segmentation",
      "summary": "This paper proposes a large-scale multi-modal dataset for referring motion expression video segmentation, focusing on segmenting and tracking target objects in videos based on language description of objects' motions. Existing referring video segmentation datasets often focus on salient objects and use language expressions rich in static attributes, potentially allowing the target object to be identified in a single frame. Such datasets underemphasize the role of motion in both videos and languages. To explore the feasibility of using motion expressions and motion reasoning clues for pixel-level video understanding, we introduce MeViS, a dataset containing 33,072 human-annotated motion expressions in both text and audio, covering 8,171 objects in 2,006 videos of complex scenarios. We benchmark 15 existing methods across 4 tasks supported by MeViS, including 6 referring video object segmentation (RVOS) methods, 3 audio-guided video object segmentation (AVOS) methods, 2 referring multi-object tracking (RMOT) methods, and 4 video captioning methods for the newly introduced referring motion expression generation (RMEG) task. The results demonstrate weaknesses and limitations of existing methods in addressing motion expression-guided video understanding. We further analyze the challenges and propose an approach LMPM++ for RVOS/AVOS/RMOT that achieves new state-of-the-art results. Our dataset provides a platform that facilitates the development of motion expression-guided video understanding algorithms in complex video scenes. The proposed MeViS dataset and the method's source code are publicly available at https://henghuiding.com/MeViS/",
      "url": "http://arxiv.org/abs/2512.10945v1",
      "published_time_eastern_timestamp": 1765479584.0
    },
    {
      "title": "FoundationMotion: Auto-Labeling and Reasoning about Spatial Movement in Videos",
      "summary": "Motion understanding is fundamental to physical reasoning, enabling models to infer dynamics and predict future states. However, state-of-the-art models still struggle on recent motion benchmarks, primarily due to the scarcity of large-scale, fine-grained motion datasets. Existing motion datasets are often constructed from costly manual annotation, severely limiting scalability. To address this challenge, we introduce FoundationMotion, a fully automated data curation pipeline that constructs large-scale motion datasets. Our approach first detects and tracks objects in videos to extract their trajectories, then leverages these trajectories and video frames with Large Language Models (LLMs) to generate fine-grained captions and diverse question-answer pairs about motion and spatial reasoning. Using datasets produced by this pipeline, we fine-tune open-source models including NVILA-Video-15B and Qwen2.5-7B, achieving substantial improvements in motion understanding without compromising performance on other tasks. Notably, our models outperform strong closed-source baselines like Gemini-2.5 Flash and large open-source models such as Qwen2.5-VL-72B across diverse motion understanding datasets and benchmarks. FoundationMotion thus provides a scalable solution for curating fine-grained motion datasets that enable effective fine-tuning of diverse models to enhance motion understanding and spatial reasoning capabilities.",
      "url": "http://arxiv.org/abs/2512.10927v1",
      "published_time_eastern_timestamp": 1765479195.0
    },
    {
      "title": "Decoupled Q-Chunking",
      "summary": "Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences (\"chunks\") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.",
      "url": "http://arxiv.org/abs/2512.10926v1",
      "published_time_eastern_timestamp": 1765479171.0
    },
    {
      "title": "Hermitian Yang--Mills connections on general vector bundles: geometry and physical Yukawa couplings",
      "summary": "We compute solutions to the Hermitian Yang-Mills equations on holomorphic vector bundles $V$ via an alternating optimisation procedure founded on geometric machine learning. The proposed method is fully general with respect to the rank and structure group of $V$, requiring only the ability to enumerate a basis of global sections for a given bundle. This enables us to compute the physically normalised Yukawa couplings in a broad class of heterotic string compactifications. Using this method, we carry out this computation in full for a heterotic compactification incorporating a gauge bundle with non-Abelian structure group.",
      "url": "http://arxiv.org/abs/2512.10907v1",
      "published_time_eastern_timestamp": 1765478290.0
    },
    {
      "title": "The LISA Astrophysics \"Disc-IMRI\" Code Comparison Project: Intermediate-Mass-Ratio Binaries in AGN-Like Discs",
      "summary": "Upcoming space-based gravitational wave detectors such as LISA, the Laser Interferometer Space Antenna, will be sensitive to extreme- and intermediate-mass-ratio inspirals (EMRIs and IMRIs). These binaries are comprised of a supermassive black hole and a stellar-mass object or intermediate-mass black hole. Their detection will probe the structure of galactic nuclei and enable tests of general relativity. As these events will be observed over thousands of orbital cycles, they will be extremely sensitive to both the underlying spacetime and astrophysical environment, demanding exquisite theoretical models on both fronts to avoid biased or even erroneous results. In particular, many (E/)IMRIs are expected to occur within accretion discs around supermassive black holes, and the nonlinearities present when modeling these systems require numerical simulations. In preparation for future modeling of LISA sources, we have conducted a comparison between eight different hydrodynamical codes and applied them to the problem of a q = 10^{-4} mass ratio binary interacting with an accretion disc. Thicker discs appear more lenient, and all codes at sufficiently high resolutions are in good agreement with each other and analytical predictions. For thinner discs, beyond the reach of analytical models, we find substantial disagreement between 2D and 3D simulations and between different codes, including both the magnitude and sign of the torque. With time and energy efficiency in mind, codes that leverage moving meshes or grid-based Lagrangian remapping seem preferable, as do codes that can leverage graphical processing units and other energy-efficient hardware.",
      "url": "http://arxiv.org/abs/2512.10893v1",
      "published_time_eastern_timestamp": 1765477356.0
    },
    {
      "title": "Iterative Compositional Data Generation for Robot Control",
      "summary": "Collecting robotic manipulation data is expensive, making it impractical to acquire demonstrations for the combinatorially large space of tasks that arise in multi-object, multi-robot, and multi-environment settings. While recent generative models can synthesize useful data for individual tasks, they do not exploit the compositional structure of robotic domains and struggle to generalize to unseen task combinations. We propose a semantic compositional diffusion transformer that factorizes transitions into robot-, object-, obstacle-, and objective-specific components and learns their interactions through attention. Once trained on a limited subset of tasks, we show that our model can zero-shot generate high-quality transitions from which we can learn control policies for unseen task combinations. Then, we introduce an iterative self-improvement procedure in which synthetic data is validated via offline reinforcement learning and incorporated into subsequent training rounds. Our approach substantially improves zero-shot performance over monolithic and hard-coded compositional baselines, ultimately solving nearly all held-out tasks and demonstrating the emergence of meaningful compositional structure in the learned representations.",
      "url": "http://arxiv.org/abs/2512.10891v1",
      "published_time_eastern_timestamp": 1765477249.0
    },
    {
      "title": "PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction",
      "summary": "Table extraction (TE) is a key challenge in visual document understanding. Traditional approaches detect tables first, then recognize their structure. Recently, interest has surged in developing methods, such as vision-language models (VLMs), that can extract tables directly in their full page or document context. However, progress has been difficult to demonstrate due to a lack of annotated data. To address this, we create a new large-scale dataset, PubTables-v2. PubTables-v2 supports a number of current challenging table extraction tasks. Notably, it is the first large-scale benchmark for multi-page table structure recognition. We demonstrate its usefulness by evaluating domain-specialized VLMs on these tasks and highlighting current progress. Finally, we use PubTables-v2 to create the Page-Object Table Transformer (POTATR), an image-to-graph extension of the Table Transformer to comprehensive page-level TE. Data, code, and trained models will be released.",
      "url": "http://arxiv.org/abs/2512.10888v1",
      "published_time_eastern_timestamp": 1765477140.0
    },
    {
      "title": "Optimized Measurement Schedules for the Surface Code with Dropout",
      "summary": "Recent work has shown that fabrication defects can be well-handled using a strategy relying on the mid-error-correction-cycle state. In this work we present two improvements to the original prescription. First, we quantify the impact of the choice of a more complete set of gauge operators originally proposed for the hex-grid surface code on the standard square-grid surface code, as well as a new method for excising effectively unused qubits. Second, we leverage the expressivity of the LUCI framework as an intermediate representation, using integer linear programming to find performant physical circuits from the large space of valid LUCI circuits. We show that on the $d = 11$ surface code at $1\\%(3\\%)$ dropout rate for qubits and couplers, these optimizations allow for a total improvement of $14.5\\%(23.6\\%)$ over $4d$ round of syndrome extraction using the SI1000 noise model at $0.1\\%$ noise.",
      "url": "http://arxiv.org/abs/2512.10871v1",
      "published_time_eastern_timestamp": 1765476159.0
    },
    {
      "title": "dtreg: Describing Data Analysis in Machine-Readable Format in Python and R",
      "summary": "For scientific knowledge to be findable, accessible, interoperable, and reusable, it needs to be machine-readable. Moving forward from post-publication extraction of knowledge, we adopted a pre-publication approach to write research findings in a machine-readable format at early stages of data analysis. For this purpose, we developed the package dtreg in Python and R. Registered and persistently identified data types, aka schemata, which dtreg applies to describe data analysis in a machine-readable format, cover the most widely used statistical tests and machine learning methods. The package supports (i) downloading a relevant schema as a mutable instance of a Python or R class, (ii) populating the instance object with metadata about data analysis, and (iii) converting the object into a lightweight Linked Data format. This paper outlines the background of our approach, explains the code architecture, and illustrates the functionality of dtreg with a machine-readable description of a t-test on Iris Data. We suggest that the dtreg package can enhance the methodological repertoire of researchers aiming to adhere to the FAIR principles.",
      "url": "http://arxiv.org/abs/2512.10836v1",
      "published_time_eastern_timestamp": 1765474024.0
    },
    {
      "title": "Estimating Detector Error Models on Google's Willow",
      "summary": "We consolidate recent theoretical advances in Detector Error Model (DEM) estimation and formalize several algorithms to learn DEM parameters and structure from syndromes without using a decoder, demonstrating recovery of known DEMs from simulated syndromes with precision limited only by finite-sample effects. We then apply these algorithms to estimate DEMs from Google's 72- and 105-qubit chips. Using a likelihood function that is tractable for small DEMs, we show that DEMs estimated directly from syndromes agree more closely with unseen syndromes than DEMs trained to optimize logical performance, whereas the latter outperform the former as priors for decoders in logical memory experiments. We used a time-series of estimated DEMs to track both global error and specific local errors over the course of a QEC experiment, suggesting applications in online characterization. We employ a sequence of DEM estimation techniques to discover and quantify long-range detector correlations spanning the width of the 105-qubit chip, for which DEM analysis suggests correlated measurement errors rather than high-weight Pauli errors as the most likely explanation. Finally, we present two artifacts in repetition code syndromes that are \\emph{not} well-modeled by a DEM: correlated flipping of pairs of adjacent detectors in many consecutive rounds of QEC, and signatures consistent with radiation events occurring more frequently than previously reported.",
      "url": "http://arxiv.org/abs/2512.10814v1",
      "published_time_eastern_timestamp": 1765472672.0
    },
    {
      "title": "CSI-Based User Positioning, Channel Charting, and Device Classification with an NVIDIA 5G Testbed",
      "summary": "Channel-state information (CSI)-based sensing will play a key role in future cellular systems. However, no CSI dataset has been published from a real-world 5G NR system that facilitates the development and validation of suitable sensing algorithms. To close this gap, we publish three real-world wideband multi-antenna multi-open RAN radio unit (O-RU) CSI datasets from the 5G NR uplink channel: an indoor lab/office room dataset, an outdoor campus courtyard dataset, and a device classification dataset with six commercial-off-the-shelf (COTS) user equipments (UEs). These datasets have been recorded using a software-defined 5G NR testbed based on NVIDIA Aerial RAN CoLab Over-the-Air (ARC-OTA) with COTS hardware, which we have deployed at ETH Zurich. We demonstrate the utility of these datasets for three CSI-based sensing tasks: neural UE positioning, channel charting in real-world coordinates, and closed-set device classification. For all these tasks, our results show high accuracy: neural UE positioning achieves 0.6cm (indoor) and 5.7cm (outdoor) mean absolute error, channel charting in real-world coordinates achieves 73cm mean absolute error (outdoor), and device classification achieves 99% (same day) and 95% (next day) accuracy. The CSI datasets, ground-truth UE position labels, CSI features, and simulation code are publicly available at https://caez.ethz.ch",
      "url": "http://arxiv.org/abs/2512.10809v1",
      "published_time_eastern_timestamp": 1765472160.0
    },
    {
      "title": "Interpretable and Steerable Concept Bottleneck Sparse Autoencoders",
      "summary": "Sparse autoencoders (SAEs) promise a unified approach for mechanistic interpretability, concept discovery, and model steering in LLMs and LVLMs. However, realizing this potential requires that the learned features be both interpretable and steerable. To that end, we introduce two new computationally inexpensive interpretability and steerability metrics and conduct a systematic analysis on LVLMs. Our analysis uncovers two observations; (i) a majority of SAE neurons exhibit either low interpretability or low steerability or both, rendering them ineffective for downstream use; and (ii) due to the unsupervised nature of SAEs, user-desired concepts are often absent in the learned dictionary, thus limiting their practical utility. To address these limitations, we propose Concept Bottleneck Sparse Autoencoders (CB-SAE) - a novel post-hoc framework that prunes low-utility neurons and augments the latent space with a lightweight concept bottleneck aligned to a user-defined concept set. The resulting CB-SAE improves interpretability by +32.1% and steerability by +14.5% across LVLMs and image generation tasks. We will make our code and model weights available.",
      "url": "http://arxiv.org/abs/2512.10805v1",
      "published_time_eastern_timestamp": 1765471687.0
    },
    {
      "title": "Zorya: Automated Concolic Execution of Single-Threaded Go Binaries",
      "summary": "Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.",
      "url": "http://arxiv.org/abs/2512.10799v1",
      "published_time_eastern_timestamp": 1765471431.0
    },
    {
      "title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
      "summary": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \\textit{global} \\revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \\emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa",
      "url": "http://arxiv.org/abs/2512.10794v1",
      "published_time_eastern_timestamp": 1765471193.0
    },
    {
      "title": "Replace, Don't Expand: Mitigating Context Dilution in Multi-Hop RAG via Fixed-Budget Evidence Assembly",
      "summary": "Retrieval-Augmented Generation (RAG) systems often fail on multi-hop queries when the initial retrieval misses a bridge fact. Prior corrective approaches, such as Self-RAG, CRAG, and Adaptive-$k$, typically address this by \\textit{adding} more context or pruning existing lists. However, simply expanding the context window often leads to \\textbf{context dilution}, where distractors crowd out relevant information. We propose \\textbf{SEAL-RAG}, a training-free controller that adopts a \\textbf{``replace, don't expand''} strategy to fight context dilution under a fixed retrieval depth $k$. SEAL executes a (\\textbf{S}earch $\\rightarrow$ \\textbf{E}xtract $\\rightarrow$ \\textbf{A}ssess $\\rightarrow$ \\textbf{L}oop) cycle: it performs on-the-fly, entity-anchored extraction to build a live \\textit{gap specification} (missing entities/relations), triggers targeted micro-queries, and uses \\textit{entity-first ranking} to actively swap out distractors for gap-closing evidence. We evaluate SEAL-RAG against faithful re-implementations of Basic RAG, CRAG, Self-RAG, and Adaptive-$k$ in a shared environment on \\textbf{HotpotQA} and \\textbf{2WikiMultiHopQA}. On HotpotQA ($k=3$), SEAL improves answer correctness by \\textbf{+3--13 pp} and evidence precision by \\textbf{+12--18 pp} over Self-RAG. On 2WikiMultiHopQA ($k=5$), it outperforms Adaptive-$k$ by \\textbf{+8.0 pp} in accuracy and maintains \\textbf{96\\%} evidence precision compared to 22\\% for CRAG. These gains are statistically significant ($p<0.001$). By enforcing fixed-$k$ replacement, SEAL yields a predictable cost profile while ensuring the top-$k$ slots are optimized for precision rather than mere breadth. We release our code and data at https://github.com/mosherino/SEAL-RAG.",
      "url": "http://arxiv.org/abs/2512.10787v1",
      "published_time_eastern_timestamp": 1765470689.0
    },
    {
      "title": "Chemical enrichment in LINERs from MaNGA. II. Characterizing the shape of their radial metallicity gradients",
      "summary": "Chemical abundance radial gradients provide key information on how the processes that affect chemical enrichment of the gas-phase interstellar medium (ISM) act at different galaxy scales. Whereas in the last decades there has been an increase in the number of galaxies studied with integral field spectroscopy, there is still not a clear picture on a subsequent characterization of the chemical abundance radial gradients in galaxies hosting Active Galactic Nuclei (AGNs). This lack of analysis is even more accentuated in the case of low-ionization nuclear emission-line regions (LINERs). For the first time, we analyze the chemical abundance radial gradients in a sample of LINER-like galaxies, whose nuclear emission has been previously (Paper I) discussed. We use a sample of 97 galaxies from the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA), whose nuclear regions show LINER-like emission. We use the open-source code HII-CHI-Mistry to estimate the chemical abundance ratios 12+log(O/H) and log(N/O) in the HII regions across the disks in our sample, as well as in the nuclear parts where the LINER-like activity dominates. To fit the radial profiles we use a piecewise methodology which uses a non-fixed number of breaks to find the best fit for the data. We obtain that majority of our sample of galaxies exhibits departures from the single linear gradient both in 12+log(O/H) and log(N/O) (as expected from the inside-out scenario). We investigate whether these departures are driven by galaxy properties (stellar mass, neutral gas mass, stellar velocity dispersion), finding not correlation at all. We also report that in most cases there is no correlation between the shape of the 12+log(O/H) and log(N/O) radial profiles. We propose a model in which AGN (feed)back, acting at different scales depending on the galaxy and its evolutionary stage, might be responsible for these departures.",
      "url": "http://arxiv.org/abs/2512.10769v1",
      "published_time_eastern_timestamp": 1765469302.0
    },
    {
      "title": "A \"New Hope\" for Moon Formation: Presenting a Multiple Impact Pathway",
      "summary": "The leading hypothesis for the origin of the Moon, that of a single giant impact, faces significant challenges. These include either the need for an impactor with a near-identical composition to Earth or an extremely high-mass or high-energy impact to achieve near-complete material mixing. In this paper we explore an alternative, the \"multiple impact hypothesis\", which relaxes the compositional constraints on both the target and projectile, and allows for the consideration of more probable, less extreme impacts that steadily grow the Earth and Moon to their current size over several impact events. Using the hydrodynamical code SWIFT, we simulate \"chains\" of impacts and follow the growth of a moon around a planet analogous to our own. Our results demonstrate that chains of three or more impacts can produce systems comparable to the Earth-Moon system whilst achieving higher compositional similarities than the canonical giant impact scenario. This presents the multiple impact hypothesis as a promising alternative to the single large impact scenario for the origin of the Moon.",
      "url": "http://arxiv.org/abs/2512.10757v1",
      "published_time_eastern_timestamp": 1765468255.0
    }
  ]
}
{
  "last_updated": "2025-05-17T17:10:01.713414-04:00",
  "papers": [
    {
      "title": "3D-Fixup: Advancing Photo Editing with 3D Priors",
      "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/",
      "url": "http://arxiv.org/abs/2505.10566v1",
      "published_time_eastern_timestamp": 1747331991.0
    },
    {
      "title": "Approximation-First Timeseries Monitoring Query At Scale",
      "summary": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch.",
      "url": "http://arxiv.org/abs/2505.10560v1",
      "published_time_eastern_timestamp": 1747331964.0
    },
    {
      "title": "MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal\n  Mathematical Reasoning",
      "summary": "Natural language image-caption datasets, widely used for training Large\nMultimodal Models, mainly focus on natural scenarios and overlook the intricate\ndetails of mathematical figures that are critical for problem-solving,\nhindering the advancement of current LMMs in multimodal mathematical reasoning.\nTo this end, we propose leveraging code as supervision for cross-modal\nalignment, since code inherently encodes all information needed to generate\ncorresponding figures, establishing a precise connection between the two\nmodalities. Specifically, we co-develop our image-to-code model and dataset\nwith model-in-the-loop approach, resulting in an image-to-code model,\nFigCodifier and ImgCode-8.6M dataset, the largest image-code dataset to date.\nFurthermore, we utilize FigCodifier to synthesize novel mathematical figures\nand then construct MM-MathInstruct-3M, a high-quality multimodal math\ninstruction fine-tuning dataset. Finally, we present MathCoder-VL, trained with\nImgCode-8.6M for cross-modal alignment and subsequently fine-tuned on\nMM-MathInstruct-3M for multimodal math problem solving. Our model achieves a\nnew open-source SOTA across all six metrics. Notably, it surpasses GPT-4o and\nClaude 3.5 Sonnet in the geometry problem-solving subset of MathVista,\nachieving improvements of 8.9% and 9.2%. The dataset and models will be\nreleased at https://github.com/mathllm/MathCoder.",
      "url": "http://arxiv.org/abs/2505.10557v1",
      "published_time_eastern_timestamp": 1747331961.0
    },
    {
      "title": "Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large\n  Reasoning Models",
      "summary": "Large reasoning models (LRMs) already possess a latent capacity for long\nchain-of-thought reasoning. Prior work has shown that outcome-based\nreinforcement learning (RL) can incidentally elicit advanced reasoning\nbehaviors such as self-correction, backtracking, and verification phenomena\noften referred to as the model's \"aha moment\". However, the timing and\nconsistency of these emergent behaviors remain unpredictable and\nuncontrollable, limiting the scalability and reliability of LRMs' reasoning\ncapabilities. To address these limitations, we move beyond reliance on prompts\nand coincidental \"aha moments\". Instead, we explicitly align models with three\nmeta-abilities: deduction, induction, and abduction, using automatically\ngenerated, self-verifiable tasks. Our three stage-pipeline individual\nalignment, parameter-space merging, and domain-specific reinforcement learning,\nboosting performance by over 10\\% relative to instruction-tuned baselines.\nFurthermore, domain-specific RL from the aligned checkpoint yields an\nadditional 2\\% average gain in the performance ceiling across math, coding, and\nscience benchmarks, demonstrating that explicit meta-ability alignment offers a\nscalable and dependable foundation for reasoning. Code is available at:\nhttps://github.com/zhiyuanhubj/Meta-Ability-Alignment",
      "url": "http://arxiv.org/abs/2505.10554v1",
      "published_time_eastern_timestamp": 1747331913.0
    },
    {
      "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal\n  Reasoning",
      "summary": "Foundation models can provide robust high-level reasoning on appropriate\nsafety interventions in hazardous scenarios beyond a robot's training data,\ni.e. out-of-distribution (OOD) failures. However, due to the high inference\nlatency of Large Vision and Language Models, current methods rely on manually\ndefined intervention policies to enact fallbacks, thereby lacking the ability\nto plan generalizable, semantically safe motions. To overcome these challenges\nwe present FORTRESS, a framework that generates and reasons about semantically\nsafe fallback strategies in real time to prevent OOD failures. At a low\nfrequency in nominal operations, FORTRESS uses multi-modal reasoners to\nidentify goals and anticipate failure modes. When a runtime monitor triggers a\nfallback response, FORTRESS rapidly synthesizes plans to fallback goals while\ninferring and avoiding semantically unsafe regions in real time. By bridging\nopen-world, multi-modal reasoning with dynamics-aware planning, we eliminate\nthe need for hard-coded fallbacks and human safety interventions. FORTRESS\noutperforms on-the-fly prompting of slow reasoning models in safety\nclassification accuracy on synthetic benchmarks and real-world ANYmal robot\ndata, and further improves system safety and planning success in simulation and\non quadrotor hardware for urban navigation.",
      "url": "http://arxiv.org/abs/2505.10547v1",
      "published_time_eastern_timestamp": 1747331728.0
    },
    {
      "title": "AORRTC: Almost-Surely Asymptotically Optimal Planning with RRT-Connect",
      "summary": "Finding high-quality solutions quickly is an important objective in motion\nplanning. This is especially true for high-degree-of-freedom robots.\nSatisficing planners have traditionally found feasible solutions quickly but\nprovide no guarantees on their optimality, while almost-surely asymptotically\noptimal (a.s.a.o.) planners have probabilistic guarantees on their convergence\ntowards an optimal solution but are more computationally expensive.\n  This paper uses the AO-x meta-algorithm to extend the satisficing RRT-Connect\nplanner to optimal planning. The resulting Asymptotically Optimal RRT-Connect\n(AORRTC) finds initial solutions in similar times as RRT-Connect and uses any\nadditional planning time to converge towards the optimal solution in an anytime\nmanner. It is proven to be probabilistically complete and a.s.a.o.\n  AORRTC was tested with the Panda (7 DoF) and Fetch (8 DoF) robotic arms on\nthe MotionBenchMaker dataset. These experiments show that AORRTC finds initial\nsolutions as fast as RRT-Connect and faster than the tested state-of-the-art\na.s.a.o. algorithms while converging to better solutions faster. AORRTC finds\nsolutions to difficult high-DoF planning problems in milliseconds where the\nother a.s.a.o. planners could not consistently find solutions in seconds. This\nperformance was demonstrated both with and without single instruction/multiple\ndata (SIMD) acceleration.",
      "url": "http://arxiv.org/abs/2505.10542v1",
      "published_time_eastern_timestamp": 1747331591.0
    },
    {
      "title": "Multi-Token Prediction Needs Registers",
      "summary": "Multi-token prediction has emerged as a promising objective for improving\nlanguage model pretraining, but its benefits have not consistently generalized\nto other settings such as fine-tuning. In this paper, we propose MuToR, a\nsimple and effective approach to multi-token prediction that interleaves\nlearnable register tokens into the input sequence, each tasked with predicting\nfuture targets. Compared to existing methods, MuToR offers several key\nadvantages: it introduces only a negligible number of additional parameters,\nrequires no architectural changes--ensuring compatibility with off-the-shelf\npretrained language models--and remains aligned with the next-token pretraining\nobjective, making it especially well-suited for supervised fine-tuning.\nMoreover, it naturally supports scalable prediction horizons. We demonstrate\nthe effectiveness and versatility of MuToR across a range of use cases,\nincluding supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and\npretraining, on challenging generative tasks in both language and vision\ndomains. Our code will be available at: https://github.com/nasosger/MuToR.",
      "url": "http://arxiv.org/abs/2505.10518v1",
      "published_time_eastern_timestamp": 1747329903.0
    },
    {
      "title": "A Tutorial on Structural Identifiability of Epidemic Models Using\n  StructuralIdentifiability.jl",
      "summary": "Structural identifiability -- the theoretical ability to uniquely recover\nmodel parameters from ideal, noise-free data -- is a prerequisite for reliable\nparameter estimation in epidemic modeling. Despite its importance, structural\nidentifiability analysis remains underutilized in the infectious disease\nmodeling literature. In this tutorial, we present a practical and reproducible\nworkflow for conducting structural identifiability analysis of ordinary\ndifferential equation models using the Julia package\n\\texttt{StructuralIdentifiability.jl}. We apply the tool to a range of epidemic\nmodels, including SEIR variants with asymptomatic and pre-symptomatic\ntransmission, vector-borne systems, and models incorporating hospitalization\nand disease-induced mortality. We compare results from\nStructuralIdentifiability.jl with those obtained using DAISY, a widely used\ndifferential algebra tool, and highlight cases where the Julia package succeeds\nin analyzing models that DAISY cannot handle. Our findings underscore how\nidentifiability depends on model structure, the availability of initial\nconditions, and the choice of observed states. All code and diagrams are\npublicly available, making this tutorial a valuable reference for researchers\nand educators working with dynamic disease models.",
      "url": "http://arxiv.org/abs/2505.10517v1",
      "published_time_eastern_timestamp": 1747329767.0
    },
    {
      "title": "PnPXAI: A Universal XAI Framework Providing Automatic Explanations\n  Across Diverse Modalities and Models",
      "summary": "Recently, post hoc explanation methods have emerged to enhance model\ntransparency by attributing model outputs to input features. However, these\nmethods face challenges due to their specificity to certain neural network\narchitectures and data modalities. Existing explainable artificial intelligence\n(XAI) frameworks have attempted to address these challenges but suffer from\nseveral limitations. These include limited flexibility to diverse model\narchitectures and data modalities due to hard-coded implementations, a\nrestricted number of supported XAI methods because of the requirements for\nlayer-specific operations of attribution methods, and sub-optimal\nrecommendations of explanations due to the lack of evaluation and optimization\nphases. Consequently, these limitations impede the adoption of XAI technology\nin real-world applications, making it difficult for practitioners to select the\noptimal explanation method for their domain. To address these limitations, we\nintroduce \\textbf{PnPXAI}, a universal XAI framework that supports diverse data\nmodalities and neural network models in a Plug-and-Play (PnP) manner. PnPXAI\nautomatically detects model architectures, recommends applicable explanation\nmethods, and optimizes hyperparameters for optimal explanations. We validate\nthe framework's effectiveness through user surveys and showcase its versatility\nacross various domains, including medicine and finance.",
      "url": "http://arxiv.org/abs/2505.10515v1",
      "published_time_eastern_timestamp": 1747329714.0
    },
    {
      "title": "Achievable rates for concatenated square Gottesman-Kitaev-Preskill codes",
      "summary": "The Gottesman-Kitaev-Preskill (GKP) codes are known to achieve optimal rates\nunder displacement noise and pure loss channels, which establishes theoretical\nfoundations for its optimality. However, such optimal rates are only known to\nbe achieved at a discrete set of noise strength with the current self-dual\nsymplectic lattice construction. In this work, we develop a new coding strategy\nusing concatenated continuous variable - discrete variable encodings to go\nbeyond past results and establish GKP's optimal rate over all noise strengths.\nIn particular, for displacement noise, the rate is obtained through a\nconstructive approach by concatenating GKP codes with a quantum polar code and\nanalog decoding. For pure loss channel, we prove the existence of\ncapacity-achieving GKP codes through a random coding approach. These results\nhighlight the capability of concatenation-based GKP codes and provides new\nmethods for constructing good GKP lattices.",
      "url": "http://arxiv.org/abs/2505.10499v1",
      "published_time_eastern_timestamp": 1747328511.0
    },
    {
      "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models\n  from a Code Security Perspective",
      "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.",
      "url": "http://arxiv.org/abs/2505.10494v1",
      "published_time_eastern_timestamp": 1747328021.0
    },
    {
      "title": "Logos as a Well-Tempered Pre-train for Sign Language Recognition",
      "summary": "This paper examines two aspects of the isolated sign language recognition\n(ISLR) task. First, despite the availability of a number of datasets, the\namount of data for most individual sign languages is limited. It poses the\nchallenge of cross-language ISLR model training, including transfer learning.\nSecond, similar signs can have different semantic meanings. It leads to\nambiguity in dataset labeling and raises the question of the best policy for\nannotating such signs. To address these issues, this study presents Logos, a\nnovel Russian Sign Language (RSL) dataset, the most extensive ISLR dataset by\nthe number of signers and one of the largest available datasets while also the\nlargest RSL dataset in size and vocabulary. It is shown that a model,\npre-trained on the Logos dataset can be used as a universal encoder for other\nlanguage SLR tasks, including few-shot learning. We explore cross-language\ntransfer learning approaches and find that joint training using multiple\nclassification heads benefits accuracy for the target lowresource datasets the\nmost. The key feature of the Logos dataset is explicitly annotated visually\nsimilar sign groups. We show that explicitly labeling visually similar signs\nimproves trained model quality as a visual encoder for downstream tasks. Based\non the proposed contributions, we outperform current state-of-the-art results\nfor the WLASL dataset and get competitive results for the AUTSL dataset, with a\nsingle stream model processing solely RGB video. The source code, dataset, and\npre-trained models are publicly available.",
      "url": "http://arxiv.org/abs/2505.10481v1",
      "published_time_eastern_timestamp": 1747326709.0
    },
    {
      "title": "HWA-UNETR: Hierarchical Window Aggregate UNETR for 3D Multimodal Gastric\n  Lesion Segmentation",
      "summary": "Multimodal medical image segmentation faces significant challenges in the\ncontext of gastric cancer lesion analysis. This clinical context is defined by\nthe scarcity of independent multimodal datasets and the imperative to\namalgamate inherently misaligned modalities. As a result, algorithms are\nconstrained to train on approximate data and depend on application migration,\nleading to substantial resource expenditure and a potential decline in analysis\naccuracy. To address those challenges, we have made two major contributions:\nFirst, we publicly disseminate the GCM 2025 dataset, which serves as the first\nlarge-scale, open-source collection of gastric cancer multimodal MRI scans,\nfeaturing professionally annotated FS-T2W, CE-T1W, and ADC images from 500\npatients. Second, we introduce HWA-UNETR, a novel 3D segmentation framework\nthat employs an original HWA block with learnable window aggregation layers to\nestablish dynamic feature correspondences between different modalities'\nanatomical structures, and leverages the innovative tri-orientated fusion mamba\nmechanism for context modeling and capturing long-range spatial dependencies.\nExtensive experiments on our GCM 2025 dataset and the publicly BraTS 2021\ndataset validate the performance of our framework, demonstrating that the new\napproach surpasses existing methods by up to 1.68\\% in the Dice score while\nmaintaining solid robustness. The dataset and code are public via\nhttps://github.com/JeMing-creater/HWA-UNETR.",
      "url": "http://arxiv.org/abs/2505.10464v1",
      "published_time_eastern_timestamp": 1747325880.0
    },
    {
      "title": "SMIET: Fast and accurate synthesis of radio pulses from extensive air\n  shower using simulated templates",
      "summary": "Interpreting the data from radio detectors for extensive air showers\ntypically relies on Monte-Carlo based simulation codes, which, despite their\naccuracy are computationally expensive and present bottlenecks for analyses. To\naddress this issue we developed a novel method called template synthesis, which\nsynthesises the radio emission from cosmic ray air showers in seconds. This\nhybrid approach uses a microscopically simulated, sliced shower (the origin) as\nan input. It rescales the emission from each slice individually to synthesise\nthe emission from a shower with different properties (the target). In order to\nbe able to change the arrival direction during synthesis, we adjust the phases\nbased on the expected geometrical delays. We benchmark the method by comparing\nsynthesised traces to CoREAS simulations over a wide frequency range of [30,\n500] MHz . The synthesis quality is primarily influenced by the difference in\n$X_{max}$ between the origin and target shower. When $\\Delta X_{max} \\leq 100\ng/cm^2$ , the scatter on the maximum amplitudes of the geomagnetic traces is at\nmost 4%. For the traces from the charge-excess component this scatter is\nsmaller than 6%. We also observe a bias with $\\Delta X_{max}$ up to 5% for both\ncomponents, which appears to depend on the antenna position. Since the bias is\nsymmetrical around $\\Delta X_{max} = 0 g/cm^2$, we can use an interpolation\napproach to correct for it. We have implemented the template synthesis\nalgorithm in a Python package called \\texttt{SMIET}, which includes all the\nnecessary parameters. This package has been successfully tested with air\nshowers with zenith angles up to $50^{\\circ}$ and can be used with any\natmosphere, observation level and magnetic field. We demonstrate that the\nsynthesis quality remains comparable to our main benchmarks across various\nscenarios and discuss use cases, including machine-learning-based analyses.",
      "url": "http://arxiv.org/abs/2505.10459v1",
      "published_time_eastern_timestamp": 1747325775.0
    },
    {
      "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion\n  Language Models",
      "summary": "We introduce the \\emph{Diffusion Chain of Lateral Thought (DCoLT)}, a\nreasoning framework for diffusion language models. DCoLT treats each\nintermediate step in the reverse diffusion process as a latent \"thinking\"\naction and optimizes the entire reasoning trajectory to maximize the reward on\nthe correctness of the final answer with outcome-based Reinforcement Learning\n(RL). Unlike traditional Chain-of-Thought (CoT) methods that follow a causal,\nlinear thinking process, DCoLT allows bidirectional, non-linear reasoning with\nno strict rule on grammatical correctness amid its intermediate steps of\nthought. We implement DCoLT on two representative Diffusion Language Models\n(DLMs). First, we choose SEDD as a representative continuous-time discrete\ndiffusion model, where its concrete score derives a probabilistic policy to\nmaximize the RL reward over the entire sequence of intermediate diffusion\nsteps. We further consider the discrete-time masked diffusion language model --\nLLaDA, and find that the order to predict and unmask tokens plays an essential\nrole to optimize its RL action resulting from the ranking-based Unmasking\nPolicy Module (UPM) defined by the Plackett-Luce model. Experiments on both\nmath and code generation tasks show that using only public data and 16 H800\nGPUs, DCoLT-reinforced DLMs outperform other DLMs trained by SFT or RL or even\nboth. Notably, DCoLT-reinforced LLaDA boosts its reasoning accuracy by +9.8%,\n+5.7%, +11.4%, +19.5% on GSM8K, MATH, MBPP, and HumanEval.",
      "url": "http://arxiv.org/abs/2505.10446v1",
      "published_time_eastern_timestamp": 1747325192.0
    },
    {
      "title": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?",
      "summary": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding.",
      "url": "http://arxiv.org/abs/2505.10443v1",
      "published_time_eastern_timestamp": 1747325065.0
    },
    {
      "title": "A 140 line MATLAB code for topology optimization problems with\n  probabilistic parameters",
      "summary": "We present an efficient 140 line MATLAB code for topology optimization\nproblems that include probabilistic parameters. It is built from the top99neo\ncode by Ferrari and Sigmund and incorporates a stochastic sample-based\napproach. Old gradient samples are adaptively recombined during the\noptimization process to obtain a gradient approximation with vanishing\napproximation error. The method's performance is thoroughly analyzed for\nseveral numerical examples. While we focus on applications in which stochastic\nparameters describe local material failure, we also present extensions of the\ncode to other settings, such as uncertain load positions or dynamic forces of\nunknown frequency. The complete code is included in the Appendix and can be\ndownloaded from www.topopt.dtu.dk.",
      "url": "http://arxiv.org/abs/2505.10421v1",
      "published_time_eastern_timestamp": 1747323500.0
    },
    {
      "title": "Learned Lightweight Smartphone ISP with Unpaired Data",
      "summary": "The Image Signal Processor (ISP) is a fundamental component in modern\nsmartphone cameras responsible for conversion of RAW sensor image data to RGB\nimages with a strong focus on perceptual quality. Recent work highlights the\npotential of deep learning approaches and their ability to capture details with\na quality increasingly close to that of professional cameras. A difficult and\ncostly step when developing a learned ISP is the acquisition of pixel-wise\naligned paired data that maps the raw captured by a smartphone camera sensor to\nhigh-quality reference images. In this work, we address this challenge by\nproposing a novel training method for a learnable ISP that eliminates the need\nfor direct correspondences between raw images and ground-truth data with\nmatching content. Our unpaired approach employs a multi-term loss function\nguided by adversarial training with multiple discriminators processing feature\nmaps from pre-trained networks to maintain content structure while learning\ncolor and texture characteristics from the target RGB dataset. Using\nlightweight neural network architectures suitable for mobile devices as\nbackbones, we evaluated our method on the Zurich RAW to RGB and Fujifilm\nUltraISP datasets. Compared to paired training methods, our unpaired learning\nstrategy shows strong potential and achieves high fidelity across multiple\nevaluation metrics. The code and pre-trained models are available at\nhttps://github.com/AndreiiArhire/Learned-Lightweight-Smartphone-ISP-with-Unpaired-Data .",
      "url": "http://arxiv.org/abs/2505.10420v1",
      "published_time_eastern_timestamp": 1747323471.0
    },
    {
      "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented\n  Generation",
      "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.",
      "url": "http://arxiv.org/abs/2505.10413v1",
      "published_time_eastern_timestamp": 1747323255.0
    },
    {
      "title": "Digging Deeper for RR Lyrae Stars with Low Modulation Amplitudes",
      "summary": "With the goal of searching for very low modulation amplitudes among\nfundamental mode RR Lyrae stars and assess their incidence rate, we performed a\nsurvey of 36 stars observed by the Kepler satellite during the entire four-year\nperiod of its mission. The search was conducted by a task-oriented code,\ndesigned to find low-amplitude signals in the presence of high-amplitude\ncomponents and instrumental systematics. We found 7 new modulated stars and\nnegate one earlier claimed star, whereby increasing the number of known Blazhko\nstars from 17 to 24 and yielding an observed occurrence rate of 67% for the\nKepler field. Six of the new stars have the lowest modulation amplitudes found\nso far, with ~250 ppm Fourier side-lobe amplitudes near the fundamental mode\nfrequency. Because of the small sample size in the Kepler field, we extended\nthe survey to 12 campaign fields observed by K2, the ``two-wheeled'' mission of\nKepler. From the 1061 stars we found 514 Blazhko stars. After correcting for\nthe short duration of the time spent on each field, and for the noise\ndependence of the detections, we arrived at an underlying occurrence rate of\n~75% - likely a lower limit for the true rate of Blazhko stars in the K2\nfields.",
      "url": "http://arxiv.org/abs/2505.10411v1",
      "published_time_eastern_timestamp": 1747323167.0
    }
  ]
}
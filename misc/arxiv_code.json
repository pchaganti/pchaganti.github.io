{
  "last_updated": "2025-08-14T05:15:33.655775-04:00",
  "papers": [
    {
      "title": "LLMC+: Benchmarking Vision-Language Model Compression with a\n  Plug-and-play Toolkit",
      "summary": "Large Vision-Language Models (VLMs) exhibit impressive multi-modal\ncapabilities but suffer from prohibitive computational and memory demands, due\nto their long visual token sequences and massive parameter sizes. To address\nthese issues, recent works have proposed training-free compression methods.\nHowever, existing efforts often suffer from three major limitations: (1)\nCurrent approaches do not decompose techniques into comparable modules,\nhindering fair evaluation across spatial and temporal redundancy. (2)\nEvaluation confined to simple single-turn tasks, failing to reflect performance\nin realistic scenarios. (3) Isolated use of individual compression techniques,\nwithout exploring their joint potential. To overcome these gaps, we introduce\nLLMC+, a comprehensive VLM compression benchmark with a versatile,\nplug-and-play toolkit. LLMC+ supports over 20 algorithms across five\nrepresentative VLM families and enables systematic study of token-level and\nmodel-level compression. Our benchmark reveals that: (1) Spatial and temporal\nredundancies demand distinct technical strategies. (2) Token reduction methods\ndegrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)\nCombining token and model compression achieves extreme compression with minimal\nperformance loss. We believe LLMC+ will facilitate fair evaluation and inspire\nfuture research in efficient VLM. Our code is available at\nhttps://github.com/ModelTC/LightCompress.",
      "url": "http://arxiv.org/abs/2508.09981v1",
      "published_time_eastern_timestamp": 1755107689.0
    },
    {
      "title": "Chemical composition of planetary hosts: II. Abundances of\n  neutron-capture elements",
      "summary": "We present a study of neutron-capture element abundances (Sr, Y, Zr, Ba, La,\nCe, Nd, Pr, and Eu) in a large and homogeneous sample of 160 F-, G-, and K-type\nplanet-host stars located in the northern hemisphere, including 32 stars in\nmulti-planetary systems. The sample hosts a total of 175 high-mass planets and\n47 Neptunian and super-Earth planets. High-resolution spectra were obtained\nwith the 1.65-metre telescope at the Mol\\.etai Astronomical Observatory using a\nfibre-fed spectrograph covering 4000-8500 \\r{A}. Elemental abundances were\ndetermined by differential line-by-line spectrum synthesis with the\nTURBOSPECTRUM code and MARCS model atmospheres. The analysis of\n$[\\mathrm{El}/\\mathrm{Fe}]$ ratios shows that most elements in PHSs follow the\nGalactic chemical evolution, but $[\\mathrm{Zr}/\\mathrm{Fe}]$,\n$[\\mathrm{La}/\\mathrm{Fe}]$, and $[\\mathrm{Ce}/\\mathrm{Fe}]$ are overabundant\nin PHSs relative to reference stars at a given $[\\mathrm{Fe}/\\mathrm{H}]$.\nCorrelations between $[\\mathrm{El}/\\mathrm{Fe}]$ and planet mass are generally\npositive, except for Sr, Y, and Ba, which show no significant trends. The\ndistribution of $\\Delta[\\mathrm{El}/\\mathrm{H}]$ versus condensation\ntemperature ($T_{\\mathrm{cond}}$) slopes is positively skewed for PHSs,\nindicating enrichment in refractory elements compared to analogues. While no\nstrong correlations are found between\n$\\Delta[\\mathrm{El}/\\mathrm{H}]$-$T_{\\mathrm{cond}}$ slopes and stellar or\nplanetary parameters, older dwarf stars with multiple planets tend to have\nsmaller or negative slopes, whereas younger dwarf stars exhibit larger positive\nslopes. Our results also confirm that multi-planetary systems are more frequent\naround metal-rich stars.",
      "url": "http://arxiv.org/abs/2508.09979v1",
      "published_time_eastern_timestamp": 1755107504.0
    },
    {
      "title": "Improving quantum communication rates with permutation-invariant codes",
      "summary": "In this work we improve the quantum communication rates of various quantum\nchannels of interest using permutation-invariant quantum codes. We focus in\nparticular on parametrized families of quantum channels and aim to improve\nbounds on their quantum capacity threshold, defined as the lowest noise level\nat which the quantum capacity of the channel family vanishes. These thresholds\nare important quantities as they mark the noise level up to which faithful\nquantum communication is theoretically possible. Our method exploits the fact\nthat independent and identically distributed quantum channels preserve any\npermutation symmetry present at the input. The resulting symmetric output\nstates can be described succinctly using the representation theory of the\nsymmetric and general linear groups, which we use to derive an efficient\nalgorithm for computing the channel coherent information of a\npermutation-invariant code. Our approach allows us to evaluate coherent\ninformation values for a large number of channel copies, e.g., at least 100\nchannel copies for qubit channels. We apply this method to various physically\nrelevant channel models, including general Pauli channels, the dephrasure\nchannel, the generalized amplitude damping channel, and the damping-dephasing\nchannel. For each channel family we obtain improved lower bounds on their\nquantum capacities. For example, for the 2-Pauli and BB84 channel families we\nsignificantly improve the best known quantum capacity thresholds derived in\n[Fern, Whaley 2008]. These threshold improvements are achieved using a\nrepetition code-like input state with non-orthogonal code states, which we\nfurther analyze in our representation-theoretic framework.",
      "url": "http://arxiv.org/abs/2508.09978v1",
      "published_time_eastern_timestamp": 1755107271.0
    },
    {
      "title": "A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing,\n  and Generation",
      "summary": "3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative\nto Neural Radiance Fields (NeRF) for 3D scene representation, offering\nhigh-fidelity photorealistic rendering with real-time performance. Beyond novel\nview synthesis, the explicit and compact nature of 3DGS enables a wide range of\ndownstream applications that require geometric and semantic understanding. This\nsurvey provides a comprehensive overview of recent progress in 3DGS\napplications. It first introduces 2D foundation models that support semantic\nunderstanding and control in 3DGS applications, followed by a review of\nNeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS\napplications into segmentation, editing, generation, and other functional\ntasks. For each, we summarize representative methods, supervision strategies,\nand learning paradigms, highlighting shared design principles and emerging\ntrends. Commonly used datasets and evaluation protocols are also summarized,\nalong with comparative analyses of recent methods across public benchmarks. To\nsupport ongoing research and development, a continually updated repository of\npapers, code, and resources is maintained at\nhttps://github.com/heshuting555/Awesome-3DGS-Applications.",
      "url": "http://arxiv.org/abs/2508.09977v1",
      "published_time_eastern_timestamp": 1755107079.0
    },
    {
      "title": "Laboratory Measurements of Ca XIX Dielectronic Recombination Satellites",
      "summary": "We report measurements of the K$\\alpha$ emission from the astrophysically\nvery abundant Ca XIX (He-like ion) and its satellite lines resonantly excited\nby dielectronic recombination (DR). We achieve an electron-energy resolution of\n8 eV in a cryogenic electron beam ion trap, and determine the energies of the\nexciting electrons and the emitted photons up to the KLn ($n\\le 8$) manifold\nwith $0.05\\%$ and $0.1\\%$ respective uncertainties. For the KLL satellites,\nenergies agree very well with our predictions using the Flexible Atomic Code\n(FAC) and previous state-of-the-art calculations. Our calculations also agree\nwith our experimental direct excitation cross-sections for K$\\alpha$ within\ntheir $10\\%$ uncertainty. We extract DR coefficient rates and find good\nagreement with values tabulated in the OPEN-ADAS database. As an application,\nwe experimentally benchmark Ca XIX atomic data used to model high-temperature\nastrophysical plasmas by comparing FAC synthetic spectra with recent XRISM\nobservations revealing the contributions of DR satellites to the Ca XIX lines.",
      "url": "http://arxiv.org/abs/2508.09975v1",
      "published_time_eastern_timestamp": 1755106914.0
    },
    {
      "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
      "summary": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise",
      "url": "http://arxiv.org/abs/2508.09968v1",
      "published_time_eastern_timestamp": 1755106417.0
    },
    {
      "title": "MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image\n  Classification",
      "summary": "Recent advances in histopathology vision-language foundation models (VLFMs)\nhave shown promise in addressing data scarcity for whole slide image (WSI)\nclassification via zero-shot adaptation. However, these methods remain\noutperformed by conventional multiple instance learning (MIL) approaches\ntrained on large datasets, motivating recent efforts to enhance VLFM-based WSI\nclassification through fewshot learning paradigms. While existing few-shot\nmethods improve diagnostic accuracy with limited annotations, their reliance on\nconventional classifier designs introduces critical vulnerabilities to data\nscarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)\ncomprising two core components: (1) a meta-learner that automatically optimizes\na classifier configuration from a mixture of candidate classifiers and (2) a\nclassifier bank housing diverse candidate classifiers to enable a holistic\npathological interpretation. Extensive experiments demonstrate that MOC\noutperforms prior arts in multiple few-shot benchmarks. Notably, on the\nTCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art\nfew-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,\noffering a critical advancement for clinical deployments where diagnostic\ntraining data is severely limited. Code is available at\nhttps://github.com/xmed-lab/MOC.",
      "url": "http://arxiv.org/abs/2508.09967v1",
      "published_time_eastern_timestamp": 1755106362.0
    },
    {
      "title": "LIA-X: Interpretable Latent Portrait Animator",
      "summary": "We introduce LIA-X, a novel interpretable portrait animator designed to\ntransfer facial dynamics from a driving video to a source portrait with\nfine-grained control. LIA-X is an autoencoder that models motion transfer as a\nlinear navigation of motion codes in latent space. Crucially, it incorporates a\nnovel Sparse Motion Dictionary that enables the model to disentangle facial\ndynamics into interpretable factors. Deviating from previous 'warp-render'\napproaches, the interpretability of the Sparse Motion Dictionary allows LIA-X\nto support a highly controllable 'edit-warp-render' strategy, enabling precise\nmanipulation of fine-grained facial semantics in the source portrait. This\nhelps to narrow initial differences with the driving video in terms of pose and\nexpression. Moreover, we demonstrate the scalability of LIA-X by successfully\ntraining a large-scale model with approximately 1 billion parameters on\nextensive datasets. Experimental results show that our proposed method\noutperforms previous approaches in both self-reenactment and cross-reenactment\ntasks across several benchmarks. Additionally, the interpretable and\ncontrollable nature of LIA-X supports practical applications such as\nfine-grained, user-guided image and video editing, as well as 3D-aware portrait\nvideo manipulation.",
      "url": "http://arxiv.org/abs/2508.09959v1",
      "published_time_eastern_timestamp": 1755105725.0
    },
    {
      "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models",
      "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.",
      "url": "http://arxiv.org/abs/2508.09945v1",
      "published_time_eastern_timestamp": 1755104444.0
    },
    {
      "title": "Mathematical Computation and Reasoning Errors by Large Language Models",
      "summary": "Large Language Models (LLMs) are increasingly utilized in AI-driven\neducational instruction and assessment, particularly within mathematics\neducation. The capability of LLMs to generate accurate answers and detailed\nsolutions for math problem-solving tasks is foundational for ensuring reliable\nand precise feedback and assessment in math education practices. Our study\nfocuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,\nDeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including\narithmetic, algebra, and number theory, and identifies step-level reasoning\nerrors within their solutions. Instead of relying on standard benchmarks, we\nintentionally build math tasks (via item models) that are challenging for LLMs\nand prone to errors. The accuracy of final answers and the presence of errors\nin individual solution steps were systematically analyzed and coded. Both\nsingle-agent and dual-agent configurations were tested. It is observed that the\nreasoning-enhanced OpenAI o1 model consistently achieved higher or nearly\nperfect accuracy across all three math task categories. Analysis of errors\nrevealed that procedural slips were the most frequent and significantly\nimpacted overall performance, while conceptual misunderstandings were less\nfrequent. Deploying dual-agent configurations substantially improved overall\nperformance. These findings offer actionable insights into enhancing LLM\nperformance and underscore effective strategies for integrating LLMs into\nmathematics education, thereby advancing AI-driven instructional practices and\nassessment precision.",
      "url": "http://arxiv.org/abs/2508.09932v1",
      "published_time_eastern_timestamp": 1755102782.0
    },
    {
      "title": "Fault tolerant Operations in Majorana-based Quantum Codes: Gates,\n  Measurements and High Rate Constructions",
      "summary": "Majorana-based quantum computation in nanowires and neutral atoms has gained\nprominence as a promising platform to encode qubits and protect them against\nnoise. In order to run computations reliably on such devices, a fully\nfault-tolerant scheme is needed for state preparation, gates, and measurements.\nHowever, current fault-tolerant schemes have either been limited to specific\ncode families or have not been developed fully. In this work, we develop a\ngeneral framework for fault-tolerant computation with logical degrees encoded\ninto Majorana hardware. We emphasize the division between even and odd Majorana\ncodes and how it manifests when constructing fault tolerant gadgets for these\nfamilies. We provide transversal constructions and supplement them with\nmeasurements to obtain several examples of fault tolerant Clifford gadgets. For\nthe case of odd codes, we give a novel construction for gadgets using quantum\nreference frames, that allows to implement operations that are forbidden due to\nparity superselection. We also provide a fault-tolerant measurement scheme for\nMajorana codes inspired by Steane error correction, enabling state preparation,\nmeasurement of logical operations and error correction. We also point out a\nconstruction for odd Majorana codes with transversal T gates. Finally, we\nconstruct an asympotically good quantum LDPC Majorana code with qubit degrees\nof freedom. Our work shows that all necessary elements of fault-tolerant\nquantum computation can be consistently implemented in fermionic hardware such\nas Majorana nanowires and fermionic neutral atoms.",
      "url": "http://arxiv.org/abs/2508.09928v1",
      "published_time_eastern_timestamp": 1755102533.0
    },
    {
      "title": "Towards Comprehensive Cellular Characterisation of H&E slides",
      "summary": "Cell detection, segmentation and classification are essential for analyzing\ntumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing\nmethods suffer from poor performance on understudied cell types (rare or not\npresent in public datasets) and limited cross-domain generalization. To address\nthese shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell\nanalysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei\ncovering 13 cell types. In external validation across 4 independent cohorts,\nHistoPLUS outperforms current state-of-the-art models in detection quality by\n5.2% and overall F1 classification score by 23.7%, while using 5x fewer\nparameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types\nand brings significant improvements on 8 of 13 cell types. Moreover, we show\nthat HistoPLUS robustly transfers to two oncology indications unseen during\ntraining. To support broader TME biomarker research, we release the model\nweights and inference code at https://github.com/owkin/histoplus/.",
      "url": "http://arxiv.org/abs/2508.09926v1",
      "published_time_eastern_timestamp": 1755102255.0
    },
    {
      "title": "SpeechForensics: Audio-Visual Speech Representation Learning for Face\n  Forgery Detection",
      "summary": "Detection of face forgery videos remains a formidable challenge in the field\nof digital forensics, especially the generalization to unseen datasets and\ncommon perturbations. In this paper, we tackle this issue by leveraging the\nsynergy between audio and visual speech elements, embarking on a novel approach\nthrough audio-visual speech representation learning. Our work is motivated by\nthe finding that audio signals, enriched with speech content, can provide\nprecise information effectively reflecting facial movements. To this end, we\nfirst learn precise audio-visual speech representations on real videos via a\nself-supervised masked prediction task, which encodes both local and global\nsemantic information simultaneously. Then, the derived model is directly\ntransferred to the forgery detection task. Extensive experiments demonstrate\nthat our method outperforms the state-of-the-art methods in terms of\ncross-dataset generalization and robustness, without the participation of any\nfake video in model training. Code is available at\nhttps://github.com/Eleven4AI/SpeechForensics.",
      "url": "http://arxiv.org/abs/2508.09913v1",
      "published_time_eastern_timestamp": 1755101376.0
    },
    {
      "title": "Exploring the Physics of the Plasma Liner Experiment: A\n  Multi-dimensional Study with FLASH, OSIRIS, and HELIOS",
      "summary": "The Plasma Liner Experiment (PLX) at Los Alamos National Laboratory (LANL) is\na platform that seeks to achieve fusion via a concept known as\nPlasma-Jet-Driven Magneto-Inertial Fusion (PJMIF). The experiment consists of\nthree main phases: (1) target formation in which up to four plasma guns shoot\nmagnetized hydrogen or deuterium-tritium jets to form a quasi-spherical target,\n(2) liner formation in which a constellation of 36 guns fire high-atomic number\n(e.g., xenon) jets to form a liner shell, and (3) target compression in which\nthe formed liner implodes the pre-formed target. Each phase of the PLX probes\ndifferent plasma regimes with different physics at play, thus we simulated each\nphase separately and with multiple codes. Here we highlight some of the 1D, 2D,\nand 3D simulation results of all three phases from the FLASH, OSIRIS, and\nHELIOS codes. Some of the key physical processes involved include shock\ndynamics, kinetic effects, anisotropic thermal conduction, resistive magnetic\ndiffusion, radiation transport, and magnetized jet dynamics. Our simulations\nshow that the PLX can form a preheated (~40 eV), magnetized (electron Hall\nparameter >1) target plasma, and a quasi-collisional liner shell that can\nsubsequently compress the target to fusion-relevant conditions (e.g.,\ntemperatures >1 keV).",
      "url": "http://arxiv.org/abs/2508.09895v1",
      "published_time_eastern_timestamp": 1755100514.0
    },
    {
      "title": "Beyond Scaling Law: A Data-Efficient Distillation Framework for\n  Reasoning",
      "summary": "Large language models (LLMs) demonstrate remarkable reasoning capabilities in\ntasks such as algorithmic coding and mathematical problem-solving. Recent\nmethods have improved reasoning through expanded corpus and multistage training\ncombining reinforcement learning and supervised fine-tuning. Although some\nmethods suggest that small but targeted dataset can incentivize reasoning via\nonly distillation, a reasoning scaling laws is still taking shape, increasing\ncomputational costs. To address this, we propose a data-efficient distillation\nframework (DED) that optimizes the Pareto frontier of reasoning distillation.\nInspired by the on-policy learning and diverse roll-out strategies of\nreinforcement learning, the key idea of our approach is threefold: (1) We\nidentify that benchmark scores alone do not determine an effective teacher\nmodel. Through comprehensive comparisons of leading reasoning LLMs, we develop\na method to select an optimal teacher model. (2) While scaling distillation can\nenhance reasoning, it often degrades out-of-domain performance. A carefully\ncurated, smaller corpus achieves a balanced trade-off between in-domain and\nout-of-domain capabilities. (3) Diverse reasoning trajectories encourage the\nstudent model to develop robust reasoning skills. We validate our method\nthrough evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and\ncode generation (LiveCodeBench), achieving state-of-the-art results with only\n0.8k carefully curated examples, bypassing the need for extensive scaling. Our\nsystematic analysis demonstrates that DED outperforms existing methods by\nconsidering factors beyond superficial hardness, token length, or teacher model\ncapability. This work offers a practical and efficient pathway to advanced\nreasoning while preserving general capabilities.",
      "url": "http://arxiv.org/abs/2508.09883v1",
      "published_time_eastern_timestamp": 1755099145.0
    },
    {
      "title": "Inference of germinal center evolutionary dynamics via simulation-based\n  deep learning",
      "summary": "B cells and the antibodies they produce are vital to health and survival,\nmotivating research on the details of the mutational and evolutionary processes\nin the germinal centers (GC) from which mature B cells arise. It is known that\nB cells with higher affinity for their cognate antigen (Ag) will, on average,\ntend to have more offspring. However the exact form of this relationship\nbetween affinity and fecundity, which we call the ``affinity-fitness response\nfunction'', is not known. Here we use deep learning and simulation-based\ninference to learn this function from a unique experiment that replays a\nparticular combination of GC conditions many times. All code is freely\navailable at https://github.com/matsengrp/gcdyn, while datasets and inference\nresults can be found at https://doi.org/10.5281/zenodo.15022130.",
      "url": "http://arxiv.org/abs/2508.09871v1",
      "published_time_eastern_timestamp": 1755097785.0
    },
    {
      "title": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource\n  Urdu Transcription",
      "summary": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems.",
      "url": "http://arxiv.org/abs/2508.09865v1",
      "published_time_eastern_timestamp": 1755097319.0
    },
    {
      "title": "Human-Aligned Procedural Level Generation Reinforcement Learning via\n  Text-Level-Sketch Shared Representation",
      "summary": "Human-aligned AI is a critical component of co-creativity, as it enables\nmodels to accurately interpret human intent and generate controllable outputs\nthat align with design goals in collaborative content creation. This direction\nis especially relevant in procedural content generation via reinforcement\nlearning (PCGRL), which is intended to serve as a tool for human designers.\nHowever, existing systems often fall short of exhibiting human-centered\nbehavior, limiting the practical utility of AI-driven generation tools in\nreal-world design workflows. In this paper, we propose VIPCGRL\n(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that\nincorporates three modalities-text, level, and sketches-to extend control\nmodality and enhance human-likeness. We introduce a shared embedding space\ntrained via quadruple contrastive learning across modalities and human-AI\nstyles, and align the policy using an auxiliary reward based on embedding\nsimilarity. Experimental results show that VIPCGRL outperforms existing\nbaselines in human-likeness, as validated by both quantitative metrics and\nhuman evaluations. The code and dataset will be available upon publication.",
      "url": "http://arxiv.org/abs/2508.09860v1",
      "published_time_eastern_timestamp": 1755096734.0
    },
    {
      "title": "Exploring the Potential of Large Language Models in Fine-Grained Review\n  Comment Classification",
      "summary": "Code review is a crucial practice in software development. As code review\nnowadays is lightweight, various issues can be identified, and sometimes, they\ncan be trivial. Research has investigated automated approaches to classify\nreview comments to gauge the effectiveness of code reviews. However, previous\nstudies have primarily relied on supervised machine learning, which requires\nextensive manual annotation to train the models effectively. To address this\nlimitation, we explore the potential of using Large Language Models (LLMs) to\nclassify code review comments. We assess the performance of LLMs to classify 17\ncategories of code review comments. Our results show that LLMs can classify\ncode review comments, outperforming the state-of-the-art approach using a\ntrained deep learning model. In particular, LLMs achieve better accuracy in\nclassifying the five most useful categories, which the state-of-the-art\napproach struggles with due to low training examples. Rather than relying\nsolely on a specific small training data distribution, our results show that\nLLMs provide balanced performance across high- and low-frequency categories.\nThese results suggest that the LLMs could offer a scalable solution for code\nreview analytics to improve the effectiveness of the code review process.",
      "url": "http://arxiv.org/abs/2508.09832v1",
      "published_time_eastern_timestamp": 1755094025.0
    },
    {
      "title": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface\n  Reconstruction from Point Clouds or Gaussians",
      "summary": "In this paper, we present a generalizable method for 3D surface\nreconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from\nRGB images. Unlike existing coordinate-based methods which are often\ncomputationally intensive when rendering explicit surfaces, our proposed\nmethod, named RayletDF, introduces a new technique called raylet distance\nfield, which aims to directly predict surface points from query rays. Our\npipeline consists of three key modules: a raylet feature extractor, a raylet\ndistance field predictor, and a multi-raylet blender. These components work\ntogether to extract fine-grained local geometric features, predict raylet\ndistances, and aggregate multiple predictions to reconstruct precise surface\npoints. We extensively evaluate our method on multiple public real-world\ndatasets, demonstrating superior performance in surface reconstruction from\npoint clouds or 3D Gaussians. Most notably, our method achieves exceptional\ngeneralization ability, successfully recovering 3D surfaces in a single-forward\npass across unseen datasets in testing.",
      "url": "http://arxiv.org/abs/2508.09830v1",
      "published_time_eastern_timestamp": 1755093921.0
    }
  ]
}
{
  "last_updated": "2025-11-06T10:13:52.583094-05:00",
  "papers": [
    {
      "title": "Introducing Quantum Computing into Statistical Physics: Random Walks and\n  the Ising Model with Qiskit",
      "summary": "Quantum computing offers a powerful new perspective on probabilistic and\ncollective behaviors traditionally taught in statistical physics. This paper\npresents two classroom-ready modules that integrate quantum computing into the\nundergraduate curriculum using Qiskit: the quantum random walk and the Ising\nmodel. Both modules allow students to simulate and contrast classical and\nquantum systems, deepening their understanding of concepts such as\nsuperposition, interference, and statistical distributions. We outline the\nquantum circuits involved, provide sample code and student activities, and\ndiscuss how each example can be used to enhance student engagement with\nstatistical physics. These modules are suitable for integration into courses in\nstatistical mechanics, modern physics, or as part of an introductory unit on\nquantum computing.",
      "url": "http://arxiv.org/abs/2511.03696v1",
      "published_time_eastern_timestamp": 1762367013.0
    },
    {
      "title": "The OpenHands Software Agent SDK: A Composable and Extensible Foundation\n  for Production Agents",
      "summary": "Agents are now used widely in the process of software development, but\nbuilding production-ready software engineering agents is a complex task.\nDeploying software agents effectively requires flexibility in implementation\nand experimentation, reliable and secure execution, and interfaces for users to\ninteract with agents. In this paper, we present the OpenHands Software Agent\nSDK, a toolkit for implementing software development agents that satisfy these\ndesiderata. This toolkit is a complete architectural redesign of the agent\ncomponents of the popular OpenHands framework for software development agents,\nwhich has 64k+ GitHub stars. To achieve flexibility, we design a simple\ninterface for implementing agents that requires only a few lines of code in the\ndefault case, but is easily extensible to more complex, full-featured agents\nwith features such as custom tools, memory management, and more. For security\nand reliability, it delivers seamless local-to-remote execution portability,\nintegrated REST/WebSocket services. For interaction with human users, it can\nconnect directly to a variety of interfaces, such as visual workspaces (VS\nCode, VNC, browser), command-line interfaces, and APIs. Compared with existing\nSDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native\nsandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and\nbuilt-in security analysis. Empirical results on SWE-Bench Verified and GAIA\nbenchmarks demonstrate strong performance. Put together, these elements allow\nthe OpenHands Software Agent SDK to provide a practical foundation for\nprototyping, unlocking new classes of custom applications, and reliably\ndeploying agents at scale.",
      "url": "http://arxiv.org/abs/2511.03690v1",
      "published_time_eastern_timestamp": 1762366604.0
    },
    {
      "title": "Realization of a Quantum Streaming Algorithm on Long-lived Trapped-ion\n  Qubits",
      "summary": "Large classical datasets are often processed in the streaming model, with\ndata arriving one item at a time. In this model, quantum algorithms have been\nshown to offer an unconditional exponential advantage in space. However,\nexperimentally implementing such streaming algorithms requires qubits that\nremain coherent while interacting with an external data stream. In this work,\nwe realize such a data-streaming model using Quantinuum Helios trapped-ion\nquantum computer with long-lived qubits that communicate with an external\nserver. We implement a quantum pair sketch, which is the primitive underlying\nmany quantum streaming algorithms, and use it to solve Hidden Matching, a\nproblem known to exhibit a theoretical exponential quantum advantage in space.\nFurthermore, we compile the quantum streaming algorithm to fault-tolerant\nquantum architectures based on surface and bivariate bicycle codes and show\nthat the quantum space advantage persists even with the overheads of\nfault-tolerance.",
      "url": "http://arxiv.org/abs/2511.03689v1",
      "published_time_eastern_timestamp": 1762366581.0
    },
    {
      "title": "Efficient GPU Parallelization of Electronic Transport and Nonequilibrium\n  Dynamics from Electron-Phonon Interactions in the Perturbo Code",
      "summary": "The Boltzmann transport equation (BTE) with electron-phonon (e-ph)\ninteractions computed from first principles is widely used to study electronic\ntransport and nonequilibrium dynamics in materials. Calculating the e-ph\ncollision integral is the most important step in the BTE, but it remains\ncomputationally costly, even with current MPI+OpenMP parallelization. This\nchallenge makes it difficult to study materials with large unit cells and to\nachieve high resolution in momentum space. Here, we show acceleration of BTE\ncalculations of electronic transport and ultrafast dynamics using graphical\nprocessing units (GPUs). We implement a novel data structure and algorithm,\noptimized for GPU hardware and developed using OpenACC, to process scattering\nchannels and efficiently compute the collision integral. This approach\nsignificantly reduces the overhead for data referencing, movement, and\nsynchronization. Relative to the efficient CPU implementation in the\nopen-source package Perturbo (v2.2.0), used as a baseline, this approach\nachieves a speed-up of 40 times for both transport and nonequilibrium dynamics\non GPU hardware, and achieves nearly linear scaling up to 100 GPUs. The novel\ndata structure can be generalized to other electron interactions and scattering\nprocesses. We released this GPU implementation in the latest public version\n(v3.0.0) of Perturbo. The new MPI+OpenMP+GPU parallelization enables sweeping\nstudies of e-ph physics and electron dynamics in conventional and quantum\nmaterials, and prepares Perturbo for exascale supercomputing platforms.",
      "url": "http://arxiv.org/abs/2511.03683v1",
      "published_time_eastern_timestamp": 1762365651.0
    },
    {
      "title": "ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained\n  Evaluation",
      "summary": "With the rapid advancement of natural language processing (NLP) technologies,\nthe demand for high-quality Chinese document question-answering datasets is\nsteadily growing. To address this issue, we present the Chinese Multi-Document\nQuestion Answering Dataset(ChiMDQA), specifically designed for downstream\nbusiness scenarios across prevalent domains including academic, education,\nfinance, law, medical treatment, and news. ChiMDQA encompasses long-form\ndocuments from six distinct fields, consisting of 6,068 rigorously curated,\nhigh-quality question-answer (QA) pairs further classified into ten\nfine-grained categories. Through meticulous document screening and a systematic\nquestion-design methodology, the dataset guarantees both diversity and high\nquality, rendering it applicable to various NLP tasks such as document\ncomprehension, knowledge extraction, and intelligent QA systems. Additionally,\nthis paper offers a comprehensive overview of the dataset's design objectives,\nconstruction methodologies, and fine-grained evaluation system, supplying a\nsubstantial foundation for future research and practical applications in\nChinese QA. The code and data are available at:\nhttps://anonymous.4open.science/r/Foxit-CHiMDQA/.",
      "url": "http://arxiv.org/abs/2511.03656v1",
      "published_time_eastern_timestamp": 1762362794.0
    },
    {
      "title": "nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN",
      "summary": "Tabular foundation models such as TabPFN have revolutionized predictive\nmachine learning for tabular data. At the same time, the driving factors of\nthis revolution are hard to understand. Existing open-source tabular foundation\nmodels are implemented in complicated pipelines boasting over 10,000 lines of\ncode, lack architecture documentation or code quality. In short, the\nimplementations are hard to understand, not beginner-friendly, and complicated\nto adapt for new experiments. We introduce nanoTabPFN, a simplified and\nlightweight implementation of the TabPFN v2 architecture and a corresponding\ntraining loop that uses pre-generated training data. nanoTabPFN makes tabular\nfoundation models more accessible to students and researchers alike. For\nexample, restricted to a small data setting it achieves a performance\ncomparable to traditional machine learning baselines within one minute of\npre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This\neliminated requirement of large computational resources makes pre-training\ntabular foundation models accessible for educational purposes. Our code is\navailable at https://github.com/automl/nanoTabPFN.",
      "url": "http://arxiv.org/abs/2511.03634v1",
      "published_time_eastern_timestamp": 1762361571.0
    },
    {
      "title": "Towards Formalizing Reinforcement Learning Theory",
      "summary": "In this paper, we formalize the almost sure convergence of $Q$-learning and\nlinear temporal difference (TD) learning with Markovian samples using the Lean\n4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are\namong the earliest and most influential reinforcement learning (RL) algorithms.\nThe investigation of their convergence properties is not only a major research\ntopic during the early development of the RL field but also receives increasing\nattention nowadays. This paper formally verifies their almost sure convergence\nin a unified framework based on the Robbins-Siegmund theorem. The framework\ndeveloped in this work can be easily extended to convergence rates and other\nmodes of convergence. This work thus makes an important step towards fully\nformalizing convergent RL results. The code is available at\nhttps://github.com/ShangtongZhang/rl-theory-in-lean.",
      "url": "http://arxiv.org/abs/2511.03618v1",
      "published_time_eastern_timestamp": 1762360547.0
    },
    {
      "title": "Human Mesh Modeling for Anny Body",
      "summary": "Parametric body models are central to many human-centric tasks, yet existing\nmodels often rely on costly 3D scans and learned shape spaces that are\nproprietary and demographically narrow. We introduce Anny, a simple, fully\ndifferentiable, and scan-free human body model grounded in anthropometric\nknowledge from the MakeHuman community. Anny defines a continuous,\ninterpretable shape space, where phenotype parameters (e.g. gender, age,\nheight, weight) control blendshapes spanning a wide range of human forms --\nacross ages (from infants to elders), body types, and proportions. Calibrated\nusing WHO population statistics, it provides realistic and demographically\ngrounded human shape variation within a single unified model. Thanks to its\nopenness and semantic control, Anny serves as a versatile foundation for 3D\nhuman modeling -- supporting millimeter-accurate scan fitting, controlled\nsynthetic data generation, and Human Mesh Recovery (HMR). We further introduce\nAnny-One, a collection of 800k photorealistic humans generated with Anny,\nshowing that despite its simplicity, HMR models trained with Anny can match the\nperformance of those trained with scan-based body models, while remaining\ninterpretable and broadly representative. The Anny body model and its code are\nreleased under the Apache 2.0 license, making Anny an accessible foundation for\nhuman-centric 3D modeling.",
      "url": "http://arxiv.org/abs/2511.03589v1",
      "published_time_eastern_timestamp": 1762359002.0
    },
    {
      "title": "PerfDojo: Automated ML Library Generation for Heterogeneous\n  Architectures",
      "summary": "The increasing complexity of machine learning models and the proliferation of\ndiverse hardware architectures (CPUs, GPUs, accelerators) make achieving\noptimal performance a significant challenge. Heterogeneity in instruction sets,\nspecialized kernel requirements for different data types and model features\n(e.g., sparsity, quantization), and architecture-specific optimizations\ncomplicate performance tuning. Manual optimization is resource-intensive, while\nexisting automatic approaches often rely on complex hardware-specific\nheuristics and uninterpretable intermediate representations, hindering\nperformance portability. We introduce PerfLLM, a novel automatic optimization\nmethodology leveraging Large Language Models (LLMs) and Reinforcement Learning\n(RL). Central to this is PerfDojo, an environment framing optimization as an RL\ngame using a human-readable, mathematically-inspired code representation that\nguarantees semantic validity through transformations. This allows effective\noptimization without prior hardware knowledge, facilitating both human analysis\nand RL agent training. We demonstrate PerfLLM's ability to achieve significant\nperformance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.",
      "url": "http://arxiv.org/abs/2511.03586v1",
      "published_time_eastern_timestamp": 1762358726.0
    },
    {
      "title": "Broad Iron Line as a Relativistic Reflection from Warm Corona in AGN",
      "summary": "We present that the broad feature usually observed in X-ray spectra can be\nexplained by a ray-traced emission from a two-slab system containing a\ndissipative, warm corona on top of an accretion disk in an AGN. Such an\naccretion flow is externally illuminated by X-ray radiation from a lamp located\nabove a central SMBH. Thermal lines from highly ionized iron ions (FeXXV and\nFeXXVI), caused by both internal heating and reflection from the warm corona,\ncan be integrated into an observed broad line profile due to the close vicinity\nof the SMBH. We investigate the dependence of the broad line profile by varying\nthe SMBH spin parameter, viewing angle, lamp height, and dissipation factor.\nOur results introduce a new method to probe properties of the warm corona using\nhigh-resolution spectroscopic measurements. We use the photoionization code\nTITAN to compute local ion populations and emission line profiles, and the\nray-tracing code GYOTO to include relativistic effects on the outgoing X-ray\nspectrum. In our models, the temperature of the inner atmosphere covering the\ndisk can reach values of 10^7 - 10^8 K due to internal warm corona dissipation\nand external illumination, which is adequate for generating the highly ionized\niron lines. These lines can undergo significant gravitational redshift near the\nblack hole, leading to a prominent spectral feature centered around 6.4 keV.\nFor all computed models, the relativistic corrections shift highly ionized iron\nlines to the X-ray region, usually attributed to fluorescent emission from the\nilluminated skin of an accretion disk. Hence, in the case of a warm corona\ncovering the inner disk regions, the resulting theoretical line profile under\nstrong gravity is a sum of different iron line transitions, and those\noriginating from highly ionized iron contribute the most to the observed total\nline profile in AGN.",
      "url": "http://arxiv.org/abs/2511.03575v1",
      "published_time_eastern_timestamp": 1762358320.0
    },
    {
      "title": "OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single\n  Panoramic Camera",
      "summary": "Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most\nsemantic scene completion (SSC) systems target wheeled platforms with\nforward-facing sensors. We present OneOcc, a vision-only panoramic SSC\nframework designed for gait-introduced body jitter and 360{\\deg} continuity.\nOneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular\npanorama and its equirectangular unfolding, preserving 360{\\deg} continuity and\ngrid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and\ncylindrical-polar spaces, reducing discretization bias and sharpening\nfree/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D\nfor dynamic multi-scale fusion and better long-range/occlusion reasoning; and\n(iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level\nmotion correction without extra sensors. We also release two panoramic\noccupancy benchmarks: QuadOcc (real quadruped, first-person 360{\\deg}) and\nHuman360Occ (H3O) (CARLA human-ego 360{\\deg} with RGB, Depth, semantic\noccupancy; standardized within-/cross-city splits). OneOcc sets new\nstate-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and\npopular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08\n(cross-city). Modules are lightweight, enabling deployable full-surround\nperception for legged/humanoid robots. Datasets and code will be publicly\navailable at https://github.com/MasterHow/OneOcc.",
      "url": "http://arxiv.org/abs/2511.03571v1",
      "published_time_eastern_timestamp": 1762357902.0
    },
    {
      "title": "Efficient Implementation of the Spin-Free Renormalized\n  Internally-Contracted Multireference Coupled Cluster Theory",
      "summary": "In this paper, an efficient implementation of the renormalized\ninternally-contracted multreference coupled cluster with singles and doubles\n(RIC-MRCCSD) into the ORCA quantum chemistry program suite is reported. To this\nend, Evangelista's Wick&d equation generator was combined with ORCA's native\nAGE code generator in order to implement the many-body residuals required for\nthe RIC-MRCCSD method. Substantial efficiency gains are realized by deriving a\nspin-free formulation instead of the previously reported spin-orbital version\ndeveloped by some of us. Since AGE produces parallelized code, the resulting\nimplementation can directly be run in parallel with substantial speedups when\nexecuted on multiple cores. In terms of runtime, the cost of RIC-MRCCSD is\nshown to be between single-reference RHF-CCSD and UHF-CCSD, even when active\nspace spaces as large as CAS(14,14) are considered. This achievement is largely\ndue to the fact that no reduced density matrices (RDM) or cumulants higher than\nthree-body enter the formalism. The scalability of the method to large systems\nis furthermore demonstrated by computing the ground-state of a vitamin B12\nmodel comprised of an active space of CAS(12, 12) and 809 orbitals. In terms of\naccuracy, RIC-MRCCSD is carefully compared to second- and approximate\nfourth-order $n$-electron valence state perturbation theories (NEVPT2,\nNEVPT4(SD)), to the multireference zeroth-order coupled-electron pair\napproximation (CEPA(0)), as well as to the IC-MRCCSD from Kohn. In contrast to\nRIC-MRCCSD, the IC-MRCCSD equations are entirely derived by AGE using the\nconventional projection-based approach, which, however, leads to much higher\nalgorithmic complexity than the former as well as the necessity to calculate up\nto the five-body RDMs. Remaining challenges such as the variation of the\nresults with the flow, a free parameter that enters the RIC-MRCCSD theory, are\ndiscussed.",
      "url": "http://arxiv.org/abs/2511.03567v1",
      "published_time_eastern_timestamp": 1762357776.0
    },
    {
      "title": "MultiZebraLogic: A Multilingual Logical Reasoning Benchmark",
      "summary": "Measuring the full abilities of large language models (LLMs) requires\nbenchmarks representing multiple tasks. We aim to create large, high-quality\ndatasets for comparison of logical reasoning skills across several languages\nand of suitable difficulty for LLMs of various reasoning ability. We explore\nmultiple ways of increasing difficulty. We generate zebra puzzles in multiple\nlanguages, themes, sizes and including 14 different clue types and 8 red\nherring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are\nsufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a\nreasoning model), respectively. Including 5 red herrings decreases o3-mini\npuzzle-level accuracy on 4x5 puzzles by 15$\\pm$7 %. Scores of o3-mini on 4x5\npuzzles are not significantly affected by use of English vs. Danish or the\ncommon houses theme vs. the country-specific smoerrebroed theme. We find no\ncorrelation between difficulty and the selected clue types. Datasets of\n128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic\nlanguages for sizes 2x3 and 4x5. We publish code for puzzle generation,\ndesigned for adaptablity into more languages and themes.",
      "url": "http://arxiv.org/abs/2511.03553v1",
      "published_time_eastern_timestamp": 1762356888.0
    },
    {
      "title": "The Converse Madelung Question",
      "summary": "We pose the converse Madelung question: not whether Fisher information can\nreproduce quantum mechanics, but whether it is necessary. We work with minimal,\nphysically motivated axioms on density and phase: locality, probability\nconservation, Euclidean invariance with a global phase symmetry, reversibility,\nand convex regularity. Within the resulting class of first order local\nHamiltonian field theories, these axioms single out the canonical Poisson\nbracket on density and phase under the Dubrovin and Novikov assumptions for\nlocal hydrodynamic brackets. Using a pointwise, gauge covariant complex change\nof variables that maps density and phase to a single complex field, we show\nthat the only convex, rotationally invariant, first derivative local functional\nof the density whose Euler Lagrange term yields a reversible completion that is\nexactly projectively linear is the Fisher functional. When its coefficient\nequals Planck constant squared divided by twice the mass, the dynamics reduce\nto the linear Schrodinger equation. For many body systems, a single local\ncomplex structure across sectors enforces the same relation species by species,\nfixing a single Planck constant. Galilean covariance appears through the\nBargmann central extension, with the usual superselection consequences.\nComparison with the Doebner and Goldin family identifies the reversible zero\ndiffusion corner with linear Schrodinger dynamics. We provide operational\nfalsifiers via residual diagnostics for the continuity and Hamilton Jacobi\nequations and report numerical minima at the Fisher scale that are invariant\nunder Galilean boosts. In this setting, quantum mechanics emerges as a\nreversible fixed point of Fisher regularised information hydrodynamics. A code\narchive enables direct numerical checks, including a superposition stress test\nthat preserves exact projective linearity within our axioms.",
      "url": "http://arxiv.org/abs/2511.03552v1",
      "published_time_eastern_timestamp": 1762356826.0
    },
    {
      "title": "Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code\n  Understanding",
      "summary": "Understanding the purpose of source code is a critical task in software\nmaintenance, onboarding, and modernization. While large language models (LLMs)\nhave shown promise in generating code explanations, they often lack grounding\nin the broader software engineering context. We propose a novel approach that\nleverages natural language artifacts from GitHub -- such as pull request\ndescriptions, issue descriptions and discussions, and commit messages -- to\nenhance LLM-based code understanding. Our system consists of three components:\none that extracts and structures relevant GitHub context, another that uses\nthis context to generate high-level explanations of the code's purpose, and a\nthird that validates the explanation. We implemented this as a standalone tool,\nas well as a server within the Model Context Protocol (MCP), enabling\nintegration with other AI-assisted development tools. Our main use case is that\nof enhancing a standard LLM-based code explanation with code insights that our\nsystem generates. To evaluate explanations' quality, we conducted a small scale\nuser study, with developers of several open projects, as well as developers of\nproprietary projects. Our user study indicates that when insights are generated\nthey often are helpful and non trivial, and are free from hallucinations.",
      "url": "http://arxiv.org/abs/2511.03549v1",
      "published_time_eastern_timestamp": 1762356702.0
    },
    {
      "title": "Bearing Syntactic Fruit with Stack-Augmented Neural Networks",
      "summary": "Any finite set of training data is consistent with an infinite number of\nhypothetical algorithms that could have generated it. Studies have shown that\nwhen human children learn language, they consistently favor hypotheses based on\nhierarchical syntactic rules without ever encountering disambiguating examples.\nA recent line of work has inquired as to whether common neural network\narchitectures share this bias, finding that they do so only under special\nconditions: when syntactically supervised, when pre-trained on massive corpora,\nor when trained long past convergence. In this paper, we demonstrate, for the\nfirst time, neural network architectures that are able to generalize in\nhuman-like fashion without any of the aforementioned requirements:\nstack-augmented neural networks. We test three base architectures (transformer,\nsimple RNN, LSTM) augmented with two styles of stack: the superposition stack\nof Joulin & Mikolov (2015) and a nondeterministic generalization of it proposed\nby DuSell & Chiang (2023). We find that transformers with nondeterministic\nstacks generalize best out of these architectures on a classical question\nformation task. We also propose a modification to the stack RNN architecture\nthat improves hierarchical generalization. These results suggest that\nstack-augmented neural networks may be more accurate models of human language\nacquisition than standard architectures, serving as useful objects of\npsycholinguistic study. Our code is publicly available.",
      "url": "http://arxiv.org/abs/2511.03547v1",
      "published_time_eastern_timestamp": 1762356658.0
    },
    {
      "title": "Radiative transfer modeling of the low-mass proto-binary system, IRAS\n  4A1 and 4A2",
      "summary": "NGC 1333 IRAS4A is a well-studied low-mass sun-like proto-binary system. It\nhas two components, A1 and A2, which are diverse according to their physical\nand chemical properties. We modeled this hot corino using the RATRAN radiative\ntransfer code and explained different spectral signatures observed towards A1\nand A2, specifically for CH3OH and H2CO. Our main goal is to understand the\nkinematical and chemical differences between A1 and A2 and to classify their\ndust emission and absorption properties. We considered a simple 1D spherical\ninfalling envelope consisting of collimated outflow in the source. Recent\nhigh-resolution interferometric observations of ALMA shed new light on why the\nsame molecular transitions towards A1 and A2 show different spectral profiles.\nThe significant difference between spectral profiles observed towards A1 and A2\nis mainly due to the dust opacity effect. Dust continuum emission toward A1 is\noptically thick, causing the transitions observed in absorption. Meanwhile, A2\nis optically thin, leading to the observed emission profiles, and an inverse\nP-Cygni profile suggests the presence of an infalling envelope. Using\nhigh-resolution observations from ALMA and VLA, we expanded our model from the\nmillimeter wavelength range to the centimeter wavelength range. This expansion\ndemonstrates the opacity effect, which is reduced in the centimeter wavelength\nrange, causing us to observe the lines in emission. Using our model, we\nreproduced the population inversion causing maser emission of methanol 44 GHz\nand 95 GHz transitions.",
      "url": "http://arxiv.org/abs/2511.03543v1",
      "published_time_eastern_timestamp": 1762355865.0
    },
    {
      "title": "SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across\n  Medical Specialties",
      "summary": "Medical question answering systems face deployment challenges including\nhallucinations, bias, computational demands, privacy concerns, and the need for\nspecialized expertise across diverse domains. Here, we present SOLVE-Med, a\nmulti-agent architecture combining domain-specialized small language models for\ncomplex medical queries. The system employs a Router Agent for dynamic\nspecialist selection, ten specialized models (1B parameters each) fine-tuned on\nspecific medical domains, and an Orchestrator Agent that synthesizes responses.\nEvaluated on Italian medical forum data across ten specialties, SOLVE-Med\nachieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697,\noutperforming standalone models up to 14B parameters while enabling local\ndeployment. Our code is publicly available on GitHub:\nhttps://github.com/PRAISELab-PicusLab/SOLVE-Med.",
      "url": "http://arxiv.org/abs/2511.03542v1",
      "published_time_eastern_timestamp": 1762355735.0
    },
    {
      "title": "Dark-Matter-Powered Population III Evolution: Lifetimes, Rotation, and\n  Quasi-Homogeneity in massive Stars",
      "summary": "Population III stars supplied the first light and metals in the Universe,\nsetting the pace of re-ionisation and early chemical enrichment. In dense\nhaloes their evolution can be strongly influenced by the energy released when\nWIMPs annihilate inside the stellar core. We follow the evolution of a\n\\(20\\,M_\\odot\\) Population III model with the \\textsc{genec} code, adding a\nfull treatment of spin dependent WIMP capture and annihilation. Tracks are\ncalculated for six halo densities from \\(10^{8}\\) to\n\\(3\\times10^{10}\\,\\mathrm{GeV\\,cm^{-3}}\\) and three initial rotation rates\nbetween zero and \\(0.4\\,v/v_{\\mathrm{crit}}\\). As soon as the capture product\nreaches\n\\(\\rho_\\chi\\sigma_{\\mathrm{SD}}\\simeq2\\times10^{-28}\\,\\mathrm{GeV\\,cm^{-1}}\\),\nthe dark-matter luminosity rivals hydrogen fusion, stretching the main-sequence\nlifetime from about ten million years to more than a gigayear. The extra time\nallows meridional circulation to smooth out differential rotation; a star that\nbegins at \\(0.4\\,v/v_{\\mathrm{crit}}\\) finishes core hydrogen burning with near\nsolid-body rotation and a helium core almost twice as massive as in the\ndark-matter-free case. Because the nuclear timescale is longer, chemically\nhomogeneous evolution now sets in at only \\(0.2\\,v/v_{\\mathrm{crit}}\\), rather\nthan the \\(\\gtrsim0.5\\,v/v_{\\mathrm{crit}}\\) required without WIMPs. For a star\nwith \\(0.4\\,v/v_{\\mathrm{crit}}\\), the surface hydrogen fraction drops to\n\\(X\\!\\sim\\!0.27\\), helium rises to \\(Y\\!\\sim\\!0.73\\), and primary\n\\(^{14}\\mathrm N\\) increases by four orders of magnitude at He exhaustion.\nModerate rotation combined with plausible dark-matter densities can therefore\ndrive primordial massive stars towards long-lived, quasi-homogeneous evolution\nwith distinctive chemical and spectral signatures.",
      "url": "http://arxiv.org/abs/2511.03540v1",
      "published_time_eastern_timestamp": 1762355602.0
    },
    {
      "title": "Security and Privacy Management of IoT Using Quantum Computing",
      "summary": "The convergence of the Internet of Things (IoT) and quantum computing is\nredefining the security paradigm of interconnected digital systems. Classical\ncryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and\nAdvanced Encryption Standard (AES) have long provided the foundation for\nsecuring IoT communication. However, the emergence of quantum algorithms such\nas Shor's and Grover's threatens to render these techniques vulnerable,\nnecessitating the development of quantum-resilient alternatives. This chapter\nexamines the implications of quantum computing for IoT security and explores\nstrategies for building cryptographically robust systems in the post-quantum\nera. It presents an overview of Post-Quantum Cryptographic (PQC) families,\nincluding lattice-based, code-based, hash-based, and multivariate approaches,\nanalyzing their potential for deployment in resource-constrained IoT\nenvironments. In addition, quantum-based methods such as Quantum Key\nDistribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed\nfor their ability to enhance confidentiality and privacy through physics-based\nsecurity guarantees. The chapter also highlights issues of privacy management,\nregulatory compliance, and standardization, emphasizing the need for\ncollaborative efforts across academia, industry, and governance. Overall, it\nprovides a comprehensive perspective on security IoT ecosystems against quantum\nthreats and ensures resilience in the next generation of intelligent networks.",
      "url": "http://arxiv.org/abs/2511.03538v1",
      "published_time_eastern_timestamp": 1762355335.0
    }
  ]
}
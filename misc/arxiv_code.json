{
  "last_updated": "2026-01-08T13:18:00.050552-05:00",
  "papers": [
    {
      "title": "Local simulations of common-envelope dynamical inspiral. Impact of rotation, accretion, and stratification",
      "summary": "Common envelope evolution (CEE) is a crucial phase in binary stellar evolution. Current global three-dimensional simulations lack the resolution to capture the small-scale dynamics around the embedded companion, while local wind-tunnel simulations always approximate the companion's orbital motion as linear rather than as rotation around the center of mass. We investigate how rotation, accretion, and stratification influence small-scale gas dynamics, gravitational drag and lift forces, and the spin-up rate of the companion. We perform three-dimensional local hydrodynamic simulations of a $0.2\\, M_\\odot$ compact companion plunging into the envelope of a $2\\, M_\\odot$ red giant in a reference frame rotating at the companion's orbital angular velocity, using the Athena++ code. The presence of stratification generates an inward-directed force, partially opposed by a rotation-induced outward lift force. Both the resulting inward directed force and the drag force, strongly influenced by stratification, would affect the evolution of the binary separation. We propose revised semi-analytical prescriptions for both drag and lift forces. Without accretion and for sufficiently small gravitational softening radii, a quasi-hydrostatic bubble forms around the companion, while accretion prevents its formation and converts kinetic energy into heat that could contribute to the envelope ejection. Drag and lift forces are only marginally affected by accretion. The companion spin-up rate varies non-monotonically in time, first increasing and then decreasing as it plunges deeper into the envelope. These results motivate future magnetohydrodynamic simulations to investigate how accretion, rotation, and stratification affect magnetic amplification, and how magnetic fields, in turn, influence mass and angular momentum accretion rates, as well as the drag and lift force exerted on the companion.",
      "url": "http://arxiv.org/abs/2601.04188v1",
      "published_time_eastern_timestamp": 1767812165.0
    },
    {
      "title": "ImLoc: Revisiting Visual Localization with Image-based Representation",
      "summary": "Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.",
      "url": "http://arxiv.org/abs/2601.04185v1",
      "published_time_eastern_timestamp": 1767811911.0
    },
    {
      "title": "Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation",
      "summary": "We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.",
      "url": "http://arxiv.org/abs/2601.04176v1",
      "published_time_eastern_timestamp": 1767811391.0
    },
    {
      "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
      "summary": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.",
      "url": "http://arxiv.org/abs/2601.04171v1",
      "published_time_eastern_timestamp": 1767811103.0
    },
    {
      "title": "Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images",
      "summary": "Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on",
      "url": "http://arxiv.org/abs/2601.04127v1",
      "published_time_eastern_timestamp": 1767807671.0
    },
    {
      "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
      "summary": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.",
      "url": "http://arxiv.org/abs/2601.04126v1",
      "published_time_eastern_timestamp": 1767807608.0
    },
    {
      "title": "Recovering of the Grassmann graph from the subgraph of non-degenerate subspaces",
      "summary": "Let ${\\mathbb F}$ be a (not necessarily finite) field. A subspace of the vector space ${\\mathbb F}^n$ is called {\\it non-degenerate} if it is not contained in a coordinate hyperplane. We show that the Grassmann graph of $k$-dimensional subspaces of ${\\mathbb F}^n$, $1<k<n-1$, can be recovered from the subgraph of non-degenerate subspaces if $|{\\mathbb F}|>n-k$. In the case when ${\\mathbb F}={\\mathbb F}_q$ is the field of $q$ elements, this subgraph is known as the graph of non-degenerate linear $[n,k]_q$ codes.",
      "url": "http://arxiv.org/abs/2601.04125v1",
      "published_time_eastern_timestamp": 1767807602.0
    },
    {
      "title": "Modeling the Effect of C/O Ratio on Complex Carbon Chemistry in Cold Molecular Clouds",
      "summary": "Elemental abundances, which are often depleted with respect to the solar values, are important input parameters for kinetic models of interstellar chemistry. In particular, the amount of carbon relative to oxygen is known to have a strong effect on modeled abundances of many species. While previous studies have focused on comparison of modeled and observed abundances to constrain the C/O ratio, the effects of this parameter on the underlying chemistry have not been well-studied. We investigated the role of the C/O ratio on dark cloud chemistry using the NAUTILUS code and machine learning techniques for molecular representation. We find that modeled abundances are quite sensitive to the C/O ratio, especially for carbon-rich species such as carbon chains and polycyclic aromatic hydrocarbons (PAHs). CO and simple ice-phase species are found to be major carbon reservoirs under both oxygen-poor and oxygen-rich conditions. The appearance of C3H4 isomers as significant carbon reservoirs, even under oxygen-rich conditions, indicates the efficiency of gas-phase C3 formation followed by adsorption and grain-surface hydrogenation. Our model is not able to reproduce the observed, gas-phase C/H ratio of TMC-1 CP at the time of best fit with any C/O ratio between 0.1 and 3, suggesting that the modeled freeze-out of carbon-bearing molecules may be too rapid. Future investigations are needed to understand the reactivity of major carbon reservoirs and their conversion to complex organic molecules.",
      "url": "http://arxiv.org/abs/2601.04103v1",
      "published_time_eastern_timestamp": 1767805718.0
    },
    {
      "title": "Random knotting in very long off-lattice self-avoiding polygons",
      "summary": "We present experimental results on knotting in off-lattice self-avoiding polygons in the bead-chain model. Using Clisby's tree data structure and the scale-free pivot algorithm, for each $k$ between $10$ and $27$ we generated $2^{43-k}$ polygons of size $n=2^k$. Using a new knot diagram simplification and invariant-free knot classification code, we were able to determine the precise knot type of each polygon. The results show that the number of prime summands of knot type $K$ in a random $n$-gon is very well described by a Poisson distribution. We estimate the characteristic length of knotting as $656500 \\pm 2500$. We use the count of summands for large $n$ to measure knotting rates and amplitude ratios of knot probabilities more accurately than previous experiments. Our calculations agree quite well with previous on-lattice computations, and support both knot localization and the knot entropy conjecture.",
      "url": "http://arxiv.org/abs/2601.04102v1",
      "published_time_eastern_timestamp": 1767805632.0
    },
    {
      "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
      "summary": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.",
      "url": "http://arxiv.org/abs/2601.04086v1",
      "published_time_eastern_timestamp": 1767804860.0
    },
    {
      "title": "CSSG: Measuring Code Similarity with Semantic Graphs",
      "summary": "Existing code similarity metrics, such as BLEU, CodeBLEU, and TSED, largely rely on surface-level string overlap or abstract syntax tree structures, and often fail to capture deeper semantic relationships between programs.We propose CSSG (Code Similarity using Semantic Graphs), a novel metric that leverages program dependence graphs to explicitly model control dependencies and variable interactions, providing a semantics-aware representation of code.Experiments on the CodeContests+ dataset show that CSSG consistently outperforms existing metrics in distinguishing more similar code from less similar code under both monolingual and cross-lingual settings, demonstrating that dependency-aware graph representations offer a more effective alternative to surface-level or syntax-based similarity measures.",
      "url": "http://arxiv.org/abs/2601.04085v1",
      "published_time_eastern_timestamp": 1767804842.0
    },
    {
      "title": "Cosmological constraints on viable $f(R)$ models using weak lensing",
      "summary": "The accelerated expansion of the Universe remains one of the central open problems in modern cosmology. While the $Λ$CDM model successfully describes a wide range of observations, the physical nature of dark energy is still unknown, motivating the study of alternative theories of gravity. Among these, $f(R)$ models provide a well-established extension of General Relativity, capable of reproducing a $Λ$CDM-like background evolution without introducing an explicit dark energy component. However, they can induce deviations in the growth of cosmic structures, making them testable through observables sensitive to cosmological perturbations. In this work, we use weak gravitational lensing to constrain several viable $f(R)$ gravity models. We analyze their impact on the matter power spectrum, as well as on the convergence and cosmic shear power spectra. Our analysis is carried out within a Bayesian framework using the \\textit{Cobaya} code and its modified gravity extension, \\textit{MGCobaya}, which enables consistent theoretical predictions and their comparison with current weak lensing and CMB lensing data. We find that standard cosmological parameters remain consistent with the $Λ$CDM scenario for all models considered, as expected from their background degeneracy. Nevertheless, we obtain non-trivial and model-dependent constraints on the characteristic parameters of several $f(R)$ theories.",
      "url": "http://arxiv.org/abs/2601.04048v1",
      "published_time_eastern_timestamp": 1767801886.0
    },
    {
      "title": "Serving Every Symbol: All-Symbol PIR and Batch Codes",
      "summary": "A $t$-all-symbol PIR code and a $t$-all-symbol batch code of dimension $k$ consist of $n$ servers storing linear combinations of $k$ linearly independent information symbols with the following recovery property: any symbol stored by a server can be recovered from $t$ pairwise disjoint subsets of servers. In the batch setting, we further require that any multiset of size $t$ of stored symbols can be recovered from $t$ disjoint subsets of servers. This framework unifies and extends several well-known code families, including one-step majority-logic decodable codes, (functional) PIR codes, and (functional) batch codes.\n  In this paper, we determine the minimum code length for some small values of $k$ and $t$, characterize structural properties of codes attaining this optimum, and derive bounds that show the trade-offs between length, dimension, minimum distance, and $t$. In addition, we study MDS codes and the simplex code, demonstrating how these classical families fit within our framework, and establish new cases of an open conjecture from \\cite{YAAKOBI2020} concerning the minimal $t$ for which the simplex code is a $t$-functional batch code.",
      "url": "http://arxiv.org/abs/2601.04041v1",
      "published_time_eastern_timestamp": 1767801423.0
    },
    {
      "title": "HI-bearing dark galaxies predictions from constrained Local Group simulations: how many and where to find them",
      "summary": "Dark galaxies are small, DM-dominated halos whose gas remains in hydrostatic and thermal equilibrium and has never formed stars. They are of particular interest because they represent a strong prediction of the LCDM model. As of today, only a handful of candidates have been observed, the most intriguing of which being Cloud-9. Using several state-of-the-art hydrodynamical simulations, we aim to predict the abundance of dark galaxies expected within our Local Group (LG), characterise their properties and provide guidance for their potential detection. We analyse LG simulations with constrained initial conditions, run with different codes, implementing different baryonic physics, feedback prescriptions, and employing two distinct values of SF density threshold, n_th=0.13 and 10 cm^-3, to select samples of dark and bright galaxies harboured in haloes of similar mass. We demonstrate that dark galaxies exist in such simulations, though their number is larger in simulations that use a higher, more realistic n_th. These galaxies, whose gas remains diffuse and never forms stars, predominantly inhabit less-concentrated, higher-spin DM halos than their luminous counterparts. Dark galaxies are typically found in low-density regions at the outskirts of the LG, and their evolution across z indicate that both the DM and gas densities in their surroundings were consistently lower than those found around bright galaxies, making them less susceptible to interactions, mergers, or gas inflows. We estimate that up to 8 dark galaxies should be detectable in HI emission within 2.5 Mpc of the LG, with the FAST telescope, accounting for its sky coverage and minimum M_HI and N_HI. Current hydrodynamical simulations of galaxies, combined with upcoming HI surveys, will offer a direct and powerful test of LCDM through their ability to predict and measure properties of dark galaxies within and beyond the LG.",
      "url": "http://arxiv.org/abs/2601.04024v1",
      "published_time_eastern_timestamp": 1767800317.0
    },
    {
      "title": "Padé Neurons for Efficient Neural Models",
      "summary": "Neural networks commonly employ the McCulloch-Pitts neuron model, which is a linear model followed by a point-wise non-linear activation. Various researchers have already advanced inherently non-linear neuron models, such as quadratic neurons, generalized operational neurons, generative neurons, and super neurons, which offer stronger non-linearity compared to point-wise activation functions. In this paper, we introduce a novel and better non-linear neuron model called Padé neurons (Paons), inspired by Padé approximants. Paons offer several advantages, such as diversity of non-linearity, since each Paon learns a different non-linear function of its inputs, and layer efficiency, since Paons provide stronger non-linearity in much fewer layers compared to piecewise linear approximation. Furthermore, Paons include all previously proposed neuron models as special cases, thus any neuron model in any network can be replaced by Paons. We note that there has been a proposal to employ the Padé approximation as a generalized point-wise activation function, which is fundamentally different from our model. To validate the efficacy of Paons, in our experiments, we replace classic neurons in some well-known neural image super-resolution, compression, and classification models based on the ResNet architecture with Paons. Our comprehensive experimental results and analyses demonstrate that neural models built by Paons provide better or equal performance than their classic counterparts with a smaller number of layers. The PyTorch implementation code for Paon is open-sourced at https://github.com/onur-keles/Paon.",
      "url": "http://arxiv.org/abs/2601.04005v1",
      "published_time_eastern_timestamp": 1767798930.0
    },
    {
      "title": "PosterVerse: A Full-Workflow Framework for Commercial-Grade Poster Generation with HTML-Based Scalable Typography",
      "summary": "Commercial-grade poster design demands the seamless integration of aesthetic appeal with precise, informative content delivery. Current automated poster generation systems face significant limitations, including incomplete design workflows, poor text rendering accuracy, and insufficient flexibility for commercial applications. To address these challenges, we propose PosterVerse, a full-workflow, commercial-grade poster generation method that seamlessly automates the entire design process while delivering high-density and scalable text rendering. PosterVerse replicates professional design through three key stages: (1) blueprint creation using fine-tuned LLMs to extract key design elements from user requirements, (2) graphical background generation via customized diffusion models to create visually appealing imagery, and (3) unified layout-text rendering with an MLLM-powered HTML engine to guarantee high text accuracy and flexible customization. In addition, we introduce PosterDNA, a commercial-grade, HTML-based dataset tailored for training and validating poster design models. To the best of our knowledge, PosterDNA is the first Chinese poster generation dataset to introduce HTML typography files, enabling scalable text rendering and fundamentally solving the challenges of rendering small and high-density text. Experimental results demonstrate that PosterVerse consistently produces commercial-grade posters with appealing visuals, accurate text alignment, and customizable layouts, making it a promising solution for automating commercial poster design. The code and model are available at https://github.com/wuhaer/PosterVerse.",
      "url": "http://arxiv.org/abs/2601.03993v1",
      "published_time_eastern_timestamp": 1767798264.0
    },
    {
      "title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures",
      "summary": "Background: Extracting the stages that structure Machine Learning (ML) pipelines from source code is key for gaining a deeper understanding of data science practices. However, the diversity caused by the constant evolution of the ML ecosystem (e.g., algorithms, libraries, datasets) makes this task challenging. Existing approaches either depend on non-scalable, manual labeling, or on ML classifiers that do not properly support the diversity of the domain. These limitations highlight the need for more flexible and reliable solutions.\n  Objective: We evaluate whether Small Language Models (SLMs) can leverage their code understanding and classification abilities to address these limitations, and subsequently how they can advance our understanding of data science practices.\n  Method: We conduct a confirmatory study based on two reference works selected for their relevance regarding current state-of-the-art's limitations. First, we compare several SLMs using Cochran's Q test. The best-performing model is then evaluated against the reference studies using two distinct McNemar's tests. We further analyze how variations in taxonomy definitions affect performance through an additional Cochran's Q test. Finally, a goodness-of-fit analysis is conducted using Pearson's chi-squared tests to compare our insights on data science practices with those from prior studies.",
      "url": "http://arxiv.org/abs/2601.03988v1",
      "published_time_eastern_timestamp": 1767798022.0
    },
    {
      "title": "Unique Decoding of Hyperderivative Reed-Solomon Codes",
      "summary": "Error-correcting codes are combinatorial objects designed to cope with the problem of reliable transmission of information on a noisy channel. A fundamental problem in coding theory and practice is to efficiently decode the received word with errors to obtain the transmitted codeword. In this paper, we consider the decoding problem of Hyperderivative Reed-Solomon (HRS) codes with respect to the NRT metric. Specifically, we propose a Welch-Berlekamp algorithm for the unique decoding of NRT HRS codes.",
      "url": "http://arxiv.org/abs/2601.03982v1",
      "published_time_eastern_timestamp": 1767797745.0
    },
    {
      "title": "The Gaia All-Sky Stellar Parameters Service (GASPS)",
      "summary": "Temperature and luminosity are the two key diagnostics of a star, yet these cannot come directly from survey data, but must be imputed by comparing those data to models. SED fitting offers a high-precision method to obtain both parameters for stars where both their distance and extinction are well known. The recent publication of many all-sky or large-area surveys coincides the publication of parallaxes and 3D extinction cubes from the Gaia satellite, making it possible to perform SED fitting of truly large ($>10^8$) numbers of Galactic stars for the first time. The analysis of this data requires a high level of automation. Here, we describe the ongoing Gaia All-Sky Stellar Parameters Service (GASPS): the fitting of 240 million SEDs from Gaia DR3 and the extraction of temperatures and luminosities for the corresponding stars using the PySSED code. We demonstrate the quality of the initial results, and the promise that these data show, from wavelength-specific information such as the ultraviolet and infrared excess of each star, to stellar classification, to expansion of the project beyond our own Galaxy, and mineralogical mapping of the Milky Way's interstellar medium.",
      "url": "http://arxiv.org/abs/2601.03978v1",
      "published_time_eastern_timestamp": 1767797240.0
    },
    {
      "title": "FUSION: Full-Body Unified Motion Prior for Body and Hands via Diffusion",
      "summary": "Hands are central to interacting with our surroundings and conveying gestures, making their inclusion essential for full-body motion synthesis. Despite this, existing human motion synthesis methods fall short: some ignore hand motions entirely, while others generate full-body motions only for narrowly scoped tasks under highly constrained settings. A key obstacle is the lack of large-scale datasets that jointly capture diverse full-body motion with detailed hand articulation. While some datasets capture both, they are limited in scale and diversity. Conversely, large-scale datasets typically focus either on body motion without hands or on hand motions without the body. To overcome this, we curate and unify existing hand motion datasets with large-scale body motion data to generate full-body sequences that capture both hand and body. We then propose the first diffusion-based unconditional full-body motion prior, FUSION, which jointly models body and hand motion. Despite using a pose-based motion representation, FUSION surpasses state-of-the-art skeletal control models on the Keypoint Tracking task in the HumanML3D dataset and achieves superior motion naturalness. Beyond standard benchmarks, we demonstrate that FUSION can go beyond typical uses of motion priors through two applications: (1) generating detailed full-body motion including fingers during interaction given the motion of an object, and (2) generating Self-Interaction motions using an LLM to transform natural language cues into actionable motion constraints. For these applications, we develop an optimization pipeline that refines the latent space of our diffusion model to generate task-specific motions. Experiments on these tasks highlight precise control over hand motion while maintaining plausible full-body coordination. The code will be public.",
      "url": "http://arxiv.org/abs/2601.03959v1",
      "published_time_eastern_timestamp": 1767795539.0
    }
  ]
}
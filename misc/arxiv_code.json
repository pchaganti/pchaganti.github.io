{
  "last_updated": "2025-11-02T22:45:28.786170-05:00",
  "papers": [
    {
      "title": "Continuous Autoregressive Language Models",
      "summary": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.",
      "url": "http://arxiv.org/abs/2510.27688v1",
      "published_time_eastern_timestamp": 1761933491.0
    },
    {
      "title": "Phased DMD: Few-step Distribution Matching Distillation via Score\n  Matching within Subintervals",
      "summary": "Distribution Matching Distillation (DMD) distills score-based generative\nmodels into efficient one-step generators, without requiring a one-to-one\ncorrespondence with the sampling trajectories of their teachers. However,\nlimited model capacity causes one-step distilled models underperform on complex\ngenerative tasks, e.g., synthesizing intricate object motions in text-to-video\ngeneration. Directly extending DMD to multi-step distillation increases memory\nusage and computational depth, leading to instability and reduced efficiency.\nWhile prior works propose stochastic gradient truncation as a potential\nsolution, we observe that it substantially reduces the generation diversity of\nmulti-step distilled models, bringing it down to the level of their one-step\ncounterparts. To address these limitations, we propose Phased DMD, a multi-step\ndistillation framework that bridges the idea of phase-wise distillation with\nMixture-of-Experts (MoE), reducing learning difficulty while enhancing model\ncapacity. Phased DMD is built upon two key ideas: progressive distribution\nmatching and score matching within subintervals. First, our model divides the\nSNR range into subintervals, progressively refining the model to higher SNR\nlevels, to better capture complex distributions. Next, to ensure the training\nobjective within each subinterval is accurate, we have conducted rigorous\nmathematical derivations. We validate Phased DMD by distilling state-of-the-art\nimage and video generation models, including Qwen-Image (20B parameters) and\nWan2.2 (28B parameters). Experimental results demonstrate that Phased DMD\npreserves output diversity better than DMD while retaining key generative\ncapabilities. We will release our code and models.",
      "url": "http://arxiv.org/abs/2510.27684v1",
      "published_time_eastern_timestamp": 1761933310.0
    },
    {
      "title": "Social learning moderates the tradeoffs between efficiency, stability,\n  and equity in group foraging",
      "summary": "Social learning shapes collective search by influencing how individuals use\npeer information. Empirical and computational studies show that optimal\ninformation sharing that is neither too localized nor too diffuse, can enhance\nresource detection and coordination. Building on these insights, we develop a\nrandomized search model that integrates social learning with area-restricted\nsearch (ARS) to investigate how communication distance affects collective\nforaging. The model includes three behavioral modes: exploration, exploitation,\nand targeted walk, which are governed by a single parameter, $\\rho$, that\nbalances exploration and exploitation at the group level. We quantify how\n$\\rho$ influences group efficiency ($\\eta$), temporal variability/burstiness\n($B$), and agent variability/equity in resource distribution ($\\sigma$),\nrevealing a clear trade-off among these outcomes. When $\\rho \\to 0$, agents\nexplore independently, maximizing collective exploration. As $\\rho$ increases,\nindividuals preferentially exploit patches discovered by others: $\\eta$ first\nrises and then declines, while $B$ shows the opposite trend. Group efficiency\nis optimized at interior $\\rho$ values that balance exploration and\nexploitation. At the largest $\\rho$, equality among agents is highest, but\nefficiency declines and burstiness is maximized too. Finally, by introducing\nnegative rewards, we examine how social learning mitigates risk.",
      "url": "http://arxiv.org/abs/2510.27683v1",
      "published_time_eastern_timestamp": 1761933180.0
    },
    {
      "title": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability\n  Detection",
      "summary": "Large language models (LLMs) have demonstrated impressive capabilities for\nmany coding tasks, including summarization, translation, completion, and code\ngeneration. However, detecting code vulnerabilities remains a challenging task\nfor LLMs. An effective way to improve LLM performance is in-context learning\n(ICL) - providing few-shot examples similar to the query, along with correct\nanswers, can improve an LLM's ability to generate correct solutions. However,\nchoosing the few-shot examples appropriately is crucial to improving model\nperformance. In this paper, we explore two criteria for choosing few-shot\nexamples for ICL used in the code vulnerability detection task. The first\ncriterion considers if the LLM (consistently) makes a mistake or not on a\nsample with the intuition that LLM performance on a sample is informative about\nits usefulness as a few-shot example. The other criterion considers similarity\nof the examples with the program under query and chooses few-shot examples\nbased on the $k$-nearest neighbors to the given sample. We perform evaluations\nto determine the benefits of these criteria individually as well as under\nvarious combinations, using open-source models on multiple datasets.",
      "url": "http://arxiv.org/abs/2510.27675v1",
      "published_time_eastern_timestamp": 1761932518.0
    },
    {
      "title": "Teaching competencies in physics for engineering education: A\n  qualitative analysis from teaching practice",
      "summary": "Physics teaching in engineering programmes poses discipline-specific demands\nthat intertwine conceptual modelling, experimental inquiry, and computational\nanalysis. This study examines nine teaching competences for physics instruction\nderived from international and regional frameworks and interpreted within\nengineering contexts. Nineteen university instructors from the Technological\nInstitute of Toluca completed an open-ended questionnaire; responses were\nanalysed using a grounded theory approach (open and axial coding) complemented\nby descriptive frequencies. Results indicate stronger development in technical\nmastery, methodological/digital integration, technology-mediated communication,\nand innovation (C1, C2, C6, C9), while information literacy for digital content\ncreation/adaptation and digital ethics/safety (C7, C8) remain underdeveloped. A\nrecurrent understanding-application gap was identified, revealing uneven\ntransfer from conceptual awareness to enacted classroom practice. We conclude\nthat advancing physics education for engineers requires institutionally\nsupported, discipline-specific professional development that aligns modelling,\nlaboratory work, and computation with ethical and reproducible digital\npractices; such alignment can move instructors from adoption/adaptation toward\nsustained appropriation and innovation in multimodal settings.",
      "url": "http://arxiv.org/abs/2510.27674v1",
      "published_time_eastern_timestamp": 1761932409.0
    },
    {
      "title": "Gaussian Combined Distance: A Generic Metric for Object Detection",
      "summary": "In object detection, a well-defined similarity metric can significantly\nenhance model performance. Currently, the IoU-based similarity metric is the\nmost commonly preferred choice for detectors. However, detectors using IoU as a\nsimilarity metric often perform poorly when detecting small objects because of\ntheir sensitivity to minor positional deviations. To address this issue, recent\nstudies have proposed the Wasserstein Distance as an alternative to IoU for\nmeasuring the similarity of Gaussian-distributed bounding boxes. However, we\nhave observed that the Wasserstein Distance lacks scale invariance, which\nnegatively impacts the model's generalization capability. Additionally, when\nused as a loss function, its independent optimization of the center attributes\nleads to slow model convergence and unsatisfactory detection precision. To\naddress these challenges, we introduce the Gaussian Combined Distance (GCD).\nThrough analytical examination of GCD and its gradient, we demonstrate that GCD\nnot only possesses scale invariance but also facilitates joint optimization,\nwhich enhances model localization performance. Extensive experiments on the\nAI-TOD-v2 dataset for tiny object detection show that GCD, as a bounding box\nregression loss function and label assignment metric, achieves state-of-the-art\nperformance across various detectors. We further validated the generalizability\nof GCD on the MS-COCO-2017 and Visdrone-2019 datasets, where it outperforms the\nWasserstein Distance across diverse scales of datasets. Code is available at\nhttps://github.com/MArKkwanGuan/mmdet-GCD.",
      "url": "http://arxiv.org/abs/2510.27649v1",
      "published_time_eastern_timestamp": 1761931471.0
    },
    {
      "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training",
      "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.",
      "url": "http://arxiv.org/abs/2510.27630v1",
      "published_time_eastern_timestamp": 1761930022.0
    },
    {
      "title": "DiffstarPop: A generative physical model of galaxy star formation\n  history",
      "summary": "We present DiffstarPop, a differentiable forward model of cosmological\npopulations of galaxy star formation histories (SFH). In the model, individual\ngalaxy SFH is parametrized by Diffstar, which has parameters $\\theta_{\\rm SFH}$\nthat have a direct interpretation in terms of galaxy formation physics, such as\nstar formation efficiency and quenching. DiffstarPop is a model for the\nstatistical connection between $\\theta_{\\rm SFH}$ and the mass assembly history\n(MAH) of dark matter halos. We have formulated DiffstarPop to have the minimal\nflexibility needed to accurately reproduce the statistical distributions of\ngalaxy SFH predicted by a diverse range of simulations, including the\nIllustrisTNG hydrodynamical simulation, the Galacticus semi-analytic model, and\nthe UniverseMachine semi-empirical model. Our publicly available code written\nin JAX includes Monte Carlo generators that supply statistical samples of\ngalaxy assembly histories that mimic the populations seen in each simulation,\nand can generate SFHs for $10^6$ galaxies in 1.1 CPU-seconds, or 0.03\nGPU-seconds. We conclude the paper with a discussion of applications of\nDiffstarPop, which we are using to generate catalogs of synthetic galaxies\npopulating the merger trees in cosmological N-body simulations.",
      "url": "http://arxiv.org/abs/2510.27604v1",
      "published_time_eastern_timestamp": 1761928139.0
    },
    {
      "title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research",
      "summary": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark.",
      "url": "http://arxiv.org/abs/2510.27598v1",
      "published_time_eastern_timestamp": 1761927743.0
    },
    {
      "title": "Learned Static Function Data Structures",
      "summary": "We consider the task of constructing a data structure for associating a\nstatic set of keys with values, while allowing arbitrary output values for\nqueries involving keys outside the set. Compared to hash tables, these\nso-called static function data structures do not need to store the key set and\nthus use significantly less memory. Several techniques are known, with\ncompressed static functions approaching the zero-order empirical entropy of the\nvalue sequence. In this paper, we introduce learned static functions, which use\nmachine learning to capture correlations between keys and values. For each key,\na model predicts a probability distribution over the values, from which we\nderive a key-specific prefix code to compactly encode the true value. The\nresulting codeword is stored in a classic static function data structure. This\ndesign allows learned static functions to break the zero-order entropy barrier\nwhile still supporting point queries. Our experiments show substantial space\nsavings: up to one order of magnitude on real data, and up to three orders of\nmagnitude on synthetic data.",
      "url": "http://arxiv.org/abs/2510.27588v1",
      "published_time_eastern_timestamp": 1761926993.0
    }
  ]
}
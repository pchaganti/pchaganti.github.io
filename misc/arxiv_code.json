{
  "last_updated": "2025-07-29T21:04:07.099809-04:00",
  "papers": [
    {
      "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super\n  Intelligence",
      "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.",
      "url": "http://arxiv.org/abs/2507.21046v1",
      "published_time_eastern_timestamp": 1753725545.0
    },
    {
      "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via\n  Code-Driven Gene Expression Analysis",
      "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",
      "url": "http://arxiv.org/abs/2507.21035v1",
      "published_time_eastern_timestamp": 1753725308.0
    },
    {
      "title": "Quantum Simulation of Molecular Dynamics Processes -- A Benchmark Study\n  Using Classical Simulator and Present-Day Quantum Hardware",
      "summary": "We explore how the fundamental problems in quantum molecular dynamics can be\nmodelled using classical simulators (emulators) of quantum computers and the\nactual quantum hardware available to us today. The list of problems we tackle\nincludes propagation of a free wave packet, vibration of a harmonic oscillator,\nand tunneling through a barrier. Each of these problems starts with the initial\nwave packet setup. Although Qiskit provides a general method for initializing\nwavefunctions, in most cases it generates deep quantum circuits. While these\ncircuits perform well on noiseless simulators, they suffer from excessive noise\non quantum hardware. To overcome this issue, we designed a shallower quantum\ncircuit for preparing a Gaussian-like initial wave packet, which improves the\nperformance on real hardware. Next, quantum circuits are implemented to apply\nthe kinetic and potential energy operators for the evolution of a wavefunction\nover time. The results of our modelling on classical emulators of quantum\nhardware agree perfectly with the results obtained using the traditional\n(classical) methods. This serves as a benchmark and demonstrates that the\nquantum algorithms and Qiskit codes we developed are accurate. However, the\nresults obtained on the actual quantum hardware available today, such as IBM's\nsuperconducting qubits and IonQ's trapped ions, indicate large discrepancies\ndue to hardware limitations. This work highlights both the potential and\nchallenges of using quantum computers to solve fundamental quantum molecular\ndynamics problems.",
      "url": "http://arxiv.org/abs/2507.21030v1",
      "published_time_eastern_timestamp": 1753725024.0
    },
    {
      "title": "Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment:\n  A Benchmark",
      "summary": "Automated assessment of human motion plays a vital role in rehabilitation,\nenabling objective evaluation of patient performance and progress. Unlike\ngeneral human activity recognition, rehabilitation motion assessment focuses on\nanalyzing the quality of movement within the same action class, requiring the\ndetection of subtle deviations from ideal motion. Recent advances in deep\nlearning and video-based skeleton extraction have opened new possibilities for\naccessible, scalable motion assessment using affordable devices such as\nsmartphones or webcams. However, the field lacks standardized benchmarks,\nconsistent evaluation protocols, and reproducible methodologies, limiting\nprogress and comparability across studies. In this work, we address these gaps\nby (i) aggregating existing rehabilitation datasets into a unified archive\ncalled Rehab-Pile, (ii) proposing a general benchmarking framework for\nevaluating deep learning methods in this domain, and (iii) conducting extensive\nbenchmarking of multiple architectures across classification and regression\ntasks. All datasets and implementations are released to the community to\nsupport transparency and reproducibility. This paper aims to establish a solid\nfoundation for future research in automated rehabilitation assessment and\nfoster the development of reliable, accessible, and personalized rehabilitation\nsolutions. The datasets, source-code and results of this article are all\npublicly available.",
      "url": "http://arxiv.org/abs/2507.21018v1",
      "published_time_eastern_timestamp": 1753724343.0
    },
    {
      "title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them",
      "summary": "Hallucinations pose critical risks for large language model (LLM)-based\nagents, often manifesting as hallucinative actions resulting from fabricated or\nmisinterpreted information within the cognitive context. While recent studies\nhave exposed such failures, existing evaluations remain fragmented and lack a\nprincipled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions\nin Risky AGEnt settings--the first unified benchmark for eliciting and\nevaluating hallucinations in interactive LLM-agent scenarios. We begin by\nintroducing a three-part taxonomy to address agentic hallucinations: actions\nthat are unfaithful to (i) task instructions, (ii) execution history, or (iii)\nenvironment observations. To analyze, we first elicit such failures by\nperforming a systematic audit of existing agent benchmarks, then synthesize\ntest cases using a snapshot strategy that isolates decision points in\ndeterministic and reproducible manners. To evaluate hallucination behaviors, we\nadopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware\nprompts, enabling scalable, high-fidelity assessment of agent actions without\nenumerating full action spaces. MIRAGE-Bench provides actionable insights on\nfailure modes of LLM agents and lays the groundwork for principled progress in\nmitigating hallucinations in interactive environments.",
      "url": "http://arxiv.org/abs/2507.21017v1",
      "published_time_eastern_timestamp": 1753724309.0
    },
    {
      "title": "Learning Transferable Facial Emotion Representations from Large-Scale\n  Semantically Rich Captions",
      "summary": "Current facial emotion recognition systems are predominately trained to\npredict a fixed set of predefined categories or abstract dimensional values.\nThis constrained form of supervision hinders generalization and applicability,\nas it reduces the rich and nuanced spectrum of emotions into oversimplified\nlabels or scales. In contrast, natural language provides a more flexible,\nexpressive, and interpretable way to represent emotions, offering a much\nbroader source of supervision. Yet, leveraging semantically rich natural\nlanguage captions as supervisory signals for facial emotion representation\nlearning remains relatively underexplored, primarily due to two key challenges:\n1) the lack of large-scale caption datasets with rich emotional semantics, and\n2) the absence of effective frameworks tailored to harness such rich\nsupervision. To this end, we introduce EmoCap100K, a large-scale facial emotion\ncaption dataset comprising over 100,000 samples, featuring rich and structured\nsemantic descriptions that capture both global affective states and\nfine-grained local facial behaviors. Building upon this dataset, we further\npropose EmoCapCLIP, which incorporates a joint global-local contrastive\nlearning framework enhanced by a cross-modal guided positive mining module.\nThis design facilitates the comprehensive exploitation of multi-level caption\ninformation while accommodating semantic similarities between closely related\nexpressions. Extensive evaluations on over 20 benchmarks covering five tasks\ndemonstrate the superior performance of our method, highlighting the promise of\nlearning facial emotion representations from large-scale semantically rich\ncaptions. The code and data will be available at\nhttps://github.com/sunlicai/EmoCapCLIP.",
      "url": "http://arxiv.org/abs/2507.21015v1",
      "published_time_eastern_timestamp": 1753723688.0
    },
    {
      "title": "User-Centered Design with AI in the Loop: A Case Study of Rapid User\n  Interface Prototyping with \"Vibe Coding\"",
      "summary": "We present a case study of using generative user interfaces, or ``vibe\ncoding,'' a method leveraging large language models (LLMs) for generating code\nvia natural language prompts, to support rapid prototyping in user-centered\ndesign (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop\nideate-prototyping process. We share insights from an empirical experience\nintegrating this process to develop an interactive data analytics interface for\nhighway traffic engineers to effectively retrieve and analyze historical\ntraffic data. With generative UIs, the team was able to elicit rich user\nfeedback and test multiple alternative design ideas from user evaluation\ninterviews and real-time collaborative sessions with domain experts. We discuss\nthe advantages and pitfalls of vibe coding for bridging the gaps between design\nexpertise and domain-specific expertise.",
      "url": "http://arxiv.org/abs/2507.21012v1",
      "published_time_eastern_timestamp": 1753723414.0
    },
    {
      "title": "Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety\n  to Vision in LVLM",
      "summary": "Large visual-language models (LVLMs) integrate aligned large language models\n(LLMs) with visual modules to process multimodal inputs. However, the safety\nmechanisms developed for text-based LLMs do not naturally extend to visual\nmodalities, leaving LVLMs vulnerable to harmful image inputs. To address this\ncross-modal safety gap, we introduce security tensors - trainable input vectors\napplied during inference through either the textual or visual modality. These\ntensors transfer textual safety alignment to visual processing without\nmodifying the model's parameters. They are optimized using a curated dataset\ncontaining (i) malicious image-text pairs requiring rejection, (ii) contrastive\nbenign pairs with text structurally similar to malicious queries, with the\npurpose of being contrastive examples to guide visual reliance, and (iii)\ngeneral benign samples preserving model functionality. Experimental results\ndemonstrate that both textual and visual security tensors significantly enhance\nLVLMs' ability to reject diverse harmful visual inputs while maintaining\nnear-identical performance on benign tasks. Further internal analysis towards\nhidden-layer representations reveals that security tensors successfully\nactivate the language module's textual \"safety layers\" in visual inputs,\nthereby effectively extending text-based safety to the visual modality.",
      "url": "http://arxiv.org/abs/2507.20994v1",
      "published_time_eastern_timestamp": 1753721993.0
    },
    {
      "title": "PyBird-JAX: Accelerated inference in large-scale structure with\n  model-independent emulation of one-loop galaxy power spectra",
      "summary": "We present $\\texttt{PyBird-JAX}$, a differentiable, $\\texttt{JAX}$-based\nimplementation of $\\texttt{PyBird}$, using internal neural network emulators to\naccelerate computationally costly operations for rapid large-scale structure\n(LSS) analysis. $\\texttt{PyBird-JAX}$ computes one-loop EFTofLSS predictions\nfor redshift-space galaxy power spectrum multipoles in 1.2 ms on a CPU and 0.2\nms on a GPU, achieving 3-4 orders of magnitude speed-up over $\\texttt{PyBird}$.\nThe emulators take a compact spline-based representation of the input linear\npower spectrum $P(k)$ as feature vectors, making the approach applicable to a\nwide range of cosmological models. We rigorously validate its accuracy against\nlarge-volume simulations and on BOSS data, including cosmologies not explicitly\nrepresented in the training set. Leveraging automatic differentiation,\n$\\texttt{PyBird-JAX}$ supports Fisher forecasting, Taylor expansion of model\npredictions, gradient-based searches, and vectorised ensemble sampling.\nInterfaced with a variety of samplers and Boltzmann solvers,\n$\\texttt{PyBird-JAX}$ provides a high-performance, end-to-end inference\npipeline. Combined with a symbolic-$P(k)$ generator, a typical Stage-4 LSS MCMC\nconverges in minutes on a GPU. Our results demonstrate that\n$\\texttt{PyBird-JAX}$ delivers the precision and speed required for upcoming\nLSS surveys, opening the door to accelerated cosmological inference with\nminimal accuracy loss and no pretraining. In a companion paper [1], we put\n$\\texttt{PyBird-JAX}$ to use in achieving LSS marginalised constraints free\nfrom volume projection effects through non-flat measures.",
      "url": "http://arxiv.org/abs/2507.20990v1",
      "published_time_eastern_timestamp": 1753721433.0
    },
    {
      "title": "Repairing vulnerabilities without invisible hands. A differentiated\n  replication study on LLMs",
      "summary": "Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of\nprogram repair. Recent studies show that large language models (LLMs)\noutperform traditional techniques, extending their success beyond code\ngeneration and fault detection.\n  Hypothesis: These gains may be driven by hidden factors -- \"invisible hands\"\nsuch as training-data leakage or perfect fault localization -- that let an LLM\nreproduce human-authored fixes for the same code.\n  Objective: We replicate prior AVR studies under controlled conditions by\ndeliberately adding errors to the reported vulnerability location in the\nprompt. If LLMs merely regurgitate memorized fixes, both small and large\nlocalization errors should yield the same number of correct patches, because\nany offset should divert the model from the original fix.\n  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans\nbenchmarks after shifting the fault location by n lines from the ground truth.\nA first LLM generates a patch, a second LLM reviews it, and we validate the\nresult with regression and proof-of-vulnerability tests. Finally, we manually\naudit a sample of patches and estimate the error rate with the\nAgresti-Coull-Wilson method.",
      "url": "http://arxiv.org/abs/2507.20977v1",
      "published_time_eastern_timestamp": 1753720756.0
    },
    {
      "title": "PySHRED: A Python package for SHallow REcurrent Decoding for sparse\n  sensing, model reduction and scientific discovery",
      "summary": "SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for\nmodeling high-dimensional dynamical systems and/or spatiotemporal data from\ndynamical system snapshot observations. PySHRED is a Python package that\nimplements SHRED and several of its major extensions, including for robust\nsensing, reduced order modeling and physics discovery. In this paper, we\nintroduce the version 1.0 release of PySHRED, which includes data preprocessors\nand a number of cutting-edge SHRED methods specifically designed to handle\nreal-world data that may be noisy, multi-scale, parameterized, prohibitively\nhigh-dimensional, and strongly nonlinear. The package is easy to install,\nthoroughly-documented, supplemented with extensive code examples, and\nmodularly-structured to support future additions. The entire codebase is\nreleased under the MIT license and is available at\nhttps://github.com/pyshred-dev/pyshred.",
      "url": "http://arxiv.org/abs/2507.20954v1",
      "published_time_eastern_timestamp": 1753718654.0
    },
    {
      "title": "A multivariate spatial model for ordinal survey-based data",
      "summary": "Health surveys provide valuable information for monitoring population health,\nidentifying risk factors and informing public health policies. Most of the\nquestions included are coded as ordinal variables and organized into thematic\nblocks. Accordingly, multivariate modeling provides a natural framework for\nconsidering these variables as true groups, thereby accounting for potential\ndependencies among the responses within each block. In this paper, we propose a\nmultivariate spatial analysis of ordinal survey-based data. This multivariate\napproach enables the joint analysis of sets of ordinal responses that are\nlikely to be correlated, accounting for individual-level effects, while\nsimultaneously improving the estimation of the geographical patterns for each\nvariable and capturing their interdependencies. We apply this methodology to\ndescribe the spatial distribution of several mental health indicators from the\nHealth Survey of the Region of Valencia (Spain) for the year 2022.\nSpecifically, we analyze the block of questions from the 12-item General Health\nQuestionnaire included in the survey.",
      "url": "http://arxiv.org/abs/2507.20944v1",
      "published_time_eastern_timestamp": 1753718190.0
    },
    {
      "title": "FRED: Financial Retrieval-Enhanced Detection and Editing of\n  Hallucinations in Language Models",
      "summary": "Hallucinations in large language models pose a critical challenge for\napplications requiring factual reliability, particularly in high-stakes domains\nsuch as finance. This work presents an effective approach for detecting and\nediting factually incorrect content in model-generated responses based on the\nprovided context. Given a user-defined domain-specific error taxonomy, we\nconstruct a synthetic dataset by inserting tagged errors into financial\nquestion-answering corpora and then fine-tune four language models, Phi-4,\nPhi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual\ninaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%\nimprovement in binary F1 score and a 30% gain in overall detection performance\ncompared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having\nonly 4 billion parameters, maintains competitive performance with just a 2%\ndrop in binary detection and a 0.1% decline in overall detection compared to\nOpenAI-o3. Our work provides a practical solution for detecting and editing\nfactual inconsistencies in financial text generation while introducing a\ngeneralizable framework that can enhance the trustworthiness and alignment of\nlarge language models across diverse applications beyond finance. Our code and\ndata are available at https://github.com/pegasi-ai/fine-grained-editting.",
      "url": "http://arxiv.org/abs/2507.20930v1",
      "published_time_eastern_timestamp": 1753717313.0
    },
    {
      "title": "Pareto-Grid-Guided Large Language Models for Fast and High-Quality\n  Heuristics Design in Multi-Objective Combinatorial Optimization",
      "summary": "Multi-objective combinatorial optimization problems (MOCOP) frequently arise\nin practical applications that require the simultaneous optimization of\nconflicting objectives. Although traditional evolutionary algorithms can be\neffective, they typically depend on domain knowledge and repeated parameter\ntuning, limiting flexibility when applied to unseen MOCOP instances. Recently,\nintegration of Large Language Models (LLMs) into evolutionary computation has\nopened new avenues for automatic heuristic generation, using their advanced\nlanguage understanding and code synthesis capabilities. Nevertheless, most\nexisting approaches predominantly focus on single-objective tasks, often\nneglecting key considerations such as runtime efficiency and heuristic\ndiversity in multi-objective settings. To bridge this gap, we introduce\nMulti-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a\nnovel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO)\nframework that leverages LLMs and Pareto Front Grid (PFG) technique. By\npartitioning the objective space into grids and retaining top-performing\ncandidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize\nheuristics with semantically distinct logical structures during variation, thus\npromoting diversity and mitigating redundancy within the population. Through\nextensive evaluations, MPaGE demonstrates superior performance over existing\nLLM-based frameworks, and achieves competitive results to traditional\nMulti-objective evolutionary algorithms (MOEAs), with significantly faster\nruntime. Our code is available at: https://github.com/langkhachhoha/MPaGE.",
      "url": "http://arxiv.org/abs/2507.20923v1",
      "published_time_eastern_timestamp": 1753716403.0
    },
    {
      "title": "RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image\n  Segmentation",
      "summary": "Referring Image Segmentation (RIS), which aims to segment specific objects\nbased on natural language descriptions, plays an essential role in\nvision-language understanding. Despite its progress in remote sensing\napplications, RIS in Low-Altitude Drone (LAD) scenarios remains underexplored.\nExisting datasets and methods are typically designed for high-altitude and\nstatic-view imagery. They struggle to handle the unique characteristics of LAD\nviews, such as diverse viewpoints and high object density. To fill this gap, we\npresent RIS-LAD, the first fine-grained RIS benchmark tailored for LAD\nscenarios. This dataset comprises 13,871 carefully annotated image-text-mask\ntriplets collected from realistic drone footage, with a focus on small,\ncluttered, and multi-viewpoint scenes. It highlights new challenges absent in\nprevious benchmarks, such as category drift caused by tiny objects and object\ndrift under crowded same-class objects. To tackle these issues, we propose the\nSemantic-Aware Adaptive Reasoning Network (SAARN). Rather than uniformly\ninjecting all linguistic features, SAARN decomposes and routes semantic\ninformation to different stages of the network. Specifically, the\nCategory-Dominated Linguistic Enhancement (CDLE) aligns visual features with\nobject categories during early encoding, while the Adaptive Reasoning Fusion\nModule (ARFM) dynamically selects semantic cues across scales to improve\nreasoning in complex scenes. The experimental evaluation reveals that RIS-LAD\npresents substantial challenges to state-of-the-art RIS algorithms, and also\ndemonstrates the effectiveness of our proposed model in addressing these\nchallenges. The dataset and code will be publicly released soon at:\nhttps://github.com/AHideoKuzeA/RIS-LAD/.",
      "url": "http://arxiv.org/abs/2507.20920v1",
      "published_time_eastern_timestamp": 1753716063.0
    },
    {
      "title": "$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\n  Attention-Guided Refinement",
      "summary": "Img2LaTeX is a practically significant task that involves converting\nmathematical expressions or tabular data from images into LaTeX code. In recent\nyears, vision-language models (VLMs) have demonstrated strong performance\nacross a variety of visual understanding tasks, owing to their generalization\ncapabilities. While some studies have explored the use of VLMs for the\nImg2LaTeX task, their performance often falls short of expectations.\nEmpirically, VLMs sometimes struggle with fine-grained visual elements, leading\nto inaccurate LaTeX predictions. To address this challenge, we propose\n$A^2R^2$: Advancing Img2LaTeX Conversion via Visual Reasoning with\nAttention-Guided Refinement, a framework that effectively integrates attention\nlocalization and iterative refinement within a visual reasoning framework,\nenabling VLMs to perform self-correction and progressively improve prediction\nquality. For effective evaluation, we introduce a new dataset,\nImg2LaTex-Hard-1K, consisting of 1,100 carefully curated and challenging\nexamples designed to rigorously evaluate the capabilities of VLMs within this\ntask domain. Extensive experimental results demonstrate that: (1) $A^2R^2$\nsignificantly improves model performance across six evaluation metrics spanning\nboth textual and visual levels, consistently outperforming other baseline\nmethods; (2) Increasing the number of inference rounds yields notable\nperformance gains, underscoring the potential of $A^2R^2$ in test-time scaling\nscenarios; (3) Ablation studies and human evaluations validate the practical\neffectiveness of our approach, as well as the strong synergy among its core\ncomponents during inference.",
      "url": "http://arxiv.org/abs/2507.20890v1",
      "published_time_eastern_timestamp": 1753713717.0
    },
    {
      "title": "Enhancing Project-Specific Code Completion by Inferring Internal API\n  Information",
      "summary": "Project-specific code completion is a critical task that leverages context\nfrom a project to generate accurate code. State-of-the-art methods use\nretrieval-augmented generation (RAG) with large language models (LLMs) and\nproject information for code completion. However, they often struggle to\nincorporate internal API information, which is crucial for accuracy, especially\nwhen APIs are not explicitly imported in the file.\n  To address this, we propose a method to infer internal API information\nwithout relying on imports. Our method extends the representation of APIs by\nconstructing usage examples and semantic descriptions, building a knowledge\nbase for LLMs to generate relevant completions. We also introduce ProjBench, a\nbenchmark that avoids leaked imports and consists of large-scale real-world\nprojects.\n  Experiments on ProjBench and CrossCodeEval show that our approach\nsignificantly outperforms existing methods, improving code exact match by\n22.72% and identifier exact match by 18.31%. Additionally, integrating our\nmethod with existing baselines boosts code match by 47.80% and identifier match\nby 35.55%.",
      "url": "http://arxiv.org/abs/2507.20888v1",
      "published_time_eastern_timestamp": 1753713586.0
    },
    {
      "title": "Ensemble Foreground Management for Unsupervised Object Discovery",
      "summary": "Unsupervised object discovery (UOD) aims to detect and segment objects in 2D\nimages without handcrafted annotations. Recent progress in self-supervised\nrepresentation learning has led to some success in UOD algorithms. However, the\nabsence of ground truth provides existing UOD methods with two challenges: 1)\ndetermining if a discovered region is foreground or background, and 2) knowing\nhow many objects remain undiscovered. To address these two problems, previous\nsolutions rely on foreground priors to distinguish if the discovered region is\nforeground, and conduct one or fixed iterations of discovery. However, the\nexisting foreground priors are heuristic and not always robust, and a fixed\nnumber of discoveries leads to under or over-segmentation, since the number of\nobjects in images varies. This paper introduces UnionCut, a robust and\nwell-grounded foreground prior based on min-cut and ensemble methods that\ndetects the union of foreground areas of an image, allowing UOD algorithms to\nidentify foreground objects and stop discovery once the majority of the\nforeground union in the image is segmented. In addition, we propose UnionSeg, a\ndistilled transformer of UnionCut that outputs the foreground union more\nefficiently and accurately. Our experiments show that by combining with\nUnionCut or UnionSeg, previous state-of-the-art UOD methods witness an increase\nin the performance of single object discovery, saliency detection and\nself-supervised instance segmentation on various benchmarks. The code is\navailable at https://github.com/YFaris/UnionCut.",
      "url": "http://arxiv.org/abs/2507.20860v1",
      "published_time_eastern_timestamp": 1753712046.0
    },
    {
      "title": "$S^3$LAM: Surfel Splatting SLAM for Geometrically Accurate Tracking and\n  Mapping",
      "summary": "We propose $S^3$LAM, a novel RGB-D SLAM system that leverages 2D surfel\nsplatting to achieve highly accurate geometric representations for simultaneous\ntracking and mapping. Unlike existing 3DGS-based SLAM approaches that rely on\n3D Gaussian ellipsoids, we utilize 2D Gaussian surfels as primitives for more\nefficient scene representation. By focusing on the surfaces of objects in the\nscene, this design enables $S^3$LAM to reconstruct high-quality geometry,\nbenefiting both mapping and tracking. To address inherent SLAM challenges\nincluding real-time optimization under limited viewpoints, we introduce a novel\nadaptive surface rendering strategy that improves mapping accuracy while\nmaintaining computational efficiency. We further derive camera pose Jacobians\ndirectly from 2D surfel splatting formulation, highlighting the importance of\nour geometrically accurate representation that improves tracking convergence.\nExtensive experiments on both synthetic and real-world datasets validate that\n$S^3$LAM achieves state-of-the-art performance. Code will be made publicly\navailable.",
      "url": "http://arxiv.org/abs/2507.20854v1",
      "published_time_eastern_timestamp": 1753711748.0
    },
    {
      "title": "Latent Inter-User Difference Modeling for LLM Personalization",
      "summary": "Large language models (LLMs) are increasingly integrated into users' daily\nlives, leading to a growing demand for personalized outputs. Previous work\nfocuses on leveraging a user's own history, overlooking inter-user differences\nthat are crucial for effective personalization. While recent work has attempted\nto model such differences, the reliance on language-based prompts often hampers\nthe effective extraction of meaningful distinctions. To address these issues,\nwe propose Difference-aware Embedding-based Personalization (DEP), a framework\nthat models inter-user differences in the latent space instead of relying on\nlanguage prompts. DEP constructs soft prompts by contrasting a user's embedding\nwith those of peers who engaged with similar content, highlighting relative\nbehavioral signals. A sparse autoencoder then filters and compresses both\nuser-specific and difference-aware embeddings, preserving only task-relevant\nfeatures before injecting them into a frozen LLM. Experiments on personalized\nreview generation show that DEP consistently outperforms baseline methods\nacross multiple metrics. Our code is available at\nhttps://github.com/SnowCharmQ/DEP.",
      "url": "http://arxiv.org/abs/2507.20849v1",
      "published_time_eastern_timestamp": 1753711257.0
    }
  ]
}
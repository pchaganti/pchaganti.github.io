{
  "last_updated": "2025-05-19T20:58:59.706626-04:00",
  "papers": [
    {
      "title": "QVGen: Pushing the Limit of Quantized Video Generative Models",
      "summary": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet,\ntheir substantial computational and memory demands pose serious challenges to\nreal-world deployment, even on high-end GPUs. As a commonly adopted solution,\nquantization has proven notable success in reducing cost for image DMs, while\nits direct application to video DMs remains ineffective. In this paper, we\npresent QVGen, a novel quantization-aware training (QAT) framework tailored for\nhigh-performance and inference-efficient video DMs under extremely low-bit\nquantization (e.g., 4-bit or below). We begin with a theoretical analysis\ndemonstrating that reducing the gradient norm is essential to facilitate\nconvergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to\nmitigate large quantization errors, leading to significantly enhanced\nconvergence. To eliminate the inference overhead of $\\Phi$, we propose a\nrank-decay strategy that progressively eliminates $\\Phi$. Specifically, we\nrepeatedly employ singular value decomposition (SVD) and a proposed rank-based\nregularization $\\mathbf{\\gamma}$ to identify and decay low-contributing\ncomponents. This strategy retains performance while zeroing out inference\noverhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs,\nwith parameter sizes ranging from $1.3$B $\\sim14$B, show that QVGen is the\nfirst to reach full-precision comparable quality under 4-bit settings.\nMoreover, it significantly outperforms existing methods. For instance, our\n3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and\n$+8.43$ in Scene Consistency on VBench.",
      "url": "http://arxiv.org/abs/2505.11497v1",
      "published_time_eastern_timestamp": 1747418380.0
    },
    {
      "title": "Area and volume from entangled qubits",
      "summary": "In this paper a relation between entangled states and geometry is studied. In\nparticular, the area of 2D parallelogram is obtained from an entangled 4-qubit\nstate. In addition, the vector area of a 3D parallelogram is derived from\nentangled 6-qubit states. Moreover, the volume of a 3D parallelepiped is\ndeduced from an entangled 9-qubit state. Furthermore, it has been provided the\nquantum circuit in qiskit code for these entangled states. It is worth\nmentioning that parallelograms and parallelepipeds serve as fundamental\nbuilding blocks for more sophisticated geometric structures.",
      "url": "http://arxiv.org/abs/2505.11487v1",
      "published_time_eastern_timestamp": 1747417828.0
    },
    {
      "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
      "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT.",
      "url": "http://arxiv.org/abs/2505.11484v1",
      "published_time_eastern_timestamp": 1747417670.0
    },
    {
      "title": "Improving Assembly Code Performance with Large Language Models via\n  Reinforcement Learning",
      "summary": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.",
      "url": "http://arxiv.org/abs/2505.11480v1",
      "published_time_eastern_timestamp": 1747417245.0
    },
    {
      "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across\n  Diverse Tasks and Languages",
      "summary": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference",
      "url": "http://arxiv.org/abs/2505.11475v1",
      "published_time_eastern_timestamp": 1747416679.0
    },
    {
      "title": "HumaniBench: A Human-Centric Framework for Large Multimodal Models\n  Evaluation",
      "summary": "Large multimodal models (LMMs) now excel on many vision language benchmarks,\nhowever, they still struggle with human centered criteria such as fairness,\nethics, empathy, and inclusivity, key to aligning with human values. We\nintroduce HumaniBench, a holistic benchmark of 32K real-world image question\npairs, annotated via a scalable GPT4o assisted pipeline and exhaustively\nverified by domain experts. HumaniBench evaluates seven Human Centered AI\n(HCAI) principles: fairness, ethics, understanding, reasoning, language\ninclusivity, empathy, and robustness, across seven diverse tasks, including\nopen and closed ended visual question answering (VQA), multilingual QA, visual\ngrounding, empathetic captioning, and robustness tests. Benchmarking 15 state\nof the art LMMs (open and closed source) reveals that proprietary models\ngenerally lead, though robustness and visual grounding remain weak points. Some\nopen-source models also struggle to balance accuracy with adherence to\nhuman-aligned principles. HumaniBench is the first benchmark purpose built\naround HCAI principles. It provides a rigorous testbed for diagnosing alignment\ngaps and guiding LMMs toward behavior that is both accurate and socially\nresponsible. Dataset, annotation prompts, and evaluation code are available at:\nhttps://vectorinstitute.github.io/HumaniBench",
      "url": "http://arxiv.org/abs/2505.11454v1",
      "published_time_eastern_timestamp": 1747415384.0
    },
    {
      "title": "Polarization Signatures of Quasi-Periodic Oscillations in Simulated\n  Tilted, Truncated Disks",
      "summary": "We utilize the Monte-Carlo radiation transport code, \\pan, to create images,\nspectra, polarization maps, and light curves from a set of general relativistic\nmagnetohydrodynamic simulations of tilted, truncated, black hole accretion\ndisks. Truncation can have spectral and polarization signatures all its own;\ntilt introduces both inclination and azimuthal dependencies into the spectra\nand polarization; and precession and oscillations of the tilted accretion flow\ninside the truncation radius (what we posit to be the \"corona\") introduce time\ndependencies or periodicity to all of this. We use the ray-traced results from\nour simulations to evaluate the feasibility of measuring these effects,\nparticularly in the context of current and future X-ray polarization\nobservatories. Such detections could greatly improve our understanding of the\ngeometry of accretion disks and coronae in the hard state, the physics of\nquasi-periodic oscillations (QPOs), and how system properties evolve as sources\napproach the hard-to-soft state transition.",
      "url": "http://arxiv.org/abs/2505.11446v1",
      "published_time_eastern_timestamp": 1747414977.0
    },
    {
      "title": "A Generative Framework for Causal Estimation via Importance-Weighted\n  Diffusion Distillation",
      "summary": "Estimating individualized treatment effects from observational data is a\ncentral challenge in causal inference, largely due to covariate imbalance and\nconfounding bias from non-randomized treatment assignment. While inverse\nprobability weighting (IPW) is a well-established solution to this problem, its\nintegration into modern deep learning frameworks remains limited. In this work,\nwe propose Importance-Weighted Diffusion Distillation (IWDD), a novel\ngenerative framework that combines the pretraining of diffusion models with\nimportance-weighted score distillation to enable accurate and fast causal\nestimation-including potential outcome prediction and treatment effect\nestimation. We demonstrate how IPW can be naturally incorporated into the\ndistillation of pretrained diffusion models, and further introduce a\nrandomization-based adjustment that eliminates the need to compute IPW\nexplicitly-thereby simplifying computation and, more importantly, provably\nreducing the variance of gradient estimates. Empirical results show that IWDD\nachieves state-of-the-art out-of-sample prediction performance, with the\nhighest win rates compared to other baselines, significantly improving causal\nestimation and supporting the development of individualized treatment\nstrategies. We will release our PyTorch code for reproducibility and future\nresearch.",
      "url": "http://arxiv.org/abs/2505.11444v1",
      "published_time_eastern_timestamp": 1747414852.0
    },
    {
      "title": "Is Compression Really Linear with Code Intelligence?",
      "summary": "Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain.",
      "url": "http://arxiv.org/abs/2505.11441v1",
      "published_time_eastern_timestamp": 1747414754.0
    },
    {
      "title": "GODBench: A Benchmark for Multimodal Large Language Models in Video\n  Comment Art",
      "summary": "Video Comment Art enhances user engagement by providing creative content that\nconveys humor, satire, or emotional resonance, requiring a nuanced and\ncomprehensive grasp of cultural and contextual subtleties. Although Multimodal\nLarge Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated\nstrong reasoning abilities in STEM tasks (e.g. mathematics and coding), they\nstill struggle to generate creative expressions such as resonant jokes and\ninsightful satire. Moreover, existing benchmarks are constrained by their\nlimited modalities and insufficient categories, hindering the exploration of\ncomprehensive creativity in video-based Comment Art creation. To address these\nlimitations, we introduce GODBench, a novel benchmark that integrates video and\ntext modalities to systematically evaluate MLLMs' abilities to compose Comment\nArt. Furthermore, inspired by the propagation patterns of waves in physics, we\npropose Ripple of Thought (RoT), a multi-step reasoning framework designed to\nenhance the creativity of MLLMs. Extensive experiments reveal that existing\nMLLMs and CoT methods still face significant challenges in understanding and\ngenerating creative video comments. In contrast, RoT provides an effective\napproach to improve creative composing, highlighting its potential to drive\nmeaningful advancements in MLLM-based creativity. GODBench is publicly\navailable at https://github.com/stan-lei/GODBench-ACL2025.",
      "url": "http://arxiv.org/abs/2505.11436v1",
      "published_time_eastern_timestamp": 1747414600.0
    },
    {
      "title": "Energy efficiency analysis of Spiking Neural Networks for space\n  applications",
      "summary": "While the exponential growth of the space sector and new operative concepts\nask for higher spacecraft autonomy, the development of AI-assisted space\nsystems was so far hindered by the low availability of power and energy typical\nof space applications. In this context, Spiking Neural Networks (SNN) are\nhighly attractive due to their theoretically superior energy efficiency due to\ntheir inherently sparse activity induced by neurons communicating by means of\nbinary spikes. Nevertheless, the ability of SNN to reach such efficiency on\nreal world tasks is still to be demonstrated in practice. To evaluate the\nfeasibility of utilizing SNN onboard spacecraft, this work presents a numerical\nanalysis and comparison of different SNN techniques applied to scene\nclassification for the EuroSAT dataset. Such tasks are of primary importance\nfor space applications and constitute a valuable test case given the abundance\nof competitive methods available to establish a benchmark. Particular emphasis\nis placed on models based on temporal coding, where crucial information is\nencoded in the timing of neuron spikes. These models promise even greater\nefficiency of resulting networks, as they maximize the sparsity properties\ninherent in SNN. A reliable metric capable of comparing different architectures\nin a hardware-agnostic way is developed to establish a clear theoretical\ndependence between architecture parameters and the energy consumption that can\nbe expected onboard the spacecraft. The potential of this novel method and his\nflexibility to describe specific hardware platforms is demonstrated by its\napplication to predicting the energy consumption of a BrainChip Akida AKD1000\nneuromorphic processor.",
      "url": "http://arxiv.org/abs/2505.11418v1",
      "published_time_eastern_timestamp": 1747412990.0
    },
    {
      "title": "The Future is Sparse: Embedding Compression for Scalable Retrieval in\n  Recommender Systems",
      "summary": "Industry-scale recommender systems face a core challenge: representing\nentities with high cardinality, such as users or items, using dense embeddings\nthat must be accessible during both training and inference. However, as\nembedding sizes grow, memory constraints make storage and access increasingly\ndifficult. We describe a lightweight, learnable embedding compression technique\nthat projects dense embeddings into a high-dimensional, sparsely activated\nspace. Designed for retrieval tasks, our method reduces memory requirements\nwhile preserving retrieval performance, enabling scalable deployment under\nstrict resource constraints. Our results demonstrate that leveraging sparsity\nis a promising approach for improving the efficiency of large-scale\nrecommenders. We release our code at https://github.com/recombee/CompresSAE.",
      "url": "http://arxiv.org/abs/2505.11388v1",
      "published_time_eastern_timestamp": 1747410712.0
    },
    {
      "title": "TensorMixedStates: a Julia library for simulating pure and mixed quantum\n  states using matrix product states",
      "summary": "We introduce TensorMixedStates, a Julia library built on top of ITensor which\nallows the simulation of quantum systems in presence of dissipation using\nmatrix product states (MPS). It offers three key features: i) it implements the\nMPS representation for mixed states along with associated operations, in\nparticular the time evolution according to a Lindblad equation or discrete time\nevolution using non-unitary gates (quantum channels), ii) it is based on\nITensor, which has proven its effectiveness and which gives access to efficient\nlow-level tensor manipulation as well state-of-the-art algorithms (like DMRG,\nTDVP, quantum numbers conservation and automated parallelization), finally iii)\nit presents a user-friendly interface allowing writing sophisticated\nsimulations for pure and mixed quantum states in a few lines of code.",
      "url": "http://arxiv.org/abs/2505.11377v1",
      "published_time_eastern_timestamp": 1747410065.0
    },
    {
      "title": "APE: An analytical protostellar environment to provide physical\n  conditions to chemical models and synthetic observations",
      "summary": "Chemical modeling and synthetic observations are powerful methods to\ninterpret observations, both requiring a knowledge of the physical conditions.\nIn this paper, we present the Analytical Protostellar Environment (APE) code,\nwhich aims at making chemical simulations and synthetic observations\naccessible. APE contains a physical model of protostellar evolution (including\nthe central object, the envelope, the protoplanetary disk and the outflow) as\nwell as interfaces to publicly available codes to perform chemical simulations,\nradiative transfer calculations, and synthetic interferometry imaging. APE\nproduces density and temperature maps of protostellar systems. The code can\nalso follow individual particles throughout their journey in a collapsing core.\nAPE includes a treatment of the dust grain size-distribution to compute\nopacities self-consistently for subsequent radiative transfer. We show an\nexample of application of APE by computing chemical abundance maps of CO, CN,\nCS, H2CO, and CH3OH in a Class I protostellar system. We also performed\nsynthetic ALMA observations of their molecular emission assuming an edge-on\nsource inclination. The moment 0 maps of CO, CS, and H2CO display an X-shaped\nemission similar to what is observed toward the Class I source IRAS 04302+2247.",
      "url": "http://arxiv.org/abs/2505.11364v1",
      "published_time_eastern_timestamp": 1747409428.0
    },
    {
      "title": "Channel coding against quantum jammers via minimax",
      "summary": "We introduce a minimax approach for characterizing the capacities of fully\nquantum arbitrarily varying channels (FQAVCs) under different shared resource\nmodels. In contrast to previous methods, our technique avoids de Finetti-type\nreductions, allowing us to treat quantum jammers with infinite-dimensional\nsystems. Consequently, we show that the entanglement-assisted and\nshared-randomness-assisted capacities of FQAVCs match those of the\ncorresponding compound channels, even in the presence of general quantum\nadversaries.",
      "url": "http://arxiv.org/abs/2505.11362v1",
      "published_time_eastern_timestamp": 1747409336.0
    },
    {
      "title": "Fractal Graph Contrastive Learning",
      "summary": "While Graph Contrastive Learning (GCL) has attracted considerable attention\nin the field of graph self-supervised learning, its performance heavily relies\non data augmentations that are expected to generate semantically consistent\npositive pairs. Existing strategies typically resort to random perturbations or\nlocal structure preservation, yet lack explicit control over global structural\nconsistency between augmented views. To address this limitation, we propose\nFractal Graph Contrastive Learning (FractalGCL), a theory-driven framework that\nleverages fractal self-similarity to enforce global topological coherence.\nFractalGCL introduces two key innovations: a renormalisation-based augmentation\nthat generates structurally aligned positive views via box coverings; and a\nfractal-dimension-aware contrastive loss that aligns graph embeddings according\nto their fractal dimensions. While combining the two innovations markedly\nboosts graph-representation quality, it also adds non-trivial computational\noverhead. To mitigate the computational overhead of fractal dimension\nestimation, we derive a one-shot estimator by proving that the dimension\ndiscrepancy between original and renormalised graphs converges weakly to a\ncentred Gaussian distribution. This theoretical insight enables a reduction in\ndimension computation cost by an order of magnitude, cutting overall training\ntime by approximately 61%. The experiments show that FractalGCL not only\ndelivers state-of-the-art results on standard benchmarks but also outperforms\ntraditional baselines on traffic networks by an average margin of about\nremarkably 7%. Codes are available at\n(https://anonymous.4open.science/r/FractalGCL-0511).",
      "url": "http://arxiv.org/abs/2505.11356v1",
      "published_time_eastern_timestamp": 1747408750.0
    },
    {
      "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task\n  for Large Language Models",
      "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose assumptions and\nchallenge the reasoning in arguments. Despite growing interest in this area,\nprogress has been hindered by the lack of suitable datasets and automatic\nevaluation standards. This work presents a comprehensive approach to support\nthe development and benchmarking of systems for this task. We construct the\nfirst large-scale manually-annotated dataset. We also investigate automatic\nevaluation methods and identify a reference-based technique using large\nlanguage models (LLMs) as the strategy that best correlates with human\njudgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline\nwhile showcasing the difficulty of the task. Data, code, and a public\nleaderboard are provided to encourage further research not only in terms of\nmodel performance, but also to explore the practical benefits of CQs-Gen for\nboth automated reasoning and human critical thinking.",
      "url": "http://arxiv.org/abs/2505.11341v1",
      "published_time_eastern_timestamp": 1747408084.0
    },
    {
      "title": "DecompileBench: A Comprehensive Benchmark for Evaluating Decompilers in\n  Real-World Scenarios",
      "summary": "Decompilers are fundamental tools for critical security tasks, from\nvulnerability discovery to malware analysis, yet their evaluation remains\nfragmented. Existing approaches primarily focus on syntactic correctness\nthrough synthetic micro-benchmarks or subjective human ratings, failing to\naddress real-world requirements for semantic fidelity and analyst usability. We\npresent DecompileBench, the first comprehensive framework that enables\neffective evaluation of decompilers in reverse engineering workflows through\nthree key components: \\textit{real-world function extraction} (comprising\n23,400 functions from 130 real-world programs), \\textit{runtime-aware\nvalidation}, and \\textit{automated human-centric assessment} using LLM-as-Judge\nto quantify the effectiveness of decompilers in reverse engineering workflows.\nThrough a systematic comparison between six industrial-strength decompilers and\nsix recent LLM-powered approaches, we demonstrate that LLM-based methods\nsurpass commercial tools in code understandability despite 52.2% lower\nfunctionality correctness. These findings highlight the potential of LLM-based\napproaches to transform human-centric reverse engineering. We open source\n\\href{https://github.com/Jennieett/DecompileBench}{DecompileBench} to provide a\nframework to advance research on decompilers and assist security experts in\nmaking informed tool selections based on their specific requirements.",
      "url": "http://arxiv.org/abs/2505.11340v1",
      "published_time_eastern_timestamp": 1747408063.0
    },
    {
      "title": "MARRS: Masked Autoregressive Unit-based Reaction Synthesis",
      "summary": "This work aims at a challenging task: human action-reaction synthesis, i.e.,\ngenerating human reactions based on the action sequence of the other as\nconditions. Currently, autoregressive modeling approaches have achieved\nremarkable performance in motion generation tasks, e.g. text-to-motion.\nHowever, vector quantization (VQ) accompanying autoregressive generation has\ninherent disadvantages, including loss of quantization information, low\ncodebook utilization, etc. Moreover, unlike text-to-motion, which focuses\nsolely on the movement of body joints, human action-reaction synthesis also\nencompasses fine-grained hand movements. In this work, we propose MARRS, a\nnovel framework designed to generate coordinated and fine-grained reaction\nmotions in continuous representations. Initially, we present the\nUnit-distinguished Motion Variational AutoEncoder (UD-VAE), which segments the\nentire body into distinct body and hand units, encoding them independently.\nSubsequently, we propose Action-Conditioned Fusion (ACF), which involves\nrandomly masking a subset of reactive tokens and extracting specific\ninformation about the body and hands from the active tokens. Furthermore, we\nintroduce Adaptive Unit Modulation (AUM) to facilitate interaction between body\nand hand units by using the information from one unit to adaptively modulate\nthe other. Finally, for the diffusion model, we employ a compact MLP as a noise\npredictor for each distinct body unit and incorporate the diffusion loss to\nmodel the probability distribution of each token. Quantitative and qualitative\nresults demonstrate that our method achieves superior performance. The code\nwill be released upon acceptance.",
      "url": "http://arxiv.org/abs/2505.11334v1",
      "published_time_eastern_timestamp": 1747407646.0
    },
    {
      "title": "Temporally-Grounded Language Generation: A Benchmark for Real-Time\n  Vision-Language Models",
      "summary": "Vision-language models (VLMs) have shown remarkable progress in offline tasks\nsuch as image captioning and video question answering. However, real-time\ninteractive environments impose new demands on VLMs, requiring them to generate\nutterances that are not only semantically accurate but also precisely timed. We\nidentify two core capabilities necessary for such settings --\n$\\textit{perceptual updating}$ and $\\textit{contingency awareness}$ -- and\npropose a new benchmark task, $\\textbf{Temporally-Grounded Language Generation\n(TGLG)}$, to evaluate them. TGLG requires models to generate utterances in\nresponse to streaming video such that both content and timing align with\ndynamic visual input. To support this benchmark, we curate evaluation datasets\nfrom sports broadcasting and egocentric human interaction domains, and\nintroduce a new metric, $\\textbf{TRACE}$, to evaluate TGLG by jointly measuring\nsemantic similarity and temporal alignment. Finally, we present\n$\\textbf{Vision-Language Model with Time-Synchronized Interleaving (VLM-TSI)}$,\na model that interleaves visual and linguistic tokens in a time-synchronized\nmanner, enabling real-time language generation without relying on turn-based\nassumptions. Experimental results show that VLM-TSI significantly outperforms a\nstrong baseline, yet overall performance remains modest -- highlighting the\ndifficulty of TGLG and motivating further research in real-time VLMs. Code and\ndata available $\\href{https://github.com/yukw777/tglg}{here}$.",
      "url": "http://arxiv.org/abs/2505.11326v1",
      "published_time_eastern_timestamp": 1747406910.0
    }
  ]
}
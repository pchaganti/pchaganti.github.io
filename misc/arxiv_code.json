{
  "last_updated": "2025-10-07T02:17:50.170581-04:00",
  "papers": [
    {
      "title": "Rapid event extraction and tensorial event adaption: Libraries for\n  efficient access and generic reweighting of parton-level events and their\n  implementation in the MadtRex module",
      "summary": "We present Rex and teaRex, C++17 libraries for efficient management of\nparton-level hard scattering event information and completely generic\nreweighting of such events, respectively. Rex is primarily an interfacing and\nI/O library for Les Houches Event format files and provides an internal event\nformat designed with data parallelism in mind, and teaRex extends this format\nto provide full parton-level reweighting functionality with minimal code\nneeding to be written by the end user. These libraries serve as the foundation\nfor the MadtRex reweighting module for MadGraph5_aMC@NLO, extending the\nfunctionality of the CUDACPP plugin to allow for data-parallel model-generic\nleading order parameter reweighting on SIMD-enabled CPUs and SIMT GPUs,\nspeeding up reweighting by more than two orders of magnitude compared to\nMadGraph5_aMC@NLO running on the exact same hardware while providing trivial\nscalability to larger and distributed systems.",
      "url": "http://arxiv.org/abs/2510.05100v1",
      "published_time_eastern_timestamp": 1759773559.0
    },
    {
      "title": "Pulp Motion: Framing-aware multimodal camera and human motion generation",
      "summary": "Treating human motion and camera trajectory generation separately overlooks a\ncore principle of cinematography: the tight interplay between actor performance\nand camera work in the screen space. In this paper, we are the first to cast\nthis task as a text-conditioned joint generation, aiming to maintain consistent\non-screen framing while producing two heterogeneous, yet intrinsically linked,\nmodalities: human motion and camera trajectories. We propose a simple,\nmodel-agnostic framework that enforces multimodal coherence via an auxiliary\nmodality: the on-screen framing induced by projecting human joints onto the\ncamera. This on-screen framing provides a natural and effective bridge between\nmodalities, promoting consistency and leading to more precise joint\ndistribution. We first design a joint autoencoder that learns a shared latent\nspace, together with a lightweight linear transform from the human and camera\nlatents to a framing latent. We then introduce auxiliary sampling, which\nexploits this linear transform to steer generation toward a coherent framing\nmodality. To support this task, we also introduce the PulpMotion dataset, a\nhuman-motion and camera-trajectory dataset with rich captions, and high-quality\nhuman motions. Extensive experiments across DiT- and MAR-based architectures\nshow the generality and effectiveness of our method in generating on-frame\ncoherent human-camera motions, while also achieving gains on textual alignment\nfor both modalities. Our qualitative results yield more cinematographically\nmeaningful framings setting the new state of the art for this task. Code,\nmodels and data are available in our\n\\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project\npage}.",
      "url": "http://arxiv.org/abs/2510.05097v1",
      "published_time_eastern_timestamp": 1759773514.0
    },
    {
      "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
      "summary": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
      "url": "http://arxiv.org/abs/2510.05096v1",
      "published_time_eastern_timestamp": 1759773482.0
    },
    {
      "title": "VChain: Chain-of-Visual-Thought for Reasoning in Video Generation",
      "summary": "Recent video generation models can produce smooth and visually appealing\nclips, but they often struggle to synthesize complex dynamics with a coherent\nchain of consequences. Accurately modeling visual outcomes and state\ntransitions over time remains a core challenge. In contrast, large language and\nmultimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and\nfuture prediction capabilities. To bridge these strengths, we introduce VChain,\na novel inference-time chain-of-visual-thought framework that injects visual\nreasoning signals from multimodal models into video generation. Specifically,\nVChain contains a dedicated pipeline that leverages large multimodal models to\ngenerate a sparse set of critical keyframes as snapshots, which are then used\nto guide the sparse inference-time tuning of a pre-trained video generator only\nat these key moments. Our approach is tuning-efficient, introduces minimal\noverhead and avoids dense supervision. Extensive experiments on complex,\nmulti-step scenarios show that VChain significantly enhances the quality of\ngenerated videos.",
      "url": "http://arxiv.org/abs/2510.05094v1",
      "published_time_eastern_timestamp": 1759773479.0
    },
    {
      "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for\n  Diffusion Large Language Models",
      "summary": "Diffusion large language models (dLLMs) have recently emerged as a promising\nalternative to autoregressive (AR) models, offering advantages such as\naccelerated parallel decoding and bidirectional context modeling. However, the\nvanilla decoding strategy in discrete dLLMs suffers from a critical limitation:\nonce a token is accepted, it can no longer be revised in subsequent steps. As a\nresult, early mistakes persist across iterations, harming both intermediate\npredictions and final output quality. To address this issue, we propose\nTolerator (Token-Level Cross-Validation Refinement), a training-free decoding\nstrategy that leverages cross-validation among predicted tokens. Unlike\nexisting methods that follow a single progressive unmasking procedure,\nTolerator introduces a two-stage process: (i) sequence fill-up and (ii)\niterative refinement by remasking and decoding a subset of tokens while\ntreating the remaining as context. This design enables previously accepted\ntokens to be reconsidered and corrected when necessary, leading to more\nreliable diffusion decoding outputs. We evaluate Tolerator on five standard\nbenchmarks covering language understanding, code generation, and mathematics.\nExperiments show that our method achieves consistent improvements over the\nbaselines under the same computational budget. These findings suggest that\ndecoding algorithms are crucial to realizing the full potential of diffusion\nlarge language models. Code and data are publicly available.",
      "url": "http://arxiv.org/abs/2510.05090v1",
      "published_time_eastern_timestamp": 1759773406.0
    },
    {
      "title": "NeoPDF: A fast interpolation library for collinear and transverse\n  momentum-dependent parton distributions",
      "summary": "We present NeoPDF, an interpolation library that supports both collinear and\ntransverse momentum-dependent parton distribution functions. NeoPDF is designed\nto be fast and reliable, with modern functionalities that target both current\nand future hadron collider experiments. It aims to address the shortcomings of\nexisting interpolation libraries while providing additional features to support\ngeneric non-perturbative functions. Some of the features include a new\ninterpolation based on Chebyshev polynomials, as well as the ability to\ninterpolate along the nucleon number $A$, the reference strong coupling\n$\\alpha_s(M_Z)$, and the parton's intrinsic transverse momentum $k_T$. NeoPDF\nimplements its own file format using binary serialisation and lossless\ncompression, prioritising speed and efficiency. A no-code migration design is\nprovided for LHAPDF in order to remove the frictions associated with\ntransitioning to NeoPDF. The library is written in Rust with interfaces for\nvarious programming languages such as Fortran, C, C++, Python, and Mathematica.\nWe benchmark NeoPDF against LHAPDF and TMDlib for various sets and show that it\nis both fast and accurate.",
      "url": "http://arxiv.org/abs/2510.05079v1",
      "published_time_eastern_timestamp": 1759773046.0
    },
    {
      "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior\n  Reasoning LLMs",
      "summary": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.",
      "url": "http://arxiv.org/abs/2510.05069v1",
      "published_time_eastern_timestamp": 1759772794.0
    },
    {
      "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation",
      "summary": "Large language models (LLMs) are typically deployed under diverse memory and\ncompute constraints. Existing approaches build model families by training each\nsize independently, which is prohibitively expensive and provides only\ncoarse-grained size options. In this work, we identify a novel phenomenon that\nwe call boomerang distillation: starting from a large base model (the teacher),\none first distills down to a small student and then progressively reconstructs\nintermediate-sized models by re-incorporating blocks of teacher layers into the\nstudent without any additional training. This process produces zero-shot\ninterpolated models of many intermediate sizes whose performance scales\nsmoothly between the student and teacher, often matching or surpassing\npretrained or distilled models of the same size. We further analyze when this\ntype of interpolation succeeds, showing that alignment between teacher and\nstudent through pruning and distillation is essential. Boomerang distillation\nthus provides a simple and efficient way to generate fine-grained model\nfamilies, dramatically reducing training cost while enabling flexible\nadaptation across deployment environments. The code and models are available at\nhttps://github.com/dcml-lab/boomerang-distillation.",
      "url": "http://arxiv.org/abs/2510.05064v1",
      "published_time_eastern_timestamp": 1759772480.0
    },
    {
      "title": "Modeling Student Learning with 3.8 Million Program Traces",
      "summary": "As programmers write code, they often edit and retry multiple times, creating\nrich \"interaction traces\" that reveal how they approach coding tasks and\nprovide clues about their level of skill development. For novice programmers in\nparticular, these traces reflect the diverse reasoning processes they employ to\ncode, such as exploratory behavior to understand how a programming concept\nworks, re-strategizing in response to bugs, and personalizing stylistic\nchoices. In this work, we explore what can be learned from training language\nmodels on such reasoning traces: not just about code, but about coders, and\nparticularly students learning to program. We introduce a dataset of over 3.8\nmillion programming reasoning traces from users of Pencil Code, a free online\neducational platform used by students to learn simple programming concepts.\nCompared to models trained only on final programs or synthetically-generated\ntraces, we find that models trained on real traces are stronger at modeling\ndiverse student behavior. Through both behavioral and probing analyses, we also\nfind that many properties of code traces, such as goal backtracking or number\nof comments, can be predicted from learned representations of the students who\nwrite them. Building on this result, we show that we can help students recover\nfrom mistakes by steering code generation models to identify a sequence of\nedits that will results in more correct code while remaining close to the\noriginal student's style. Together, our results suggest that many properties of\ncode are properties of individual students and that training on edit traces can\nlead to models that are more steerable, more predictive of student behavior\nwhile programming, and better at generating programs in their final states.\nCode and data is available at https://github.com/meghabyte/pencilcode-public",
      "url": "http://arxiv.org/abs/2510.05056v1",
      "published_time_eastern_timestamp": 1759772237.0
    },
    {
      "title": "KEEP: Integrating Medical Ontologies with Clinical Data for Robust Code\n  Embeddings",
      "summary": "Machine learning in healthcare requires effective representation of\nstructured medical codes, but current methods face a trade off: knowledge graph\nbased approaches capture formal relationships but miss real world patterns,\nwhile data driven methods learn empirical associations but often overlook\nstructured knowledge in medical terminologies. We present KEEP (Knowledge\npreserving and Empirically refined Embedding Process), an efficient framework\nthat bridges this gap by combining knowledge graph embeddings with adaptive\nlearning from clinical data. KEEP first generates embeddings from knowledge\ngraphs, then employs regularized training on patient records to adaptively\nintegrate empirical patterns while preserving ontological relationships.\nImportantly, KEEP produces final embeddings without task specific auxiliary or\nend to end training enabling KEEP to support multiple downstream applications\nand model architectures. Evaluations on structured EHR from UK Biobank and\nMIMIC IV demonstrate that KEEP outperforms both traditional and Language Model\nbased approaches in capturing semantic relationships and predicting clinical\noutcomes. Moreover, KEEP's minimal computational requirements make it\nparticularly suitable for resource constrained environments.",
      "url": "http://arxiv.org/abs/2510.05049v1",
      "published_time_eastern_timestamp": 1759771674.0
    },
    {
      "title": "Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time\n  Optimization",
      "summary": "Multimodal encoders have pushed the boundaries of visual document retrieval,\nmatching textual query tokens directly to image patches and achieving\nstate-of-the-art performance on public benchmarks. Recent models relying on\nthis paradigm have massively scaled the sizes of their query and document\nrepresentations, presenting obstacles to deployment and scalability in\nreal-world pipelines. Furthermore, purely vision-centric approaches may be\nconstrained by the inherent modality gap still exhibited by modern\nvision-language models. In this work, we connect these challenges to the\nparadigm of hybrid retrieval, investigating whether a lightweight dense text\nretriever can enhance a stronger vision-centric model. Existing hybrid methods,\nwhich rely on coarse-grained fusion of ranks or scores, fail to exploit the\nrich interactions within each model's representation space. To address this, we\nintroduce Guided Query Refinement (GQR), a novel test-time optimization method\nthat refines a primary retriever's query embedding using guidance from a\ncomplementary retriever's scores. Through extensive experiments on visual\ndocument retrieval benchmarks, we demonstrate that GQR allows vision-centric\nmodels to match the performance of models with significantly larger\nrepresentations, while being up to 14x faster and requiring 54x less memory.\nOur findings show that GQR effectively pushes the Pareto frontier for\nperformance and efficiency in multimodal retrieval. We release our code at\nhttps://github.com/IBM/test-time-hybrid-retrieval",
      "url": "http://arxiv.org/abs/2510.05038v1",
      "published_time_eastern_timestamp": 1759770773.0
    },
    {
      "title": "Chroma+ model stellar surface intensities: Spherical formal solution",
      "summary": "We announce V. 2025-08-08 of the Chroma+ suite of stellar atmosphere and\nspectrum modelling codes for fast, approximate, effectively\nplatform-independent stellar spectrum synthesis, written in a number of free\nwell-supported programming languages. The Chroma+ suite now computes the\nemergent surface intensity and flux distributions and the hydrostatic pressure\nstructure assuming a spherical atmosphere rather than local flatness by\nimplementing the analytic formal solution of the 1D spherical radiative\ntransfer equation of Chapman (1966} based on an integration factor. We present\nour adaptation and discretization of the solution and demonstrate the resulting\nimpact of our sphericity treatment on a number of computed observables,\nincluding exo-planet transit light-curves. All codes are available from the\nOpenStars www site: www.ap.smu.ca/OpenStars.",
      "url": "http://arxiv.org/abs/2510.05035v1",
      "published_time_eastern_timestamp": 1759770688.0
    },
    {
      "title": "Imperceptible Jailbreaking against Large Language Models",
      "summary": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.",
      "url": "http://arxiv.org/abs/2510.05025v1",
      "published_time_eastern_timestamp": 1759770230.0
    },
    {
      "title": "Inoculation Prompting: Instructing LLMs to misbehave at train-time\n  improves test-time alignment",
      "summary": "Large language models are sometimes trained with imperfect oversight signals,\nleading to undesired behaviors such as reward hacking and sycophancy. Improving\noversight quality can be expensive or infeasible, motivating methods that\nimprove learned behavior despite an imperfect training signal. We introduce\nInoculation Prompting (IP), a simple but counterintuitive technique that\nprevents learning of an undesired behavior by modifying training prompts to\nexplicitly request it. For example, to inoculate against reward hacking, we\nmodify the prompts used in supervised fine-tuning to request code that only\nworks on provided test cases but fails on other inputs. Across four settings we\nfind that IP reduces the learning of undesired behavior without substantially\nreducing the learning of desired capabilities. We also show that prompts which\nmore strongly elicit the undesired behavior prior to fine-tuning more\neffectively inoculate against the behavior when used during training; this\nserves as a heuristic to identify promising inoculation prompts. Overall, IP is\na simple yet effective way to control how models generalize from fine-tuning,\npreventing learning of undesired behaviors without substantially disrupting\ndesired capabilities.",
      "url": "http://arxiv.org/abs/2510.05024v1",
      "published_time_eastern_timestamp": 1759770179.0
    },
    {
      "title": "Correcting quantum errors using a classical code and one additional\n  qubit",
      "summary": "Classical error-correcting codes are powerful but incompatible with quantum\nnoise, which includes both bit-flips and phase-flips. We introduce\nHadamard-based Virtual Error Correction (H-VEC), a protocol that empowers any\nclassical bit-flip code to correct arbitrary Pauli noise with the addition of\nonly a single ancilla qubit and two layers of controlled-Hadamard gates.\nThrough classical post-processing, H-VEC virtually filters the error channel,\nprojecting the noise into pure Y-type errors that are subsequently corrected\nusing the classical code's native decoding algorithm. We demonstrate this by\napplying H-VEC to the classical repetition code. Under a code-capacity noise\nmodel, the resulting protocol not only provides full quantum protection but\nalso achieves an exponentially stronger error suppression (in distance) than\nthe original classical code, and even larger improvements over the surface code\nwhile using much fewer qubits, simpler checks and straight-forward decoding.\nH-VEC comes with a sampling overhead due to its post-processing nature. It\nrepresents a new hybrid quantum error correction and mitigation framework that\nredefines the trade-offs between physical hardware requirements and classical\nprocessing for error suppression.",
      "url": "http://arxiv.org/abs/2510.05008v1",
      "published_time_eastern_timestamp": 1759769525.0
    },
    {
      "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM\n  Training",
      "summary": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.",
      "url": "http://arxiv.org/abs/2510.04996v1",
      "published_time_eastern_timestamp": 1759768449.0
    },
    {
      "title": "NatGVD: Natural Adversarial Example Attack towards Graph-based\n  Vulnerability Detection",
      "summary": "Graph-based models learn rich code graph structural information and present\nsuperior performance on various code analysis tasks. However, the robustness of\nthese models against adversarial example attacks in the context of\nvulnerability detection remains an open question. This paper proposes NatGVD, a\nnovel attack methodology that generates natural adversarial vulnerable code to\ncircumvent GNN-based and graph-aware transformer-based vulnerability detectors.\nNatGVD employs a set of code transformations that modify graph structure while\npreserving code semantics. Instead of injecting dead or unrelated code like\nprevious works, NatGVD considers naturalness requirements: generated examples\nshould not be easily recognized by humans or program analysis tools. With\nextensive evaluation of NatGVD on state-of-the-art vulnerability detection\nsystems, the results reveal up to 53.04% evasion rate across GNN-based\ndetectors and graph-aware transformer-based detectors. We also explore\npotential defense strategies to enhance the robustness of these systems against\nNatGVD.",
      "url": "http://arxiv.org/abs/2510.04987v1",
      "published_time_eastern_timestamp": 1759767859.0
    },
    {
      "title": "Bridging Clinical Narratives and ACR Appropriateness Guidelines: A\n  Multi-Agent RAG System for Medical Imaging Decisions",
      "summary": "The selection of appropriate medical imaging procedures is a critical and\ncomplex clinical decision, guided by extensive evidence-based standards such as\nthe ACR Appropriateness Criteria (ACR-AC). However, the underutilization of\nthese guidelines, stemming from the difficulty of mapping unstructured patient\nnarratives to structured criteria, contributes to suboptimal patient outcomes\nand increased healthcare costs. To bridge this gap, we introduce a multi-agent\ncognitive architecture that automates the translation of free-text clinical\nscenarios into specific, guideline-adherent imaging recommendations. Our system\nleverages a novel, domain-adapted dense retrieval model, ColBERT, fine-tuned on\na synthetically generated dataset of 8,840 clinical scenario-recommendation\npairs to achieve highly accurate information retrieval from the ACR-AC\nknowledge base. This retriever identifies candidate guidelines with a 93.9%\ntop-10 recall, which are then processed by a sequence of LLM-based agents for\nselection and evidence-based synthesis. We evaluate our architecture using\nGPT-4.1 and MedGemma agents, demonstrating a state-of-the-art exact match\naccuracy of 81%, meaning that in 81% of test cases the predicted procedure set\nwas identical to the guideline's reference set, and an F1-score of 0.879. This\nrepresents a 67-percentage-point absolute improvement in accuracy over a strong\nstandalone GPT-4.1 baseline, underscoring the contribution that our\narchitecture makes to a frontier model. These results were obtained on a\nchallenging test set with substantial lexical divergence from the source\nguidelines. Our code is available at\nhttps://anonymous.4open.science/r/demo-iclr-B567/",
      "url": "http://arxiv.org/abs/2510.04969v1",
      "published_time_eastern_timestamp": 1759766668.0
    },
    {
      "title": "Galaxy Model Subtraction with a Convolutional Denoising Autoencoder",
      "summary": "Galaxy model subtraction removes the smooth light of nearby galaxies so that\nfainter sources (e.g., stars, star clusters, background galaxies) can be\nidentified and measured. Traditional approaches (isophotal or parametric\nfitting) are semi-automated and can be challenging for large data sets. We\nbuild a convolutional denoising autoencoder (DAE) for galaxy model subtraction:\nimages are compressed to a latent representation and reconstructed to yield the\nsmooth galaxy, suppressing other objects. The DAE is trained on\nGALFIT-generated model galaxies injected into real sky backgrounds and tested\non real images from the Next Generation Virgo Cluster Survey (NGVS). To\nquantify performance, we conduct an injection-recovery experiment on residual\nimages by adding mock globular clusters (GCs) with known fluxes and positions.\nOur tests confirm a higher recovery rate of mock GCs near galaxy centers for\ncomplex morphologies, while matching ellipse fitting for smooth ellipticals.\nOverall, the DAE achieves subtraction equivalent to isophotal ellipse fitting\nfor regular ellipticals and superior results for galaxies with high\nellipticities or spiral features. Photometry of small-scale sources on DAE\nresiduals is consistent with that on ellipse-subtracted residuals. Once\ntrained, the DAE processes an image cutout in $\\lesssim 0.1$ s, enabling fast,\nfully automatic analysis of large data sets. We make our code available for\ndownload and use.",
      "url": "http://arxiv.org/abs/2510.04957v1",
      "published_time_eastern_timestamp": 1759766193.0
    },
    {
      "title": "ONNX-Net: Towards Universal Representations and Instant Performance\n  Prediction for Neural Architectures",
      "summary": "Neural architecture search (NAS) automates the design process of\nhigh-performing architectures, but remains bottlenecked by expensive\nperformance evaluation. Most existing studies that achieve faster evaluation\nare mostly tied to cell-based search spaces and graph encodings tailored to\nthose individual search spaces, limiting their flexibility and scalability when\napplied to more expressive search spaces. In this work, we aim to close the gap\nof individual search space restrictions and search space dependent network\nrepresentations. We present ONNX-Bench, a benchmark consisting of a collection\nof neural networks in a unified format based on ONNX files. ONNX-Bench includes\nall open-source NAS-bench-based neural networks, resulting in a total size of\nmore than 600k {architecture, accuracy} pairs. This benchmark allows creating a\nshared neural network representation, ONNX-Net, able to represent any neural\narchitecture using natural language descriptions acting as an input to a\nperformance predictor. This text-based encoding can accommodate arbitrary layer\ntypes, operation parameters, and heterogeneous topologies, enabling a single\nsurrogate to generalise across all neural architectures rather than being\nconfined to cell-based search spaces. Experiments show strong zero-shot\nperformance across disparate search spaces using only a small amount of\npretraining samples, enabling the unprecedented ability to evaluate any neural\nnetwork architecture instantly.",
      "url": "http://arxiv.org/abs/2510.04938v1",
      "published_time_eastern_timestamp": 1759765416.0
    }
  ]
}
{
  "last_updated": "2025-10-14T02:18:39.789904-04:00",
  "papers": [
    {
      "title": "BayeSN-TD: Time Delay and $H_0$ Estimation for Lensed SN H0pe",
      "summary": "We present BayeSN-TD, an enhanced implementation of the probabilistic type Ia\nsupernova (SN Ia) BayeSN SED model, designed for fitting multiply-imaged,\ngravitationally lensed type Ia supernovae (glSNe Ia). BayeSN-TD fits for\nmagnifications and time-delays across multiple images while marginalising over\nan achromatic, Gaussian process-based treatment of microlensing, to allow for\ntime-dependent deviations from a typical SN Ia SED caused by gravitational\nlensing by stars in the lensing system. BayeSN-TD is able to robustly infer\ntime delays and produce well-calibrated uncertainties, even when applied to\nsimulations based on a different SED model and incorporating chromatic\nmicrolensing, strongly validating its suitability for time-delay cosmography.\nWe then apply BayeSN-TD to publicly available photometry of the glSN Ia SN\nH0pe, inferring time delays between images BA and BC of $\\Delta\nT_{BA}=121.9^{+9.5}_{-7.5}$ days and $\\Delta T_{BC}=63.2^{+3.2}_{-3.3}$ days\nalong with absolute magnifications $\\beta$ for each image, $\\beta_A =\n2.38^{+0.72}_{-0.54}$, $\\beta_B=5.27^{+1.25}_{-1.02}$ and\n$\\beta_C=3.93^{+1.00}_{-0.75}$. Combining our constraints on time-delays and\nmagnifications with existing lens models of this system, we infer\n$H_0=69.3^{+12.6}_{-7.8}$ km s$^{-1}$ Mpc$^{-1}$, consistent with previous\nanalysis of this system; incorporating additional constraints based on\nspectroscopy yields $H_0=66.8^{+13.4}_{-5.4}$ km s$^{-1}$ Mpc$^{-1}$. While\nthis is not yet precise enough to draw a meaningful conclusion with regard to\nthe `Hubble tension', upcoming analysis of SN H0pe with more accurate\nphotometry enabled by template images, and other glSNe, will provide stronger\nconstraints on $H_0$; BayeSN-TD will be a valuable tool for these analyses.",
      "url": "http://arxiv.org/abs/2510.11719v1",
      "published_time_eastern_timestamp": 1760378399.0
    },
    {
      "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven\n  Images",
      "summary": "Recent advances in Large Language Models (LLMs) and Vision Language Models\n(VLMs) have shown significant progress in mathematical reasoning, yet they\nstill face a critical bottleneck with problems requiring visual assistance,\nsuch as drawing auxiliary lines or plotting functions to solve the problems.\nMost LLMs and VLMs are constrained to text-only reasoning chains, while\nmultimodal unified models that can generate interleaved text and images lack\nthe necessary precision and controllability for such tasks. To address this, we\npropose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking\nwith images\" in mathematics. Our approach leverages the VLM to generate text\nreasoning as well as executable plotting code, which is then rendered into\nimages as \"visual thought\", to solve mathematical problems. To achieve this, we\nfirst construct Math-VR, the first large-scale, bilingual dataset and benchmark\nfor Mathematics problems with Visual Reasoning, comprising 178K samples.\nSecond, to create high-quality training data, we develop a state-of-the-art\nimage-to-code converter specialized for parsing complex mathematical figures\ninto codes. Finally, using these training data, we train the CodePlot-CoT model\nfor solving mathematical problems. Experimental results show that our model\nachieves up to 21% increase over base model on our new benchmark, fully\nvalidating the efficacy of our proposed code-driven reasoning paradigm. Our\nwork opens a new direction for multimodal mathematical reasoning and provides\nthe community with the first large-scale dataset, comprehensive benchmark, and\nstrong approach for such problems. To facilitate future research, we make our\ndatasets, code, and pretrained models publicly available at\nhttps://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
      "url": "http://arxiv.org/abs/2510.11718v1",
      "published_time_eastern_timestamp": 1760378395.0
    },
    {
      "title": "Are Large Reasoning Models Interruptible?",
      "summary": "Large Reasoning Models (LRMs) excel at complex reasoning but are\ntraditionally evaluated in static, \"frozen world\" settings: model responses are\nassumed to be instantaneous, and the context of a request is presumed to be\nimmutable over the duration of the response. While generally true for\nshort-term tasks, the \"frozen world\" assumption breaks down in modern reasoning\ntasks such as assistive programming, where models may take hours to think\nthrough problems and code may change dramatically from the time the model\nstarts thinking to the model's final output. In this work, we challenge the\nfrozen world assumption and evaluate LRM robustness under two realistic dynamic\nscenarios: interruptions, which test the quality of the model's partial outputs\non a limited budget, and dynamic context, which tests model adaptation to\nin-flight changes. Across mathematics and programming benchmarks that require\nlong-form reasoning, static evaluations consistently overestimate robustness:\neven state-of-the-art LRMs, which achieve high accuracy in static settings, can\nfail unpredictably when interrupted or exposed to changing context, with\nperformance dropping by up to 60% when updates are introduced late in the\nreasoning process. Our analysis further reveals several novel failure modes,\nincluding reasoning leakage, where models fold the reasoning into their final\nanswer when interrupted; panic, where under time pressure models abandon\nreasoning entirely and return incorrect answers; and self-doubt, where\nperformance degrades while incorporating updated information.",
      "url": "http://arxiv.org/abs/2510.11713v1",
      "published_time_eastern_timestamp": 1760378375.0
    },
    {
      "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
      "summary": "In this work, we propose DiT360, a DiT-based framework that performs hybrid\ntraining on perspective and panoramic data for panoramic image generation. For\nthe issues of maintaining geometric fidelity and photorealism in generation\nquality, we attribute the main reason to the lack of large-scale, high-quality,\nreal-world panoramic data, where such a data-centric view differs from prior\nmethods that focus on model design. Basically, DiT360 has several key modules\nfor inter-domain transformation and intra-domain augmentation, applied at both\nthe pre-VAE image level and the post-VAE token level. At the image level, we\nincorporate cross-domain knowledge through perspective image guidance and\npanoramic refinement, which enhance perceptual quality while regularizing\ndiversity and photorealism. At the token level, hybrid supervision is applied\nacross multiple modules, which include circular padding for boundary\ncontinuity, yaw loss for rotational robustness, and cube loss for distortion\nawareness. Extensive experiments on text-to-panorama, inpainting, and\noutpainting tasks demonstrate that our method achieves better boundary\nconsistency and image fidelity across eleven quantitative metrics. Our code is\navailable at https://github.com/Insta360-Research-Team/DiT360.",
      "url": "http://arxiv.org/abs/2510.11712v1",
      "published_time_eastern_timestamp": 1760378355.0
    },
    {
      "title": "Reinforced sequential Monte Carlo for amortised sampling",
      "summary": "This paper proposes a synergy of amortised and particle-based methods for\nsampling from distributions defined by unnormalised density functions. We state\na connection between sequential Monte Carlo (SMC) and neural sequential\nsamplers trained by maximum-entropy reinforcement learning (MaxEnt RL), wherein\nlearnt sampling policies and value functions define proposal kernels and twist\nfunctions. Exploiting this connection, we introduce an off-policy RL training\nprocedure for the sampler that uses samples from SMC -- using the learnt\nsampler as a proposal -- as a behaviour policy that better explores the target\ndistribution. We describe techniques for stable joint training of proposals and\ntwist functions and an adaptive weight tempering scheme to reduce training\nsignal variance. Furthermore, building upon past attempts to use experience\nreplay to guide the training of neural samplers, we derive a way to combine\nhistorical samples with annealed importance sampling weights within a replay\nbuffer. On synthetic multi-modal targets (in both continuous and discrete\nspaces) and the Boltzmann distribution of alanine dipeptide conformations, we\ndemonstrate improvements in approximating the true distribution as well as\ntraining stability compared to both amortised and Monte Carlo methods.",
      "url": "http://arxiv.org/abs/2510.11711v1",
      "published_time_eastern_timestamp": 1760378351.0
    },
    {
      "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
      "summary": "Recently, the emergence of agentic RL has showcased that RL could also\neffectively improve the agentic reasoning ability of LLMs, yet the key design\nprinciples and optimal practices remain unclear. In this work, we conduct a\ncomprehensive and systematic investigation to demystify reinforcement learning\nin agentic reasoning from three key perspectives: data, algorithm, and\nreasoning mode. We highlight our key insights: (i) Replacing stitched synthetic\ntrajectories with real end-to-end tool-use trajectories yields a far stronger\nSFT initialization; high-diversity, model-aware datasets sustain exploration\nand markedly improve RL performance. (ii) Exploration-friendly techniques are\ncrucial for agentic RL, such as clip higher, overlong reward shaping, and\nmaintaining adequate policy entropy could improve the training efficiency.\n(iii) A deliberative strategy with fewer tool calls outperforms frequent tool\ncalls or verbose self-reasoning, improving tool efficiency and final accuracy.\nTogether, these simple practices consistently enhance agentic reasoning and\ntraining efficiency, achieving strong results on challenging benchmarks with\nsmaller models, and establishing a practical baseline for future agentic RL\nresearch. Beyond these empirical insights, we further contribute a\nhigh-quality, real end-to-end agentic SFT dataset along with a high-quality RL\ndataset, and demonstrate the effectiveness of our insights in boosting the\nagentic reasoning ability of LLMs across four challenging benchmarks, including\nAIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,\n4B-sized models could also achieve superior agentic reasoning performance\ncompared to 32B-sized models. Code and models:\nhttps://github.com/Gen-Verse/Open-AgentRL",
      "url": "http://arxiv.org/abs/2510.11701v1",
      "published_time_eastern_timestamp": 1760378235.0
    },
    {
      "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning\n  for LLMs",
      "summary": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for\nlarge language models (LLMs). While RL is essential for LLMs' reasoning\ncapabilities, it is resource-intensive, requiring substantial GPU memory and\nlong rollout durations. QeRL addresses these issues by combining NVFP4\nquantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL\nwhile reducing memory overhead. Beyond efficiency, our findings show that\nquantization noise increases policy entropy, enhancing exploration, and\nenabling the discovery of better strategies during RL. To further optimize\nexploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,\nwhich dynamically adjusts noise during training. Experiments demonstrate that\nQeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is\nthe first framework to enable RL training of a 32B LLM on a single H100 80GB\nGPU, while delivering overall speedups for RL training. It also achieves faster\nreward growth and higher final accuracy than 16-bit LoRA and QLoRA, while\nmatching the performance of full-parameter fine-tuning on mathematical\nbenchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These\nresults establish QeRL as an efficient and effective framework for RL training\nin LLMs.",
      "url": "http://arxiv.org/abs/2510.11696v1",
      "published_time_eastern_timestamp": 1760378109.0
    },
    {
      "title": "Scaling Language-Centric Omnimodal Representation Learning",
      "summary": "Recent multimodal embedding approaches leveraging multimodal large language\nmodels (MLLMs) fine-tuned with contrastive learning (CL) have shown promising\nresults, yet the underlying reasons behind their superiority remain\nunderexplored. This work argues that a crucial advantage of MLLM-based\napproaches stems from implicit cross-modal alignment achieved during generative\npretraining, where the language decoder learns to exploit multimodal signals\nwithin a shared representation space for generating unimodal outputs. Through\nanalysis of anisotropy and kernel similarity structure, we empirically confirm\nthat latent alignment emerges within MLLM representations, allowing CL to serve\nas a lightweight refinement stage. Leveraging this insight, we propose a\nLanguage-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive\nexperiments across diverse backbones and benchmarks demonstrate its\neffectiveness, achieving state-of-the-art performance across modalities.\nFurthermore, we identify a Generation-Representation Scaling Law (GRSL),\nshowing that the representational capabilities gained through contrastive\nrefinement scales positively with the MLLM's generative capabilities. This\nsuggests that improving generative abilities evolves as an effective paradigm\nfor enhancing representation quality. We provide a theoretical explanation of\nGRSL, which formally links the MLLM's generative quality to the upper bound on\nits representation performance, and validate it on a challenging, low-resource\nvisual-document retrieval task, showing that continual generative pretraining\nbefore CL can further enhance the potential of a model's embedding\ncapabilities. Codes, models, and resources are available at\nhttps://github.com/LCO-Embedding/LCO-Embedding.",
      "url": "http://arxiv.org/abs/2510.11693v1",
      "published_time_eastern_timestamp": 1760378032.0
    },
    {
      "title": "Representation-Based Exploration for Language Models: From Test-Time to\n  Post-Training",
      "summary": "Reinforcement learning (RL) promises to expand the capabilities of language\nmodels, but it is unclear if current RL techniques promote the discovery of\nnovel behaviors, or simply sharpen those already present in the base model. In\nthis paper, we investigate the value of deliberate exploration -- explicitly\nincentivizing the model to discover novel and diverse behaviors -- and aim to\nunderstand how the knowledge in pre-trained models can guide this search. Our\nmain finding is that exploration with a simple, principled,\nrepresentation-based bonus derived from the pre-trained language model's hidden\nstates significantly improves diversity and pass@k rates -- both for\npost-training, and in a novel inference-time scaling setting we introduce. For\ninference-time, exploration with representation-based diversity improves\nefficiency, consistently improving pass@k rates across a variety of models and\nreasoning tasks. For example, for Qwen-2.5-14b-Instruct we obtain over 50%\nimprovement in verifier efficiency on almost all tasks. For post-training, we\nshow that integrating this exploration strategy into an RL pipeline improves\nreasoning performance over that of the initial model and over standard RL\npost-training. For example, on AIME 2024, our post-trained\nQwen-2.5-7b-Instruct's pass@80 matches the pass@256 of GRPO on the same model,\ndemonstrating a 3x improvement in test-time sample efficiency. Overall, our\nfindings suggest that deliberate exploration -- with the right notion of\ndiversity -- is a practical path toward discovery of new behaviors beyond\nsharpening.",
      "url": "http://arxiv.org/abs/2510.11686v1",
      "published_time_eastern_timestamp": 1760377745.0
    },
    {
      "title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion\n  Large Language Models",
      "summary": "A key challenge in applying reinforcement learning (RL) to diffusion large\nlanguage models (dLLMs) lies in the intractability of their likelihood\nfunctions, which are essential for the RL objective, necessitating\ncorresponding approximation in each training step. While existing methods\napproximate the log-likelihoods by their evidence lower bounds (ELBOs) via\ncustomized Monte Carlo (MC) sampling, the forward computational graphs of all\nMC samples need to be retained for the gradient computation of non-linear terms\nin the RL objective, resulting in significant memory overhead. This constraint\nrestricts feasible sample sizes, leading to imprecise likelihood approximations\nand ultimately distorting the RL objective. To overcome this limitation, we\npropose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient\nRL algorithm that maximizes a specially constructed lower bound of the\nELBO-based objective. This lower bound is carefully designed to satisfy two key\nproperties: (1) Linearity: it is formulated in a linear sum where each term\ndepends only on a single MC sample, thereby enabling gradient accumulation\nacross samples and ensuring constant memory usage; (2) Equivalence: Both the\nvalue and gradient of this lower bound are equal to those of the ELBO-based\nobjective in on-policy training, making it also an effective approximation for\nthe original RL objective. These properties allow BGPO to adopt a large MC\nsample size, resulting in more accurate likelihood approximations and improved\nRL objective estimation, which in turn leads to enhanced performance.\nExperiments show that BGPO significantly outperforms previous RL algorithms for\ndLLMs in math problem solving, code generation, and planning tasks.",
      "url": "http://arxiv.org/abs/2510.11683v1",
      "published_time_eastern_timestamp": 1760377670.0
    },
    {
      "title": "Integral Matrices of Fixed Rank over Number Fields",
      "summary": "We prove an asymptotic formula for the number of fixed rank matrices with\ninteger coefficients over a number field K/Q and bounded norm. As an\napplication, we derive an approximate Rogers integral formula for discrete sets\nof module lattices obtained from lifts of algebraic codes. This in turn implies\nthat the moment estimates of random lattices with a number field structure also\ncarry through for large enough discrete sets of module lattices.",
      "url": "http://arxiv.org/abs/2510.11673v1",
      "published_time_eastern_timestamp": 1760377438.0
    },
    {
      "title": "SR-Scientist: Scientific Equation Discovery With Agentic AI",
      "summary": "Recently, Large Language Models (LLMs) have been applied to scientific\nequation discovery, leveraging their embedded scientific knowledge for\nhypothesis generation. However, current methods typically confine LLMs to the\nrole of an equation proposer within search algorithms like genetic programming.\nIn this paper, we present SR-Scientist, a framework that elevates the LLM from\na simple equation proposer to an autonomous AI scientist that writes code to\nanalyze data, implements the equation as code, submits it for evaluation, and\noptimizes the equation based on experimental feedback. Specifically, we wrap\nthe code interpreter into a set of tools for data analysis and equation\nevaluation. The agent is instructed to optimize the equation by utilizing these\ntools over a long horizon with minimal human-defined pipelines. Empirical\nresults show that SR-Scientist outperforms baseline methods by an absolute\nmargin of 6% to 35% on datasets covering four science disciplines.\nAdditionally, we demonstrate our method's robustness to noise, the\ngeneralization of the discovered equations to out-of-domain data, and their\nsymbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning\nframework to enhance the agent's capabilities.",
      "url": "http://arxiv.org/abs/2510.11661v1",
      "published_time_eastern_timestamp": 1760376923.0
    },
    {
      "title": "Automatically Generating Questions About Scratch Programs",
      "summary": "When learning to program, students are usually assessed based on the code\nthey wrote. However, the mere completion of a programming task does not\nguarantee actual comprehension of the underlying concepts. Asking learners\nquestions about the code they wrote has therefore been proposed as a means to\nassess program comprehension. As creating targeted questions for individual\nstudent programs can be tedious and challenging, prior work has proposed to\ngenerate such questions automatically. In this paper we generalize this idea to\nthe block-based programming language Scratch. We propose a set of 30 different\nquestions for Scratch code covering an established program comprehension model,\nand extend the LitterBox static analysis tool to automatically generate\ncorresponding questions for a given Scratch program. On a dataset of 600,913\nprojects we generated 54,118,694 questions automatically. Our initial\nexperiments with 34 ninth graders demonstrate that this approach can indeed\ngenerate meaningful questions for Scratch programs, and we find that the\nability of students to answer these questions on their programs relates to\ntheir overall performance.",
      "url": "http://arxiv.org/abs/2510.11658v1",
      "published_time_eastern_timestamp": 1760376852.0
    },
    {
      "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic\n  Research Problems",
      "summary": "In recent years, the research focus of large language models (LLMs) and\nagents has shifted increasingly from demonstrating novel capabilities to\ncomplex reasoning and tackling challenging tasks. However, existing evaluations\nfocus mainly on math/code contests or general tasks, while existing\nmulti-domain academic benchmarks lack sufficient reasoning depth, leaving the\nfield without a rigorous benchmark for high-level reasoning. To fill this gap,\nwe introduce the Acadreason benchmark, designed to evaluate the ability of LLMs\nand agents to acquire and reason over academic knowledge. It consists of 50\nexpert-annotated academic problems across five high-reasoning domains,\nincluding computer science, economics, law, mathematics, and philosophy. All\nquestions are sourced from top-tier publications in recent years and undergo\nrigorous annotation and quality control to ensure they are both challenging and\nanswerable. We conduct systematic evaluations of over 10 mainstream LLMs and\nagents. The results show that most LLMs scored below 20 points, with even the\ncutting-edge GPT-5 achieving only 16 points. While agents achieved higher\nscores, none exceeded 40 points. This demonstrates the current capability gap\nbetween LLMs and agents in super-intelligent academic research tasks and\nhighlights the challenges of Acadreason.",
      "url": "http://arxiv.org/abs/2510.11652v1",
      "published_time_eastern_timestamp": 1760376636.0
    },
    {
      "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing\n  Assessment",
      "summary": "Instruction-guided video editing has emerged as a rapidly advancing research\ndirection, offering new opportunities for intuitive content transformation\nwhile also posing significant challenges for systematic evaluation. Existing\nvideo editing benchmarks fail to support the evaluation of instruction-guided\nvideo editing adequately and further suffer from limited source diversity,\nnarrow task coverage and incomplete evaluation metrics. To address the above\nlimitations, we introduce IVEBench, a modern benchmark suite specifically\ndesigned for instruction-guided video editing assessment. IVEBench comprises a\ndiverse database of 600 high-quality source videos, spanning seven semantic\ndimensions, and covering video lengths ranging from 32 to 1,024 frames. It\nfurther includes 8 categories of editing tasks with 35 subcategories, whose\nprompts are generated and refined through large language models and expert\nreview. Crucially, IVEBench establishes a three-dimensional evaluation protocol\nencompassing video quality, instruction compliance and video fidelity,\nintegrating both traditional metrics and multimodal large language model-based\nassessments. Extensive experiments demonstrate the effectiveness of IVEBench in\nbenchmarking state-of-the-art instruction-guided video editing methods, showing\nits ability to provide comprehensive and human-aligned evaluation outcomes.",
      "url": "http://arxiv.org/abs/2510.11647v1",
      "published_time_eastern_timestamp": 1760376428.0
    },
    {
      "title": "LRQ-Solver: A Transformer-Based Neural Operator for Fast and Accurate\n  Solving of Large-scale 3D PDEs",
      "summary": "Solving large-scale Partial Differential Equations (PDEs) on complex\nthree-dimensional geometries represents a central challenge in scientific and\nengineering computing, often impeded by expensive pre-processing stages and\nsubstantial computational overhead. We introduce Low-Rank Query-based PDE\nSolver (LRQ-Solver), a physics-integrated framework engineered for rapid,\naccurate, and highly scalable simulations of industrial-grade models. This\nframework is built upon two primary technical innovations. First, our Parameter\nConditioned Lagrangian Modeling (PCLM) approach explicitly couples local\nphysical states with global design parameters, enabling robust predictions\nacross varied simulation configurations. By embedding physical consistency\ndirectly into the learning architecture, PCLM ensures that predictions remain\nphysically meaningful even under unseen design conditions, significantly\nenhancing generalization and reliability. Second, the Low-Rank Query Attention\n(LR-QA) module leverages the second-order statistics of physical fields to\nconstruct a global coherence kernel, reducing the computational complexity of\nattention from O(N2) to O(NC2 + C3). By replacing point-wise clustering with\ncovariance decomposition, LRQ-Solver achieves exceptional scalability\nefficiently processing up to 2 million points on a single GPU. Validated on\nstandard benchmarks, LRQ-Solver achieves a 38.9% error reduction on the\nDrivAer++ dataset and 28.76% on the 3D Beam dataset, alongside a training\nspeedup of up to 50 times. Our results establish that LRQ-Solver offers a\npowerful paradigm for multi-configuration physics simulations, delivering a\nSOTA combination of accuracy, scalability, and efficiency. Code to reproduce\nthe experiments is available at https://github.com/LilaKen/LRQ-Solver.",
      "url": "http://arxiv.org/abs/2510.11636v1",
      "published_time_eastern_timestamp": 1760375910.0
    },
    {
      "title": "EvoCAD: Evolutionary CAD Code Generation with Vision Language Models",
      "summary": "Combining large language models with evolutionary computation algorithms\nrepresents a promising research direction leveraging the remarkable generative\nand in-context learning capabilities of LLMs with the strengths of evolutionary\nalgorithms. In this work, we present EvoCAD, a method for generating\ncomputer-aided design (CAD) objects through their symbolic representations\nusing vision language models and evolutionary optimization. Our method samples\nmultiple CAD objects, which are then optimized using an evolutionary approach\nwith vision language and reasoning language models. We assess our method using\nGPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and\ncomparing it to prior methods. Additionally, we introduce two new metrics based\non topological properties defined by the Euler characteristic, which capture a\nform of semantic similarity between 3D objects. Our results demonstrate that\nEvoCAD outperforms previous approaches on multiple metrics, particularly in\ngenerating topologically correct objects, which can be efficiently evaluated\nusing our two novel metrics that complement existing spatial metrics.",
      "url": "http://arxiv.org/abs/2510.11631v1",
      "published_time_eastern_timestamp": 1760375522.0
    },
    {
      "title": "High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid\n  Network",
      "summary": "Photo enhancement plays a crucial role in augmenting the visual aesthetics of\na photograph. In recent years, photo enhancement methods have either focused on\nenhancement performance, producing powerful models that cannot be deployed on\nedge devices, or prioritized computational efficiency, resulting in inadequate\nperformance for real-world applications. To this end, this paper introduces a\npyramid network called LLF-LUT++, which integrates global and local operators\nthrough closed-form Laplacian pyramid decomposition and reconstruction. This\napproach enables fast processing of high-resolution images while also achieving\nexcellent performance. Specifically, we utilize an image-adaptive 3D LUT that\ncapitalizes on the global tonal characteristics of downsampled images, while\nincorporating two distinct weight fusion strategies to achieve coarse global\nimage enhancement. To implement this strategy, we designed a spatial-frequency\ntransformer weight predictor that effectively extracts the desired distinct\nweights by leveraging frequency features. Additionally, we apply local\nLaplacian filters to adaptively refine edge details in high-frequency\ncomponents. After meticulously redesigning the network structure and\ntransformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on\nthe HDR+ dataset, but also further reduces runtime, with 4K resolution images\nprocessed in just 13 ms on a single GPU. Extensive experimental results on two\nbenchmark datasets further show that the proposed approach performs favorably\ncompared to state-of-the-art methods. The source code will be made publicly\navailable at https://github.com/fengzhang427/LLF-LUT.",
      "url": "http://arxiv.org/abs/2510.11613v1",
      "published_time_eastern_timestamp": 1760374352.0
    },
    {
      "title": "ParaCook: On Time-Efficient Planning for Multi-Agent Systems",
      "summary": "Large Language Models (LLMs) exhibit strong reasoning abilities for planning\nlong-horizon, real-world tasks, yet existing agent benchmarks focus on task\ncompletion while neglecting time efficiency in parallel and asynchronous\noperations. To address this, we present ParaCook, a benchmark for\ntime-efficient collaborative planning. Inspired by the Overcooked game,\nParaCook provides an environment for various challenging interaction planning\nof multi-agent systems that are instantiated as cooking tasks, with a\nsimplified action space to isolate the core challenge of strategic parallel\nplanning. Through a comprehensive evaluation of state-of-the-art LLMs, we find\nthat current approaches achieve suboptimal plans, which struggle with parallel\nactions or coordination. Our analysis also reveals LLMs' potential on abstract\ntasks where they can focus on high-level parallel optimization. ParaCook\nprovides a scalable evaluation framework with adjustable complexity,\nestablishing a foundation for developing and assessing time efficiency-aware\nmulti-agent planning. The code and data are available at\nhttps://github.com/zsq259/ParaCook.",
      "url": "http://arxiv.org/abs/2510.11608v1",
      "published_time_eastern_timestamp": 1760374027.0
    },
    {
      "title": "ExpVid: A Benchmark for Experiment Video Understanding & Reasoning",
      "summary": "Multimodal Large Language Models (MLLMs) hold promise for accelerating\nscientific discovery by interpreting complex experimental procedures. However,\ntheir true capabilities are poorly understood, as existing benchmarks neglect\nthe fine-grained and long-horizon nature of authentic laboratory work,\nespecially in wet-lab settings. To bridge this gap, we introduce ExpVid, the\nfirst benchmark designed to systematically evaluate MLLMs on scientific\nexperiment videos. Curated from peer-reviewed video publications, ExpVid\nfeatures a new three-level task hierarchy that mirrors the scientific process:\n(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural\nUnderstanding of step order and completeness; and (3) Scientific Reasoning that\nconnects the full experiment to its published conclusions. Our vision-centric\nannotation pipeline, combining automated generation with multi-disciplinary\nexpert validation, ensures that tasks require visual grounding. We evaluate 19\nleading MLLMs on ExpVid and find that while they excel at coarse-grained\nrecognition, they struggle with disambiguating fine details, tracking state\nchanges over time, and linking experimental procedures to scientific outcomes.\nOur results reveal a notable performance gap between proprietary and\nopen-source models, particularly in high-order reasoning. ExpVid not only\nprovides a diagnostic tool but also charts a roadmap for developing MLLMs\ncapable of becoming trustworthy partners in scientific experimentation.",
      "url": "http://arxiv.org/abs/2510.11606v1",
      "published_time_eastern_timestamp": 1760373928.0
    }
  ]
}
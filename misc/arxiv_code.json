{
  "last_updated": "2025-10-01T11:10:58.401676-04:00",
  "papers": [
    {
      "title": "Stitch: Training-Free Position Control in Multimodal Diffusion\n  Transformers",
      "summary": "Text-to-Image (T2I) generation models have advanced rapidly in recent years,\nbut accurately capturing spatial relationships like \"above\" or \"to the right\nof\" poses a persistent challenge. Earlier methods improved spatial relationship\nfollowing with external position control. However, as architectures evolved to\nenhance image quality, these techniques became incompatible with modern models.\nWe propose Stitch, a training-free method for incorporating external position\ncontrol into Multi-Modal Diffusion Transformers (MMDiT) via\nautomatically-generated bounding boxes. Stitch produces images that are both\nspatially accurate and visually appealing by generating individual objects\nwithin designated bounding boxes and seamlessly stitching them together. We\nfind that targeted attention heads capture the information necessary to isolate\nand cut out individual objects mid-generation, without needing to fully\ncomplete the image. We evaluate Stitch on PosEval, our benchmark for\nposition-based T2I generation. Featuring five new tasks that extend the concept\nof Position beyond the basic GenEval task, PosEval demonstrates that even top\nmodels still have significant room for improvement in position-based\ngeneration. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances\nbase models, even improving FLUX by 218% on GenEval's Position task and by 206%\non PosEval. Stitch achieves state-of-the-art results with Qwen-Image on\nPosEval, improving over previous models by 54%, all accomplished while\nintegrating position control into leading models training-free. Code is\navailable at https://github.com/ExplainableML/Stitch.",
      "url": "http://arxiv.org/abs/2509.26644v1",
      "published_time_eastern_timestamp": 1759255191.0
    },
    {
      "title": "TTT3R: 3D Reconstruction as Test-Time Training",
      "summary": "Modern Recurrent Neural Networks have become a competitive architecture for\n3D reconstruction due to their linear-time complexity. However, their\nperformance degrades significantly when applied beyond the training context\nlength, revealing limited length generalization. In this work, we revisit the\n3D reconstruction foundation models from a Test-Time Training perspective,\nframing their designs as an online learning problem. Building on this\nperspective, we leverage the alignment confidence between the memory state and\nincoming observations to derive a closed-form learning rate for memory updates,\nto balance between retaining historical information and adapting to new\nobservations. This training-free intervention, termed TTT3R, substantially\nimproves length generalization, achieving a $2\\times$ improvement in global\npose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU\nmemory to process thousands of images. Code available in\nhttps://rover-xingyu.github.io/TTT3R",
      "url": "http://arxiv.org/abs/2509.26645v1",
      "published_time_eastern_timestamp": 1759255191.0
    },
    {
      "title": "AccidentBench: Benchmarking Multimodal Understanding and Reasoning in\n  Vehicle Accidents and Beyond",
      "summary": "Rapid advances in multimodal models demand benchmarks that rigorously\nevaluate understanding and reasoning in safety-critical, dynamic real-world\nsettings. We present AccidentBench, a large-scale benchmark that combines\nvehicle accident scenarios with Beyond domains, safety-critical settings in air\nand water that emphasize spatial and temporal reasoning (e.g., navigation,\norientation, multi-vehicle motion). The benchmark contains approximately 2000\nvideos and over 19000 human-annotated question--answer pairs spanning multiple\nvideo lengths (short/medium/long) and difficulty levels (easy/medium/hard).\nTasks systematically probe core capabilities: temporal, spatial, and intent\nunderstanding and reasoning. By unifying accident-centric traffic scenes with\nbroader safety-critical scenarios in air and water, AccidentBench offers a\ncomprehensive, physically grounded testbed for evaluating models under\nreal-world variability. Evaluations of state-of-the-art models (e.g.,\nGemini-2.5 Pro and GPT-5) show that even the strongest models achieve only\nabout 18% accuracy on the hardest tasks and longest videos, revealing\nsubstantial gaps in real-world temporal, spatial, and intent reasoning.\nAccidentBench is designed to expose these critical gaps and drive the\ndevelopment of multimodal models that are safer, more robust, and better\naligned with real-world safety-critical challenges. The code and dataset are\navailable at: https://github.com/SafeRL-Lab/AccidentBench",
      "url": "http://arxiv.org/abs/2509.26636v1",
      "published_time_eastern_timestamp": 1759255153.0
    },
    {
      "title": "Branching Out: Broadening AI Measurement and Evaluation with Measurement\n  Trees",
      "summary": "This paper introduces \\textit{measurement trees}, a novel class of metrics\ndesigned to combine various constructs into an interpretable multi-level\nrepresentation of a measurand. Unlike conventional metrics that yield single\nvalues, vectors, surfaces, or categories, measurement trees produce a\nhierarchical directed graph in which each node summarizes its children through\nuser-defined aggregation methods. In response to recent calls to expand the\nscope of AI system evaluation, measurement trees enhance metric transparency\nand facilitate the integration of heterogeneous evidence, including, e.g.,\nagentic, business, energy-efficiency, sociotechnical, or security signals. We\npresent definitions and examples, demonstrate practical utility through a\nlarge-scale measurement exercise, and provide accompanying open-source Python\ncode. By operationalizing a transparent approach to measurement of complex\nconstructs, this work offers a principled foundation for broader and more\ninterpretable AI evaluation.",
      "url": "http://arxiv.org/abs/2509.26632v1",
      "published_time_eastern_timestamp": 1759255139.0
    },
    {
      "title": "Recursive Self-Aggregation Unlocks Deep Thinking in Large Language\n  Models",
      "summary": "Test-time scaling methods improve the capabilities of large language models\n(LLMs) by increasing the amount of compute used during inference to make a\nprediction. Inference-time compute can be scaled in parallel by choosing among\nmultiple independent solutions or sequentially through self-refinement. We\npropose Recursive Self-Aggregation (RSA), a test-time scaling method inspired\nby evolutionary methods that combines the benefits of both parallel and\nsequential scaling. Each step of RSA refines a population of candidate\nreasoning chains through aggregation of subsets to yield a population of\nimproved solutions, which are then used as the candidate pool for the next\niteration. RSA exploits the rich information embedded in the reasoning chains\n-- not just the final answers -- and enables bootstrapping from partially\ncorrect intermediate steps within different chains of thought. Empirically, RSA\ndelivers substantial performance gains with increasing compute budgets across\ndiverse tasks, model families and sizes. Notably, RSA enables\nQwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning\nmodels, including DeepSeek-R1 and o3-mini (high), while outperforming purely\nparallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning\nGym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the\nmodel to combine solutions via a novel aggregation-aware reinforcement learning\napproach yields significant performance gains. Code available at\nhttps://github.com/HyperPotatoNeo/RSA.",
      "url": "http://arxiv.org/abs/2509.26626v1",
      "published_time_eastern_timestamp": 1759255083.0
    },
    {
      "title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from\n  Language Pre-training",
      "summary": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.",
      "url": "http://arxiv.org/abs/2509.26625v1",
      "published_time_eastern_timestamp": 1759255064.0
    },
    {
      "title": "HART: Human Aligned Reconstruction Transformer",
      "summary": "We introduce HART, a unified framework for sparse-view human reconstruction.\nGiven a small set of uncalibrated RGB images of a person as input, it outputs a\nwatertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat\nrepresentation for photorealistic novel-view rendering. Prior methods for\nclothed human reconstruction either optimize parametric templates, which\noverlook loose garments and human-object interactions, or train implicit\nfunctions under simplified camera assumptions, limiting applicability in real\nscenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body\ncorrespondences, and employs an occlusion-aware Poisson reconstruction to\nrecover complete geometry, even in self-occluded regions. These predictions\nalso align with a parametric SMPL-X body model, ensuring that reconstructed\ngeometry remains consistent with human structure while capturing loose clothing\nand interactions. These human-aligned meshes initialize Gaussian splats to\nfurther enable sparse-view rendering. While trained on only 2.3K synthetic\nscans, HART achieves state-of-the-art results: Chamfer Distance improves by\n18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for\nSMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on\na wide range of datasets. These results suggest that feed-forward transformers\ncan serve as a scalable model for robust human reconstruction in real-world\nsettings. Code and models will be released.",
      "url": "http://arxiv.org/abs/2509.26621v1",
      "published_time_eastern_timestamp": 1759254962.0
    },
    {
      "title": "DA$^2$: Depth Anything in Any Direction",
      "summary": "Panorama has a full FoV (360$^\\circ\\times$180$^\\circ$), offering a more\ncomplete visual description than perspective images. Thanks to this\ncharacteristic, panoramic depth estimation is gaining increasing traction in 3D\nvision. However, due to the scarcity of panoramic data, previous methods are\noften restricted to in-domain settings, leading to poor zero-shot\ngeneralization. Furthermore, due to the spherical distortions inherent in\npanoramas, many approaches rely on perspective splitting (e.g., cubemaps),\nwhich leads to suboptimal efficiency. To address these challenges, we propose\n$\\textbf{DA}$$^{\\textbf{2}}$: $\\textbf{D}$epth $\\textbf{A}$nything in\n$\\textbf{A}$ny $\\textbf{D}$irection, an accurate, zero-shot generalizable, and\nfully end-to-end panoramic depth estimator. Specifically, for scaling up\npanoramic data, we introduce a data curation engine for generating high-quality\npanoramic depth data from perspective, and create $\\sim$543K panoramic\nRGB-depth pairs, bringing the total to $\\sim$607K. To further mitigate the\nspherical distortions, we present SphereViT, which explicitly leverages\nspherical coordinates to enforce the spherical geometric consistency in\npanoramic image features, yielding improved performance. A comprehensive\nbenchmark on multiple datasets clearly demonstrates DA$^{2}$'s SoTA\nperformance, with an average 38% improvement on AbsRel over the strongest\nzero-shot baseline. Surprisingly, DA$^{2}$ even outperforms prior in-domain\nmethods, highlighting its superior zero-shot generalization. Moreover, as an\nend-to-end solution, DA$^{2}$ exhibits much higher efficiency over fusion-based\napproaches. Both the code and the curated panoramic data will be released.\nProject page: https://depth-any-in-any-dir.github.io/.",
      "url": "http://arxiv.org/abs/2509.26618v1",
      "published_time_eastern_timestamp": 1759254937.0
    },
    {
      "title": "Video Object Segmentation-Aware Audio Generation",
      "summary": "Existing multimodal audio generation models often lack precise user control,\nwhich limits their applicability in professional Foley workflows. In\nparticular, these models focus on the entire video and do not provide precise\nmethods for prioritizing a specific object within a scene, generating\nunnecessary background sounds, or focusing on the wrong objects. To address\nthis gap, we introduce the novel task of video object segmentation-aware audio\ngeneration, which explicitly conditions sound synthesis on object-level\nsegmentation maps. We present SAGANet, a new multimodal generative model that\nenables controllable audio generation by leveraging visual segmentation masks\nalong with video and textual cues. Our model provides users with fine-grained\nand visually localized control over audio generation. To support this task and\nfurther research on segmentation-aware Foley, we propose Segmented Music Solos,\na benchmark dataset of musical instrument performance videos with segmentation\ninformation. Our method demonstrates substantial improvements over current\nstate-of-the-art methods and sets a new standard for controllable,\nhigh-fidelity Foley synthesis. Code, samples, and Segmented Music Solos are\navailable at https://saganet.notion.site",
      "url": "http://arxiv.org/abs/2509.26604v1",
      "published_time_eastern_timestamp": 1759254581.0
    },
    {
      "title": "DeepScientist: Advancing Frontier-Pushing Scientific Findings\n  Progressively",
      "summary": "While previous AI Scientist systems can generate novel findings, they often\nlack the focus to produce scientifically valuable contributions that address\npressing human-defined challenges. We introduce DeepScientist, a system\ndesigned to overcome this by conducting goal-oriented, fully autonomous\nscientific discovery over month-long timelines. It formalizes discovery as a\nBayesian Optimization problem, operationalized through a hierarchical\nevaluation process consisting of \"hypothesize, verify, and analyze\". Leveraging\na cumulative Findings Memory, this loop intelligently balances the exploration\nof novel hypotheses with exploitation, selectively promoting the most promising\nfindings to higher-fidelity levels of validation. Consuming over 20,000 GPU\nhours, the system generated about 5,000 unique scientific ideas and\nexperimentally validated approximately 1100 of them, ultimately surpassing\nhuman-designed state-of-the-art (SOTA) methods on three frontier AI tasks by\n183.7\\%, 1.9\\%, and 7.9\\%. This work provides the first large-scale evidence of\nan AI achieving discoveries that progressively surpass human SOTA on scientific\ntasks, producing valuable findings that genuinely push the frontier of\nscientific discovery. To facilitate further research into this process, we will\nopen-source all experimental logs and system code at\nhttps://github.com/ResearAI/DeepScientist/.",
      "url": "http://arxiv.org/abs/2509.26603v1",
      "published_time_eastern_timestamp": 1759254572.0
    },
    {
      "title": "Graphite: A GPU-Accelerated Mixed-Precision Graph Optimization Framework",
      "summary": "We present Graphite, a GPU-accelerated nonlinear graph optimization\nframework. It provides a CUDA C++ interface to enable the sharing of code\nbetween a realtime application, such as a SLAM system, and its optimization\ntasks. The framework supports techniques to reduce memory usage, including\nin-place optimization, support for multiple floating point types and\nmixed-precision modes, and dynamically computed Jacobians. We evaluate Graphite\non well-known bundle adjustment problems and find that it achieves similar\nperformance to MegBA, a solver specialized for bundle adjustment, while\nmaintaining generality and using less memory. We also apply Graphite to global\nvisual-inertial bundle adjustment on maps generated from stereo-inertial SLAM\ndatasets, and observe speed ups of up to 59x compared to a CPU baseline. Our\nresults indicate that our solver enables faster large-scale optimization on\nboth desktop and resource-constrained devices.",
      "url": "http://arxiv.org/abs/2509.26581v1",
      "published_time_eastern_timestamp": 1759253993.0
    },
    {
      "title": "Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics\n  Research Benchmark",
      "summary": "While large language models (LLMs) with reasoning capabilities are\nprogressing rapidly on high-school math competitions and coding, can they\nreason effectively through complex, open-ended challenges found in frontier\nphysics research? And crucially, what kinds of reasoning tasks do physicists\nwant LLMs to assist with? To address these questions, we present the CritPt\n(Complex Research using Integrated Thinking - Physics Test, pronounced\n\"critical point\"), the first benchmark designed to test LLMs on unpublished,\nresearch-level reasoning tasks that broadly covers modern physics research\nareas, including condensed matter, quantum physics, atomic, molecular & optical\nphysics, astrophysics, high energy physics, mathematical physics, statistical\nphysics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics.\nCritPt consists of 71 composite research challenges designed to simulate\nfull-scale research projects at the entry level, which are also decomposed to\n190 simpler checkpoint tasks for more fine-grained insights. All problems are\nnewly created by 50+ active physics researchers based on their own research.\nEvery problem is hand-curated to admit a guess-resistant and machine-verifiable\nanswer and is evaluated by an automated grading pipeline heavily customized for\nadvanced physics-specific output formats. We find that while current\nstate-of-the-art LLMs show early promise on isolated checkpoints, they remain\nfar from being able to reliably solve full research-scale challenges: the best\naverage accuracy among base models is only 4.0% , achieved by GPT-5 (high),\nmoderately rising to around 10% when equipped with coding tools. Through the\nrealistic yet standardized evaluation offered by CritPt, we highlight a large\ndisconnect between current model capabilities and realistic physics research\ndemands, offering a foundation to guide the development of scientifically\ngrounded AI tools.",
      "url": "http://arxiv.org/abs/2509.26574v1",
      "published_time_eastern_timestamp": 1759253643.0
    },
    {
      "title": "Radio-based Multi-Robot Odometry and Relative Localization",
      "summary": "Radio-based methods such as Ultra-Wideband (UWB) and RAdio Detection And\nRanging (radar), which have traditionally seen limited adoption in robotics,\nare experiencing a boost in popularity thanks to their robustness to harsh\nenvironmental conditions and cluttered environments. This work proposes a\nmulti-robot UGV-UAV localization system that leverages the two technologies\nwith inexpensive and readily-available sensors, such as Inertial Measurement\nUnits (IMUs) and wheel encoders, to estimate the relative position of an aerial\nrobot with respect to a ground robot. The first stage of the system pipeline\nincludes a nonlinear optimization framework to trilaterate the location of the\naerial platform based on UWB range data, and a radar pre-processing module with\nloosely coupled ego-motion estimation which has been adapted for a multi-robot\nscenario. Then, the pre-processed radar data as well as the relative\ntransformation are fed to a pose-graph optimization framework with odometry and\ninter-robot constraints. The system, implemented for the Robotic Operating\nSystem (ROS 2) with the Ceres optimizer, has been validated in\nSoftware-in-the-Loop (SITL) simulations and in a real-world dataset. The\nproposed relative localization module outperforms state-of-the-art closed-form\nmethods which are less robust to noise. Our SITL environment includes a custom\nGazebo plugin for generating realistic UWB measurements modeled after real\ndata. Conveniently, the proposed factor graph formulation makes the system\nreadily extensible to full Simultaneous Localization And Mapping (SLAM).\nFinally, all the code and experimental data is publicly available to support\nreproducibility and to serve as a common open dataset for benchmarking.",
      "url": "http://arxiv.org/abs/2509.26558v1",
      "published_time_eastern_timestamp": 1759253033.0
    },
    {
      "title": "Automated and Scalable SEM Image Analysis of Perovskite Solar Cell\n  Materials via a Deep Segmentation Framework",
      "summary": "Scanning Electron Microscopy (SEM) is indispensable for characterizing the\nmicrostructure of thin films during perovskite solar cell fabrication. Accurate\nidentification and quantification of lead iodide and perovskite phases are\ncritical because residual lead iodide strongly influences crystallization\npathways and defect formation, while the morphology of perovskite grains\ngoverns carrier transport and device stability. Yet current SEM image analysis\nis still largely manual, limiting throughput and consistency. Here, we present\nan automated deep learning-based framework for SEM image segmentation that\nenables precise and efficient identification of lead iodide, perovskite and\ndefect domains across diverse morphologies. Built upon an improved YOLOv8x\narchitecture, our model named PerovSegNet incorporates two novel modules: (i)\nAdaptive Shuffle Dilated Convolution Block, which enhances multi-scale and\nfine-grained feature extraction through group convolutions and channel mixing;\nand (ii) Separable Adaptive Downsampling module, which jointly preserves\nfine-scale textures and large-scale structures for more robust boundary\nrecognition. Trained on an augmented dataset of 10,994 SEM images, PerovSegNet\nachieves a mean Average Precision of 87.25% with 265.4 Giga Floating Point\nOperations, outperforming the baseline YOLOv8x-seg by 4.08%, while reducing\nmodel size and computational load by 24.43% and 25.22%, respectively. Beyond\nsegmentation, the framework provides quantitative grain-level metrics, such as\nlead iodide/perovskite area and count, which can serve as reliable indicators\nof crystallization efficiency and microstructural quality. These capabilities\nestablish PerovSegNet as a scalable tool for real-time process monitoring and\ndata-driven optimization of perovskite thin-film fabrication.The source code is\navailable at:https://github.com/wlyyj/PerovSegNet/tree/master.",
      "url": "http://arxiv.org/abs/2509.26548v1",
      "published_time_eastern_timestamp": 1759252731.0
    },
    {
      "title": "Towards Verified Code Reasoning by LLMs",
      "summary": "While LLM-based agents are able to tackle a wide variety of code reasoning\nquestions, the answers are not always correct. This prevents the agent from\nbeing useful in situations where high precision is desired: (1) helping a\nsoftware engineer understand a new code base, (2) helping a software engineer\nduring code review sessions, and (3) ensuring that the code generated by an\nautomated code generation system meets certain requirements (e.g. fixes a bug,\nimproves readability, implements a feature).\n  As a result of this lack of trustworthiness, the agent's answers need to be\nmanually verified before they can be trusted. Manually confirming responses\nfrom a code reasoning agent requires human effort and can result in slower\ndeveloper productivity, which weakens the assistance benefits of the agent. In\nthis paper, we describe a method to automatically validate the answers provided\nby a code reasoning agent by verifying its reasoning steps. At a very high\nlevel, the method consists of extracting a formal representation of the agent's\nresponse and, subsequently, using formal verification and program analysis\ntools to verify the agent's reasoning steps.\n  We applied this approach to a benchmark set of 20 uninitialized variable\nerrors detected by sanitizers and 20 program equivalence queries. For the\nuninitialized variable errors, the formal verification step was able to\nvalidate the agent's reasoning on 13/20 examples, and for the program\nequivalence queries, the formal verification step successfully caught 6/8\nincorrect judgments made by the agent.",
      "url": "http://arxiv.org/abs/2509.26546v1",
      "published_time_eastern_timestamp": 1759252671.0
    },
    {
      "title": "Voice Evaluation of Reasoning Ability: Diagnosing the Modality-Induced\n  Performance Gap",
      "summary": "We present Voice Evaluation of Reasoning Ability (VERA), a benchmark for\nevaluating reasoning ability in voice-interactive systems under real-time\nconversational constraints. VERA comprises 2,931 voice-native episodes derived\nfrom established text benchmarks and organized into five tracks (Math, Web,\nScience, Long-Context, Factual). Each item is adapted for speech interaction\nwhile preserving reasoning difficulty. VERA enables direct text-voice\ncomparison within model families and supports analysis of how architectural\nchoices affect reliability. We assess 12 contemporary voice systems alongside\nstrong text baselines and observe large, consistent modality gaps: on\ncompetition mathematics a leading text model attains 74.8% accuracy while its\nvoice counterpart reaches 6.1%; macro-averaged across tracks the best text\nmodels achieve 54.0% versus 11.3% for voice. Latency-accuracy analyses reveal a\nlow-latency plateau, where fast voice systems cluster around ~10% accuracy,\nwhile approaching text performance requires sacrificing real-time interaction.\nDiagnostic experiments indicate that common mitigations are insufficient.\nIncreasing \"thinking time\" yields negligible gains; a decoupled cascade that\nseparates reasoning from narration improves accuracy but still falls well short\nof text and introduces characteristic grounding/consistency errors. Failure\nanalyses further show distinct error signatures across native streaming,\nend-to-end, and cascade designs. VERA provides a reproducible testbed and\ntargeted diagnostics for architectures that decouple thinking from speaking,\noffering a principled way to measure progress toward real-time voice assistants\nthat are both fluent and reliably reasoned.",
      "url": "http://arxiv.org/abs/2509.26542v1",
      "published_time_eastern_timestamp": 1759252629.0
    },
    {
      "title": "TASP: Topology-aware Sequence Parallelism",
      "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
      "url": "http://arxiv.org/abs/2509.26541v1",
      "published_time_eastern_timestamp": 1759252527.0
    },
    {
      "title": "OceanGym: A Benchmark Environment for Underwater Embodied Agents",
      "summary": "We introduce OceanGym, the first comprehensive benchmark for ocean underwater\nembodied agents, designed to advance AI in one of the most demanding real-world\nenvironments. Unlike terrestrial or aerial domains, underwater settings present\nextreme perceptual and decision-making challenges, including low visibility,\ndynamic ocean currents, making effective agent deployment exceptionally\ndifficult. OceanGym encompasses eight realistic task domains and a unified\nagent framework driven by Multi-modal Large Language Models (MLLMs), which\nintegrates perception, memory, and sequential decision-making. Agents are\nrequired to comprehend optical and sonar data, autonomously explore complex\nenvironments, and accomplish long-horizon objectives under these harsh\nconditions. Extensive experiments reveal substantial gaps between\nstate-of-the-art MLLM-driven agents and human experts, highlighting the\npersistent difficulty of perception, planning, and adaptability in ocean\nunderwater environments. By providing a high-fidelity, rigorously designed\nplatform, OceanGym establishes a testbed for developing robust embodied AI and\ntransferring these capabilities to real-world autonomous ocean underwater\nvehicles, marking a decisive step toward intelligent agents capable of\noperating in one of Earth's last unexplored frontiers. The code and data are\navailable at https://github.com/OceanGPT/OceanGym.",
      "url": "http://arxiv.org/abs/2509.26536v1",
      "published_time_eastern_timestamp": 1759252172.0
    },
    {
      "title": "TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal\n  Foundation Models in Federated Learning",
      "summary": "Federated Learning (FL), despite demonstrating impressive capabilities in the\ntraining of multiple models in a decentralized manner, has been shown to\nproduce a final model not necessarily well-suited to the needs of each client.\nWhile extensive work has been conducted on how to create tailored personalized\nmodels, called Personalized Federated Learning (PFL), less attention has been\ngiven to personalization via fine-tuning of foundation models with multi-task\nand multi-modal properties. Moreover, there exists a lack of understanding in\nthe literature on how to fine-tune and personalize such models in a setting\nthat is heterogeneous across clients not only in data, but also in tasks and\nmodalities. To address this gap in the literature, we propose TAP (Two-Stage\nAdaptive Personalization), which (i) leverages mismatched model architectures\nbetween the clients and server to selectively conduct replacement operations\nwhen it benefits a client's local tasks and (ii) engages in post-FL knowledge\ndistillation for capturing beneficial general knowledge without compromising\npersonalization. We also introduce the first convergence analysis of the server\nmodel under its modality-task pair architecture, and demonstrate that as the\nnumber of modality-task pairs increases, its ability to cater to all tasks\nsuffers. Through extensive experiments, we demonstrate the effectiveness of our\nproposed algorithm across a variety of datasets and tasks in comparison to a\nmultitude of baselines. Implementation code is publicly available at\nhttps://github.com/lee3296/TAP.",
      "url": "http://arxiv.org/abs/2509.26524v1",
      "published_time_eastern_timestamp": 1759251692.0
    },
    {
      "title": "Neural Network-Based Single-Carrier Joint Communication and Sensing:\n  Loss Design, Constellation Shaping and Precoding",
      "summary": "We investigate the impact of higher-order modulation formats on the sensing\nperformance of single-carrier joint communication and sensing (JCAS) systems.\nSeveral separate components such as a beamformer, a modulator, a target\ndetector, an angle of arrival (AoA) estimator and a communication demapper are\nimplemented as trainable neural networks (NNs). We compare geometrically shaped\nmodulation formats to a classical quadrature amplitude modulation (QAM) scheme.\nWe assess the influence of multi-snapshot sensing and varying signal-to-noise\nratio (SNR) on the overall performance of the autoencoder-based system. To\nimprove the training behavior of the system, we decouple the loss functions\nfrom the respective SNR values and the number of sensing snapshots, using upper\nbounds of the sensing and communication performance, namely the Cram\\'er-Rao\nbound for AoA estimation and the mutual information for communication. The\nNN-based sensing outperforms classical algorithms, such as a Neyman-Pearson\nbased power detector for object detection and ESPRIT for AoA estimation for\nboth the trained constellations and QAM at low SNRs. We show that the gap in\nsensing performance between classical and shaped modulation formats can be\nsignificantly reduced through multi-snapshot sensing. Lastly, we demonstrate\nsystem extension to multi-user multiple-input multiple-output to address the\nimprovement of spatial efficiency when servicing multiple user equipments. Our\ncontribution emphasizes the importance of estimation bounds for training neural\nnetworks, especially when the trained solutions are deployed in varying SNR\nconditions.",
      "url": "http://arxiv.org/abs/2509.26508v1",
      "published_time_eastern_timestamp": 1759250977.0
    }
  ]
}
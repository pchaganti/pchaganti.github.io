{
  "last_updated": "2025-09-30T21:00:53.104068-04:00",
  "papers": [
    {
      "title": "MGM-Omni: Scaling Omni LLMs to Personalized Long-Horizon Speech",
      "summary": "We present MGM-Omni, a unified Omni LLM for omni-modal understanding and\nexpressive, long-horizon speech generation. Unlike cascaded pipelines that\nisolate speech synthesis, MGM-Omni adopts a \"brain-mouth\" design with a\ndual-track, token-based architecture that cleanly decouples multimodal\nreasoning from real-time speech generation. This design enables efficient\ncross-modal interaction and low-latency, streaming speech generation. For\nunderstanding, a unified training strategy coupled with a dual audio encoder\ndesign enables long-form audio perception across diverse acoustic conditions.\nFor generation, a chunk-based parallel decoding scheme narrows the text speech\ntoken-rate gap, accelerating inference and supporting streaming zero-shot voice\ncloning with stable timbre over extended durations. Compared to concurrent\nwork, MGM-Omni achieves these capabilities with markedly data-efficient\ntraining. Extensive experiments demonstrate that MGM-Omni outperforms existing\nopen source models in preserving timbre identity across extended sequences,\nproducing natural and context-aware speech, and achieving superior long-form\naudio and omnimodal understanding. MGM-Omni establishes an efficient,\nend-to-end paradigm for omnimodal understanding and controllable, personalised\nlong-horizon speech generation.",
      "url": "http://arxiv.org/abs/2509.25131v1",
      "published_time_eastern_timestamp": 1759168108.0
    },
    {
      "title": "gCAMB: A GPU-accelerated Boltzmann solver for next-generation\n  cosmological surveys",
      "summary": "Inferring cosmological parameters from Cosmic Microwave Background (CMB) data\nrequires repeated and computationally expensive calculations of theoretical\nangular power spectra using Boltzmann solvers like CAMB. This creates a\nsignificant bottleneck, particularly for non-standard cosmological models and\nthe high-accuracy demands of future surveys. While emulators based on deep\nneural networks can accelerate this process by several orders of magnitude,\nthey first require large, pre-computed training datasets, which are costly to\ngenerate and model-specific. To address this challenge, we introduce gCAMB, a\nversion of the CAMB code ported to GPUs, which preserves all the features of\nthe original CPU-only code. By offloading the most computationally intensive\nmodules to the GPU, gCAMB significantly accelerates the generation of power\nspectra, saving massive computational time, halving the power consumption in\nhigh-accuracy settings and, among other purposes, facilitating the creation of\nextensive training sets needed for robust cosmological analyses. We make the\ngCAMB software available to the community at\nhttps://github.com/lstorchi/CAMB/tree/gpuport.",
      "url": "http://arxiv.org/abs/2509.25110v1",
      "published_time_eastern_timestamp": 1759167591.0
    },
    {
      "title": "Benchmarking ECG Foundational Models: A Reality Check Across Clinical\n  Tasks",
      "summary": "The 12-lead electrocardiogram (ECG) is a long-standing diagnostic tool. Yet\nmachine learning for ECG interpretation remains fragmented, often limited to\nnarrow tasks or datasets. Foundation models promise broader adaptability, but\ntheir generalization across diverse ECG tasks is not well understood. We\nbenchmarked eight ECG foundation models on 26 clinically relevant tasks using\n12 public datasets comprising 1,650 regression and classification targets.\nModels were evaluated under fine-tuning and frozen settings, with scaling\nanalyses across dataset sizes. Results show heterogeneous performance across\ndomains: in the most widely studied domain, adult ECG interpretation, three\nfoundation models consistently outperformed strong supervised baselines. In\ncontrast, ECG-CPC, a compact structured state-space model pretrained on HEEDB,\ndominated other categories where most foundation models failed to surpass\nsupervised learning. Foundation models also displayed distinct scaling\nbehaviors with dataset size, which are critical for small-scale clinical\napplications. Overall, while foundation models show promise for adult ECG\nanalysis, substantial gaps remain in cardiac structure, outcome prediction, and\npatient characterization. Notably, ECG-CPC's strong performance despite being\norders of magnitude smaller and consuming minimal computational resources\nhighlights untapped opportunities for advancing ECG foundation models.",
      "url": "http://arxiv.org/abs/2509.25095v1",
      "published_time_eastern_timestamp": 1759166988.0
    },
    {
      "title": "Unsupervised Representation Learning for 3D Mesh Parameterization with\n  Semantic and Visibility Objectives",
      "summary": "Recent 3D generative models produce high-quality textures for 3D mesh\nobjects. However, they commonly rely on the heavy assumption that input 3D\nmeshes are accompanied by manual mesh parameterization (UV mapping), a manual\ntask that requires both technical precision and artistic judgment. Industry\nsurveys show that this process often accounts for a significant share of asset\ncreation, creating a major bottleneck for 3D content creators. Moreover,\nexisting automatic methods often ignore two perceptually important criteria:\n(1) semantic awareness (UV charts should align semantically similar 3D parts\nacross shapes) and (2) visibility awareness (cutting seams should lie in\nregions unlikely to be seen). To overcome these shortcomings and to automate\nthe mesh parameterization process, we present an unsupervised differentiable\nframework that augments standard geometry-preserving UV learning with semantic-\nand visibility-aware objectives. For semantic-awareness, our pipeline (i)\nsegments the mesh into semantic 3D parts, (ii) applies an unsupervised learned\nper-part UV-parameterization backbone, and (iii) aggregates per-part charts\ninto a unified UV atlas. For visibility-awareness, we use ambient occlusion\n(AO) as an exposure proxy and back-propagate a soft differentiable AO-weighted\nseam objective to steer cutting seams toward occluded regions. By conducting\nqualitative and quantitative evaluations against state-of-the-art methods, we\nshow that the proposed method produces UV atlases that better support texture\ngeneration and reduce perceptible seam artifacts compared to recent baselines.\nOur implementation code is publicly available at:\nhttps://github.com/AHHHZ975/Semantic-Visibility-UV-Param.",
      "url": "http://arxiv.org/abs/2509.25094v1",
      "published_time_eastern_timestamp": 1759166938.0
    },
    {
      "title": "Distributed Quantum Error Correction with Permutation-Invariant\n  Approximate Codes",
      "summary": "Modular quantum computing architectures require error correction schemes that\nremain effective in the presense of noisy inter-processor operations. We\nintroduce a distributed quantum error correction framework based on approximate\ncodes to address this challenge. Our approach enables concatenation of distinct\nlocal codes across modules while allowing logical operations composed primarily\nof processor-local gates. We derive a lower bound and present corresponding\nsimulations which indicate that this nontraditional approach can provide marked\nadvantage over existing approaches in the highly non-uniform error landscape of\na distributed quantum computer. As a concrete realization, we present encoding\nand decoding circuits for the permutation-invariant W- state code and propose\nefficient methods for its preparation. These results highlight the potential of\napproximate distributed error correction strategies for scalable, modular,\nfault-tolerant quantum computation.",
      "url": "http://arxiv.org/abs/2509.25093v1",
      "published_time_eastern_timestamp": 1759166919.0
    },
    {
      "title": "Scaling Generalist Data-Analytic Agents",
      "summary": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.",
      "url": "http://arxiv.org/abs/2509.25084v1",
      "published_time_eastern_timestamp": 1759166588.0
    },
    {
      "title": "Towards a Certificate of Trust: Task-Aware OOD Detection for Scientific\n  AI",
      "summary": "Data-driven models are increasingly adopted in critical scientific fields\nlike weather forecasting and fluid dynamics. These methods can fail on\nout-of-distribution (OOD) data, but detecting such failures in regression tasks\nis an open challenge. We propose a new OOD detection method based on estimating\njoint likelihoods using a score-based diffusion model. This approach considers\nnot just the input but also the regression model's prediction, providing a\ntask-aware reliability score. Across numerous scientific datasets, including\nPDE datasets, satellite imagery and brain tumor segmentation, we show that this\nlikelihood strongly correlates with prediction error. Our work provides a\nfoundational step towards building a verifiable 'certificate of trust', thereby\noffering a practical tool for assessing the trustworthiness of AI-based\nscientific predictions. Our code is publicly available at\nhttps://github.com/bogdanraonic3/OOD_Detection_ScientificML",
      "url": "http://arxiv.org/abs/2509.25080v1",
      "published_time_eastern_timestamp": 1759166485.0
    },
    {
      "title": "UniLat3D: Geometry-Appearance Unified Latents for Single-Stage 3D\n  Generation",
      "summary": "High-fidelity 3D asset generation is crucial for various industries. While\nrecent 3D pretrained models show strong capability in producing realistic\ncontent, most are built upon diffusion models and follow a two-stage pipeline\nthat first generates geometry and then synthesizes appearance. Such a decoupled\ndesign tends to produce geometry-texture misalignment and non-negligible cost.\nIn this paper, we propose UniLat3D, a unified framework that encodes geometry\nand appearance in a single latent space, enabling direct single-stage\ngeneration. Our key contribution is a geometry-appearance Unified VAE, which\ncompresses high-resolution sparse features into a compact latent representation\n-- UniLat. UniLat integrates structural and visual information into a dense\nlow-resolution latent, which can be efficiently decoded into diverse 3D\nformats, e.g., 3D Gaussians and meshes. Based on this unified representation,\nwe train a single flow-matching model to map Gaussian noise directly into\nUniLat, eliminating redundant stages. Trained solely on public datasets,\nUniLat3D produces high-quality 3D assets in seconds from a single image,\nachieving superior appearance fidelity and geometric quality. More demos \\&\ncode are available at https://unilat3d.github.io/",
      "url": "http://arxiv.org/abs/2509.25079v1",
      "published_time_eastern_timestamp": 1759166483.0
    },
    {
      "title": "BRIDGE -- Building Reinforcement-Learning Depth-to-Image Data Generation\n  Engine for Monocular Depth Estimation",
      "summary": "Monocular Depth Estimation (MDE) is a foundational task for computer vision.\nTraditional methods are limited by data scarcity and quality, hindering their\nrobustness. To overcome this, we propose BRIDGE, an RL-optimized depth-to-image\n(D2I) generation framework that synthesizes over 20M realistic and\ngeometrically accurate RGB images, each intrinsically paired with its ground\ntruth depth, from diverse source depth maps. Then we train our depth estimation\nmodel on this dataset, employing a hybrid supervision strategy that integrates\nteacher pseudo-labels with ground truth depth for comprehensive and robust\ntraining. This innovative data generation and training paradigm enables BRIDGE\nto achieve breakthroughs in scale and domain diversity, consistently\noutperforming existing state-of-the-art approaches quantitatively and in\ncomplex scene detail capture, thereby fostering general and robust depth\nfeatures. Code and models are available at\nhttps://dingning-liu.github.io/bridge.github.io/.",
      "url": "http://arxiv.org/abs/2509.25077v1",
      "published_time_eastern_timestamp": 1759166385.0
    },
    {
      "title": "GEM: 3D Gaussian Splatting for Efficient and Accurate Cryo-EM\n  Reconstruction",
      "summary": "Cryo-electron microscopy (cryo-EM) has become a central tool for\nhigh-resolution structural biology, yet the massive scale of datasets (often\nexceeding 100k particle images) renders 3D reconstruction both computationally\nexpensive and memory intensive. Traditional Fourier-space methods are efficient\nbut lose fidelity due to repeated transforms, while recent real-space\napproaches based on neural radiance fields (NeRFs) improve accuracy but incur\ncubic memory and computation overhead. Therefore, we introduce GEM, a novel\ncryo-EM reconstruction framework built on 3D Gaussian Splatting (3DGS) that\noperates directly in real-space while maintaining high efficiency. Instead of\nmodeling the entire density volume, GEM represents proteins with compact 3D\nGaussians, each parameterized by only 11 values. To further improve the\ntraining efficiency, we designed a novel gradient computation to 3D Gaussians\nthat contribute to each voxel. This design substantially reduced both memory\nfootprint and training cost. On standard cryo-EM benchmarks, GEM achieves up to\n48% faster training and 12% lower memory usage compared to state-of-the-art\nmethods, while improving local resolution by as much as 38.8%. These results\nestablish GEM as a practical and scalable paradigm for cryo-EM reconstruction,\nunifying speed, efficiency, and high-resolution accuracy. Our code is available\nat https://github.com/UNITES-Lab/GEM.",
      "url": "http://arxiv.org/abs/2509.25075v1",
      "published_time_eastern_timestamp": 1759166273.0
    },
    {
      "title": "Unsourced Random Access",
      "summary": "Current wireless networks are designed to optimize spectral efficiency for\nhuman users, who typically require sustained connections for high-data-rate\napplications like file transfers and video streaming. However, these networks\nare increasingly inadequate for the emerging era of machine-type communications\n(MTC). With a vast number of devices exhibiting sporadic traffic patterns\nconsisting of short packets, the grant-based multiple access procedures\nutilized by existing networks lead to significant delays and inefficiencies. To\naddress this issue the unsourced random access (URA) paradigm has been\nproposed. This paradigm assumes the devices to share a common encoder thus\nsimplifying the reception process by eliminating the identification procedure.\nThe URA paradigm not only addresses the computational challenges but it also\nconsiders the random access (RA) as a coding problem, i.e., takes into account\nboth medium access protocols and physical layer effects. In this monograph we\nprovide a comprehensive overview of the URA problem in noisy channels, with the\nmain task being to explain the major ideas rather than to list all existing\nsolutions.",
      "url": "http://arxiv.org/abs/2509.25074v1",
      "published_time_eastern_timestamp": 1759166242.0
    },
    {
      "title": "AlphaSAGE: Structure-Aware Alpha Mining via GFlowNets for Robust\n  Exploration",
      "summary": "The automated mining of predictive signals, or alphas, is a central challenge\nin quantitative finance. While Reinforcement Learning (RL) has emerged as a\npromising paradigm for generating formulaic alphas, existing frameworks are\nfundamentally hampered by a triad of interconnected issues. First, they suffer\nfrom reward sparsity, where meaningful feedback is only available upon the\ncompletion of a full formula, leading to inefficient and unstable exploration.\nSecond, they rely on semantically inadequate sequential representations of\nmathematical expressions, failing to capture the structure that determine an\nalpha's behavior. Third, the standard RL objective of maximizing expected\nreturns inherently drives policies towards a single optimal mode, directly\ncontradicting the practical need for a diverse portfolio of non-correlated\nalphas. To overcome these challenges, we introduce AlphaSAGE (Structure-Aware\nAlpha Mining via Generative Flow Networks for Robust Exploration), a novel\nframework is built upon three cornerstone innovations: (1) a structure-aware\nencoder based on Relational Graph Convolutional Network (RGCN); (2) a new\nframework with Generative Flow Networks (GFlowNets); and (3) a dense,\nmulti-faceted reward structure. Empirical results demonstrate that AlphaSAGE\noutperforms existing baselines in mining a more diverse, novel, and highly\npredictive portfolio of alphas, thereby proposing a new paradigm for automated\nalpha mining. Our code is available at https://github.com/BerkinChen/AlphaSAGE.",
      "url": "http://arxiv.org/abs/2509.25055v1",
      "published_time_eastern_timestamp": 1759165567.0
    },
    {
      "title": "Advantage Weighted Matching: Aligning RL with Pretraining in Diffusion\n  Models",
      "summary": "Reinforcement Learning (RL) has emerged as a central paradigm for advancing\nLarge Language Models (LLMs), where pre-training and RL post-training share the\nsame log-likelihood formulation. In contrast, recent RL approaches for\ndiffusion models, most notably Denoising Diffusion Policy Optimization (DDPO),\noptimize an objective different from the pretraining objectives--score/flow\nmatching loss. In this work, we establish a novel theoretical analysis: DDPO is\nan implicit form of score/flow matching with noisy targets, which increases\nvariance and slows convergence. Building on this analysis, we introduce\n\\textbf{Advantage Weighted Matching (AWM)}, a policy-gradient method for\ndiffusion. It uses the same score/flow-matching loss as pretraining to obtain a\nlower-variance objective and reweights each sample by its advantage. In effect,\nAWM raises the influence of high-reward samples and suppresses low-reward ones\nwhile keeping the modeling objective identical to pretraining. This unifies\npretraining and RL conceptually and practically, is consistent with\npolicy-gradient theory, reduces variance, and yields faster convergence. This\nsimple yet effective design yields substantial benefits: on GenEval, OCR, and\nPickScore benchmarks, AWM delivers up to a $24\\times$ speedup over Flow-GRPO\n(which builds on DDPO), when applied to Stable Diffusion 3.5 Medium and FLUX,\nwithout compromising generation quality. Code is available at\nhttps://github.com/scxue/advantage_weighted_matching.",
      "url": "http://arxiv.org/abs/2509.25050v1",
      "published_time_eastern_timestamp": 1759165340.0
    },
    {
      "title": "Large Language Models for Software Testing: A Research Roadmap",
      "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most\nsignificant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks\nsuch as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of\nnew contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no\nprior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we\naim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the\nmost promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature\nreview, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing\nthe open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the\nwhole software testing field.",
      "url": "http://arxiv.org/abs/2509.25043v1",
      "published_time_eastern_timestamp": 1759165101.0
    },
    {
      "title": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for\n  Efficient Distributed MoE Inference",
      "summary": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance.",
      "url": "http://arxiv.org/abs/2509.25041v1",
      "published_time_eastern_timestamp": 1759165053.0
    },
    {
      "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence\n  Instruct",
      "summary": "Fast generation of language texts is the holy grail that people pursue in the\nAI era. In this work, we introduced Discrete Diffusion Divergence Instruct\n(DiDi-Instruct), a training-based method that leads to fast language generation\nmodels by initializing from a pre-trained (masked) discrete diffusion language\nmodel (dLLM). The resulting DiDi-Instruct model outperforms the dLLM\ncounterparts and the GPT-2 baseline with 64x acceleration. In the theoretical\npart of the paper, we build the foundation of DiDi-Instruct in a framework of\nintegral KL-divergence minimization, with practical training algorithms. We\nalso introduce techniques like grouped reward normalization, intermediate-state\nmatching, and the reward-guided ancestral sampler (RGAS) that significantly\nimprove the training stability, the model coverage, and the inference\nperformances. On OpenWebText, DiDi-Instruct outperforms all accelerated\nlanguage generation models as well as the GPT-2 baseline and the standard\ndLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128\nNFEs). These performance gains are accomplished with a negligible entropy loss\nof about 1% and 20x less additional training wall-clock time. We further\nvalidate the robustness and effectiveness of DiDi-Instruct through extensive\nablation studies, model scaling, and the generation of discrete protein\nsequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct.",
      "url": "http://arxiv.org/abs/2509.25035v1",
      "published_time_eastern_timestamp": 1759164944.0
    },
    {
      "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning",
      "summary": "Few-shot learning (FSL) aims to recognize novel concepts from only a few\nlabeled support samples. Recent studies enhance support features by\nincorporating additional semantic information or designing complex semantic\nfusion modules. However, they still suffer from hallucinating semantics that\ncontradict the visual evidence due to the lack of grounding in actual\ninstances, resulting in noisy guidance and costly corrections. To address these\nissues, we propose a novel framework, bridging Vision and Text with LLMs for\nFew-Shot Learning (VT-FSL), which constructs precise cross-modal prompts\nconditioned on Large Language Models (LLMs) and support images, seamlessly\nintegrating them through a geometry-aware alignment. It mainly consists of\nCross-modal Iterative Prompting (CIP) and Cross-modal Geometric Alignment\n(CGA). Specifically, the CIP conditions an LLM on both class names and support\nimages to generate precise class descriptions iteratively in a single\nstructured reasoning pass. These descriptions not only enrich the semantic\nunderstanding of novel classes but also enable the zero-shot synthesis of\nsemantically consistent images. The descriptions and synthetic images act\nrespectively as complementary textual and visual prompts, providing high-level\nclass semantics and low-level intra-class diversity to compensate for limited\nsupport data. Furthermore, the CGA jointly aligns the fused textual, support,\nand synthetic visual representations by minimizing the kernelized volume of the\n3-dimensional parallelotope they span. It captures global and nonlinear\nrelationships among all representations, enabling structured and consistent\nmultimodal integration. The proposed VT-FSL method establishes new\nstate-of-the-art performance across ten diverse benchmarks, including standard,\ncross-domain, and fine-grained few-shot learning scenarios. Code is available\nat https://github.com/peacelwh/VT-FSL.",
      "url": "http://arxiv.org/abs/2509.25033v1",
      "published_time_eastern_timestamp": 1759164767.0
    },
    {
      "title": "Bayesian Surrogates for Risk-Aware Pre-Assessment of Aging Bridge\n  Portfolios",
      "summary": "Aging infrastructure portfolios pose a critical resource allocation\nchallenge: deciding which structures require intervention and which can safely\nremain in service. Structural assessments must balance the trade-off between\ncheaper, conservative analysis methods and accurate but costly simulations that\ndo not scale portfolio-wide. We propose Bayesian neural network (BNN)\nsurrogates for rapid structural pre-assessment of worldwide common bridge\ntypes, such as reinforced concrete frame bridges. Trained on a large-scale\ndatabase of non-linear finite element analyses generated via a parametric\npipeline and developed based on the Swiss Federal Railway's bridge portfolio,\nthe models accurately and efficiently estimate high-fidelity structural\nanalysis results by predicting code compliance factors with calibrated\nepistemic uncertainty. Our BNN surrogate enables fast, uncertainty-aware\ntriage: flagging likely critical structures and providing guidance where\nrefined analysis is pertinent. We demonstrate the framework's effectiveness in\na real-world case study of a railway underpass, showing its potential to\nsignificantly reduce costs and emissions by avoiding unnecessary analyses and\nphysical interventions across entire infrastructure portfolios.",
      "url": "http://arxiv.org/abs/2509.25031v1",
      "published_time_eastern_timestamp": 1759164662.0
    },
    {
      "title": "STAGE: Stable and Generalizable GRPO for Autoregressive Image Generation",
      "summary": "Reinforcement learning has recently been explored to improve text-to-image\ngeneration, yet applying existing GRPO algorithms to autoregressive (AR) image\nmodels remains challenging. The instability of the training process easily\ndisrupts the pretrained model capability during long runs, resulting in\nmarginal gains, degraded image quality, and poor generalization. In this work,\nwe revisit GRPO for AR image generation and identify two key issues:\ncontradictory gradients from unnecessary tokens and unstable policy entropy\ndynamics. To address these, we introduce STAGE, a stable and generalizable\nframework that leverages two targeted solutions: 1) Advantage/KL reweighting.\nSimilarity-aware reweighting to alleviate conflicting updates; and 2) Entropy\nreward. An entropy-based reward corresponding to reference model to stabilize\nlearning. With the help of alleviating conflicts between tokens and an entropy\nreward for stabilizing training, we reduce disruption of the pretrained\ndistribution and mitigate reward hacking, which in turn improves generalization\nand transfer better to other benchmarks. Experiments across multiple benchmarks\nshow that STAGE consistently improves visual quality, stability, and cross-task\ngeneralization compared to baseline GRPO.",
      "url": "http://arxiv.org/abs/2509.25027v1",
      "published_time_eastern_timestamp": 1759164621.0
    },
    {
      "title": "GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing\n  Reasoning",
      "summary": "Recent advances in reinforcement learning (RL) have delivered strong\nreasoning capabilities in natural image domains, yet their potential for Earth\nObservation (EO) remains largely unexplored. EO tasks introduce unique\nchallenges, spanning referred object detection, image or region captioning,\nchange detection, grounding, and temporal analysis, that demand task aware\nreasoning. We propose a novel post training framework that incorporates task\naware rewards to enable effective adaptation of reasoning based RL models to\ndiverse EO tasks. This training strategy enhances reasoning capabilities for\nremote sensing images, stabilizes optimization, and improves robustness.\nExtensive experiments across multiple EO benchmarks show consistent performance\ngains over state of the art generic and specialized vision language models.\nCode and models will be released publicly at\nhttps://mustansarfiaz.github.io/GeoVLM-R1/ .",
      "url": "http://arxiv.org/abs/2509.25026v1",
      "published_time_eastern_timestamp": 1759164534.0
    }
  ]
}
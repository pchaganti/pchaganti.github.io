{
  "last_updated": "2025-08-12T05:15:05.403583-04:00",
  "papers": [
    {
      "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting",
      "summary": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task\nthat aims to segment target objects in a 3D Gaussian scene based on natural\nlanguage descriptions, which often contain spatial relationships or object\nattributes. This task requires the model to identify newly described objects\nthat may be occluded or not directly visible in a novel view, posing a\nsignificant challenge for 3D multi-modal understanding. Developing this\ncapability is crucial for advancing embodied AI. To support research in this\narea, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that\n3D multi-modal understanding and spatial relationship modeling are key\nchallenges for R3DGS. To address these challenges, we propose ReferSplat, a\nframework that explicitly models 3D Gaussian points with natural language\nexpressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art\nperformance on both the newly proposed R3DGS task and 3D open-vocabulary\nsegmentation benchmarks. Dataset and code are available at\nhttps://github.com/heshuting555/ReferSplat.",
      "url": "http://arxiv.org/abs/2508.08252v1",
      "published_time_eastern_timestamp": 1754935170.0
    },
    {
      "title": "Sensitivity toward dark matter annihilation imprints on 21-cm signal\n  with SKA-Low: A convolutional neural network approach",
      "summary": "This study investigates the sensitivity of the radio interferometers to\nidentify imprints of spatially inhomogeneous dark matter annihilation\nsignatures in the 21-cm signal during the pre-reionization era. We focus on the\nupcoming low-mode survey of the Square Kilometre Array (SKA-Low) telescope.\nUsing CNNs, we analyze simulated 3D 21-cm differential brightness temperature\nmaps generated via the DM21cm code, which is based on 21cmFAST and DarkHistory,\nto distinguish between spatially homogeneous and inhomogeneous energy\ninjection/deposition scenarios arising from dark matter annihilation. The\ninhomogeneous case accounts for local dark matter density contrasts and gas\nproperties, such as thermal and ionization states, while the homogeneous model\nassumes uniform energy deposition. Our study focuses on two primary\nannihilation channels to electron-positron pairs ($e^+e^-$) and photons (\n$\\gamma \\gamma$), exploring dark matter masses from 1 MeV to 100 MeV and a\nrange of annihilation cross-sections. For $\\gamma \\gamma$ channel, the\ndistinction across dark matter models is less pronounced due to the larger mean\nfree path of the emitted photons, resulting in a more uniform energy\ndeposition. For $e^+e^-$ channel, the results indicate that the CNNs can\neffectively differentiate between the inhomogeneous and homogeneous cases.\nDespite observational challenges, the results demonstrate that these effects\nremain detectable even after incorporating noise from next-generation radio\ninterferometers, such as the SKA. We find that the inhomogeneous dark matter\nannihilation models can leave measurable imprints on the 21-cm signal maps\ndistinguishable from the homogeneous scenarios for the dark matter masses\n$m_{\\rm DM}=1$ MeV and the annihilation cross-sections of $\\geq 5 \\times\n10^{-30}~{\\rm cm^3/sec}$ ($\\geq 5 \\times 10^{-29}~{\\rm cm^3/sec}$ for $m_{\\rm\nDM}=100$ MeV) for moderate SKA-Low noise.",
      "url": "http://arxiv.org/abs/2508.08251v1",
      "published_time_eastern_timestamp": 1754935168.0
    },
    {
      "title": "Composable Quantum Fault-Tolerance",
      "summary": "Proving threshold theorems for fault-tolerant quantum computation is a\nburdensome endeavor with many moving parts that come together in relatively\nformulaic but lengthy ways. It is difficult and rare to combine elements from\nmultiple papers into a single formal threshold proof, due to the use of\ndifferent measures of fault-tolerance. In this work, we introduce composable\nfault-tolerance, a framework that decouples the probabilistic analysis of the\nnoise distribution from the combinatorial analysis of circuit correctness, and\nenables threshold proofs to compose independently analyzed gadgets easily and\nrigorously. Within this framework, we provide a library of standard and\ncommonly used gadgets such as memory and logic implemented by constant-depth\ncircuits for quantum low-density parity check codes and distillation. As sample\napplications, we explicitly write down a threshold proof for computation with\nsurface code and re-derive the constant space-overhead fault-tolerant scheme of\nGottesman using gadgets from this library. We expect that future\nfault-tolerance proofs may focus on the analysis of novel techniques while\nleaving the standard components to the composable fault-tolerance framework,\nwith the formal proof following the intuitive ``napkin math'' exactly.",
      "url": "http://arxiv.org/abs/2508.08246v1",
      "published_time_eastern_timestamp": 1754935094.0
    },
    {
      "title": "Coupled Time-Dependent Proton Acceleration and Leptonic-Hadronic\n  Radiation in Turbulent Supermassive Black Hole Coronae",
      "summary": "Turbulent coronae of supermassive black holes can accelerate non-thermal\nparticles to high energies and produce observable radiation, but capturing this\nprocess is challenging due to comparable timescales of acceleration, cooling,\nand the development of cascades. We present a time-dependent numerical\nframework that self-consistently couples proton acceleration -- modeled by the\nFokker-Planck equation -- with leptonic-hadronic radiation. For the\nneutrino-emitting Seyfert galaxy NGC 1068, we reproduce the neutrino spectrum\nobserved by IceCube, while satisfying gamma-ray constraints. We also consider a\ntransient corona scenario, potentially emerging in non-jetted tidal disruption\nevents like AT 2019dsg, and show that early-stage cascade feedback can impact\nproton acceleration and radiation processes in weaker coronae, producing\ndelayed optical/ultraviolet, X-ray, and neutrino emissions of $\\mathcal\nO(100~\\rm d)$. This flexible code efficiently models multi-messenger signals\nfrom both steady and transient astrophysical sources, providing insights in\ncombining particle acceleration and radiation mechanisms.",
      "url": "http://arxiv.org/abs/2508.08233v1",
      "published_time_eastern_timestamp": 1754934622.0
    },
    {
      "title": "Mamba-FCS: Joint Spatio- Frequency Feature Fusion, Change-Guided\n  Attention, and SeK Loss for Enhanced Semantic Change Detection in Remote\n  Sensing",
      "summary": "Semantic Change Detection (SCD) from remote sensing imagery requires models\nbalancing extensive spatial context, computational efficiency, and sensitivity\nto class-imbalanced land-cover transitions. While Convolutional Neural Networks\nexcel at local feature extraction but lack global context, Transformers provide\nglobal modeling at high computational costs. Recent Mamba architectures based\non state-space models offer compelling solutions through linear complexity and\nefficient long-range modeling. In this study, we introduce Mamba-FCS, a SCD\nframework built upon Visual State Space Model backbone incorporating, a Joint\nSpatio-Frequency Fusion block incorporating log-amplitude frequency domain\nfeatures to enhance edge clarity and suppress illumination artifacts, a\nChange-Guided Attention (CGA) module that explicitly links the naturally\nintertwined BCD and SCD tasks, and a Separated Kappa (SeK) loss tailored for\nclass-imbalanced performance optimization. Extensive evaluation on SECOND and\nLandsat-SCD datasets shows that Mamba-FCS achieves state-of-the-art metrics,\n88.62% Overall Accuracy, 65.78% F_scd, and 25.50% SeK on SECOND, 96.25% Overall\nAccuracy, 89.27% F_scd, and 60.26% SeK on Landsat-SCD. Ablation analyses\nconfirm distinct contributions of each novel component, with qualitative\nassessments highlighting significant improvements in SCD. Our results underline\nthe substantial potential of Mamba architectures, enhanced by proposed\ntechniques, setting a new benchmark for effective and scalable semantic change\ndetection in remote sensing applications. The complete source code,\nconfiguration files, and pre-trained models will be publicly available upon\npublication.",
      "url": "http://arxiv.org/abs/2508.08232v1",
      "published_time_eastern_timestamp": 1754934599.0
    },
    {
      "title": "LL3M: Large Language 3D Modelers",
      "summary": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.",
      "url": "http://arxiv.org/abs/2508.08228v1",
      "published_time_eastern_timestamp": 1754934482.0
    },
    {
      "title": "Industrial Viewpoints on RAN Technologies for 6G",
      "summary": "6G standardization is to start imminently, with commercial deployments\nexpected before 2030. Its technical components and performance requirements are\nthe focus of this article. Our emphasis is on the 6G radio access, especially\nMIMO, AI, waveforms, coding, signal constellations and integration with\nnon-terrestrial networks. Whilst standardization has not yet formally started,\nthe scope of the 6G study items has been defined. Our predictions in this paper\nare speculative as there are no results of the study yet, but our views are\nguided by implementation and deployment aspects. We expect that the views here\nwill guide researchers and industry practitioners.",
      "url": "http://arxiv.org/abs/2508.08225v1",
      "published_time_eastern_timestamp": 1754934264.0
    },
    {
      "title": "Cross-Subject and Cross-Montage EEG Transfer Learning via Individual\n  Tangent Space Alignment and Spatial-Riemannian Feature Fusion",
      "summary": "Personalised music-based interventions offer a powerful means of supporting\nmotor rehabilitation by dynamically tailoring auditory stimuli to provide\nexternal timekeeping cues, modulate affective states, and stabilise gait\npatterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for\nadapting these interventions across individuals. However, inter-subject\nvariability in EEG signals, further compounded by movement-induced artefacts\nand motor planning differences, hinders the generalisability of BCIs and\nresults in lengthy calibration processes. We propose Individual Tangent Space\nAlignment (ITSA), a novel pre-alignment strategy incorporating subject-specific\nrecentering, distribution matching, and supervised rotational alignment to\nenhance cross-subject generalisation. Our hybrid architecture fuses Regularised\nCommon Spatial Patterns (RCSP) with Riemannian geometry in parallel and\nsequential configurations, improving class separability while maintaining the\ngeometric structure of covariance matrices for robust statistical computation.\nUsing leave-one-subject-out cross-validation, `ITSA' demonstrates significant\nperformance improvements across subjects and conditions. The parallel fusion\napproach shows the greatest enhancement over its sequential counterpart, with\nrobust performance maintained across varying data conditions and electrode\nconfigurations. The code will be made publicly available at the time of\npublication.",
      "url": "http://arxiv.org/abs/2508.08216v1",
      "published_time_eastern_timestamp": 1754933837.0
    },
    {
      "title": "Color it, Code it, Cancel it: k-local dynamical decoupling from\n  classical additive codes",
      "summary": "Dynamical decoupling is a central technique in quantum computing for actively\nsuppressing decoherence and systematic imperfections through sequences of\nsingle-qubit operations. Conventional sequences typically aim to completely\nfreeze system dynamics, often resulting in long protocols whose length scales\nexponentially with system size. In this work, we introduce a general framework\nfor constructing time-optimal, selectively-tailored sequences that remove only\nspecific local interactions. By combining techniques from graph coloring and\nclassical coding theory, our approach enables compact and hardware-tailored\nsequences across diverse qubit platforms, efficiently canceling undesired\nHamiltonian terms while preserving target interactions. This opens up broad\napplications in quantum computing and simulation. At the core of our method is\na mapping between dynamical decoupling sequence design and error-detecting\ncodes, which allows us to leverage powerful coding-theoretic tools to construct\ncustomized sequences. To overcome exponential overheads, we exploit symmetries\nin colored interaction hypergraphs, extending graph-coloring strategies to\narbitrary many-body Hamiltonians. We demonstrate the effectiveness of our\nframework through concrete examples, including compact sequences that suppress\nresidual ZZ and ZZZ interactions in superconducting qubits and Heisenberg\nexchange coupling in spin qubits. We also show how it enables Hamiltonian\nengineering by simulating the anisotropic Kitaev honeycomb model using only\nisotropic Heisenberg interactions.",
      "url": "http://arxiv.org/abs/2508.08213v1",
      "published_time_eastern_timestamp": 1754933699.0
    },
    {
      "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling",
      "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.",
      "url": "http://arxiv.org/abs/2508.08211v1",
      "published_time_eastern_timestamp": 1754933598.0
    },
    {
      "title": "The Yin-Yang Magnetic Flux Eruption (Yin-Yang-MFE) Code: A Global Corona\n  Magnetohydrodynamic Code with the Yin-Yang grid",
      "summary": "We describe the numerical algorithms of a global magnetohydrodynamic (MHD)\ncode utilizing the Yin-Yang grid, called the Yin-Yang Magnetic Flux Eruption\n(Yin-Yang-MFE) code, suitable for modeling the large-scale dynamical processes\nof the solar corona and the solar wind. It is a single-fluid MHD code taking\ninto account the non-adiabatic effects of the solar corona, including the\nelectron heat conduction, optically thin radiative cooling, and empirical\ncoronal heating. We describe the numerical algorithms used to solve the set of\nMHD equations (with the semi-relativistic correction, or the Boris correction)\nin each of the partial spherical shell Yin Yang domains, and the method for\nupdating the boundary conditions in the ghost-zones of the two overlapping\ndomains with the code parallelized with the message passing interface (MPI). We\nvalidate the code performance with a set of standard test problems, and finally\npresent a solar wind solution with a dipolar magnetic flux distribution at the\nsolar surface, representative of solar minimum configuration.",
      "url": "http://arxiv.org/abs/2508.08210v1",
      "published_time_eastern_timestamp": 1754933561.0
    },
    {
      "title": "Differential rotation of solar Î±-sunspots and implications for\n  stellar light curves",
      "summary": "Differential rotation is a key driver of magnetic activity and dynamo\nprocesses in the Sun and other stars, especially as the rate differs across the\nsolar layers, but also in active regions. We aim to accurately quantify the\nvelocity at which round {\\alpha}-spots traverse the solar disk as a function of\ntheir latitude, and compare these rates to those of the quiet-Sun and other\nsunspot types. We then extend this work to other stars and investigate how\ndifferential rotation affects the modulation of stellar light curves by\nintroducing a generalized stellar differential rotation law. We manually\nidentify and track 105 {\\alpha}-sunspots in the 6173 {\\AA} continuum using the\nHelioseismic and Magnetic Imager (HMI) aboard the Solar Dynamics Observatory\n(SDO). We measure the angular velocities of each spot through center-of-mass\nand geometric ellipse-fitting methods to derive a differential rotation law for\nround {\\alpha}-sunspots. Results. Using over a decade of HMI data we derive a\ndifferential rotation law for {\\alpha}-sunspots. When compared to previous\nmeasurements we find that {\\alpha}-sunspots rotate 1.56% faster than the\nsurrounding quiet-Sun, but 1.35% slower than the average sunspot population.\nThis supports the hypothesis that the depth at which flux tubes are anchored\ninfluences sunspot motions across the solar disk. We extend this analysis to\nother stars by introducing a scaling law based on the rotation rates of these\nstars. This scaling law is implemented into the Stellar Activity Grid for\nExoplanets (SAGE) code to illustrate how differential rotation alters the\nphotometric modulation of active stars. Our findings emphasize the necessity of\nconsidering differential rotation effects when modeling stellar activity and\nexoplanet transit signatures",
      "url": "http://arxiv.org/abs/2508.08196v1",
      "published_time_eastern_timestamp": 1754932499.0
    },
    {
      "title": "Single-Shot Decoding and Fault-tolerant Gates with Trivariate Tricycle\n  Codes",
      "summary": "While quantum low-density parity check (qLDPC) codes are a low-overhead means\nof quantum information storage, it is valuable for quantum codes to possess\nfault-tolerant features beyond this resource efficiency. In this work, we\nintroduce trivariate tricycle (TT) codes, qLDPC codes that combine several\ndesirable features: high thresholds under a circuit-level noise model, partial\nsingle-shot decodability for low-time-overhead decoding, a large set of\ntransversal Clifford gates and automorphisms within and between code blocks,\nand (for several sub-constructions) constant-depth implementations of a\n(non-Clifford) $CCZ$ gate. TT codes are CSS codes based on a length-3 chain\ncomplex, and are defined from three trivariate polynomials, with the 3D toric\ncode (3DTC) belonging to this construction. We numerically search for TT codes\nand find several candidates with improved parameters relative to the 3DTC,\nusing up to 48$\\times$ fewer data qubits as equivalent 3DTC encodings. We\nconstruct syndrome-extraction circuits for these codes and numerically\ndemonstrate single-shot decoding in the X error channel in both\nphenomenological and circuit-level noise models. Under circuit-level noise, TT\ncodes have a threshold of $0.3\\%$ in the Z error channel and $1\\%$ in the X\nerror channel (with single-shot decoding). All TT codes possess several\ntransversal $CZ$ gates that can partially address logical qubits between two\ncode blocks. Additionally, the codes possess a large set of automorphisms that\ncan perform Clifford gates within a code block. Finally, we establish several\nTT code polynomial constructions that allows for a constant-depth\nimplementation of logical $CCZ$ gates. We find examples of error-correcting and\nerror-detecting codes using these constructions whose parameters out-perform\nthose of the 3DTC, using up to $4\\times$ fewer data qubits for\nequivalent-distance 3DTC encodings.",
      "url": "http://arxiv.org/abs/2508.08191v1",
      "published_time_eastern_timestamp": 1754932283.0
    },
    {
      "title": "Characterization of syndrome-dependent logical noise in detector regions",
      "summary": "Characterizing how quantum error correction circuits behave under realistic\nhardware noise is essential for testing the premises that enable scalable fault\ntolerance. Logical error rates conditioned on syndrome outcomes are needed to\nenable noise-aware decoding and validate threshold-relevant assumptions. We\nintroduce a protocol to directly estimate the logical Pauli channels (and pure\nerrors) associated with detector regions formed of two or more syndrome\nextraction gadgets, conditioned on observing a particular parity in the\nsyndrome outcomes. The method is SPAM-robust and most suitable for flag-based\nsyndrome measurement schemes. For classical processing of the experimental data\nwe implement a Bayesian modelling approach. We validate this new protocol on a\nsmall error-detecting code using Quantinuum H1-1, a trapped-ion device, and\ndemonstrate that several noise diagnostic tests for fault tolerance improve\nsignificantly when using noise tailoring and mitigation strategies, such as\nswapped measurements for leakage protection, and Pauli frame randomization.",
      "url": "http://arxiv.org/abs/2508.08188v1",
      "published_time_eastern_timestamp": 1754932074.0
    },
    {
      "title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold\n  Representation Learning",
      "summary": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.",
      "url": "http://arxiv.org/abs/2508.08186v1",
      "published_time_eastern_timestamp": 1754932015.0
    },
    {
      "title": "THAT: Token-wise High-frequency Augmentation Transformer for\n  Hyperspectral Pansharpening",
      "summary": "Transformer-based methods have demonstrated strong potential in hyperspectral\npansharpening by modeling long-range dependencies. However, their effectiveness\nis often limited by redundant token representations and a lack of multi-scale\nfeature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,\nabundance sparsity) and spatial priors (e.g., non-local similarity), which are\ncritical for accurate reconstruction. From a spectral-spatial perspective,\nVision Transformers (ViTs) face two major limitations: they struggle to\npreserve high-frequency components--such as material edges and texture\ntransitions--and suffer from attention dispersion across redundant tokens.\nThese issues stem from the global self-attention mechanism, which tends to\ndilute high-frequency signals and overlook localized details. To address these\nchallenges, we propose the Token-wise High-frequency Augmentation Transformer\n(THAT), a novel framework designed to enhance hyperspectral pansharpening\nthrough improved high-frequency feature representation and token selection.\nSpecifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to\nprioritize informative tokens and suppress redundancy; (2) a Multi-level\nVariance-aware Feed-forward Network (MVFN) to enhance high-frequency detail\nlearning. Experiments on standard benchmarks show that THAT achieves\nstate-of-the-art performance with improved reconstruction quality and\nefficiency. The source code is available at https://github.com/kailuo93/THAT.",
      "url": "http://arxiv.org/abs/2508.08183v1",
      "published_time_eastern_timestamp": 1754931790.0
    },
    {
      "title": "RedDino: A foundation model for red blood cell analysis",
      "summary": "Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc",
      "url": "http://arxiv.org/abs/2508.08180v1",
      "published_time_eastern_timestamp": 1754931571.0
    },
    {
      "title": "3D Human Mesh Estimation from Single View RGBD",
      "summary": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released.",
      "url": "http://arxiv.org/abs/2508.08178v1",
      "published_time_eastern_timestamp": 1754931554.0
    },
    {
      "title": "CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce\n  High-Resolution Time-Varying Data",
      "summary": "Large-scale scientific simulations require significant resources to generate\nhigh-resolution time-varying data (TVD). While super-resolution is an efficient\npost-processing strategy to reduce costs, existing methods rely on a large\namount of HR training data, limiting their applicability to diverse simulation\nscenarios. To address this constraint, we proposed CD-TVD, a novel framework\nthat combines contrastive learning and an improved diffusion-based\nsuper-resolution model to achieve accurate 3D super-resolution from limited\ntime-step high-resolution data. During pre-training on historical simulation\ndata, the contrastive encoder and diffusion superresolution modules learn\ndegradation patterns and detailed features of high-resolution and\nlow-resolution samples. In the training phase, the improved diffusion model\nwith a local attention mechanism is fine-tuned using only one newly generated\nhigh-resolution timestep, leveraging the degradation knowledge learned by the\nencoder. This design minimizes the reliance on large-scale high-resolution\ndatasets while maintaining the capability to recover fine-grained details.\nExperimental results on fluid and atmospheric simulation datasets confirm that\nCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a\nsignificant advancement in data augmentation for large-scale scientific\nsimulations. The code is available at\nhttps://github.com/Xin-Gao-private/CD-TVD.",
      "url": "http://arxiv.org/abs/2508.08173v1",
      "published_time_eastern_timestamp": 1754931088.0
    },
    {
      "title": "PyVeritas: On Verifying Python via LLM-Based Transpilation and Bounded\n  Model Checking for C",
      "summary": "Python has become the dominant language for general-purpose programming, yet\nit lacks robust tools for formal verification. In contrast, programmers working\nin languages such as C benefit from mature model checkers, for example CBMC,\nwhich enable exhaustive symbolic reasoning and fault localisation. The inherent\ncomplexity of Python, coupled with the verbosity and low-level nature of\nexisting transpilers (e.g., Cython), have historically limited the\napplicability of formal verification to Python programs.\n  In this paper, we propose PyVeritas, a novel framework that leverages Large\nLanguage Models (LLMs) for high-level transpilation from Python to C, followed\nby bounded model checking and MaxSAT-based fault localisation in the generated\nC code. PyVeritas enables verification and bug localisation for Python code\nusing existing model checking tools for C. Our empirical evaluation on two\nPython benchmarks demonstrates that LLM-based transpilation can achieve a high\ndegree of accuracy, up to 80--90% for some LLMs, enabling effective development\nenvironment that supports assertion-based verification and interpretable fault\ndiagnosis for small yet non-trivial Python programs.",
      "url": "http://arxiv.org/abs/2508.08171v1",
      "published_time_eastern_timestamp": 1754930947.0
    }
  ]
}
{
  "last_updated": "2025-11-10T10:14:21.081064-05:00",
  "papers": [
    {
      "title": "TimeSearch-R: Adaptive Temporal Search for Long-Form Video Understanding\n  via Self-Verification Reinforcement Learning",
      "summary": "Temporal search aims to identify a minimal set of relevant frames from tens\nof thousands based on a given query, serving as a foundation for accurate\nlong-form video understanding. Existing works attempt to progressively narrow\nthe search space. However, these approaches typically rely on a hand-crafted\nsearch process, lacking end-to-end optimization for learning optimal search\nstrategies. In this paper, we propose TimeSearch-R, which reformulates temporal\nsearch as interleaved text-video thinking, seamlessly integrating searching\nvideo clips into the reasoning process through reinforcement learning (RL).\nHowever, applying RL training methods, such as Group Relative Policy\nOptimization (GRPO), to video reasoning can result in unsupervised intermediate\nsearch decisions. This leads to insufficient exploration of the video content\nand inconsistent logical reasoning. To address these issues, we introduce GRPO\nwith Completeness Self-Verification (GRPO-CSV), which gathers searched video\nframes from the interleaved reasoning process and utilizes the same policy\nmodel to verify the adequacy of searched frames, thereby improving the\ncompleteness of video reasoning. Additionally, we construct datasets\nspecifically designed for the SFT cold-start and RL training of GRPO-CSV,\nfiltering out samples with weak temporal dependencies to enhance task\ndifficulty and improve temporal search capabilities. Extensive experiments\ndemonstrate that TimeSearch-R achieves significant improvements on temporal\nsearch benchmarks such as Haystack-LVBench and Haystack-Ego4D, as well as\nlong-form video understanding benchmarks like VideoMME and MLVU. Notably,\nTimeSearch-R establishes a new state-of-the-art on LongVideoBench with 4.1%\nimprovement over the base model Qwen2.5-VL and 2.0% over the advanced video\nreasoning model Video-R1. Our code is available at\nhttps://github.com/Time-Search/TimeSearch-R.",
      "url": "http://arxiv.org/abs/2511.05489v1",
      "published_time_eastern_timestamp": 1762541905.0
    },
    {
      "title": "Further improvements to stabilizer simulation theory: classical\n  rewriting of CSS-preserving stabilizer circuits, quadratic form expansions of\n  stabilizer operations, and framed hidden variable models",
      "summary": "Simulation of stabilizer circuits is a well-studied problem in quantum\ninformation processing, with a number of highly optimized algorithms available.\nYet, we argue that further improvements can arise from the theoretical\nstructure of stabilizer operations themselves. We focus on the subclass of\nstabilizer circuits composed of Calderbank-Shor-Steane (CSS)-preserving\nstabilizer operations, which naturally appear in fault-tolerant computations\nover CSS stabilizer codes. Using elementary circuit-transformation techniques,\nwe show that such circuits can be exactly rewritten as classical probabilistic\ncircuits that reproduce measurement statistics. This rewriting introduces no\ncomputational overhead, in contrast to the general case of stabilizer circuits.\nTo clarify the origin of this simplification, we introduce the standard\nquadratic-form representation of general stabilizer operations (Clifford\nchannels). It provides an efficient way to describe compositions of stabilizer\noperations and thus to simulate stabilizer circuits. CSS-preserving operations\ncorrespond to purely linear forms, which under a Walsh-Hadamard-Fourier\ntransform yield a noncontextual hidden variable model, providing an alternative\nproof of the introduced rewriting. Finally, we develop a theory of reference\nframes for multiqubit systems, where frames are encoded by quadratic forms.\nThis allows us to express stabilizer operations as probabilistic maps for\nproper reference frames. Non-CSS-preserving stabilizer circuits require\ndynamical modifications of reference frames, embodying a contextuality resource\nthat leads to the computational overhead. This framework provides a new\nperspective on simulating stabilizer and near-stabilizer circuits within\ndynamically evolving quasiprobability models.",
      "url": "http://arxiv.org/abs/2511.05478v1",
      "published_time_eastern_timestamp": 1762540802.0
    },
    {
      "title": "A Metamorphic Testing Perspective on Knowledge Distillation for Language\n  Models of Code: Does the Student Deeply Mimic the Teacher?",
      "summary": "Transformer-based language models of code have achieved state-of-the-art\nperformance across a wide range of software analytics tasks, but their\npractical deployment remains limited due to high computational costs, slow\ninference speeds, and significant environmental impact. To address these\nchallenges, recent research has increasingly explored knowledge distillation as\na method for compressing a large language model of code (the teacher) into a\nsmaller model (the student) while maintaining performance. However, the degree\nto which a student model deeply mimics the predictive behavior and internal\nrepresentations of its teacher remains largely unexplored, as current\naccuracy-based evaluation provides only a surface-level view of model quality\nand often fails to capture more profound discrepancies in behavioral fidelity\nbetween the teacher and student models. To address this gap, we empirically\nshow that the student model often fails to deeply mimic the teacher model,\nresulting in up to 285% greater performance drop under adversarial attacks,\nwhich is not captured by traditional accuracy-based evaluation. Therefore, we\npropose MetaCompress, a metamorphic testing framework that systematically\nevaluates behavioral fidelity by comparing the outputs of teacher and student\nmodels under a set of behavior-preserving metamorphic relations. We evaluate\nMetaCompress on two widely studied tasks, using compressed versions of popular\nlanguage models of code, obtained via three different knowledge distillation\ntechniques: Compressor, AVATAR, and MORPH. The results show that MetaCompress\nidentifies up to 62% behavioral discrepancies in student models, underscoring\nthe need for behavioral fidelity evaluation within the knowledge distillation\npipeline and establishing MetaCompress as a practical framework for testing\ncompressed language models of code derived through knowledge distillation.",
      "url": "http://arxiv.org/abs/2511.05476v1",
      "published_time_eastern_timestamp": 1762540734.0
    },
    {
      "title": "The Potential of Copernicus Satellites for Disaster Response: Retrieving\n  Building Damage from Sentinel-1 and Sentinel-2",
      "summary": "Natural disasters demand rapid damage assessment to guide humanitarian\nresponse. Here, we investigate whether medium-resolution Earth observation\nimages from the Copernicus program can support building damage assessment,\ncomplementing very-high resolution imagery with often limited availability. We\nintroduce xBD-S12, a dataset of 10,315 pre- and post-disaster image pairs from\nboth Sentinel-1 and Sentinel-2, spatially and temporally aligned with the\nestablished xBD benchmark. In a series of experiments, we demonstrate that\nbuilding damage can be detected and mapped rather well in many disaster\nscenarios, despite the moderate 10$\\,$m ground sampling distance. We also find\nthat, for damage mapping at that resolution, architectural sophistication does\nnot seem to bring much advantage: more complex model architectures tend to\nstruggle with generalization to unseen disasters, and geospatial foundation\nmodels bring little practical benefit. Our results suggest that Copernicus\nimages are a viable data source for rapid, wide-area damage assessment and\ncould play an important role alongside VHR imagery. We release the xBD-S12\ndataset, code, and trained models to support further research.",
      "url": "http://arxiv.org/abs/2511.05461v1",
      "published_time_eastern_timestamp": 1762538527.0
    },
    {
      "title": "SWE-Compass: Towards Unified Evaluation of Agentic Coding Abilities for\n  Large Language Models",
      "summary": "Evaluating large language models (LLMs) for software engineering has been\nlimited by narrow task coverage, language bias, and insufficient alignment with\nreal-world developer workflows. Existing benchmarks often focus on algorithmic\nproblems or Python-centric bug fixing, leaving critical dimensions of software\nengineering underexplored. To address these gaps, we introduce SWE-Compass1, a\ncomprehensive benchmark that unifies heterogeneous code-related evaluations\ninto a structured and production-aligned framework. SWE-Compass spans 8 task\ntypes, 8 programming scenarios, and 10 programming languages, with 2000\nhigh-quality instances curated from authentic GitHub pull requests and refined\nthrough systematic filtering and validation. We benchmark ten state-of-the-art\nLLMs under two agentic frameworks, SWE-Agent and Claude Code, revealing a clear\nhierarchy of difficulty across task types, languages, and scenarios. Moreover,\nby aligning evaluation with real-world developer practices, SWE-Compass\nprovides a rigorous and reproducible foundation for diagnosing and advancing\nagentic coding capabilities in large language models.",
      "url": "http://arxiv.org/abs/2511.05459v1",
      "published_time_eastern_timestamp": 1762538492.0
    },
    {
      "title": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?",
      "summary": "Recent advances in 3D point cloud transformers have led to state-of-the-art\nresults in tasks such as semantic segmentation and reconstruction. However,\nthese models typically rely on dense token representations, incurring high\ncomputational and memory costs during training and inference. In this work, we\npresent the finding that tokens are remarkably redundant, leading to\nsubstantial inefficiency. We introduce gitmerge3D, a globally informed graph\ntoken merging method that can reduce the token count by up to 90-95% while\nmaintaining competitive performance. This finding challenges the prevailing\nassumption that more tokens inherently yield better performance and highlights\nthat many current models are over-tokenized and under-optimized for\nscalability. We validate our method across multiple 3D vision tasks and show\nconsistent improvements in computational efficiency. This work is the first to\nassess redundancy in large-scale 3D transformer models, providing insights into\nthe development of more efficient 3D foundation architectures. Our code and\ncheckpoints are publicly available at https://gitmerge3d.github.io",
      "url": "http://arxiv.org/abs/2511.05449v1",
      "published_time_eastern_timestamp": 1762537081.0
    },
    {
      "title": "Shortest self-orthogonal embeddings of binary linear codes",
      "summary": "There has been recent interest in the study of shortest self-orthogonal\nembeddings of binary linear codes, since many such codes are optimal\nself-orthogonal codes. Several authors have studied the length of a shortest\nself-orthogonal embedding of a given binary code $\\mathcal C$, or equivalently,\nthe minimum number of columns that must be added to a generator matrix of\n$\\mathcal C$ to form a generator matrix of a self-orthogonal code. In this\npaper, we use properties of the hull of a linear code to determine the length\nof a shortest self-orthogonal embedding of any binary linear code. We focus on\nthe examples of Hamming codes and Reed-Muller codes. We show that a shortest\nself-orthogonal embedding of a binary Hamming code is self-dual, and propose\ntwo algorithms to construct self-dual codes from Hamming codes $\\mathcal H_r$.\nUsing these algorithms, we construct a self-dual $[22, 11, 6]$ code, called the\nshortened Golay code, from the binary $[15, 11, 3]$ Hamming code $\\mathcal\nH_4$, and construct a self-dual $[52, 26, 8]$ code from the binary $[31, 26,\n3]$ Hamming code $\\mathcal H_5$. We use shortest SO embeddings of linear codes\nto obtain many inequivalent optimal self-orthogonal codes of dimension $7$ and\n$8$ for several lengths. Four of the codes of dimension $8$ that we construct\nare codes with new parameters such as $[91, 8, 42],\\, [98, 8, 46],\\,[114, 8,\n54]$, and $[191, 8, 94]$.",
      "url": "http://arxiv.org/abs/2511.05440v1",
      "published_time_eastern_timestamp": 1762535841.0
    },
    {
      "title": "\"I Like That You Have to Poke Around\": Instructors on How Experiential\n  Approaches to AI Literacy Spark Inquiry and Critical Thinking",
      "summary": "As artificial intelligence (AI) increasingly shapes decision-making across\ndomains, there is a growing need to support AI literacy among learners beyond\ncomputer science. However, many current approaches rely on programming-heavy\ntools or abstract lecture-based content, limiting accessibility for non-STEM\naudiences. This paper presents findings from a study of AI User, a modular,\nweb-based curriculum that teaches core AI concepts through interactive, no-code\nprojects grounded in real-world scenarios. The curriculum includes eight\nprojects; this study focuses on instructor feedback on Projects 5-8, which\naddress applied topics such as natural language processing, computer vision,\ndecision support, and responsible AI. Fifteen community college instructors\nparticipated in structured focus groups, completing the projects as learners\nand providing feedback through individual reflection and group discussion.\nUsing thematic analysis, we examined how instructors evaluated the design,\ninstructional value, and classroom applicability of these experiential\nactivities. Findings highlight instructors' appreciation for exploratory tasks,\nrole-based simulations, and real-world relevance, while also surfacing design\ntrade-offs around cognitive load, guidance, and adaptability for diverse\nlearners. This work extends prior research on AI literacy by centering\ninstructor perspectives on teaching complex AI topics without code. It offers\nactionable insights for designing inclusive, experiential AI learning resources\nthat scale across disciplines and learner backgrounds.",
      "url": "http://arxiv.org/abs/2511.05430v1",
      "published_time_eastern_timestamp": 1762535158.0
    },
    {
      "title": "Sharing the Learned Knowledge-base to Estimate Convolutional Filter\n  Parameters for Continual Image Restoration",
      "summary": "Continual learning is an emerging topic in the field of deep learning, where\na model is expected to learn continuously for new upcoming tasks without\nforgetting previous experiences. This field has witnessed numerous\nadvancements, but few works have been attempted in the direction of image\nrestoration. Handling large image sizes and the divergent nature of various\ndegradation poses a unique challenge in the restoration domain. However,\nexisting works require heavily engineered architectural modifications for new\ntask adaptation, resulting in significant computational overhead.\nRegularization-based methods are unsuitable for restoration, as different\nrestoration challenges require different kinds of feature processing. In this\ndirection, we propose a simple modification of the convolution layer to adapt\nthe knowledge from previous restoration tasks without touching the main\nbackbone architecture. Therefore, it can be seamlessly applied to any deep\narchitecture without any structural modifications. Unlike other approaches, we\ndemonstrate that our model can increase the number of trainable parameters\nwithout significantly increasing computational overhead or inference time.\nExperimental validation demonstrates that new restoration tasks can be\nintroduced without compromising the performance of existing tasks. We also show\nthat performance on new restoration tasks improves by adapting the knowledge\nfrom the knowledge base created by previous restoration tasks. The code is\navailable at https://github.com/aupendu/continual-restore.",
      "url": "http://arxiv.org/abs/2511.05421v1",
      "published_time_eastern_timestamp": 1762534362.0
    },
    {
      "title": "Story Arena: A Multi-Agent Environment for Envisioning the Future of\n  Software Engineering",
      "summary": "What better way to understand the impact of AI on software engineering than\nto ask AI itself? We constructed Story Arena, a multi-agent \"writer's room\" in\nwhich multiple AI agents, independently imbued with a position statement on the\nfuture of software engineering, converse with each other to develop a shared\nvision. They then use this shared vision to collaboratively construct a design\nfiction that depicts this vision in narrative form. We present \"The Code of\nTrust,\" a short fiction that investigates themes of human comprehension, trust,\ncontent ownership, augmentation vs. replacement, and uncertain futures in\nhuman-AI co-creation.",
      "url": "http://arxiv.org/abs/2511.05410v1",
      "published_time_eastern_timestamp": 1762533321.0
    },
    {
      "title": "Multi-modal Loop Closure Detection with Foundation Models in Severely\n  Unstructured Environments",
      "summary": "Robust loop closure detection is a critical component of Simultaneous\nLocalization and Mapping (SLAM) algorithms in GNSS-denied environments, such as\nin the context of planetary exploration. In these settings, visual place\nrecognition often fails due to aliasing and weak textures, while LiDAR-based\nmethods suffer from sparsity and ambiguity. This paper presents MPRF, a\nmultimodal pipeline that leverages transformer-based foundation models for both\nvision and LiDAR modalities to achieve robust loop closure in severely\nunstructured environments. Unlike prior work limited to retrieval, MPRF\nintegrates a two-stage visual retrieval strategy with explicit 6-DoF pose\nestimation, combining DINOv2 features with SALAD aggregation for efficient\ncandidate screening and SONATA-based LiDAR descriptors for geometric\nverification. Experiments on the S3LI dataset and S3LI Vulcano dataset show\nthat MPRF outperforms state-of-the-art retrieval methods in precision while\nenhancing pose estimation robustness in low-texture regions. By providing\ninterpretable correspondences suitable for SLAM back-ends, MPRF achieves a\nfavorable trade-off between accuracy, efficiency, and reliability,\ndemonstrating the potential of foundation models to unify place recognition and\npose estimation. Code and models will be released at github.com/DLR-RM/MPRF.",
      "url": "http://arxiv.org/abs/2511.05404v1",
      "published_time_eastern_timestamp": 1762533035.0
    },
    {
      "title": "PreResQ-R1: Towards Fine-Grained Rank-and-Score Reinforcement Learning\n  for Visual Quality Assessment via Preference-Response Disentangled Policy\n  Optimization",
      "summary": "Visual Quality Assessment (QA) seeks to predict human perceptual judgments of\nvisual fidelity. While recent multimodal large language models (MLLMs) show\npromise in reasoning about image and video quality, existing approaches mainly\nrely on supervised fine-tuning or rank-only objectives, resulting in shallow\nreasoning, poor score calibration, and limited cross-domain generalization. We\npropose PreResQ-R1, a Preference-Response Disentangled Reinforcement Learning\nframework that unifies absolute score regression and relative ranking\nconsistency within a single reasoning-driven optimization scheme. Unlike prior\nQA methods, PreResQ-R1 introduces a dual-branch reward formulation that\nseparately models intra-sample response coherence and inter-sample preference\nalignment, optimized via Group Relative Policy Optimization (GRPO). This design\nencourages fine-grained, stable, and interpretable chain-of-thought reasoning\nabout perceptual quality. To extend beyond static imagery, we further design a\nglobal-temporal and local-spatial data flow strategy for Video Quality\nAssessment. Remarkably, with reinforcement fine-tuning on only 6K images and\n28K videos, PreResQ-R1 achieves state-of-the-art results across 10 IQA and 5\nVQA benchmarks under both SRCC and PLCC metrics, surpassing by margins of 5.30%\nand textbf2.15% in IQA task, respectively. Beyond quantitative gains, it\nproduces human-aligned reasoning traces that reveal the perceptual cues\nunderlying quality judgments. Code and model are available.",
      "url": "http://arxiv.org/abs/2511.05393v1",
      "published_time_eastern_timestamp": 1762532390.0
    },
    {
      "title": "TeaRAG: A Token-Efficient Agentic Retrieval-Augmented Generation\n  Framework",
      "summary": "Retrieval-Augmented Generation (RAG) utilizes external knowledge to augment\nLarge Language Models' (LLMs) reliability. For flexibility, agentic RAG employs\nautonomous, multi-round retrieval and reasoning to resolve queries. Although\nrecent agentic RAG has improved via reinforcement learning, they often incur\nsubstantial token overhead from search and reasoning processes. This trade-off\nprioritizes accuracy over efficiency. To address this issue, this work proposes\nTeaRAG, a token-efficient agentic RAG framework capable of compressing both\nretrieval content and reasoning steps. 1) First, the retrieved content is\ncompressed by augmenting chunk-based semantic retrieval with a graph retrieval\nusing concise triplets. A knowledge association graph is then built from\nsemantic similarity and co-occurrence. Finally, Personalized PageRank is\nleveraged to highlight key knowledge within this graph, reducing the number of\ntokens per retrieval. 2) Besides, to reduce reasoning steps, Iterative\nProcess-aware Direct Preference Optimization (IP-DPO) is proposed.\nSpecifically, our reward function evaluates the knowledge sufficiency by a\nknowledge matching mechanism, while penalizing excessive reasoning steps. This\ndesign can produce high-quality preference-pair datasets, supporting iterative\nDPO to improve reasoning conciseness. Across six datasets, TeaRAG improves the\naverage Exact Match by 4% and 2% while reducing output tokens by 61% and 59% on\nLlama3-8B-Instruct and Qwen2.5-14B-Instruct, respectively. Code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/TeaRAG.",
      "url": "http://arxiv.org/abs/2511.05385v1",
      "published_time_eastern_timestamp": 1762531714.0
    },
    {
      "title": "Modeling Solar Atmosphere Dynamics with MAGEC",
      "summary": "Modeling the solar atmosphere is challenging due to its layered structure and\nmulti-scale dynamics. We aim to validate the new radiative MHD code MAGEC,\nwhich combines the MANCHA and MAGNUS codes into a finite-volume,\nshock-capturing framework, and to test its performance through 2D simulations\nof magneto-convection.\n  MAGEC is MPI-parallelized and includes improvements for coronal modeling,\nsuch as LTE radiative losses and a hyperbolic treatment of thermal conduction\nthat mitigates restrictive time steps. We also estimated its numerical\nviscosity and resistivity. To assess robustness, we performed 2D simulations\ncovering a domain from 2 Mm below the surface to 18.16 Mm into the corona,\nusing both open and closed magnetic-field configurations. For each case, we\nanalyzed steady-state temperature profiles and the contributions to the\ninternal-energy balance at different heights. A separate experiment examined\nthe role of perpendicular thermal conduction.\n  MAGEC reproduced the expected temperature stratification set by boundary\nconditions and magnetic geometry, and all simulations reached thermal\nequilibrium. Open-field cases produced higher coronal temperatures than closed,\narcade-like fields. Analysis of the explicit and implicit energy terms\nclarified their relative effects on heating and cooling. Perpendicular thermal\nconduction, often neglected in coronal models, was found to influence plasma\ndynamics near reconnection; although local effects are small, they can\ncumulatively modify the average coronal temperature.\n  These results show that MAGEC is a reliable and efficient tool for radiative\nMHD simulations, well suited to capturing the shocks and dynamic processes of\nthe solar atmosphere.",
      "url": "http://arxiv.org/abs/2511.05380v1",
      "published_time_eastern_timestamp": 1762531317.0
    },
    {
      "title": "AI Literacy for Community Colleges: Instructors' Perspectives on\n  Scenario-Based and Interactive Approaches to Teaching AI",
      "summary": "This research category full paper investigates how community college\ninstructors evaluate interactive, no-code AI literacy resources designed for\nnon-STEM learners. As artificial intelligence becomes increasingly integrated\ninto everyday technologies, AI literacy - the ability to evaluate AI systems,\ncommunicate with them, and understand their broader impacts - has emerged as a\ncritical skill across disciplines. Yet effective, scalable approaches for\nteaching these concepts in higher education remain limited, particularly for\nstudents outside STEM fields.\n  To address this gap, we developed AI User, an interactive online curriculum\nthat introduces core AI concepts through scenario - based activities set in\nreal - world contexts. This study presents findings from four focus groups with\ninstructors who engaged with AI User materials and participated in structured\nfeedback activities. Thematic analysis revealed that instructors valued\nexploratory tasks that simulated real - world AI use cases and fostered\nexperimentation, while also identifying challenges related to scaffolding,\naccessibility, and multi-modal support. A ranking task for instructional\nsupport materials showed a strong preference for interactive demonstrations\nover traditional educational materials like conceptual guides or lecture\nslides.\n  These findings offer insights into instructor perspectives on making AI\nconcepts more accessible and relevant for broad learner audiences. They also\ninform the design of AI literacy tools that align with diverse teaching\ncontexts and support critical engagement with AI in higher education.",
      "url": "http://arxiv.org/abs/2511.05363v1",
      "published_time_eastern_timestamp": 1762530713.0
    },
    {
      "title": "Diffusion-Based Electromagnetic Inverse Design of Scattering Structured\n  Media",
      "summary": "We present a conditional diffusion model for electromagnetic inverse design\nthat generates structured media geometries directly from target differential\nscattering cross-section profiles, bypassing expensive iterative optimization.\nOur 1D U-Net architecture with Feature-wise Linear Modulation learns to map\ndesired angular scattering patterns to 2x2 dielectric sphere structure,\nnaturally handling the non-uniqueness of inverse problems by sampling diverse\nvalid designs. Trained on 11,000 simulated metasurfaces, the model achieves\nmedian MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES\nevolutionary optimization while reducing design time from hours to seconds.\nThese results demonstrate that employing diffusion models is promising for\nadvancing electromagnetic inverse design research, potentially enabling rapid\nexploration of complex metasurface architectures and accelerating the\ndevelopment of next-generation photonic and wireless communication systems. The\ncode is publicly available at\nhttps://github.com/mikzuker/inverse_design_metasurface_generation.",
      "url": "http://arxiv.org/abs/2511.05357v1",
      "published_time_eastern_timestamp": 1762530530.0
    },
    {
      "title": "Efficient CNN Inference on Ultra-Low-Power MCUs via Saturation-Aware\n  Convolution",
      "summary": "Deploying lightweight CNN inference tasks on ultra-low-power MCUs is often\nnot limited by space constraint, thanks to the compact size of models, yet\ninference latency is crucial for preserving energy. We reveal that quantized\nCNN inference on ultra-low-power MCUs executes unnecessary computations in\nneurons that produce saturated output values: often times, these neurons still\nproduce the correct output value without fully completing the computation,\nsince the neuron value is too extreme and is eventually systematically clamped\nat the boundaries allowed by the neuron. We show that with carefully designed\ncondition checks, it is possible to identify and skip these unnecessary\ncomputations without impacting the neuron output. Based on this, we present\nsaturation-aware convolution: an inference technique whereby computations in\nconvolution kernels are executed in an altered order to induce earlier\nsaturation, and saturation checks are inserted to omit unnecessary\ncomputations. We integrate our implementation into MCUNet's TinyEngine, the\nstate-of-the-art neural network code generation and inference framework, and\nconduct experiments on a Cortex-M0+ MCU. The result based on 7 open-source CNN\nmodels displays up to 24% inference time saving, with strictly zero impact on\nneural network accuracy.",
      "url": "http://arxiv.org/abs/2511.05347v1",
      "published_time_eastern_timestamp": 1762530087.0
    },
    {
      "title": "EMPEROR I. Exoplanet MCMC parallel tempering for RV orbit retrieval",
      "summary": "We present \\texttt{EMPEROR}, an open-source Python framework designed for\nefficient exoplanet detection and characterisation with radial velocities (RV).\n\\texttt{EMPEROR} integrates Dynamic Nested Sampling (DNS) and Adaptive Parallel\nTempering (APT) Markov Chain Monte Carlo (MCMC), supporting multiple noise\nmodels such as Gaussian Processes (GPs) and Moving Averages (MA). The framework\nenables systematic model comparison using statistical metrics, including\nBayesian evidence ($\\ln{\\mathcal{Z}}$) and Bayesian Information Criterion\n(BIC), while providing automated, publish-ready visualisations.\n\\texttt{EMPEROR} is evaluated across three distinct systems to assess its\ncapabilities in different detection scenarios. Sampling performance, model\nselection, and the search for Earth-mass planets are evaluated in data for 51\nPegasi, HD 55693 and Barnard's Star (GJ 699). For 51 Pegasi, APT achieves an\neffective sampling increase over DNS by a factor 3.76, while retrieving tighter\nparameter estimates. For HD 55693 the stellar rotation\n$P_{\\text{rot}}=29.72^{+0.01}_{-0.02}$ and magnetic cycle\n$P_{\\text{mag}}=2557.0^{+70.1}_{-36.7}$ are recovered, while demonstrating the\nsensitivity of $\\ln{\\mathcal{Z}}$ to prior selection. For Barnard's star,\nseveral noise models are compared, and the confirmed planet parameters are\nsuccessfully retrieved with all of them. The best model shows a period of\n3.1536$\\pm$0.0003~d, minimum mass of 0.38$\\pm$0.03 M$_{\\rm{\\oplus}}$, and\nsemi-major axis of 0.02315$\\pm$0.00039~AU. Purely statistical inference might\nbe insufficient on its own for robust exoplanet detection. Effective\nmethodologies must integrate domain knowledge, heuristic criteria, and\nmulti-faceted model comparisons. The versatility of \\texttt{EMPEROR} in\nhandling diverse noise structures, its systematic model selection, and its\nimproved performance make it a valuable tool for RV exoplanetary studies.",
      "url": "http://arxiv.org/abs/2511.05331v1",
      "published_time_eastern_timestamp": 1762529315.0
    },
    {
      "title": "$\\mathbf{S^2LM}$: Towards Semantic Steganography via Large Language\n  Models",
      "summary": "Although steganography has made significant advancements in recent years, it\nstill struggles to embed semantically rich, sentence-level information into\ncarriers. However, in the era of AIGC, the capacity of steganography is more\ncritical than ever. In this work, we present Sentence-to-Image Steganography,\nan instance of Semantic Steganography, a novel task that enables the hiding of\narbitrary sentence-level messages within a cover image. Furthermore, we\nestablish a benchmark named Invisible Text (IVT), comprising a diverse set of\nsentence-level texts as secret messages for evaluation. Finally, we present\n$\\mathbf{S^2LM}$: Semantic Steganographic Language Model, which utilizes large\nlanguage models (LLMs) to embed high-level textual information, such as\nsentences or even paragraphs, into images. Unlike traditional bit-level\ncounterparts, $\\mathrm{S^2LM}$ enables the integration of semantically rich\ncontent through a newly designed pipeline in which the LLM is involved\nthroughout the entire process. Both quantitative and qualitative experiments\ndemonstrate that our method effectively unlocks new semantic steganographic\ncapabilities for LLMs. The source code will be released soon.",
      "url": "http://arxiv.org/abs/2511.05319v1",
      "published_time_eastern_timestamp": 1762528660.0
    },
    {
      "title": "Rethinking Metrics and Diffusion Architecture for 3D Point Cloud\n  Generation",
      "summary": "As 3D point clouds become a cornerstone of modern technology, the need for\nsophisticated generative models and reliable evaluation metrics has grown\nexponentially. In this work, we first expose that some commonly used metrics\nfor evaluating generated point clouds, particularly those based on Chamfer\nDistance (CD), lack robustness against defects and fail to capture geometric\nfidelity and local shape consistency when used as quality indicators. We\nfurther show that introducing samples alignment prior to distance calculation\nand replacing CD with Density-Aware Chamfer Distance (DCD) are simple yet\nessential steps to ensure the consistency and robustness of point cloud\ngenerative model evaluation metrics. While existing metrics primarily focus on\ndirectly comparing 3D Euclidean coordinates, we present a novel metric, named\nSurface Normal Concordance (SNC), which approximates surface similarity by\ncomparing estimated point normals. This new metric, when combined with\ntraditional ones, provides a more comprehensive evaluation of the quality of\ngenerated samples. Finally, leveraging recent advancements in transformer-based\nmodels for point cloud analysis, such as serialized patch attention , we\npropose a new architecture for generating high-fidelity 3D structures, the\nDiffusion Point Transformer. We perform extensive experiments and comparisons\non the ShapeNet dataset, showing that our model outperforms previous solutions,\nparticularly in terms of quality of generated point clouds, achieving new\nstate-of-the-art. Code available at\nhttps://github.com/matteo-bastico/DiffusionPointTransformer.",
      "url": "http://arxiv.org/abs/2511.05308v1",
      "published_time_eastern_timestamp": 1762528044.0
    }
  ]
}
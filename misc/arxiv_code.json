{
  "last_updated": "2025-09-08T20:54:40.395365-04:00",
  "papers": [
    {
      "title": "FlowSeek: Optical Flow Made Easier with Depth Foundation Models and\n  Motion Bases",
      "summary": "We present FlowSeek, a novel framework for optical flow requiring minimal\nhardware resources for training. FlowSeek marries the latest advances on the\ndesign space of optical flow networks with cutting-edge single-image depth\nfoundation models and classical low-dimensional motion parametrization,\nimplementing a compact, yet accurate architecture. FlowSeek is trained on a\nsingle consumer-grade GPU, a hardware budget about 8x lower compared to most\nrecent methods, and still achieves superior cross-dataset generalization on\nSintel Final and KITTI, with a relative improvement of 10 and 15% over the\nprevious state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow\ndatasets.",
      "url": "http://arxiv.org/abs/2509.05297v1",
      "published_time_eastern_timestamp": 1757095199.0
    },
    {
      "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
      "summary": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.",
      "url": "http://arxiv.org/abs/2509.05296v1",
      "published_time_eastern_timestamp": 1757095187.0
    },
    {
      "title": "Non-Termination Proving: 100 Million LoC and Beyond",
      "summary": "We report on our tool, Pulse Infinite, that uses proof techniques to show\nnon-termination (divergence) in large programs. Pulse Infinite works\ncompositionally and under-approximately: the former supports scale, and the\nlatter ensures soundness for proving divergence. Prior work focused on small\nbenchmarks in the tens or hundreds of lines of code (LoC), and scale limits\ntheir practicality: a single company may have tens of millions, or even\nhundreds of millions of LoC or more. We report on applying Pulse Infinite to\nover a hundred million lines of open-source and proprietary software written in\nC, C++, and Hack, identifying over 30 previously unknown issues, establishing a\nnew state of the art for detecting divergence in real-world codebases.",
      "url": "http://arxiv.org/abs/2509.05293v1",
      "published_time_eastern_timestamp": 1757095125.0
    },
    {
      "title": "Learning to accelerate distributed ADMM using graph neural networks",
      "summary": "Distributed optimization is fundamental in large-scale machine learning and\ncontrol applications. Among existing methods, the Alternating Direction Method\nof Multipliers (ADMM) has gained popularity due to its strong convergence\nguarantees and suitability for decentralized computation. However, ADMM often\nsuffers from slow convergence and sensitivity to hyperparameter choices. In\nthis work, we show that distributed ADMM iterations can be naturally\nrepresented within the message-passing framework of graph neural networks\n(GNNs). Building on this connection, we propose to learn adaptive step sizes\nand communication weights by a graph neural network that predicts the\nhyperparameters based on the iterates. By unrolling ADMM for a fixed number of\niterations, we train the network parameters end-to-end to minimize the final\niterates error for a given problem class, while preserving the algorithm's\nconvergence properties. Numerical experiments demonstrate that our learned\nvariant consistently improves convergence speed and solution quality compared\nto standard ADMM. The code is available at\nhttps://github.com/paulhausner/learning-distributed-admm.",
      "url": "http://arxiv.org/abs/2509.05288v1",
      "published_time_eastern_timestamp": 1757094922.0
    },
    {
      "title": "Elucidating the Design Space of Decay in Linear Attention",
      "summary": "This paper presents a comprehensive investigation into the decay mechanisms\ninherent in linear complexity sequence models. We systematically delineate the\ndesign space of decay mechanisms across four pivotal dimensions:\nparameterization strategy, which refers to the computational methodology for\ndecay; parameter sharing, which involves the utilization of supplementary\nparameters for decay computation; decay granularity, comparing scalar versus\nvector-based decay; and compatibility with relative positional encoding\nmethods, such as Rotary Position Embedding (RoPE). Through an extensive series\nof experiments conducted on diverse language modeling tasks, we uncovered\nseveral critical insights. Firstly, the design of the parameterization strategy\nfor decay requires meticulous consideration. Our findings indicate that\neffective configurations are typically confined to a specific range of\nparameters. Secondly, parameter sharing cannot be used arbitrarily, as it may\ncause decay values to be too large or too small, thereby significantly\nimpacting performance. Thirdly, under identical parameterization strategies,\nscalar decay generally underperforms compared to its vector-based counterpart.\nHowever, in certain scenarios with alternative parameterization strategies,\nscalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our\nanalysis reveals that RoPE, a commonly employed relative positional encoding\nmethod, typically fails to provide tangible benefits to the majority of linear\nattention mechanisms.",
      "url": "http://arxiv.org/abs/2509.05282v1",
      "published_time_eastern_timestamp": 1757094506.0
    },
    {
      "title": "SpikingBrain Technical Report: Spiking Brain-inspired Large Models",
      "summary": "Mainstream Transformer-based large language models face major efficiency\nbottlenecks: training computation scales quadratically with sequence length,\nand inference memory grows linearly, limiting long-context processing. Building\nlarge models on non-NVIDIA platforms also poses challenges for stable and\nefficient training. To address this, we introduce SpikingBrain, a family of\nbrain-inspired models designed for efficient long-context training and\ninference. SpikingBrain leverages the MetaX GPU cluster and focuses on three\naspects: (1) Model Architecture: linear and hybrid-linear attention\narchitectures with adaptive spiking neurons; (2) Algorithmic Optimizations: an\nefficient, conversion-based training pipeline and a dedicated spike coding\nframework; (3) System Engineering: customized training frameworks, operator\nlibraries, and parallelism strategies tailored to MetaX hardware.\n  Using these techniques, we develop two models: SpikingBrain-7B, a linear LLM,\nand SpikingBrain-76B, a hybrid-linear MoE LLM. These models demonstrate the\nfeasibility of large-scale LLM development on non-NVIDIA platforms.\nSpikingBrain achieves performance comparable to open-source Transformer\nbaselines while using only about 150B tokens for continual pre-training. Our\nmodels significantly improve long-sequence training efficiency and deliver\ninference with (partially) constant memory and event-driven spiking behavior.\nFor example, SpikingBrain-7B attains over 100x speedup in Time to First Token\nfor 4M-token sequences. Training remains stable for weeks on hundreds of MetaX\nC550 GPUs, with the 7B model reaching a Model FLOPs Utilization of 23.4\npercent. The proposed spiking scheme achieves 69.15 percent sparsity, enabling\nlow-power operation. Overall, this work demonstrates the potential of\nbrain-inspired mechanisms to drive the next generation of efficient and\nscalable large model design.",
      "url": "http://arxiv.org/abs/2509.05276v1",
      "published_time_eastern_timestamp": 1757093640.0
    },
    {
      "title": "First ICCUB Numerical Relativity Waveform Catalog of Eccentric Black\n  Hole Binaries",
      "summary": "We release the first Numerical Relativity catalog of Institut de Ciencies del\nCosmos at University of Barcelona (ICCUB) consisting of 128 simulations for\nblack hole binaries. All simulations in this first release correspond to highly\neccentric binaries with eccentricity $e = (0.62,0.79)$ which develop\nzoom-whirls up to three close passages before merger. We consider aligned,\nequal spin configurations in the range $\\chi = (-0.5, 0.5)$ and equal mass\nratios.\n  For each simulation, we provide the modes $(\\ell, m)$ of Weyl scalar\n$\\psi_4^{(\\ell,m)}$ extrapolated to $r = \\infty$, with $\\ell \\leq 4$. In\naddition, we provide the corresponding strain modes obtained by computing a\ndouble time integral of the Weyl scalar modes. Moreover, we provide metadata\nand the parameter files required to reproduce our results using the open-source\ncode Einstein Toolkit.\n  A Python code that facilitates the access to the data is available on\nGit-Hub.",
      "url": "http://arxiv.org/abs/2509.05269v1",
      "published_time_eastern_timestamp": 1757093090.0
    },
    {
      "title": "On Evaluating the Poisoning Robustness of Federated Learning under Local\n  Differential Privacy",
      "summary": "Federated learning (FL) combined with local differential privacy (LDP)\nenables privacy-preserving model training across decentralized data sources.\nHowever, the decentralized data-management paradigm leaves LDPFL vulnerable to\nparticipants with malicious intent. The robustness of LDPFL protocols,\nparticularly against model poisoning attacks (MPA), where adversaries inject\nmalicious updates to disrupt global model convergence, remains insufficiently\nstudied. In this paper, we propose a novel and extensible model poisoning\nattack framework tailored for LDPFL settings. Our approach is driven by the\nobjective of maximizing the global training loss while adhering to local\nprivacy constraints. To counter robust aggregation mechanisms such as\nMulti-Krum and trimmed mean, we develop adaptive attacks that embed carefully\ncrafted constraints into a reverse training process, enabling evasion of these\ndefenses. We evaluate our framework across three representative LDPFL\nprotocols, three benchmark datasets, and two types of deep neural networks.\nAdditionally, we investigate the influence of data heterogeneity and privacy\nbudgets on attack effectiveness. Experimental results demonstrate that our\nadaptive attacks can significantly degrade the performance of the global model,\nrevealing critical vulnerabilities and highlighting the need for more robust\nLDPFL defense strategies against MPA. Our code is available at\nhttps://github.com/ZiJW/LDPFL-Attack",
      "url": "http://arxiv.org/abs/2509.05265v1",
      "published_time_eastern_timestamp": 1757092983.0
    },
    {
      "title": "COGITAO: A Visual Reasoning Framework To Study Compositionality &\n  Generalization",
      "summary": "The ability to compose learned concepts and apply them in novel settings is\nkey to human intelligence, but remains a persistent limitation in\nstate-of-the-art machine learning models. To address this issue, we introduce\nCOGITAO, a modular and extensible data generation framework and benchmark\ndesigned to systematically study compositionality and generalization in visual\ndomains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs\nrule-based tasks which apply a set of transformations to objects in grid-like\nenvironments. It supports composition, at adjustable depth, over a set of 28\ninteroperable transformations, along with extensive control over grid\nparametrization and object properties. This flexibility enables the creation of\nmillions of unique task rules -- surpassing concurrent datasets by several\norders of magnitude -- across a wide range of difficulties, while allowing\nvirtually unlimited sample generation per rule. We provide baseline experiments\nusing state-of-the-art vision models, highlighting their consistent failures to\ngeneralize to novel combinations of familiar elements, despite strong in-domain\nperformance. COGITAO is fully open-sourced, including all code and datasets, to\nsupport continued research in this field.",
      "url": "http://arxiv.org/abs/2509.05249v1",
      "published_time_eastern_timestamp": 1757091665.0
    },
    {
      "title": "Rethinking mass transfer: a unified semi-analytical framework for\n  circular and eccentric binaries. I. Orbital evolution due to conservative\n  mass transfer",
      "summary": "Mass transfer (MT) is a fundamental process in stellar evolution. While MT in\ncircular orbits is well studied, observations indicate that it also occurs in\neccentric ones. To date, no framework simultaneously accounts for both\nconservative and non-conservative MT across arbitrary eccentricities while also\nincorporating the donor star`s spin. We present a new semi-analytic framework\nfor the secular orbital evolution of mass-transferring binaries, treating stars\nas extended bodies and accounting for the donor star`s spin. The model is\napplicable to both circular and eccentric orbits and accommodates conservative\nand non-conservative MT across a broad range of mass ratios and stellar spins.\nWe derive secular, orbit-averaged equations describing the orbital evolution by\ntreating MT, mass loss, and angular momentum loss as perturbations to the\ngeneral two-body problem. Assuming conservative MT, we compare our results to\nprevious models and validate them against numerical integrations. Our model\npredicts stronger orbital widening at a given mass ratio than previous models.\nFor circular orbits, we find that the transitional mass ratio $q_{trans,a}$,\nwhich separates orbital widening from shrinkage, increases from $q_{trans,a} =\n1$ up to $q_{trans,a} \\sim 1.5$ when accounting for extended bodies. For\neccentric orbits, the model predicts a broader parameter space for both orbital\nwidening and eccentricity pumping. We find that stable MT naturally explains\nthe observed correlation between longer orbital periods and higher\neccentricities, providing a robust mechanism for the formation of wide and\neccentric post-interaction binaries. Our model can be integrated into binary\nevolution and population synthesis codes to consistently treat conservative and\nnon-conservative MT in arbitrarily eccentric orbits with applications ranging\nfrom MT on the main sequence to gravitational-wave progenitors.",
      "url": "http://arxiv.org/abs/2509.05243v1",
      "published_time_eastern_timestamp": 1757091495.0
    },
    {
      "title": "Testing Magnetic Field Configurations in Spider Pulsar PSR J1723-2837\n  with IXPE",
      "summary": "We present the first X-ray polarimetry observations of a redback millisecond\npulsar binary, \\src, with the Imaging X-ray Polarimetry Explorer (IXPE).\nRedbacks are compact binaries in which a rotation-powered millisecond pulsar\ninteracts with a non-degenerate companion via an intrabinary shock, forming\nideal laboratories for probing pulsar winds and relativistic shock physics,\nwhere ordered magnetic fields and particle acceleration shape the observed\nradiation. We conduct a spectro-polarimetric analysis combining IXPE data with\narchival Chandra, XMM-Newton, NuSTAR, and Swift observations. We explore two\nlimiting magnetic field configurations, parallel and perpendicular to the bulk\nflow, and simulate their expected polarization signatures using the {\\tt 3DPol}\nradiative transport code. To account for the rapid rotation of the polarization\nangle predicted by these models, we implement a phase-dependent Stokes\nalignment procedure that preserves the polarization degree while correcting for\nphase-rotating PA. We also devise a new maximum-likelihood fitting strategy to\ndetermine the phase-dependence of the polarization angle by minimizing the\npolarization degree uncertainty. This technique shows a hint the binary may be\nrotating clockwise relative to the celestial north pole. We find no significant\ndetection of polarization in the IXPE data, with PD<51% at 99% confidence\nlevel. Our results excludes the high-polarization degree scenario predicted by\nthe perpendicular field model during the brightest orbital phase bin.\nSimulations show that doubling the current exposure would make the parallel\nconfiguration detectable. The new PA rotation technique is also applicable to\nIXPE data of many sources whose intrinsic PA variation is apriori not known but\nis strictly periodic.",
      "url": "http://arxiv.org/abs/2509.05240v1",
      "published_time_eastern_timestamp": 1757091391.0
    },
    {
      "title": "Improving Solar Flare Nowcasting with the Hot Onset Precursor Event\n  (HOPE) Technique",
      "summary": "This study investigates the statistical behavior of plasma properties during\nHot Onset Precursor Events (HOPEs) of solar flares and evaluates their\npotential for improving flare nowcasting. Two datasets are analyzed: (a) new\nSoft X-Ray (SXR) spectra of 25 flares (C2.6 to M1.0) obtained from the\nDual-zone Aperture X-ray Solar Spectrometer (DAXSS), and (b) SXR irradiance\ndata from 137 flares (C5.0 to X7.1) recorded by the X-Ray Sensor on the\nGeostationary Operational Environmental Satellite (GOES-XRS). Plasma\ntemperature, emission measure (EM), and low First Ionization Potential (e.g.,\nMg, Si, Fe) elemental abundance factors (AFs) are derived from DAXSS using\nAstrophysical Plasma Emission Code model fitting. Isothermal plasma temperature\nand emission measure are derived from GOES-XRS using the XRS-A/XRS-B ratio\nmethod. Results indicate that the HOPE phase exhibits elevated temperatures\n(10-15 MK) and an order-of-magnitude increase in EM before the impulsive phase.\nElemental AFs show a transition from coronal to photospheric values as the\nflare progresses. Using GOES-XRS data, we develop an improved nowcasting\nalgorithm that detects flares utilizing HOPE signatures. The algorithm is\ntested across three categories of flares (C5.0-M1.0, M1.0-X1.0, and X1.0+),\nconsistently predicting flare alerts 5-15 minutes ahead of the flare peak. We\nalso explore the possibility of approximate flare magnitude prediction, by\ncalculating correlation between onset parameters and flare peak magnitude. This\nHOPE-based system shows potential for earlier warnings than current NOAA R3\nalerts, which could be useful for High-frequency communication systems\noperators and targeted flare observation campaigns.",
      "url": "http://arxiv.org/abs/2509.05234v1",
      "published_time_eastern_timestamp": 1757090996.0
    },
    {
      "title": "Cultivating T states on the surface code with only two-qubit gates",
      "summary": "High-fidelity T magic states are a key requirement for fault-tolerant quantum\ncomputing in 2D. It has generally been assumed that preparing high-fidelity T\nstates requires noisy injection of T states followed by lengthy distillation\nroutines. This assumption has been recently challenged by the introduction of\ncultivation, in which careful state injection and postselection alone are used\nto prepare T states close to the fidelity required for quantum algorithms.\nCultivation was originally proposed for the color code, but can also be done on\nthe $RP^2$ code, a code similar to the surface code.\n  In this work, we demonstrate how to cultivate T states directly on the\nsurface code. Besides its simplicity compared to color or $RP^2$ cultivation,\nsurface code cultivation offers a number of advantages, including: (1) It is\nmore directly compatible with neutral atom architectures than ${RP}^2$\ncultivation (2) Cultivated surface code states can be used in transversal CNOT\ngates with other surface codes, unlike color code states (3) Surface code\ncultivation can be done at any distance, unlike color and ${RP}^2$ cultivation\nwhich requires odd distances. Under a standard depolarizing error model, our\n$d=3,\\ (4),\\ (5)$ cultivation circuit reaches an error rate of $1\\cdot\n10^{-6},\\ (1\\cdot10^{-8}),\\ (2\\cdot10^{-9})$ and an acceptance rate of $34\\%,\\\n(6\\%),\\ (1\\%)$, meeting or exceeding the fidelity of color and $RP^2$\ncultivation with comparable acceptance rates.",
      "url": "http://arxiv.org/abs/2509.05232v1",
      "published_time_eastern_timestamp": 1757090936.0
    },
    {
      "title": "HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range\n  Dependency Modeling in Large Language Models",
      "summary": "Positional encoding mechanisms enable Transformers to model sequential\nstructure and long-range dependencies in text. While absolute positional\nencodings struggle with extrapolation to longer sequences due to fixed\npositional representations, and relative approaches like Alibi exhibit\nperformance degradation on extremely long contexts, the widely-used Rotary\nPositional Encoding (RoPE) introduces oscillatory attention patterns that\nhinder stable long-distance dependency modelling. We address these limitations\nthrough a geometric reformulation of positional encoding. Drawing inspiration\nfrom Lorentz transformations in hyperbolic geometry, we propose Hyperbolic\nRotary Positional Encoding (HoPE), which leverages hyperbolic functions to\nimplement Lorentz rotations on token representations. Theoretical analysis\ndemonstrates that RoPE is a special case of our generalized formulation. HoPE\nfundamentally resolves RoPE's slation issues by enforcing monotonic decay of\nattention weights with increasing token distances. Extensive experimental\nresults, including perplexity evaluations under several extended sequence\nbenchmarks, show that HoPE consistently exceeds existing positional encoding\nmethods. These findings underscore HoPE's enhanced capacity for representing\nand generalizing long-range dependencies. Data and code will be available.",
      "url": "http://arxiv.org/abs/2509.05218v1",
      "published_time_eastern_timestamp": 1757089248.0
    },
    {
      "title": "Fold-transversal surface code cultivation",
      "summary": "Magic state cultivation is a state-of-the-art protocol to prepare ultra-high\nfidelity non-Clifford resource states for universal quantum computation. It\noffers a significant reduction in spacetime overhead compared to traditional\nmagic state distillation techniques. Cultivation protocols involve measuring a\ntransversal logical Clifford operator on an initial small-distance code and\nthen rapidly growing to a larger-distance code. In this work, we present a new\ncultivation scheme in which we measure the fold-transversal Hadamard of the\nunrotated surface code, and leverage unitary techniques to grow within the\nsurface code family. Using both stabilizer and state vector simulations we find\nthat this approach achieves the lowest known spacetime overhead for magic state\ncultivation. Practical implementation of our protocol is best suited to\narchitectures with non-local connectivity, showing the strength of\narchitectures where such connectivity is readily available.",
      "url": "http://arxiv.org/abs/2509.05212v1",
      "published_time_eastern_timestamp": 1757088886.0
    },
    {
      "title": "Symbolic Graphics Programming with Large Language Models",
      "summary": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.",
      "url": "http://arxiv.org/abs/2509.05208v1",
      "published_time_eastern_timestamp": 1757088653.0
    },
    {
      "title": "List Decoding Expander-Based Codes via Fast Approximation of Expanding\n  CSPs: I",
      "summary": "We present near-linear time list decoding algorithms (in the block-length\n$n$) for expander-based code constructions. More precisely, we show that\n  (i) For every $\\delta \\in (0,1)$ and $\\epsilon > 0$, there is an explicit\nfamily of good Tanner LDPC codes of (design) distance $\\delta$ that is $(\\delta\n- \\epsilon, O_\\varepsilon(1))$ list decodable in time\n$\\widetilde{\\mathcal{O}}_{\\varepsilon}(n)$ with alphabet size $O_\\delta(1)$,\n  (ii) For every $R \\in (0,1)$ and $\\epsilon > 0$, there is an explicit family\nof AEL codes of rate $R$, distance $1-R -\\varepsilon$ that is $(1-R-\\epsilon,\nO_\\varepsilon(1))$ list decodable in time\n$\\widetilde{\\mathcal{O}}_{\\varepsilon}(n)$ with alphabet size\n$\\text{exp}(\\text{poly}(1/\\epsilon))$, and\n  (iii) For every $R \\in (0,1)$ and $\\epsilon > 0$, there is an explicit family\nof AEL codes of rate $R$, distance $1-R-\\varepsilon$ that is $(1-R-\\epsilon,\nO(1/\\epsilon))$ list decodable in time\n$\\widetilde{\\mathcal{O}}_{\\varepsilon}(n)$ with alphabet size\n$\\text{exp}(\\text{exp}(\\text{poly}(1/\\epsilon)))$ using recent near-optimal\nlist size bounds from [JMST25].\n  Our results are obtained by phrasing the decoding task as an agreement CSP\n[RWZ20,DHKNT19] on expander graphs and using the fast approximation algorithm\nfor $q$-ary expanding CSPs from [Jer23], which is based on weak regularity\ndecomposition [JST21,FK96]. Similarly to list decoding $q$-ary Ta-Shma's codes\nin [Jer23], we show that it suffices to enumerate over assignments that are\nconstant in each part (of the constantly many) of the decomposition in order to\nrecover all codewords in the list.",
      "url": "http://arxiv.org/abs/2509.05203v1",
      "published_time_eastern_timestamp": 1757088435.0
    },
    {
      "title": "Enhancing 3D Point Cloud Classification with ModelNet-R and\n  Point-SkipNet",
      "summary": "The classification of 3D point clouds is crucial for applications such as\nautonomous driving, robotics, and augmented reality. However, the commonly used\nModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D\ndata, size mismatches, and inadequate class differentiation, which hinder model\nperformance. This paper introduces ModelNet-R, a meticulously refined version\nof ModelNet40 designed to address these issues and serve as a more reliable\nbenchmark. Additionally, this paper proposes Point-SkipNet, a lightweight\ngraph-based neural network that leverages efficient sampling, neighborhood\ngrouping, and skip connections to achieve high classification accuracy with\nreduced computational overhead. Extensive experiments demonstrate that models\ntrained in ModelNet-R exhibit significant performance improvements. Notably,\nPoint-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a\nsubstantially lower parameter count compared to contemporary models. This\nresearch highlights the crucial role of dataset quality in optimizing model\nefficiency for 3D point cloud classification. For more details, see the code\nat: https://github.com/m-saeid/ModeNetR_PointSkipNet.",
      "url": "http://arxiv.org/abs/2509.05198v1",
      "published_time_eastern_timestamp": 1757087856.0
    },
    {
      "title": "AI Agents for Web Testing: A Case Study in the Wild",
      "summary": "Automated web testing plays a critical role in ensuring high-quality user\nexperiences and delivering business value. Traditional approaches primarily\nfocus on code coverage and load testing, but often fall short of capturing\ncomplex user behaviors, leaving many usability issues undetected. The emergence\nof large language models (LLM) and AI agents opens new possibilities for web\ntesting by enabling human-like interaction with websites and a general\nawareness of common usability problems. In this work, we present WebProber, a\nprototype AI agent-based web testing framework. Given a URL, WebProber\nautonomously explores the website, simulating real user interactions,\nidentifying bugs and usability issues, and producing a human-readable report.\nWe evaluate WebProber through a case study of 120 academic personal websites,\nwhere it uncovered 29 usability issues--many of which were missed by\ntraditional tools. Our findings highlight agent-based testing as a promising\ndirection while outlining directions for developing next-generation,\nuser-centered testing frameworks.",
      "url": "http://arxiv.org/abs/2509.05197v1",
      "published_time_eastern_timestamp": 1757087836.0
    },
    {
      "title": "A hybrid active galactic nucleus feedback model with spinning black\n  holes, winds and jets",
      "summary": "We present a hybrid active galactic nucleus (AGN) feedback model that\nfeatures three accretion disc states (the thick, thin, and slim discs at low,\nmoderate, and super-Eddington accretion rates, respectively), and two feedback\nmodes: thermal isotropic and kinetic jets. The model includes black hole (BH)\nspin evolution due to gas accretion, BH mergers, jet spindown, and\nLense-Thirring torques. The BH spin determines the jet directions and affects\nthe feedback efficiencies. The model is implemented in the SWIFT code and\ncoupled with the COLIBRE galaxy formation model. We present the first results\nfrom hybrid AGN feedback simulations run as part of the COLIBRE suite, focusing\non the impact of new parameters and calibration efforts. Using the new hybrid\nAGN feedback model, we find that AGN feedback affects not just massive\ngalaxies, but all galaxies down to $M_*\\approx10^8$ $\\mathrm{M}_\\odot$. BH\nspins are predicted to be near-maximal for intermediate-mass BHs\n($M_\\mathrm{BH}\\in[10^6,10^8]$ $\\mathrm{M}_\\odot$), and lower for other BH\nmasses. These trends are in good agreement with observations. The intergalactic\nmedium is hotter and impacted on larger scales in the hybrid AGN feedback\nsimulations compared to those using purely thermal feedback. In the hybrid AGN\nsimulations, we predict that half of the cumulative injected AGN energy is in\nthermal and the other half in jet form, broadly independent of BH mass and\nredshift. Jet feedback is important at all redshifts and dominates over thermal\nfeedback at $z<0.5$ and $z>1.5$, but only mildly.",
      "url": "http://arxiv.org/abs/2509.05179v1",
      "published_time_eastern_timestamp": 1757086001.0
    }
  ]
}
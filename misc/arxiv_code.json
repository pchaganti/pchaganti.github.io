{
  "last_updated": "2025-07-30T05:20:11.823430-04:00",
  "papers": [
    {
      "title": "Zak-OTFS Based Coded Random Access for Uplink mMTC",
      "summary": "This paper proposes a grant-free coded random access (CRA) scheme for uplink\nmassive machine-type communications (mMTC), based on Zak-orthogonal time\nfrequency space (Zak-OTFS) modulation in the delay-Doppler domain. The scheme\nis tailored for doubly selective wireless channels, where conventional\northogonal frequency-division multiplexing (OFDM)-based CRA suffers from\nunreliable inter-slot channel prediction due to time-frequency variability. By\nexploiting the predictable nature of Zak-OTFS, the proposed approach enables\naccurate channel estimation across slots, facilitating reliable successive\ninterference cancellation across user packet replicas. A fair comparison with\nan OFDM-based CRA baseline shows that the proposed scheme achieves\nsignificantly lower packet loss rates under high mobility and user density.\nExtensive simulations over the standardized Veh-A channel confirm the\nrobustness and scalability of Zak-OTFS-based CRA, supporting its applicability\nto future mMTC deployments.",
      "url": "http://arxiv.org/abs/2507.22013v1",
      "published_time_eastern_timestamp": 1753808614.0
    },
    {
      "title": "Random Lozenge Waterfall: Dimensional Collapse of Gibbs Measures",
      "summary": "We investigate the asymptotic behavior of the q-Racah probability measure on\nlozenge tilings of a hexagon whose side lengths scale linearly with a large\nparameter $L$, while the parameters $q\\in(0,1)$ and $\\kappa\\in\n\\mathbf{i}\\mathbb{R}$ remain fixed. This regime differs fundamentally from the\ntraditional case $q\\sim e^{-c/L}\\to1$, in which random tilings are locally\ngoverned by two-dimensional translation-invariant ergodic Gibbs measures. In\nthe fixed-q regime we uncover a new macroscopic phase, the waterfall\n(previously only observed experimentally), where the two-dimensional Gibbs\nstructure collapses into a one-dimensional random stepped interface that we\ncall a barcode.\n  We prove a law of large numbers and exponential concentration, showing that\nthe random tilings converge to a deterministic waterfall profile. We further\nconjecture an explicit correlation kernel of the one-dimensional barcode\nprocess arising in the limit. Remarkably, the limit is invariant under shifts\nby $2\\mathbb{Z}$ but not by $\\mathbb{Z}$, exhibiting an emergent period-two\nstructure absent from the original weights. Our conjectures are supported by\nextensive numerical evidence and perfect sampling simulations. The kernel is\nbuilt from a family of functions orthogonal in both spaces\n$\\ell^{2}(\\mathbb{Z})$ and $\\ell^{2}(\\mathbb{Z}+\\frac12)$, that may be of\nindependent interest.\n  Our proofs adapt the spectral projection method of Borodin-Gorin-Rains\n(https://arxiv.org/abs/0905.0679) to the regime with fixed~q. The resulting\nasymptotic analysis is substantially more involved, and leads to\nnon-self-adjoint operators. We overcome these challenges in the exponential\nconcentration result by a separate argument based on sharp bounds for the\nratios of probabilities under the q-Racah orthogonal polynomial ensemble.",
      "url": "http://arxiv.org/abs/2507.22011v1",
      "published_time_eastern_timestamp": 1753808536.0
    },
    {
      "title": "VeS: Teaching Pixels to Listen Without Supervision",
      "summary": "Recent dense audio-visual (AV) models achieve impressive retrieval and\nemergent localization, but almost all evidence comes from English-centric,\ncaption-rich web video. It is unclear whether these objectives survive in\nlow-resource, code-switched, and noisy multilingual settings that typify\ndeveloping regions. We show they do**-**and that the choice of aggregation\nfunction becomes even more critical. Using a multilingual subset of Project\nVaani spanning dozens of Indian languages and dialectal variants, we compare\nthree contrastive objectives: (i) a global mean-pooled loss (CLIP-style), (ii)\na dense max-mean token matcher (DenseAV-style), and (iii) a simple hybrid\n(motivated by frozen-vision alignment strategies). The dense objective delivers\na +59% relative R@1 (Audio Visual) improvement over global pooling and\nsubstantially lower mean/median ranks, while consistently producing sharp\nzero-shot localization heatmaps of spoken objects-despite keeping the vision\nbackbone entirely frozen (no LoRA / partial fine-tuning). Our results\ndemonstrate that dense token routing is not a luxury of high-resource English\ncorpora; it is more decisive when annotations and acoustic cleanliness are\nscarce. We release the codebase and trained models.",
      "url": "http://arxiv.org/abs/2507.22008v1",
      "published_time_eastern_timestamp": 1753808332.0
    },
    {
      "title": "See Different, Think Better: Visual Variations Mitigating Hallucinations\n  in LVLMs",
      "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable\ncapabilities in visual understanding and multimodal reasoning. However, LVLMs\nfrequently exhibit hallucination phenomena, manifesting as the generated\ntextual responses that demonstrate inconsistencies with the provided visual\ncontent. Existing hallucination mitigation methods are predominantly\ntext-centric, the challenges of visual-semantic alignment significantly limit\ntheir effectiveness, especially when confronted with fine-grained visual\nunderstanding scenarios. To this end, this paper presents ViHallu, a\nVision-Centric Hallucination mitigation framework that enhances visual-semantic\nalignment through Visual Variation Image Generation and Visual Instruction\nConstruction. ViHallu introduces \\textbf{\\textit{visual variation images}} with\ncontrollable visual alterations while maintaining the overall image structure.\nThese images, combined with carefully constructed visual instructions, enable\nLVLMs to better understand fine-grained visual content through fine-tuning,\nallowing models to more precisely capture the correspondence between visual\ncontent and text, thereby enhancing visual-semantic alignment. Extensive\nexperiments on multiple benchmarks show that ViHallu effectively enhances\nmodels' fine-grained visual understanding while significantly reducing\nhallucination tendencies. Furthermore, we release ViHallu-Instruction, a visual\ninstruction dataset specifically designed for hallucination mitigation and\nvisual-semantic alignment. Code is available at\nhttps://github.com/oliviadzy/ViHallu.",
      "url": "http://arxiv.org/abs/2507.22003v1",
      "published_time_eastern_timestamp": 1753808007.0
    },
    {
      "title": "Misspecifications in structural equation modeling: The choice of latent\n  variables, causal-formative constructs or composites",
      "summary": "Empirical research in many social disciplines involves constructs that are\nnot directly observable, such as behaviors. To model them, constructs must be\noperationalized using their relations with indicators. Structural equation\nmodeling (SEM) is the primary approach for this purpose. In SEM, three types of\nconstructs are distinguished: latent variables, causal-formative constructs,\nand composites. To estimate the parameters of the different models, various\nestimators have been developed. Many Monte Carlo studies have examined the\nestimation performances of different estimators for the construct types. One\naspect evaluated is the consequences of construct misspecification - when the\ntrue construct type differs from the modeling choice - on parameter estimates\nand model fit. For example, parameter bias in models that misspecify latent\nvariables as composites is often attributed to the chosen estimator, although\nmodel parameters depend on different estimators, making it impossible to\nexamine the factors individually. This article aims to disentangle the issues\nof construct misspecification and parameter estimation by a comprehensive Monte\nCarlo study of all combinations between true and assumed construct types. To\nfocus on misspecification, we used the same estimator for all models, namely\nthe maximum likelihood (ML) estimator. To generalize beyond ML, we replicated\nthe simulation using another estimator. We aim to examine the role of construct\nmisspecification, not estimator choice, on the estimation performance and show\nthat misspecification leads indeed to biased path coefficient estimates.\nFurther, we evaluate whether fit measures can distinguish models with correct\nfrom those with misspecified constructs. We find that none of the criteria\nconsidered is suited for this. These findings stress the importance of\nthoughtful construct specification and the need for further research.",
      "url": "http://arxiv.org/abs/2507.21998v1",
      "published_time_eastern_timestamp": 1753807592.0
    },
    {
      "title": "DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity\n  Environments",
      "summary": "We present the first unified, modular, open-source 3DGS-based simulation\nframework for Real2Sim2Real robot learning. It features a holistic Real2Sim\npipeline that synthesizes hyper-realistic geometry and appearance of complex\nreal-world scenarios, paving the way for analyzing and bridging the Sim2Real\ngap. Powered by Gaussian Splatting and MuJoCo, Discoverse enables massively\nparallel simulation of multiple sensor modalities and accurate physics, with\ninclusive supports for existing 3D assets, robot models, and ROS plugins,\nempowering large-scale robot learning and complex robotic benchmarks. Through\nextensive experiments on imitation learning, Discoverse demonstrates\nstate-of-the-art zero-shot Sim2Real transfer performance compared to existing\nsimulators. For code and demos: https://air-discoverse.github.io/.",
      "url": "http://arxiv.org/abs/2507.21981v1",
      "published_time_eastern_timestamp": 1753806812.0
    },
    {
      "title": "Motion Matters: Motion-guided Modulation Network for Skeleton-based\n  Micro-Action Recognition",
      "summary": "Micro-Actions (MAs) are an important form of non-verbal communication in\nsocial interactions, with potential applications in human emotional analysis.\nHowever, existing methods in Micro-Action Recognition often overlook the\ninherent subtle changes in MAs, which limits the accuracy of distinguishing MAs\nwith subtle changes. To address this issue, we present a novel Motion-guided\nModulation Network (MMN) that implicitly captures and modulates subtle motion\ncues to enhance spatial-temporal representation learning. Specifically, we\nintroduce a Motion-guided Skeletal Modulation module (MSM) to inject motion\ncues at the skeletal level, acting as a control signal to guide spatial\nrepresentation modeling. In parallel, we design a Motion-guided Temporal\nModulation module (MTM) to incorporate motion information at the frame level,\nfacilitating the modeling of holistic motion patterns in micro-actions.\nFinally, we propose a motion consistency learning strategy to aggregate the\nmotion cues from multi-scale features for micro-action classification.\nExperimental results on the Micro-Action 52 and iMiGUE datasets demonstrate\nthat MMN achieves state-of-the-art performance in skeleton-based micro-action\nrecognition, underscoring the importance of explicitly modeling subtle motion\ncues. The code will be available at https://github.com/momiji-bit/MMN.",
      "url": "http://arxiv.org/abs/2507.21977v1",
      "published_time_eastern_timestamp": 1753806430.0
    },
    {
      "title": "Fine-Tuning Code Language Models to Detect Cross-Language Bugs",
      "summary": "Multilingual programming, which involves using multiple programming languages\n(PLs) in a single project, is increasingly common due to its benefits. However,\nit introduces cross-language bugs (CLBs), which arise from interactions between\ndifferent PLs and are difficult to detect by single-language bug detection\ntools. This paper investigates the potential of pre-trained code language\nmodels (CodeLMs) in CLB detection. We developed CLCFinder, a cross-language\ncode identification tool, and constructed a CLB dataset involving three PL\ncombinations (Python-C/C++, Java-C/C++, and Python-Java) with nine interaction\ntypes. We fine-tuned 13 CodeLMs on this dataset and evaluated their\nperformance, analyzing the effects of dataset size, token sequence length, and\ncode comments. Results show that all CodeLMs performed poorly before\nfine-tuning, but exhibited varying degrees of performance improvement after\nfine-tuning, with UniXcoder-base achieving the best F1 score (0.7407). Notably,\nsmall fine-tuned CodeLMs tended to performe better than large ones. CodeLMs\nfine-tuned on single-language bug datasets performed poorly on CLB detection,\ndemonstrating the distinction between CLBs and single-language bugs.\nAdditionally, increasing the fine-tuning dataset size significantly improved\nperformance, while longer token sequences did not necessarily improve the model\nperformance. The impact of code comments varied across models. Some fine-tuned\nCodeLMs' performance was improved, while others showed degraded performance.",
      "url": "http://arxiv.org/abs/2507.21954v1",
      "published_time_eastern_timestamp": 1753805168.0
    },
    {
      "title": "MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile\n  Task Automation",
      "summary": "The recent advancement of autonomous agents powered by Large Language Models\n(LLMs) has demonstrated significant potential for automating tasks on mobile\ndevices through graphical user interfaces (GUIs). Despite initial progress,\nthese agents still face challenges when handling complex real-world tasks.\nThese challenges arise from a lack of knowledge about real-life mobile\napplications in LLM-based agents, which may lead to ineffective task planning\nand even cause hallucinations. To address these challenges, we propose a novel\nLLM-based agent framework called MapAgent that leverages memory constructed\nfrom historical trajectories to augment current task planning. Specifically, we\nfirst propose a trajectory-based memory mechanism that transforms task\nexecution trajectories into a reusable and structured page-memory database.\nEach page within a trajectory is extracted as a compact yet comprehensive\nsnapshot, capturing both its UI layout and functional context. Secondly, we\nintroduce a coarse-to-fine task planning approach that retrieves relevant pages\nfrom the memory database based on similarity and injects them into the LLM\nplanner to compensate for potential deficiencies in understanding real-world\napp scenarios, thereby achieving more informed and context-aware task planning.\nFinally, planned tasks are transformed into executable actions through a task\nexecutor supported by a dual-LLM architecture, ensuring effective tracking of\ntask progress. Experimental results in real-world scenarios demonstrate that\nMapAgent achieves superior performance to existing methods. The code will be\nopen-sourced to support further research.",
      "url": "http://arxiv.org/abs/2507.21953v1",
      "published_time_eastern_timestamp": 1753805132.0
    },
    {
      "title": "The Curious Case of High-Dimensional Indexing as a File Structure: A\n  Case Study of eCP-FS",
      "summary": "Modern analytical pipelines routinely deploy multiple deep learning and\nretrieval models that rely on approximate nearest-neighbor (ANN) indexes to\nsupport efficient similarity-based search. While many state-of-the-art\nANN-indexes are memory-based (e.g., HNSW and IVF), using multiple ANN indexes\ncreates a competition for limited GPU/CPU memory resources, which in turn\nnecessitates disk-based index structures (e.g., DiskANN or eCP). In typical\nindex implementations, the main component is a complex data structure that is\nserialized to disk and is read either fully at startup time, for memory-based\nindexes, or incrementally at query time, for disk-based indexes. To visualize\nthe index structure, or analyze its quality, complex coding is needed that is\neither embedded in the index implementation or replicates the code that reads\nthe data structure. In this paper, we consider an alternative approach that\nmaps the data structure to a file structure, using a file library, making the\nindex easily readable for any programming language and even human-readable. The\ndisadvantage is that the serialized index is verbose, leading to overhead of\nsearching through the index. The question addressed in this paper is how severe\nthis performance penalty is. To that end, this paper presents eCP-FS, a\nfile-based implementation of eCP, a well-known disk-based ANN index. A\ncomparison with state-of-the-art indexes shows that while eCP-FS is slower, the\nimplementation is nevertheless somewhat competitive even when memory is not\nconstrained. In a memory-constrained scenario, eCP-FS offers a minimal memory\nfootprint, making it ideal for resource-constrained or multi-index\nenvironments.",
      "url": "http://arxiv.org/abs/2507.21939v1",
      "published_time_eastern_timestamp": 1753804304.0
    },
    {
      "title": "Vibe Coding as a Reconfiguration of Intent Mediation in Software\n  Development: Definition, Implications, and Research Agenda",
      "summary": "Software development is undergoing a fundamental transformation as vibe\ncoding becomes widespread, with large portions of contemporary codebases now\nbeing AI-generated. The disconnect between rapid adoption and limited\nconceptual understanding highlights the need for an inquiry into this emerging\nparadigm. Drawing on an intent perspective and historical analysis, we define\nvibe coding as a software development paradigm where humans and generative AI\nengage in collaborative flow to co-create software artifacts through natural\nlanguage dialogue, shifting the mediation of developer intent from\ndeterministic instruction to probabilistic inference. By intent mediation, we\nrefer to the fundamental process through which developers translate their\nconceptual goals into representations that computational systems can execute.\nOur results show that vibe coding reconfigures cognitive work by redistributing\nepistemic labor between humans and machines, shifting the expertise in the\nsoftware development process away from traditional areas such as design or\ntechnical implementation toward collaborative orchestration. We identify key\nopportunities, including democratization, acceleration, and systemic leverage,\nalongside risks, such as black box codebases, responsibility gaps, and\necosystem bias. We conclude with a research agenda spanning human-,\ntechnology-, and organization-centered directions to guide future\ninvestigations of this paradigm.",
      "url": "http://arxiv.org/abs/2507.21928v1",
      "published_time_eastern_timestamp": 1753803895.0
    },
    {
      "title": "Efficient Sub-pixel Motion Compensation in Learned Video Codecs",
      "summary": "Motion compensation is a key component of video codecs. Conventional codecs\n(HEVC and VVC) have carefully refined this coding step, with an important focus\non sub-pixel motion compensation. On the other hand, learned codecs achieve\nsub-pixel motion compensation through simple bilinear filtering. This paper\noffers to improve learned codec motion compensation by drawing inspiration from\nconventional codecs. It is shown that the usage of more advanced interpolation\nfilters, block-based motion information and finite motion accuracy lead to\nbetter compression performance and lower decoding complexity. Experimental\nresults are provided on the Cool-chic video codec, where we demonstrate a rate\ndecrease of more than 10% and a lowering of motion-related decoding complexity\nfrom 391 MAC per pixel to 214 MAC per pixel. All contributions are made\nopen-source at https://github.com/Orange-OpenSource/Cool-Chic",
      "url": "http://arxiv.org/abs/2507.21926v1",
      "published_time_eastern_timestamp": 1753803677.0
    },
    {
      "title": "ArtSeek: Deep artwork understanding via multimodal in-context reasoning\n  and late interaction retrieval",
      "summary": "Analyzing digitized artworks presents unique challenges, requiring not only\nvisual interpretation but also a deep understanding of rich artistic,\ncontextual, and historical knowledge. We introduce ArtSeek, a multimodal\nframework for art analysis that combines multimodal large language models with\nretrieval-augmented generation. Unlike prior work, our pipeline relies only on\nimage input, enabling applicability to artworks without links to Wikidata or\nWikipedia-common in most digitized collections. ArtSeek integrates three key\ncomponents: an intelligent multimodal retrieval module based on late\ninteraction retrieval, a contrastive multitask classification network for\npredicting artist, genre, style, media, and tags, and an agentic reasoning\nstrategy enabled through in-context examples for complex visual question\nanswering and artwork explanation via Qwen2.5-VL. Central to this approach is\nWikiFragments, a Wikipedia-scale dataset of image-text fragments curated to\nsupport knowledge-grounded multimodal reasoning. Our framework achieves\nstate-of-the-art results on multiple benchmarks, including a +8.4% F1\nimprovement in style classification over GraphCLIP and a +7.1 BLEU@1 gain in\ncaptioning on ArtPedia. Qualitative analyses show that ArtSeek can interpret\nvisual motifs, infer historical context, and retrieve relevant knowledge, even\nfor obscure works. Though focused on visual arts, our approach generalizes to\nother domains requiring external knowledge, supporting scalable multimodal AI\nresearch. Both the dataset and the source code will be made publicly available\nat https://github.com/cilabuniba/artseek.",
      "url": "http://arxiv.org/abs/2507.21917v1",
      "published_time_eastern_timestamp": 1753803118.0
    },
    {
      "title": "Predict Patient Self-reported Race from Skin Histological Images",
      "summary": "Artificial Intelligence (AI) has demonstrated success in computational\npathology (CPath) for disease detection, biomarker classification, and\nprognosis prediction. However, its potential to learn unintended demographic\nbiases, particularly those related to social determinants of health, remains\nunderstudied. This study investigates whether deep learning models can predict\nself-reported race from digitized dermatopathology slides and identifies\npotential morphological shortcuts. Using a multisite dataset with a racially\ndiverse population, we apply an attention-based mechanism to uncover\nrace-associated morphological features. After evaluating three dataset curation\nstrategies to control for confounding factors, the final experiment showed that\nWhite and Black demographic groups retained high prediction performance (AUC:\n0.799, 0.762), while overall performance dropped to 0.663. Attention analysis\nrevealed the epidermis as a key predictive feature, with significant\nperformance declines when these regions were removed. These findings highlight\nthe need for careful data curation and bias mitigation to ensure equitable AI\ndeployment in pathology. Code available at:\nhttps://github.com/sinai-computational-pathology/CPath_SAIF.",
      "url": "http://arxiv.org/abs/2507.21912v1",
      "published_time_eastern_timestamp": 1753802859.0
    },
    {
      "title": "Evaluating Deepfake Detectors in the Wild",
      "summary": "Deepfakes powered by advanced machine learning models present a significant\nand evolving threat to identity verification and the authenticity of digital\nmedia. Although numerous detectors have been developed to address this problem,\ntheir effectiveness has yet to be tested when applied to real-world data. In\nthis work we evaluate modern deepfake detectors, introducing a novel testing\nprocedure designed to mimic real-world scenarios for deepfake detection. Using\nstate-of-the-art deepfake generation methods, we create a comprehensive dataset\ncontaining more than 500,000 high-quality deepfake images. Our analysis shows\nthat detecting deepfakes still remains a challenging task. The evaluation shows\nthat in fewer than half of the deepfake detectors tested achieved an AUC score\ngreater than 60%, with the lowest being 50%. We demonstrate that basic image\nmanipulations, such as JPEG compression or image enhancement, can significantly\nreduce model performance. All code and data are publicly available at\nhttps://github.com/messlav/Deepfake-Detectors-in-the-Wild.",
      "url": "http://arxiv.org/abs/2507.21905v1",
      "published_time_eastern_timestamp": 1753802220.0
    },
    {
      "title": "LLM-based Content Classification Approach for GitHub Repositories by the\n  README Files",
      "summary": "GitHub is the world's most popular platform for storing, sharing, and\nmanaging code. Every GitHub repository has a README file associated with it.\nThe README files should contain project-related information as per the\nrecommendations of GitHub to support the usage and improvement of repositories.\nHowever, GitHub repository owners sometimes neglected these recommendations.\nThis prevents a GitHub repository from reaching its full potential. This\nresearch posits that the comprehensiveness of a GitHub repository's README file\nsignificantly influences its adoption and utilization, with a lack of detail\npotentially hindering its full potential for widespread engagement and impact\nwithin the research community. Large Language Models (LLMs) have shown great\nperformance in many text-based tasks including text classification, text\ngeneration, text summarization and text translation. In this study, an approach\nis developed to fine-tune LLMs for automatically classifying different sections\nof GitHub README files. Three encoder-only LLMs are utilized, including BERT,\nDistilBERT and RoBERTa. These pre-trained models are then fine-tuned based on a\ngold-standard dataset consisting of 4226 README file sections. This approach\noutperforms current state-of-the-art methods and has achieved an overall F1\nscore of 0.98. Moreover, we have also investigated the use of\nParameter-Efficient Fine-Tuning (PEFT) techniques like Low-Rank Adaptation\n(LoRA) and shown an economical alternative to full fine-tuning without\ncompromising much performance. The results demonstrate the potential of using\nLLMs in designing an automatic classifier for categorizing the content of\nGitHub README files. Consequently, this study contributes to the development of\nautomated tools for GitHub repositories to improve their identifications and\npotential usages.",
      "url": "http://arxiv.org/abs/2507.21899v1",
      "published_time_eastern_timestamp": 1753801778.0
    },
    {
      "title": "Tiny-BioMoE: a Lightweight Embedding Model for Biosignal Analysis",
      "summary": "Pain is a complex and pervasive condition that affects a significant portion\nof the population. Accurate and consistent assessment is essential for\nindividuals suffering from pain, as well as for developing effective management\nstrategies in a healthcare system. Automatic pain assessment systems enable\ncontinuous monitoring, support clinical decision-making, and help minimize\npatient distress while mitigating the risk of functional deterioration.\nLeveraging physiological signals offers objective and precise insights into a\nperson's state, and their integration in a multimodal framework can further\nenhance system performance. This study has been submitted to the \\textit{Second\nMultimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)}. The\nproposed approach introduces \\textit{Tiny-BioMoE}, a lightweight pretrained\nembedding model for biosignal analysis. Trained on $4.4$ million biosignal\nimage representations and consisting of only $7.3$ million parameters, it\nserves as an effective tool for extracting high-quality embeddings for\ndownstream tasks. Extensive experiments involving electrodermal activity, blood\nvolume pulse, respiratory signals, peripheral oxygen saturation, and their\ncombinations highlight the model's effectiveness across diverse modalities in\nautomatic pain recognition tasks. \\textit{\\textcolor{blue}{The model's\narchitecture (code) and weights are available at\nhttps://github.com/GkikasStefanos/Tiny-BioMoE.",
      "url": "http://arxiv.org/abs/2507.21875v1",
      "published_time_eastern_timestamp": 1753800399.0
    },
    {
      "title": "A Neuro-Symbolic Approach for Probabilistic Reasoning on Graph Data",
      "summary": "Graph neural networks (GNNs) excel at predictive tasks on graph-structured\ndata but often lack the ability to incorporate symbolic domain knowledge and\nperform general reasoning. Relational Bayesian Networks (RBNs), in contrast,\nenable fully generative probabilistic modeling over graph-like structures and\nsupport rich symbolic knowledge and probabilistic inference. This paper\npresents a neuro-symbolic framework that seamlessly integrates GNNs into RBNs,\ncombining the learning strength of GNNs with the flexible reasoning\ncapabilities of RBNs.\n  We develop two implementations of this integration: one compiles GNNs\ndirectly into the native RBN language, while the other maintains the GNN as an\nexternal component. Both approaches preserve the semantics and computational\nproperties of GNNs while fully aligning with the RBN modeling paradigm. We also\npropose a maximum a-posteriori (MAP) inference method for these neuro-symbolic\nmodels.\n  To demonstrate the framework's versatility, we apply it to two distinct\nproblems. First, we transform a GNN for node classification into a collective\nclassification model that explicitly models homo- and heterophilic label\npatterns, substantially improving accuracy. Second, we introduce a\nmulti-objective network optimization problem in environmental planning, where\nMAP inference supports complex decision-making. Both applications include new\npublicly available benchmark datasets.\n  This work introduces a powerful and coherent neuro-symbolic approach to graph\ndata, bridging learning and reasoning in ways that enable novel applications\nand improved performance across diverse tasks.",
      "url": "http://arxiv.org/abs/2507.21873v1",
      "published_time_eastern_timestamp": 1753800205.0
    },
    {
      "title": "Gamma rays as leptonic portals to energetic neutrinos: a new Monte Carlo\n  approach",
      "summary": "High center-of-mass electromagnetic~(EM) interactions could produce decaying\nheavy leptons and hadrons, leading to neutrino generation. These processes\nmight occur in the most extreme astrophysical scenarios, potentially altering\nthe expected gamma-ray and neutrino fluxes in both the hadronic and the\nleptonic pictures. For instance, neutrinos could arise from high-redshift EM\ncascades, triggered by gamma rays beyond $10^{18} \\; \\text{eV}$ scattering\nbackground photons, from radio to ultraviolet energy bands. Such energetic\ngamma rays are predicted in cosmogenic models and in scenarios involving\nnon-standard physics. On astrophysical scales, leptonic production of neutrinos\ncould take place in active galactic nuclei cores, where several-TeV gamma rays\ninteract with the X-ray photons from the hot corona. We explore these scenarios\nwithin the CRPropa Monte Carlo code framework, developing dedicated tools to\naccount for leptonic production and decay of heavy leptons and hadrons. In\nparticular, the latter are performed by interfacing with the PYTHIA event\ngenerator. With these novel tools, we characterise the spectrum and flavour\ncomposition of neutrinos emerging from cosmological EM cascades and from\nleptonic processes in the core of active galactic nuclei. Finally, we\ninvestigate the leptonic production of neutrinos in the context of the IceCube\ndetection of NGC~1068.",
      "url": "http://arxiv.org/abs/2507.21867v1",
      "published_time_eastern_timestamp": 1753799982.0
    },
    {
      "title": "VidFuncta: Towards Generalizable Neural Representations for Ultrasound\n  Videos",
      "summary": "Ultrasound is widely used in clinical care, yet standard deep learning\nmethods often struggle with full video analysis due to non-standardized\nacquisition and operator bias. We offer a new perspective on ultrasound video\nanalysis through implicit neural representations (INRs). We build on Functa, an\nINR framework in which each image is represented by a modulation vector that\nconditions a shared neural network. However, its extension to the temporal\ndomain of medical videos remains unexplored. To address this gap, we propose\nVidFuncta, a novel framework that leverages Functa to encode variable-length\nultrasound videos into compact, time-resolved representations. VidFuncta\ndisentangles each video into a static video-specific vector and a sequence of\ntime-dependent modulation vectors, capturing both temporal dynamics and\ndataset-level redundancies. Our method outperforms 2D and 3D baselines on video\nreconstruction and enables downstream tasks to directly operate on the learned\n1D modulation vectors. We validate VidFuncta on three public ultrasound video\ndatasets -- cardiac, lung, and breast -- and evaluate its downstream\nperformance on ejection fraction prediction, B-line detection, and breast\nlesion classification. These results highlight the potential of VidFuncta as a\ngeneralizable and efficient representation framework for ultrasound videos. Our\ncode is publicly available under\nhttps://github.com/JuliaWolleb/VidFuncta_public.",
      "url": "http://arxiv.org/abs/2507.21863v1",
      "published_time_eastern_timestamp": 1753799708.0
    }
  ]
}
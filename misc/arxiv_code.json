{
  "last_updated": "2025-05-11T17:10:01.774518-04:00",
  "papers": [
    {
      "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
      "summary": "We propose Flow-GRPO, the first method integrating online reinforcement\nlearning (RL) into flow matching models. Our approach uses two key strategies:\n(1) an ODE-to-SDE conversion that transforms a deterministic Ordinary\nDifferential Equation (ODE) into an equivalent Stochastic Differential Equation\n(SDE) that matches the original model's marginal distribution at all timesteps,\nenabling statistical sampling for RL exploration; and (2) a Denoising Reduction\nstrategy that reduces training denoising steps while retaining the original\ninference timestep number, significantly improving sampling efficiency without\nperformance degradation. Empirically, Flow-GRPO is effective across multiple\ntext-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly\nperfect object counts, spatial relations, and fine-grained attributes, boosting\nGenEval accuracy from $63\\%$ to $95\\%$. In visual text rendering, its accuracy\nimproves from $59\\%$ to $92\\%$, significantly enhancing text generation.\nFlow-GRPO also achieves substantial gains in human preference alignment.\nNotably, little to no reward hacking occurred, meaning rewards did not increase\nat the cost of image quality or diversity, and both remained stable in our\nexperiments.",
      "url": "http://arxiv.org/abs/2505.05470v1",
      "published_time_eastern_timestamp": 1746727125.0
    },
    {
      "title": "Generating Physically Stable and Buildable LEGO Designs from Text",
      "summary": "We introduce LegoGPT, the first approach for generating physically stable\nLEGO brick models from text prompts. To achieve this, we construct a\nlarge-scale, physically stable dataset of LEGO designs, along with their\nassociated captions, and train an autoregressive large language model to\npredict the next brick to add via next-token prediction. To improve the\nstability of the resulting designs, we employ an efficient validity check and\nphysics-aware rollback during autoregressive inference, which prunes infeasible\ntoken predictions using physics laws and assembly constraints. Our experiments\nshow that LegoGPT produces stable, diverse, and aesthetically pleasing LEGO\ndesigns that align closely with the input text prompts. We also develop a\ntext-based LEGO texturing method to generate colored and textured designs. We\nshow that our designs can be assembled manually by humans and automatically by\nrobotic arms. We also release our new dataset, StableText2Lego, containing over\n47,000 LEGO structures of over 28,000 unique 3D objects accompanied by detailed\ncaptions, along with our code and models at the project website:\nhttps://avalovelace1.github.io/LegoGPT/.",
      "url": "http://arxiv.org/abs/2505.05469v1",
      "published_time_eastern_timestamp": 1746727098.0
    },
    {
      "title": "Bring Reason to Vision: Understanding Perception and Reasoning through\n  Model Merging",
      "summary": "Vision-Language Models (VLMs) combine visual perception with the general\ncapabilities, such as reasoning, of Large Language Models (LLMs). However, the\nmechanisms by which these two abilities can be combined and contribute remain\npoorly understood. In this work, we explore to compose perception and reasoning\nthrough model merging that connects parameters of different models. Unlike\nprevious works that often focus on merging models of the same kind, we propose\nmerging models across modalities, enabling the incorporation of the reasoning\ncapabilities of LLMs into VLMs. Through extensive experiments, we demonstrate\nthat model merging offers a successful pathway to transfer reasoning abilities\nfrom LLMs to VLMs in a training-free manner. Moreover, we utilize the merged\nmodels to understand the internal mechanism of perception and reasoning and how\nmerging affects it. We find that perception capabilities are predominantly\nencoded in the early layers of the model, whereas reasoning is largely\nfacilitated by the middle-to-late layers. After merging, we observe that all\nlayers begin to contribute to reasoning, whereas the distribution of perception\nabilities across layers remains largely unchanged. These observations shed\nlight on the potential of model merging as a tool for multimodal integration\nand interpretation.",
      "url": "http://arxiv.org/abs/2505.05464v1",
      "published_time_eastern_timestamp": 1746726983.0
    },
    {
      "title": "Resolution of the Solar Convective Conundrum? New Results Using the\n  Time-Distance Deep-Focus Method",
      "summary": "We re-examine the deep-focus methodology of time-distance helioseismology\npreviously used to estimate the power spectrum of the solar convection at a\ndepth of about 30 Mm, which was found to be significantly weaker than predicted\nby theory and simulations. The Global Acoustic, Linearized Euler (GALE) and\nEulerian Lagrangian (EULAG) codes are used to generate ground-truth simulations\nthrough which the accuracy of the convective power spectrum can be evaluated.\nThis validation process shows that the power spectrum diverges significantly\nfrom ground truth beyond spatial scales corresponding to the spherical harmonic\ndegree $\\ell=15$ - $30$ because of the limited resolution of helioseismic\nmeasurements. However, the power estimated at larger spatial scales ($\\ell<15$)\nis sufficiently accurate. We then apply the methodology to solar data and find\na spectrum that is substantially stronger than previously reported. We discuss\nsome possible differences in methodology that might have led to the initial\nunder-estimation of solar convective power. The new spectra are in line with\nrecent hydrodynamic and magnetohydrodynamic simulations of solar convection and\nalso consistent with the previous inferences obtained by the ring-diagram\nlocal-helioseismology method.",
      "url": "http://arxiv.org/abs/2505.05454v1",
      "published_time_eastern_timestamp": 1746726317.0
    },
    {
      "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual\n  Document Understanding",
      "summary": "Visual Document Understanding has become essential with the increase of\ntext-rich visual content. This field poses significant challenges due to the\nneed for effective integration of visual perception and textual comprehension,\nparticularly across diverse document types with complex layouts. Moreover,\nexisting fine-tuning datasets for this domain often fall short in providing the\ndetailed contextual information for robust understanding, leading to\nhallucinations and limited comprehension of spatial relationships among visual\nelements. To address these challenges, we propose an innovative pipeline that\nutilizes adaptive generation of markup languages, such as Markdown, JSON, HTML,\nand TiKZ, to build highly structured document representations and deliver\ncontextually-grounded responses. We introduce two fine-grained structured\ndatasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs\nfor document parsing, and DocMark-Instruct, featuring 624k fine-tuning data\nannotations for grounded instruction following. Extensive experiments\ndemonstrate that our proposed model significantly outperforms existing\nstate-of-theart MLLMs across a range of visual document understanding\nbenchmarks, facilitating advanced reasoning and comprehension capabilities in\ncomplex visual scenarios. Our code and models are released at https://github.\ncom/Euphoria16/DocMark.",
      "url": "http://arxiv.org/abs/2505.05446v1",
      "published_time_eastern_timestamp": 1746725856.0
    },
    {
      "title": "Artifact Sharing for Information Retrieval Research",
      "summary": "Sharing artifacts -- such as trained models, pre-built indexes, and the code\nto use them -- aids in reproducibility efforts by allowing researchers to\nvalidate intermediate steps and improves the sustainability of research by\nallowing multiple groups to build off one another's prior computational work.\nAlthough there are de facto consensuses on how to share research code (through\na git repository linked to from publications) and trained models (via\nHuggingFace Hub), there is no consensus for other types of artifacts, such as\nbuilt indexes. Given the practical utility of using shared indexes, researchers\nhave resorted to self-hosting these resources or performing ad hoc file\ntransfers upon request, ultimately limiting the artifacts' discoverability and\nreuse. This demonstration introduces a flexible and interoperable way to share\nartifacts for Information Retrieval research, improving both their\naccessibility and usability.",
      "url": "http://arxiv.org/abs/2505.05434v1",
      "published_time_eastern_timestamp": 1746725012.0
    },
    {
      "title": "TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and\n  Generation",
      "summary": "Pioneering token-based works such as Chameleon and Emu3 have established a\nfoundation for multimodal unification but face challenges of high training\ncomputational overhead and limited comprehension performance due to a lack of\nhigh-level semantics. In this paper, we introduce TokLIP, a visual tokenizer\nthat enhances comprehension by semanticizing vector-quantized (VQ) tokens and\nincorporating CLIP-level semantics while enabling end-to-end multimodal\nautoregressive training with standard VQ tokens. TokLIP integrates a low-level\ndiscrete VQ tokenizer with a ViT-based token encoder to capture high-level\ncontinuous semantics. Unlike previous approaches (e.g., VILA-U) that discretize\nhigh-level features, TokLIP disentangles training objectives for comprehension\nand generation, allowing the direct application of advanced VQ tokenizers\nwithout the need for tailored quantization operations. Our empirical results\ndemonstrate that TokLIP achieves exceptional data efficiency, empowering visual\ntokens with high-level semantic understanding while enhancing low-level\ngenerative capacity, making it well-suited for autoregressive Transformers in\nboth comprehension and generation tasks. The code and models are available at\nhttps://github.com/TencentARC/TokLIP.",
      "url": "http://arxiv.org/abs/2505.05422v1",
      "published_time_eastern_timestamp": 1746724339.0
    },
    {
      "title": "PillarMamba: Learning Local-Global Context for Roadside Point Cloud via\n  Hybrid State Space Model",
      "summary": "Serving the Intelligent Transport System (ITS) and Vehicle-to-Everything\n(V2X) tasks, roadside perception has received increasing attention in recent\nyears, as it can extend the perception range of connected vehicles and improve\ntraffic safety. However, roadside point cloud oriented 3D object detection has\nnot been effectively explored. To some extent, the key to the performance of a\npoint cloud detector lies in the receptive field of the network and the ability\nto effectively utilize the scene context. The recent emergence of Mamba, based\non State Space Model (SSM), has shaken up the traditional convolution and\ntransformers that have long been the foundational building blocks, due to its\nefficient global receptive field. In this work, we introduce Mamba to\npillar-based roadside point cloud perception and propose a framework based on\nCross-stage State-space Group (CSG), called PillarMamba. It enhances the\nexpressiveness of the network and achieves efficient computation through\ncross-stage feature fusion. However, due to the limitations of scan directions,\nstate space model faces local connection disrupted and historical relationship\nforgotten. To address this, we propose the Hybrid State-space Block (HSB) to\nobtain the local-global context of roadside point cloud. Specifically, it\nenhances neighborhood connections through local convolution and preserves\nhistorical memory through residual attention. The proposed method outperforms\nthe state-of-the-art methods on the popular large scale roadside benchmark:\nDAIR-V2X-I. The code will be released soon.",
      "url": "http://arxiv.org/abs/2505.05397v1",
      "published_time_eastern_timestamp": 1746721984.0
    },
    {
      "title": "Threshold Modulation for Online Test-Time Adaptation of Spiking Neural\n  Networks",
      "summary": "Recently, spiking neural networks (SNNs), deployed on neuromorphic chips,\nprovide highly efficient solutions on edge devices in different scenarios.\nHowever, their ability to adapt to distribution shifts after deployment has\nbecome a crucial challenge. Online test-time adaptation (OTTA) offers a\npromising solution by enabling models to dynamically adjust to new data\ndistributions without requiring source data or labeled target samples.\nNevertheless, existing OTTA methods are largely designed for traditional\nartificial neural networks and are not well-suited for SNNs. To address this\ngap, we propose a low-power, neuromorphic chip-friendly online test-time\nadaptation framework, aiming to enhance model generalization under distribution\nshifts. The proposed approach is called Threshold Modulation (TM), which\ndynamically adjusts the firing threshold through neuronal dynamics-inspired\nnormalization, being more compatible with neuromorphic hardware. Experimental\nresults on benchmark datasets demonstrate the effectiveness of this method in\nimproving the robustness of SNNs against distribution shifts while maintaining\nlow computational cost. The proposed method offers a practical solution for\nonline test-time adaptation of SNNs, providing inspiration for the design of\nfuture neuromorphic chips. The demo code is available at\ngithub.com/NneurotransmitterR/TM-OTTA-SNN.",
      "url": "http://arxiv.org/abs/2505.05375v1",
      "published_time_eastern_timestamp": 1746720580.0
    },
    {
      "title": "From Sleep Staging to Spindle Detection: Evaluating End-to-End Automated\n  Sleep Analysis",
      "summary": "Automation of sleep analysis, including both macrostructural (sleep stages)\nand microstructural (e.g., sleep spindles) elements, promises to enable\nlarge-scale sleep studies and to reduce variance due to inter-rater\nincongruencies. While individual steps, such as sleep staging and spindle\ndetection, have been studied separately, the feasibility of automating\nmulti-step sleep analysis remains unclear. Here, we evaluate whether a fully\nautomated analysis using state-of-the-art machine learning models for sleep\nstaging (RobustSleepNet) and subsequent spindle detection (SUMOv2) can\nreplicate findings from an expert-based study of bipolar disorder. The\nautomated analysis qualitatively reproduced key findings from the expert-based\nstudy, including significant differences in fast spindle densities between\nbipolar patients and healthy controls, accomplishing in minutes what previously\ntook months to complete manually. While the results of the automated analysis\ndiffered quantitatively from the expert-based study, possibly due to biases\nbetween expert raters or between raters and the models, the models individually\nperformed at or above inter-rater agreement for both sleep staging and spindle\ndetection. Our results demonstrate that fully automated approaches have the\npotential to facilitate large-scale sleep research. We are providing public\naccess to the tools used in our automated analysis by sharing our code and\nintroducing SomnoBot, a privacy-preserving sleep analysis platform.",
      "url": "http://arxiv.org/abs/2505.05371v1",
      "published_time_eastern_timestamp": 1746720430.0
    },
    {
      "title": "Walrus: An Efficient Decentralized Storage Network",
      "summary": "Decentralized storage systems face a fundamental trade-off between\nreplication overhead, recovery efficiency, and security guarantees. Current\napproaches either rely on full replication, incurring substantial storage\ncosts, or employ trivial erasure coding schemes that struggle with efficient\nrecovery especially under high storage-node churn. We present Walrus, a novel\ndecentralized blob storage system that addresses these limitations through\nmultiple technical innovations. At the core of Walrus is RedStuff, a\ntwo-dimensional erasure coding protocol that achieves high security with only\n4.5x replication factor, while enabling self-healing recovery that requires\nbandwidth proportional to only the lost data $(O(|blob|/n)$ versus $O(|blob|)$\nin traditional systems). Crucially, RedStuff is the first protocol to support\nstorage challenges in asynchronous networks, preventing adversaries from\nexploiting network delays to pass verification without actually storing data.\nWalrus also introduces a novel multi-stage epoch change protocol that\nefficiently handles storage node churn while maintaining uninterrupted\navailability during committee transitions. Our system incorporates\nauthenticated data structures to defend against malicious clients and ensures\ndata consistency throughout storage and retrieval processes. Experimental\nevaluation demonstrates that Walrus achieves practical performance at scale,\nmaking it suitable for a wide range of decentralized applications requiring\nhigh-integrity, available blob storage with reasonable overhead.",
      "url": "http://arxiv.org/abs/2505.05370v1",
      "published_time_eastern_timestamp": 1746720401.0
    },
    {
      "title": "SDR-RDMA: Software-Defined Reliability Architecture for Planetary Scale\n  RDMA Communication",
      "summary": "RDMA is vital for efficient distributed training across datacenters, but\nmillisecond-scale latencies complicate the design of its reliability layer. We\nshow that depending on long-haul link characteristics, such as drop rate,\ndistance and bandwidth, the widely used Selective Repeat algorithm can be\ninefficient, warranting alternatives like Erasure Coding. To enable such\nalternatives on existing hardware, we propose SDR-RDMA, a software-defined\nreliability stack for RDMA. Its core is a lightweight SDR SDK that extends\nstandard point-to-point RDMA semantics -- fundamental to AI networking stacks\n-- with a receive buffer bitmap. SDR bitmap enables partial message completion\nto let applications implement custom reliability schemes tailored to specific\ndeployments, while preserving zero-copy RDMA benefits. By offloading the SDR\nbackend to NVIDIA's Data Path Accelerator (DPA), we achieve line-rate\nperformance, enabling efficient inter-datacenter communication and advancing\nreliability innovation for intra-datacenter training.",
      "url": "http://arxiv.org/abs/2505.05366v1",
      "published_time_eastern_timestamp": 1746720215.0
    },
    {
      "title": "Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound\n  Source Localization",
      "summary": "Large-scale vision-language models demonstrate strong multimodal alignment\nand generalization across diverse tasks. Among them, CLIP stands out as one of\nthe most successful approaches. In this work, we extend the application of CLIP\nto sound source localization, proposing a self-supervised method operates\nwithout explicit text input. We introduce a framework that maps audios into\ntokens compatible with CLIP's text encoder, producing audio-driven embeddings.\nThese embeddings are used to generate sounding region masks, from which visual\nfeatures are extracted and aligned with the audio embeddings through a\ncontrastive audio-visual correspondence objective. Our findings show that\nalignment knowledge of pre-trained multimodal foundation model enables our\nmethod to generate more complete and compact localization for sounding objects.\nWe further propose an LLM-guided extension that distills object-aware\naudio-visual scene understanding into the model during training to enhance\nalignment. Extensive experiments across five diverse tasks demonstrate that our\nmethod, in all variants, outperforms state-of-the-art approaches and achieves\nstrong generalization in zero-shot settings.",
      "url": "http://arxiv.org/abs/2505.05343v1",
      "published_time_eastern_timestamp": 1746718324.0
    },
    {
      "title": "TS-Detector : Detecting Feature Toggle Usage Patterns",
      "summary": "Feature toggles enable developers to control feature states, allowing the\nfeatures to be released to a limited group of users while preserving overall\nsoftware functionality. The absence of comprehensive best practices for feature\ntoggle usage often results in improper implementation, causing code quality\nissues. Although certain feature toggle usage patterns are prone to toggle\nsmells, there is no tool as of today for software engineers to detect toggle\nusage patterns from the source code. This paper presents a tool TS-Detector to\ndetect five different toggle usage patterns across ten open-source software\nprojects in six different programming languages. We conducted a manual\nevaluation and results show that the true positive rates of detecting Spread,\nNested, and Dead toggles are 80%, 86.4%, and 66.6% respectively, and the true\nnegative rate of Mixed and Enum usages was 100%. The tool can be downloaded\nfrom its GitHub repository and can be used following the instructions provided\nthere.",
      "url": "http://arxiv.org/abs/2505.05326v1",
      "published_time_eastern_timestamp": 1746717319.0
    },
    {
      "title": "CottonSim: Development of an autonomous visual-guided robotic\n  cotton-picking system in the Gazebo",
      "summary": "In this study, an autonomous visual-guided robotic cotton-picking system,\nbuilt on a Clearpath's Husky robot platform and the Cotton-Eye perception\nsystem, was developed in the Gazebo robotic simulator. Furthermore, a virtual\ncotton farm was designed and developed as a Robot Operating System (ROS 1)\npackage to deploy the robotic cotton picker in the Gazebo environment for\nsimulating autonomous field navigation. The navigation was assisted by the map\ncoordinates and an RGB-depth camera, while the ROS navigation algorithm\nutilized a trained YOLOv8n-seg model for instance segmentation. The model\nachieved a desired mean Average Precision (mAP) of 85.2%, a recall of 88.9%,\nand a precision of 93.0% for scene segmentation. The developed ROS navigation\npackages enabled our robotic cotton-picking system to autonomously navigate\nthrough the cotton field using map-based and GPS-based approaches, visually\naided by a deep learning-based perception system. The GPS-based navigation\napproach achieved a 100% completion rate (CR) with a threshold of 5 x 10^-6\ndegrees, while the map-based navigation approach attained a 96.7% CR with a\nthreshold of 0.25 m. This study establishes a fundamental baseline of\nsimulation for future agricultural robotics and autonomous vehicles in cotton\nfarming and beyond. CottonSim code and data are released to the research\ncommunity via GitHub: https://github.com/imtheva/CottonSim",
      "url": "http://arxiv.org/abs/2505.05317v1",
      "published_time_eastern_timestamp": 1746716555.0
    },
    {
      "title": "Augmented Deep Contexts for Spatially Embedded Video Coding",
      "summary": "Most Neural Video Codecs (NVCs) only employ temporal references to generate\ntemporal-only contexts and latent prior. These temporal-only NVCs fail to\nhandle large motions or emerging objects due to limited contexts and misaligned\nlatent prior. To relieve the limitations, we propose a Spatially Embedded Video\nCodec (SEVC), in which the low-resolution video is compressed for spatial\nreferences. Firstly, our SEVC leverages both spatial and temporal references to\ngenerate augmented motion vectors and hybrid spatial-temporal contexts.\nSecondly, to address the misalignment issue in latent prior and enrich the\nprior information, we introduce a spatial-guided latent prior augmented by\nmultiple temporal latent representations. At last, we design a joint\nspatial-temporal optimization to learn quality-adaptive bit allocation for\nspatial references, further boosting rate-distortion performance. Experimental\nresults show that our SEVC effectively alleviates the limitations in handling\nlarge motions or emerging objects, and also reduces 11.9% more bitrate than the\nprevious state-of-the-art NVC while providing an additional low-resolution\nbitstream. Our code and model are available at https://github.com/EsakaK/SEVC.",
      "url": "http://arxiv.org/abs/2505.05309v1",
      "published_time_eastern_timestamp": 1746716272.0
    },
    {
      "title": "HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic\n  Text-to-SQL Workflow",
      "summary": "Recent advances in leveraging the agentic paradigm of large language models\n(LLMs) utilization have significantly enhanced Text-to-SQL capabilities,\nenabling users without specialized database expertise to query data\nintuitively. However, deploying these agentic LLM-based Text-to-SQL systems in\nproduction poses substantial challenges due to their inherently multi-stage\nworkflows, stringent latency constraints, and potentially heterogeneous GPU\ninfrastructure in enterprise environments. Current LLM serving frameworks lack\neffective mechanisms for handling interdependent inference tasks, dynamic\nlatency variability, and resource heterogeneity, leading to suboptimal\nperformance and frequent service-level objective (SLO) violations. In this\npaper, we introduce HEXGEN-TEXT2SQL, a novel framework designed explicitly to\nschedule and execute agentic multi-stage LLM-based Text-to-SQL workflows on\nheterogeneous GPU clusters that handle multi-tenant end-to-end queries.\nHEXGEN-TEXT2SQL introduce a hierarchical scheduling approach combining global\nworkload-balanced task dispatching and local adaptive urgency-guided\nprioritization, guided by a systematic analysis of agentic Text-to-SQL\nworkflows. Additionally, we propose a lightweight simulation-based method for\ntuning critical scheduling hyperparameters, further enhancing robustness and\nadaptability. Our extensive evaluation on realistic Text-to-SQL benchmarks\ndemonstrates that HEXGEN-TEXT2SQL significantly outperforms state-of-the-art\nLLM serving frameworks. Specifically, HEXGEN-TEXT2SQL reduces latency deadlines\nby up to 1.67$\\times$ (average: 1.41$\\times$) and improves system throughput by\nup to 1.75$\\times$ (average: 1.65$\\times$) compared to vLLM under diverse,\nrealistic workload conditions. Our code is available at\nhttps://github.com/Relaxed-System-Lab/Hexgen-Flow.",
      "url": "http://arxiv.org/abs/2505.05286v1",
      "published_time_eastern_timestamp": 1746714527.0
    },
    {
      "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for\n  CodeLLMs and Agents",
      "summary": "Code large language models (CodeLLMs) and agents have shown great promise in\ntackling complex software engineering tasks.Compared to traditional software\nengineering methods, CodeLLMs and agents offer stronger abilities, and can\nflexibly process inputs and outputs in both natural and code. Benchmarking\nplays a crucial role in evaluating the capabilities of CodeLLMs and agents,\nguiding their development and deployment. However, despite their growing\nsignificance, there remains a lack of comprehensive reviews of benchmarks for\nCodeLLMs and agents. To bridge this gap, this paper provides a comprehensive\nreview of existing benchmarks for CodeLLMs and agents, studying and analyzing\n181 benchmarks from 461 relevant papers, covering the different phases of the\nsoftware development life cycle (SDLC). Our findings reveal a notable imbalance\nin the coverage of current benchmarks, with approximately 60% focused on the\nsoftware development phase in SDLC, while requirements engineering and software\ndesign phases receive minimal attention at only 5% and 3%, respectively.\nAdditionally, Python emerges as the dominant programming language across the\nreviewed benchmarks. Finally, this paper highlights the challenges of current\nresearch and proposes future directions, aiming to narrow the gap between the\ntheoretical capabilities of CodeLLMs and agents and their application in\nreal-world scenarios.",
      "url": "http://arxiv.org/abs/2505.05283v1",
      "published_time_eastern_timestamp": 1746714465.0
    },
    {
      "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
      "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
      "url": "http://arxiv.org/abs/2505.05251v1",
      "published_time_eastern_timestamp": 1746712580.0
    },
    {
      "title": "Bounds on $k$-hash distances and rates of linear codes",
      "summary": "In this paper, we bound the rate of linear codes in $\\mathbb{F}_q^n$ with the\nproperty that any $k \\leq q$ codewords are all simultaneously distinct in at\nleast $d_k$ coordinates. For the particular case $d_k=1$, this leads to bounds\non the rate of linear $q$-ary $k$-hash codes which generalize, with a simpler\nproof, results recently obtained for the case $q=k=3$ by Pohoata and Zakharov\nand by Bishnoi D'haeseleeer and Gijswijt. We finally discuss some related open\nproblems on the list-decoding zero-error capacity of discrete memoryless\nchannels.",
      "url": "http://arxiv.org/abs/2505.05239v1",
      "published_time_eastern_timestamp": 1746711242.0
    }
  ]
}
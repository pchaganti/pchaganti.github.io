{
  "last_updated": "2025-10-23T02:18:29.905869-04:00",
  "papers": [
    {
      "title": "Supermassive Black Hole Growth in Massive Galaxies at Cosmic Dawn",
      "summary": "Among the emerging excess of massive, bright galaxies at Cosmic Dawn $z\n\\gtrsim 9$ seen by the James Webb Space Telescope, several exhibit spectral\nfeatures associated with active galactic nuclei (AGN). These AGN candidates\nsuggest that supermassive black holes (SMBHs) grow rapidly in the early\nUniverse. In a series of numerical experiments, we investigate how SMBHs grow\nwithin and influence the most massive galaxies at Cosmic Dawn using\ncosmological hydrodynamic zoom-in simulations run with the adaptive mesh\nrefinement code RAMSES. Our suite of simulations explore how super-Eddington\naccretion, seed mass, and the strength of feedback influence SMBH-galaxy\nco-evolution in the early Universe. We find that SMBH growth is sensitive to\nstellar feedback which generates a turbulent-multiphase interstellar medium\n(ISM) that stochastically starves the SMBH. In the absence of AGN feedback, we\nfind that the SMBH is starved $\\sim 50\\%$ of the time after the onset of star\nformation in the galaxy. SMBH growth can become self-regulated by AGN feedback\nif the SMBH becomes massive enough, either by accretion or seeding, for its\nfeedback to dominate the surrounding nuclear region. We find no evidence of\ngalaxy-scale, AGN-driven quenching in the star formation rate (SFR) across all\nsimulations in our suite.",
      "url": "http://arxiv.org/abs/2510.19822v1",
      "published_time_eastern_timestamp": 1761155970.0
    },
    {
      "title": "olmOCR 2: Unit Test Rewards for Document OCR",
      "summary": "We present olmOCR 2, the latest in our family of powerful OCR systems for\nconverting digitized print documents, like PDFs, into clean, naturally ordered\nplain text. olmOCR 2 is powered by olmOCR-2-7B-1025, a specialized, 7B vision\nlanguage model (VLM) trained using reinforcement learning with verifiable\nrewards (RLVR), where our rewards are a diverse set of binary unit tests. To\nscale unit test creation, we develop a pipeline for generating synthetic\ndocuments with diverse and challenging layouts, known ground-truth HTML source\ncode, and extracted test cases. We show that RL training on these test cases\nresults in state-of-the-art performance on olmOCR-Bench, our English-language\nOCR benchmark, with the largest improvements in math formula conversion, table\nparsing, and multi-column layouts compared to previous versions. We release our\nmodel, data and code under permissive open licenses.",
      "url": "http://arxiv.org/abs/2510.19817v1",
      "published_time_eastern_timestamp": 1761155582.0
    },
    {
      "title": "How to Evaluate Monocular Depth Estimation?",
      "summary": "Monocular depth estimation is an important task with rapid progress, but how\nto evaluate it remains an open question, as evidenced by a lack of\nstandardization in existing literature and a large selection of evaluation\nmetrics whose trade-offs and behaviors are not well understood. This paper\ncontributes a novel, quantitative analysis of existing metrics in terms of\ntheir sensitivity to various types of perturbations of ground truth,\nemphasizing comparison to human judgment. Our analysis reveals that existing\nmetrics are severely under-sensitive to curvature perturbation such as making\nflat surfaces wavy. To remedy this, we introduce a new metric based on relative\nsurface normals, along with new depth visualization tools and a principled\nmethod to create composite metrics with better human alignment. Code and data\nare available at: https://github.com/princeton-vl/evalmde.",
      "url": "http://arxiv.org/abs/2510.19814v1",
      "published_time_eastern_timestamp": 1761155484.0
    },
    {
      "title": "Good quantum codes with addressable and parallelizable non-Clifford\n  gates",
      "summary": "We revisit a family of good quantum error-correcting codes presented in He\n$\\textit{et al.}$ (2025), and we show that various sets of addressable and\ntransversal non-Clifford multi-control-$Z$ gates can be performed in parallel.\nThe construction relies on the good classical codes of Stichtenoth (IEEE Trans.\nInf. Theory, 2006), which were previously instantiated in He $\\textit{et al.}$\n(2025), to yield quantum CSS codes over which addressable logical\n$\\mathsf{CCZ}$ gates can be performed at least one at a time. Here, we show\nthat for any $m$, there exists a family of good quantum error-correcting codes\nover qudits for which logical $\\mathsf{C}^{m}\\mathsf{Z}$ gates can address\nspecific logical qudits and be performed in parallel. This leads to a\nsignificant advantage in the depth overhead of multi-control-$Z$ circuits.",
      "url": "http://arxiv.org/abs/2510.19809v1",
      "published_time_eastern_timestamp": 1761155223.0
    },
    {
      "title": "Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing\n  LLM Reasoning",
      "summary": "Reinforcement learning from verifiable rewards has emerged as a powerful\ntechnique for enhancing the complex reasoning abilities of Large Language\nModels (LLMs). However, these methods are fundamentally constrained by the\n''learning cliff'' phenomenon: when faced with problems far beyond their\ncurrent capabilities, models consistently fail, yielding a persistent\nzero-reward signal. In policy optimization algorithms like GRPO, this collapses\nthe advantage calculation to zero, rendering these difficult problems invisible\nto the learning gradient and stalling progress. To overcome this, we introduce\nScaf-GRPO (Scaffolded Group Relative Policy Optimization), a progressive\ntraining framework that strategically provides minimal guidance only when a\nmodel's independent learning has plateaued. The framework first diagnoses\nlearning stagnation and then intervenes by injecting tiered in-prompt hints,\nranging from abstract concepts to concrete steps, enabling the model to\nconstruct a valid solution by itself. Extensive experiments on challenging\nmathematics benchmarks demonstrate Scaf-GRPO's effectiveness, boosting the\npass@1 score of the Qwen2.5-Math-7B model on the AIME24 benchmark by a relative\n44.3% over a vanilla GRPO baseline. This result demonstrates our framework\nprovides a robust and effective methodology for unlocking a model's ability to\nsolve problems previously beyond its reach, a critical step towards extending\nthe frontier of autonomous reasoning in LLM.",
      "url": "http://arxiv.org/abs/2510.19807v1",
      "published_time_eastern_timestamp": 1761154890.0
    },
    {
      "title": "Forbidden Sidon subsets of perfect difference sets, featuring a\n  human-assisted proof",
      "summary": "We resolve a $1000 Erd\\H{o}s prize problem, complete with formal verification\ngenerated by a large language model.\n  In over a dozen papers, beginning in 1976 and spanning two decades, Paul\nErd\\H{o}s repeatedly posed one of his \"favourite\" conjectures: every finite\nSidon set can be extended to a finite perfect difference set. We establish that\n{1, 2, 4, 8, 13} is a counterexample to this conjecture.\n  During the preparation of this paper, we discovered that although this\nproblem was presumed to be open for half a century, Marshall Hall, Jr.\npublished a different counterexample three decades before Erd\\H{o}s first posed\nthe problem. With a healthy skepticism of this apparent oversight, and out of\nan abundance of caution, we used ChatGPT to vibe code a Lean proof of both\nHall's and our counterexamples.",
      "url": "http://arxiv.org/abs/2510.19804v1",
      "published_time_eastern_timestamp": 1761154779.0
    },
    {
      "title": "Passive quantum error correction of photon loss at breakeven",
      "summary": "Physical qubits in a quantum computer are often represented by superposition\nstates of single particles or excitations. Decay of the excitation itself is a\nfundamental error channel that is difficult to overcome via external drive or\ncontrol techniques. Quantum error correcting codes, which encode information in\nsuperpositions involving multiple excitations, provide a path to preserve\ninformation beyond the capacity of individual excitations, but typically\nrequire exquisite active operations on the system. Here, we demonstrate a\nsteady-state driven dissipative quantum system, composed of a superconducting\ncavity and a transmon ancilla, that preserves a logical qubit beyond the\nphoton-lifetime limit by about 5% using a binomial encoding. This realization\nof continuous quantum error correction at the breakeven point highlights the\nquantitative competitiveness of passive correction strategies while\ncircumventing some demanding hardware requirements of its active counterparts.",
      "url": "http://arxiv.org/abs/2510.19794v1",
      "published_time_eastern_timestamp": 1761154107.0
    },
    {
      "title": "On Controlled Change: Generative AI's Impact on Professional Authority\n  in Journalism",
      "summary": "Using (generative) artificial intelligence tools and systems in journalism is\nexpected to increase journalists' production rates, transform newsrooms'\neconomic models, and further personalize the audience's news consumption\npractices. Since its release in 2022, OpenAI's ChatGPT and other large language\nmodels have raised the alarms inside news organizations, not only for bringing\nnew challenges to news reporting and fact-checking but also for what these\ntechnologies would mean for journalists' professional authority in journalism.\nThis paper examines how journalists in Dutch media manage the integration of AI\ntechnologies into their daily routines. Drawing from 13 interviews with\neditors, journalists, and innovation managers in different news outlets and\nmedia companies, we propose the concept of controlled change. as a heuristic to\nexplain how journalists are proactively setting guidelines, experimenting with\nAI tools, and identifying their limitations and capabilities. Using\nprofessional authority as a theoretical framework, we argue that journalists\nanticipate and integrate AI technologies in a supervised manner and identify\nthree primary mechanisms through which journalists manage this integration: (1)\ndeveloping adaptive guidelines that align AI use with ethical codes, (2)\nexperimenting with AI technologies to determine their necessity and fit, and\n(3) critically assessing the capabilities and limitations of AI systems.",
      "url": "http://arxiv.org/abs/2510.19792v1",
      "published_time_eastern_timestamp": 1761154052.0
    },
    {
      "title": "Adapting Multilingual Models to Code-Mixed Tasks via Model Merging",
      "summary": "We study model merging as a practical alternative to conventional adaptation\nstrategies for code-mixed NLP. Starting from a multilingual base model, we: (i)\nperform continued pre-training (CPT) on unlabeled code-mixed text to obtain an\nadapted checkpoint, (ii) merge checkpoint with the base model, and (iii)\nfine-tune (FT) on the downstream task data. We evaluate our approach for\nsentence classification (sentiment and hate speech) task in English-Hindi\n(En-Hi) and English-Spanish (En-Es) using XLM-R and Llama-3.2-1B models. Our\nresults show that merged models consistently outperform full fine-tuning and\nCPT->FT. We observe gains of 2--5 points in F1 over full fine-tuning and ~1-2\npoints over CPT->FT, indicating that unlabeled data is leveraged more\neffectively via merging than via CPT alone. Zero-/few-shot prompting with\nlarger LLMs (e.g., Llama-3.3-70B) lags behind fine-tuned and merged\ncheckpoints, underscoring limits of in-context learning for code-mixed inputs.\nWe further test cross-pair transfer by training on En-Hi and evaluating on\nEn-Ta and En-Ml: merged checkpoints transfer more strongly than\nmonolingual-English baselines (e.g., TV/TIES variants reaching 0.65-0.68 F1 vs\n0.61-0.63 for full fine-tuning), suggesting that code-mixed knowledge is a more\nreliable substrate for low-resource pairs. We conclude with adaptation recipes\nmatched to common data regimes (labeled only; labeled+unlabeled; transfer-only)\nand discuss limitations and scaling considerations for broader tasks and larger\nmodels.",
      "url": "http://arxiv.org/abs/2510.19782v1",
      "published_time_eastern_timestamp": 1761153383.0
    },
    {
      "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative\n  Decoders",
      "summary": "Speculative Decoding (SD) accelerates large language model inference by\nemploying a small draft model to generate predictions, which are then verified\nby a larger target model. The effectiveness of SD hinges on the alignment\nbetween these models, which is typically enhanced by Knowledge Distillation\n(KD). However, conventional KD methods aim to minimize the KL divergence\nbetween the draft and target models across all tokens, a goal that is\nmisaligned with the true objective of SD, which is to maximize token acceptance\nrate. Therefore, draft models often struggle to fully assimilate the target\nmodel's knowledge due to capacity constraints, leading to suboptimal\nperformance. To address this challenge, we propose AdaSPEC, a novel method that\nincorporates selective token filtering into the KD process. AdaSPEC utilizes a\nreference model to identify and filter out difficult-to-fit tokens, enabling\nthe distillation of a draft model that better aligns with the target model on\nsimpler tokens. This approach improves the overall token acceptance rate\nwithout compromising generation quality. We evaluate AdaSPEC across diverse\ntasks, including arithmetic reasoning, instruction-following, coding, and\nsummarization, using model configurations of 31M/1.4B and 350M/2.7B parameters.\nOur results demonstrate that AdaSPEC consistently outperforms the\nstate-of-the-art DistillSpec method, achieving higher acceptance rates across\nall tasks (up to 15\\%). The code is publicly available at\nhttps://github.com/yuezhouhu/adaspec.",
      "url": "http://arxiv.org/abs/2510.19779v1",
      "published_time_eastern_timestamp": 1761153180.0
    },
    {
      "title": "BOSQTGEN: Breaking the Sound Barrier in Test Generation",
      "summary": "Modern software is increasingly built by composing APIs, elevating the API\ncontract to a critical role. Inadequate contracts, however, lead to mismatched\nexpectations and failures, creating a pressing need for robust conformance\ntesting. Current test generation techniques are hindered by key challenges:\npolyglot systems, source code inaccessibility, a cost-reliability trade-off,\nand, most critically, the difficulty of generating structured inputs.\n  We introduce BOSQTGEN, a novel black-box methodology and tool for API test\ngeneration. BOSQTGEN utilizes a novel approach for decomposing API\nspecifications into primitives, using LLMs to suggest coherent strata for them,\nand employing combinatorial testing to efficiently sample over these values.\nThis approach ensures coverage of critical interactions while avoiding the\nredundancy of random sampling.\n  The resulting BOSQTGEN system achieves an average of 82% code coverage on\nRESTful benchmarks, often a 20% or more increase over prior state-of-the-art\nsystems and nearing parity with hand-written test suites. Providing a fully\nAPI-driven approach to test generation, enables developers to automatically\ncreate high-quality test cases for validation or test-driven development.",
      "url": "http://arxiv.org/abs/2510.19777v1",
      "published_time_eastern_timestamp": 1761153090.0
    },
    {
      "title": "SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via\n  Promoting Deeper Thought Exploration",
      "summary": "The long chain-of-thought (LongCoT) capability is central to the recent\nbreakthroughs achieved by large language models in complex reasoning tasks.\nHowever, the accompanying issue of ''underthinking'', where models exhibit\nshallow reasoning by frequently switching thoughts without sufficient\nexploration, limits both performance and token efficiency. To address this\nproblem, we propose a simple yet effective reasoning strategy: the SmartSwitch\ninference framework. This framework can be easily integrated into any large\nlanguage model as a plug-and-play solution, continuously monitoring the model's\nreasoning process to detect underthinking and guide it toward deeper\nexploration of promising but overlooked thoughts. Specifically, the perception\nmodule identifies points where thoughts switch and evaluates the potential of\nthe preceding thought using an off-the-shelf process reward model (PRM). If a\nhigh-potential thought is found to be prematurely abandoned, the intervention\nmodule interrupts the ongoing inference, backtracks to the point before the\nswitch, and inserts a \"deepening prompt\" to encourage further exploration along\nthat promising path. Extensive experiments on challenging mathematical\nreasoning benchmarks demonstrate that our method significantly enhances the\nperformance of various large language models of different sizes.",
      "url": "http://arxiv.org/abs/2510.19767v1",
      "published_time_eastern_timestamp": 1761152161.0
    },
    {
      "title": "Review of Tools for Zero-Code LLM Based Application Development",
      "summary": "Large Language Models (LLMs) are transforming software creation by enabling\nzero code development platforms. Our survey reviews recent platforms that let\nusers build applications without writing code, by leveraging LLMs as the brains\nof the development process. We adopt a broad survey methodology, categorizing\nplatforms based on key dimensions such as interface style, backend integration,\noutput type, and extensibility. We analyze both dedicated LLM based app\nbuilders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and\ngeneral no code platforms (e.g., Bubble, Glide) that integrate LLM\ncapabilities. We present a taxonomy categorizing these platforms by their\ninterface (conversational, visual, etc.), supported LLM backends, output type\n(chatbot, full application, workflow), and degree of extensibility. Core\nfeatures such as autonomous agents, memory management, workflow orchestration,\nand API integrations are in scope of the survey. We provide a detailed\ncomparison, highlighting each platform's strengths and limitations. Trade offs\n(customizability, scalability, vendor lock-in) are discussed in comparison with\ntraditional and low code development approaches. Finally, we outline future\ndirections, including multimodal interfaces, on device LLMs, and improved\norchestration for democratizing app creation with AI. Our findings indicate\nthat while zero code LLM platforms greatly reduce the barrier to creating AI\npowered applications, they still face challenges in flexibility and\nreliability. Overall, the landscape is rapidly evolving, offering exciting\nopportunities to empower non programmers to create sophisticated software.",
      "url": "http://arxiv.org/abs/2510.19747v1",
      "published_time_eastern_timestamp": 1761151276.0
    },
    {
      "title": "Explicitly Quantum-parallel Computation by Displacements",
      "summary": "We introduce an encoding of information in the relative displacement or\nphoton number of different optical modes. Since the loss rate to interference\nis insensitive to squeezing and many non-Gaussian fluctuations, such a space is\nrelatively protected from imperfections. We show that photon subtraction\nprotocols can be used to create high-quality quantum superpositions of squeezed\nstates with much higher fidelity than when the protocol is restricted to\nproducing only cat states (superpositions of coherent states). We also show\nthat the amount of squeezing and anti-squeezing introduced is moderate, and\nunlikely to dominate the photon number. This parallel processing allows for\nexplicit use of non-Gaussian interference as opposed to the more incidental\nrole played by non-Gaussianity in all-optical coherent Ising machines. A key\nobservation we make is that displacements of optical states provide a\nconvenient degree of freedom to encode information for quantum parallel\nprocessing. Furthermore, we discuss important considerations for realizing an\noptical quantum annealer based on differential photon number encoding. In\nparticular, we discuss the need to perform quantum erasure on loss channels\nfrom interference, as well as the ability to correct degrees of freedom not\nused for the encoding without disrupting the processed quantum information.",
      "url": "http://arxiv.org/abs/2510.19730v1",
      "published_time_eastern_timestamp": 1761150059.0
    },
    {
      "title": "The magnetic sensitivity of the Ca II resonance and subordinate lines in\n  the solar atmosphere",
      "summary": "Aims: The polarization of the Ca II resonant doublet (H and K lines) and the\nsubordinate infrared triplet lines are key observables for diagnosing solar\nchromospheric magnetism. It is thus necessary to understand the physical\nmechanisms that shape their Stokes profiles in magnetic environments.\n  Methods: Using the spectral synthesis module of the HanleRT-TIC code, we\nstudy the effects of anisotropic radiation pumping with partial frequency\nredistribution (PRD) and J-state interference (JSI) in a plane-parallel\nsemi-empirical static solar atmospheric model. We also analyze the sensitivity\nof these lines to magnetic fields of varying strengths and orientations,\naccounting for the combined action of the Hanle and Zeeman effects.\n  Results: Including PRD is crucial to model the polarization in the core\nregions of the resonant lines, while JSI strongly affects their far wings. The\nmetastable lower levels of the subordinate lines also influence the scattering\npolarization of the K line. With horizontal magnetic fields, the resonant lines\nrespond to field strengths from sub-gauss to tens of gauss, whereas the\ninfrared triplet scattering polarization is mainly sensitive to milligauss\nfields. At a near-limb line of sight (LOS) with $\\mu = 0.1$, the Hanle effect\nmodifies the scattering polarization via a depolarization and a rotation in the\nplane of linear polarization. At disk center, horizontal fields generate linear\npolarization in the 1D model: for the K line, the Hanle effect dominates from\nsub-gauss to a few tens of gauss, and the Zeeman effect dominates in stronger\nfields. For vertical fields, the Hanle effect vanishes, but magneto-optical\neffects affect the linear polarization wings. Finally, atomic level\npolarization impacts the outer circular polarization lobes of the resonant\nlines, and the weak-field approximation overestimates the LOS magnetic\ncomponent in this frequency range.",
      "url": "http://arxiv.org/abs/2510.19719v1",
      "published_time_eastern_timestamp": 1761149179.0
    },
    {
      "title": "SEMPO: Lightweight Foundation Models for Time Series Forecasting",
      "summary": "The recent boom of large pre-trained models witnesses remarkable success in\ndeveloping foundation models (FMs) for time series forecasting. Despite\nimpressive performance across diverse downstream forecasting tasks, existing\ntime series FMs possess massive network architectures and require substantial\npre-training on large-scale datasets, which significantly hinders their\ndeployment in resource-constrained environments. In response to this growing\ntension between versatility and affordability, we propose SEMPO, a novel\nlightweight foundation model that requires pretraining on relatively\nsmall-scale data, yet exhibits strong general time series forecasting.\nConcretely, SEMPO comprises two key modules: 1) energy-aware SpEctral\ndecomposition module, that substantially improves the utilization of\npre-training data by modeling not only the high-energy frequency signals but\nalso the low-energy yet informative frequency signals that are ignored in\ncurrent methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learns\nheterogeneous temporal patterns through small dataset-specific prompts and\nadaptively routes time series tokens to prompt-based experts for\nparameter-efficient model adaptation across different datasets and domains.\nEquipped with these modules, SEMPO significantly reduces both pre-training data\nscale and model size, while achieving strong generalization. Extensive\nexperiments on two large-scale benchmarks covering 16 datasets demonstrate the\nsuperior performance of SEMPO in both zero-shot and few-shot forecasting\nscenarios compared with state-of-the-art methods. Code and data are available\nat https://github.com/mala-lab/SEMPO.",
      "url": "http://arxiv.org/abs/2510.19710v1",
      "published_time_eastern_timestamp": 1761148724.0
    },
    {
      "title": "Dictionary learning methods for brain activity mapping with MEG data",
      "summary": "A central goal in many brain studies is the identification of those brain\nregions that are activated during an observation window that may correspond to\na motor task, a stimulus, or simply a resting state. While functional MRI is\ncurrently the most commonly employed modality for such task, methods based on\nthe electromagnetic activity of the brain are valuable alternatives because of\ntheir excellent time resolution and of the fact that the measured signals are\ndirectly related to brain activation and not to a secondary effect such as the\nhemodynamic response. In this work we focus on the MEG modality, investigating\nthe performance of a recently proposed Bayesian dictionary learning (BDL)\nalgorithm for brain region identification. The partitioning of the source space\ninto the 148 regions of interest (ROI) corresponding to parcellation of the\nDestrieux atlas provides a natural determination of the subdictionaries\nnecessary for the BDL algorithm. We design a simulation protocol where a small\nrandomly selected patch in each ROI is activated, the MEG signal is computed\nand the inverse problem of active brain region identification is solved using\nthe BDL algorithm. The BDL algorithm consists of two phases, the first one\ncomprising dictionary compression and Bayesian compression error analysis, and\nthe second one performing dictionary coding with a deflated dictionary built on\nthe output of the first phase, both steps relying on Bayesian sparsity\npromoting computations. For assessing the performance, we give a probabilistic\ninterpretation of the confusion matrix, and consider different impurity\nmeasures for a multi-class classifier.",
      "url": "http://arxiv.org/abs/2510.19702v1",
      "published_time_eastern_timestamp": 1761148402.0
    },
    {
      "title": "Toward Agentic Software Engineering Beyond Code: Framing Vision, Values,\n  and Vocabulary",
      "summary": "Agentic AI is poised to usher in a seismic paradigm shift in Software\nEngineering (SE). As technologists rush head-along to make agentic AI a\nreality, SE researchers are driven to establish agentic SE as a research area.\nWhile early visions of agentic SE are primarily focused on code-related\nactivities, early empirical evidence calls for a consideration of a range of\nsocio-technical concerns to make it work in practice. This paper contributes to\nthe emerging community vision by: (a) recommending an expansion of its scope\nbeyond code, toward a 'whole of process' vision, grounding it in SE foundations\nand evolution and emerging agentic SE frameworks, (b) proposing a preliminary\nset of values and principles to guide efforts, and (c) sharing guidance on\ndesigning/using well-defined vocabulary for agentic SE. It is hoped that these\nideas will encourage community collaborations and steer the SE community\ntowards laying strong foundations of agentic SE so its not only inevitable but\nalso deliberate and desirable in the long run.",
      "url": "http://arxiv.org/abs/2510.19692v1",
      "published_time_eastern_timestamp": 1761147598.0
    },
    {
      "title": "CircuitGuard: Mitigating LLM Memorization in RTL Code Generation Against\n  IP Leakage",
      "summary": "Large Language Models (LLMs) have achieved remarkable success in generative\ntasks, including register-transfer level (RTL) hardware synthesis. However,\ntheir tendency to memorize training data poses critical risks when proprietary\nor security-sensitive designs are unintentionally exposed during inference.\nWhile prior work has examined memorization in natural language, RTL introduces\nunique challenges: In RTL, structurally different implementations (e.g.,\nbehavioral vs. gate-level descriptions) can realize the same hardware, leading\nto intellectual property (IP) leakage (full or partial) even without verbatim\noverlap. Conversely, even small syntactic variations (e.g., operator precedence\nor blocking vs. non-blocking assignments) can drastically alter circuit\nbehavior, making correctness preservation especially challenging. In this work,\nwe systematically study memorization in RTL code generation and propose\nCircuitGuard, a defense strategy that balances leakage reduction with\ncorrectness preservation. CircuitGuard (1) introduces a novel RTL-aware\nsimilarity metric that captures both structural and functional equivalence\nbeyond surface-level overlap, and (2) develops an activation-level steering\nmethod that identifies and attenuates transformer components most responsible\nfor memorization. Our empirical evaluation demonstrates that CircuitGuard\nidentifies (and isolates) 275 memorization-critical features across layers\n18-28 of Llama 3.1-8B model, achieving up to 80% reduction in semantic\nsimilarity to proprietary patterns while maintaining generation quality.\nCircuitGuard further shows 78-85% cross-domain transfer effectiveness, enabling\nrobust memorization mitigation across circuit categories without retraining.",
      "url": "http://arxiv.org/abs/2510.19676v1",
      "published_time_eastern_timestamp": 1761146535.0
    },
    {
      "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
      "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
      "url": "http://arxiv.org/abs/2510.19670v1",
      "published_time_eastern_timestamp": 1761146216.0
    }
  ]
}
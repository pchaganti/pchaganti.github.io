{
  "last_updated": "2025-06-09T17:11:17.358041-04:00",
  "papers": [
    {
      "title": "TerraFM: A Scalable Foundation Model for Unified Multisensor Earth\n  Observation",
      "summary": "Modern Earth observation (EO) increasingly leverages deep learning to harness\nthe scale and diversity of satellite imagery across sensors and regions. While\nrecent foundation models have demonstrated promising generalization across EO\ntasks, many remain limited by the scale, geographical coverage, and spectral\ndiversity of their training data, factors critical for learning globally\ntransferable representations. In this work, we introduce TerraFM, a scalable\nself-supervised learning model that leverages globally distributed Sentinel-1\nand Sentinel-2 imagery, combined with large spatial tiles and land-cover aware\nsampling to enrich spatial and semantic coverage. By treating sensing\nmodalities as natural augmentations in our self-supervised approach, we unify\nradar and optical inputs via modality-specific patch embeddings and adaptive\ncross-attention fusion. Our training strategy integrates local-global\ncontrastive learning and introduces a dual-centering mechanism that\nincorporates class-frequency-aware regularization to address long-tailed\ndistributions in land cover.TerraFM achieves strong generalization on both\nclassification and segmentation tasks, outperforming prior models on GEO-Bench\nand Copernicus-Bench. Our code and pretrained models are publicly available at:\nhttps://github.com/mbzuai-oryx/TerraFM .",
      "url": "http://arxiv.org/abs/2506.06281v1",
      "published_time_eastern_timestamp": 1749232790.0
    },
    {
      "title": "ExAct: A Video-Language Benchmark for Expert Action Analysis",
      "summary": "We present ExAct, a new video-language benchmark for expert-level\nunderstanding of skilled physical human activities. Our new benchmark contains\n3521 expert-curated video question-answer pairs spanning 11 physical activities\nin 6 domains: Sports, Bike Repair, Cooking, Health, Music, and Dance. ExAct\nrequires the correct answer to be selected from five carefully designed\ncandidate options, thus necessitating a nuanced, fine-grained, expert-level\nunderstanding of physical human skills. Evaluating the recent state-of-the-art\nVLMs on ExAct reveals a substantial performance gap relative to human expert\nperformance. Specifically, the best-performing GPT-4o model achieves only\n44.70% accuracy, well below the 82.02% attained by trained human\nspecialists/experts. We believe that ExAct will be beneficial for developing\nand evaluating VLMs capable of precise understanding of human skills in various\nphysical and procedural domains. Dataset and code are available at\nhttps://texaser.github.io/exact_project_page/",
      "url": "http://arxiv.org/abs/2506.06277v1",
      "published_time_eastern_timestamp": 1749232731.0
    },
    {
      "title": "Integrating Complexity and Biological Realism: High-Performance Spiking\n  Neural Networks for Breast Cancer Detection",
      "summary": "Spiking Neural Networks (SNNs) event-driven nature enables efficient encoding\nof spatial and temporal features, making them suitable for dynamic\ntime-dependent data processing. Despite their biological relevance, SNNs have\nseen limited application in medical image recognition due to difficulties in\nmatching the performance of conventional deep learning models. To address this,\nwe propose a novel breast cancer classification approach that combines SNNs\nwith Lempel-Ziv Complexity (LZC) a computationally efficient measure of\nsequence complexity. LZC enhances the interpretability and accuracy of\nspike-based models by capturing structural patterns in neural activity. Our\nstudy explores both biophysical Leaky Integrate-and-Fire (LIF) and\nprobabilistic Levy-Baxter (LB) neuron models under supervised, unsupervised,\nand hybrid learning regimes. Experiments were conducted on the Breast Cancer\nWisconsin dataset using numerical features derived from medical imaging.\nLB-based models consistently exceeded 90.00% accuracy, while LIF-based models\nreached over 85.00%. The highest accuracy of 98.25% was achieved using an\nANN-to-SNN conversion method applied to both neuron models comparable to\ntraditional deep learning with back-propagation, but at up to 100 times lower\ncomputational cost. This hybrid approach merges deep learning performance with\nthe efficiency and plausibility of SNNs, yielding top results at lower\ncomputational cost. We hypothesize that the synergy between temporal-coding,\nspike-sparsity, and LZC-driven complexity analysis enables more-efficient\nfeature extraction. Our findings demonstrate that SNNs combined with LZC offer\npromising, biologically plausible alternative to conventional neural networks\nin medical diagnostics, particularly for resource-constrained or real-time\nsystems.",
      "url": "http://arxiv.org/abs/2506.06265v1",
      "published_time_eastern_timestamp": 1749232047.0
    },
    {
      "title": "DesignBench: A Comprehensive Benchmark for MLLM-based Front-end Code\n  Generation",
      "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in automated front-end engineering, e.g., generating UI code from\nvisual designs. However, existing front-end UI code generation benchmarks have\nthe following limitations: (1) While framework-based development becomes\npredominant in modern front-end programming, current benchmarks fail to\nincorporate mainstream development frameworks. (2) Existing evaluations focus\nsolely on the UI code generation task, whereas practical UI development\ninvolves several iterations, including refining editing, and repairing issues.\n(3) Current benchmarks employ unidimensional evaluation, lacking investigation\ninto influencing factors like task difficulty, input context variations, and\nin-depth code-level analysis. To bridge these gaps, we introduce DesignBench, a\nmulti-framework, multi-task evaluation benchmark for assessing MLLMs'\ncapabilities in automated front-end engineering. DesignBench encompasses three\nwidely-used UI frameworks (React, Vue, and Angular) alongside vanilla HTML/CSS,\nand evaluates on three essential front-end tasks (generation, edit, and repair)\nin real-world development workflows. DesignBench contains 900 webpage samples\nspanning over 11 topics, 9 edit types, and 6 issue categories, enabling\ndetailed analysis of MLLM performance across multiple dimensions. Our\nsystematic evaluation reveals critical insights into MLLMs' framework-specific\nlimitations, task-related bottlenecks, and performance variations under\ndifferent conditions, providing guidance for future research in automated\nfront-end development. Our code and data are available at\nhttps://github.com/WebPAI/DesignBench.",
      "url": "http://arxiv.org/abs/2506.06251v1",
      "published_time_eastern_timestamp": 1749230481.0
    },
    {
      "title": "Scalable Language Agnostic Taint Tracking using Explicit Data\n  Dependencies",
      "summary": "Taint analysis using explicit whole-program data-dependence graphs is\npowerful for vulnerability discovery but faces two major challenges. First,\naccurately modeling taint propagation through calls to external library\nprocedures requires extensive manual annotations, which becomes impractical for\nlarge ecosystems. Second, the sheer size of whole-program graph representations\nleads to serious scalability and performance issues, particularly when quick\nanalysis is needed in continuous development pipelines.\n  This paper presents the design and implementation of a system for a\nlanguage-agnostic data-dependence representation. The system accommodates\nmissing annotations describing the behavior of library procedures by\nover-approximating data flows, allowing annotations to be added later without\nrecalculation. We contribute this data-flow analysis system to the open-source\ncode analysis platform Joern making it available to the community.",
      "url": "http://arxiv.org/abs/2506.06247v1",
      "published_time_eastern_timestamp": 1749230159.0
    },
    {
      "title": "Optimizing Cloud-to-GPU Throughput for Deep Learning With Earth\n  Observation Data",
      "summary": "Training deep learning models on petabyte-scale Earth observation (EO) data\nrequires separating compute resources from data storage. However, standard\nPyTorch data loaders cannot keep modern GPUs utilized when streaming GeoTIFF\nfiles directly from cloud storage. In this work, we benchmark GeoTIFF loading\nthroughput from both cloud object storage and local SSD, systematically testing\ndifferent loader configurations and data parameters. We focus on tile-aligned\nreads and worker thread pools, using Bayesian optimization to find optimal\nsettings for each storage type. Our optimized configurations increase remote\ndata loading throughput by 20x and local throughput by 4x compared to default\nsettings. On three public EO benchmarks, models trained with optimized remote\nloading achieve the same accuracy as local training within identical time\nbudgets. We improve validation IoU by 6-15% and maintain 85-95% GPU utilization\nversus 0-30% with standard configurations. Code is publicly available at\nhttps://github.com/microsoft/pytorch-cloud-geotiff-optimization",
      "url": "http://arxiv.org/abs/2506.06235v1",
      "published_time_eastern_timestamp": 1749228853.0
    },
    {
      "title": "Towards an Explainable Comparison and Alignment of Feature Embeddings",
      "summary": "While several feature embedding models have been developed in the literature,\ncomparisons of these embeddings have largely focused on their numerical\nperformance in classification-related downstream applications. However, an\ninterpretable comparison of different embeddings requires identifying and\nanalyzing mismatches between sample groups clustered within the embedding\nspaces. In this work, we propose the \\emph{Spectral Pairwise Embedding\nComparison (SPEC)} framework to compare embeddings and identify their\ndifferences in clustering a reference dataset. Our approach examines the kernel\nmatrices derived from two embeddings and leverages the eigendecomposition of\nthe difference kernel matrix to detect sample clusters that are captured\ndifferently by the two embeddings. We present a scalable implementation of this\nkernel-based approach, with computational complexity that grows linearly with\nthe sample size. Furthermore, we introduce an optimization problem using this\nframework to align two embeddings, ensuring that clusters identified in one\nembedding are also captured in the other model. We provide numerical results\ndemonstrating the SPEC's application to compare and align embeddings on\nlarge-scale datasets such as ImageNet and MS-COCO. The code is available at\n[https://github.com/mjalali/embedding-comparison](github.com/mjalali/embedding-comparison).",
      "url": "http://arxiv.org/abs/2506.06231v1",
      "published_time_eastern_timestamp": 1749228637.0
    },
    {
      "title": "CompilerGPT: Leveraging Large Language Models for Analyzing and Acting\n  on Compiler Optimization Reports",
      "summary": "Current compiler optimization reports often present complex, technical\ninformation that is difficult for programmers to interpret and act upon\neffectively. This paper assesses the capability of large language models (LLM)\nto understand compiler optimization reports and automatically rewrite the code\naccordingly.\n  To this end, the paper introduces CompilerGPT, a novel framework that\nautomates the interaction between compilers, LLMs, and user defined test and\nevaluation harness. CompilerGPT's workflow runs several iterations and reports\non the obtained results.\n  Experiments with two leading LLM models (GPT-4o and Claude Sonnet),\noptimization reports from two compilers (Clang and GCC), and five benchmark\ncodes demonstrate the potential of this approach. Speedups of up to 6.5x were\nobtained, though not consistently in every test. This method holds promise for\nimproving compiler usability and streamlining the software optimization\nprocess.",
      "url": "http://arxiv.org/abs/2506.06227v1",
      "published_time_eastern_timestamp": 1749228134.0
    },
    {
      "title": "STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large\n  Language Models in Autonomous Driving",
      "summary": "We introduce STSBench, a scenario-based framework to benchmark the holistic\nunderstanding of vision-language models (VLMs) for autonomous driving. The\nframework automatically mines pre-defined traffic scenarios from any dataset\nusing ground-truth annotations, provides an intuitive user interface for\nefficient human verification, and generates multiple-choice questions for model\nevaluation. Applied to the NuScenes dataset, we present STSnu, the first\nbenchmark that evaluates the spatio-temporal reasoning capabilities of VLMs\nbased on comprehensive 3D perception. Existing benchmarks typically target\noff-the-shelf or fine-tuned VLMs for images or videos from a single viewpoint\nand focus on semantic tasks such as object recognition, dense captioning, risk\nassessment, or scene understanding. In contrast, STSnu evaluates driving expert\nVLMs for end-to-end driving, operating on videos from multi-view cameras or\nLiDAR. It specifically assesses their ability to reason about both ego-vehicle\nactions and complex interactions among traffic participants, a crucial\ncapability for autonomous vehicles. The benchmark features 43 diverse scenarios\nspanning multiple views and frames, resulting in 971 human-verified\nmultiple-choice questions. A thorough evaluation uncovers critical shortcomings\nin existing models' ability to reason about fundamental traffic dynamics in\ncomplex environments. These findings highlight the urgent need for\narchitectural advances that explicitly model spatio-temporal reasoning. By\naddressing a core gap in spatio-temporal evaluation, STSBench enables the\ndevelopment of more robust and explainable VLMs for autonomous driving.",
      "url": "http://arxiv.org/abs/2506.06218v1",
      "published_time_eastern_timestamp": 1749227122.0
    },
    {
      "title": "Corrector Sampling in Language Models",
      "summary": "Autoregressive language models accumulate errors due to their fixed,\nirrevocable left-to-right token generation. To address this, we propose a new\nsampling method called Resample-Previous-Tokens (RPT). RPT mitigates error\naccumulation by iteratively revisiting and potentially replacing tokens in a\nwindow of previously generated text. This method can be integrated into\nexisting autoregressive models, preserving their next-token-prediction quality\nand speed. Fine-tuning a pretrained 8B parameter model with RPT for only 100B\nresulted in ~10% relative improvements on reasoning and coding benchmarks\ncompared to the standard sampling.",
      "url": "http://arxiv.org/abs/2506.06215v1",
      "published_time_eastern_timestamp": 1749226895.0
    },
    {
      "title": "Can Theoretical Physics Research Benefit from Language Agents?",
      "summary": "Large Language Models (LLMs) are rapidly advancing across diverse domains,\nyet their application in theoretical physics research is not yet mature. This\nposition paper argues that LLM agents can potentially help accelerate\ntheoretical, computational, and applied physics when properly integrated with\ndomain knowledge and toolbox. We analyze current LLM capabilities for physics\n-- from mathematical reasoning to code generation -- identifying critical gaps\nin physical intuition, constraint satisfaction, and reliable reasoning. We\nenvision future physics-specialized LLMs that could handle multimodal data,\npropose testable hypotheses, and design experiments. Realizing this vision\nrequires addressing fundamental challenges: ensuring physical consistency, and\ndeveloping robust verification methods. We call for collaborative efforts\nbetween physics and AI communities to help advance scientific discovery in\nphysics.",
      "url": "http://arxiv.org/abs/2506.06214v1",
      "published_time_eastern_timestamp": 1749226806.0
    },
    {
      "title": "MLOps with Microservices: A Case Study on the Maritime Domain",
      "summary": "This case study describes challenges and lessons learned on building Ocean\nGuard: a Machine Learning-Enabled System (MLES) for anomaly detection in the\nmaritime domain. First, the paper presents the system's specification, and\narchitecture. Ocean Guard was designed with a microservices' architecture to\nenable multiple teams to work on the project in parallel. Then, the paper\ndiscusses how the developers adapted contract-based design to MLOps for\nachieving that goal. As a MLES, Ocean Guard employs code, model, and data\ncontracts to establish guidelines between its services. This case study hopes\nto inspire software engineers, machine learning engineers, and data scientists\nto leverage similar approaches for their systems.",
      "url": "http://arxiv.org/abs/2506.06202v1",
      "published_time_eastern_timestamp": 1749225899.0
    },
    {
      "title": "Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with\n  a Multi-Agent Approach",
      "summary": "Large language models can translate natural-language chart descriptions into\nrunnable code, yet approximately 15\\% of the generated scripts still fail to\nexecute, even after supervised fine-tuning and reinforcement learning. We\ninvestigate whether this persistent error rate stems from model limitations or\nfrom reliance on a single-prompt design. To explore this, we propose a\nlightweight multi-agent pipeline that separates drafting, execution, repair,\nand judgment, using only an off-the-shelf GPT-4o-mini model. On the\n\\textsc{Text2Chart31} benchmark, our system reduces execution errors to 4.5\\%\nwithin three repair iterations, outperforming the strongest fine-tuned baseline\nby nearly 5 percentage points while requiring significantly less compute.\nSimilar performance is observed on the \\textsc{ChartX} benchmark, with an error\nrate of 4.6\\%, demonstrating strong generalization. Under current benchmarks,\nexecution success appears largely solved. However, manual review reveals that 6\nout of 100 sampled charts contain hallucinations, and an LLM-based\naccessibility audit shows that only 33.3\\% (\\textsc{Text2Chart31}) and 7.2\\%\n(\\textsc{ChartX}) of generated charts satisfy basic colorblindness guidelines.\nThese findings suggest that future work should shift focus from execution\nreliability toward improving chart aesthetics, semantic fidelity, and\naccessibility.",
      "url": "http://arxiv.org/abs/2506.06175v1",
      "published_time_eastern_timestamp": 1749224357.0
    },
    {
      "title": "The Lock-in Hypothesis: Stagnation by Algorithm",
      "summary": "The training and deployment of large language models (LLMs) create a feedback\nloop with human users: models learn human beliefs from data, reinforce these\nbeliefs with generated content, reabsorb the reinforced beliefs, and feed them\nback to users again and again. This dynamic resembles an echo chamber. We\nhypothesize that this feedback loop entrenches the existing values and beliefs\nof users, leading to a loss of diversity and potentially the lock-in of false\nbeliefs. We formalize this hypothesis and test it empirically with agent-based\nLLM simulations and real-world GPT usage data. Analysis reveals sudden but\nsustained drops in diversity after the release of new GPT iterations,\nconsistent with the hypothesized human-AI feedback loop. Code and data\navailable at https://thelockinhypothesis.com",
      "url": "http://arxiv.org/abs/2506.06166v1",
      "published_time_eastern_timestamp": 1749223891.0
    },
    {
      "title": "Obfuscation-Resilient Binary Code Similarity Analysis using Dominance\n  Enhanced Semantic Graph",
      "summary": "Binary code similarity analysis (BCSA) serves as a core technique for binary\nanalysis tasks such as vulnerability detection. While current graph-based BCSA\napproaches capture substantial semantics and show strong performance, their\nperformance suffers under code obfuscation due to the unstable control flow. To\naddress this issue, we develop ORCAS, an Obfuscation-Resilient BCSA model based\non Dominance Enhanced Semantic Graph (DESG). The DESG is an original binary\ncode representation, capturing more binaries' implicit semantics without\ncontrol flow structure, including inter-instruction relations, inter-basic\nblock relations, and instruction-basic block relations. ORCAS robustly scores\nsemantic similarity across binary functions from different obfuscation options,\noptimization levels, and instruction set architectures. Extensive evaluation on\nthe BinKit dataset shows ORCAS significantly outperforms eight baselines,\nachieving an average 12.1% PR-AUC gain when using combined three obfuscation\noptions compared to the state-of-the-art approaches. Furthermore, ORCAS\nimproves recall by up to 43% on an original obfuscated real-world vulnerability\ndataset, which we released to facilitate future research.",
      "url": "http://arxiv.org/abs/2506.06161v1",
      "published_time_eastern_timestamp": 1749223613.0
    },
    {
      "title": "Masked Language Models are Good Heterogeneous Graph Generalizers",
      "summary": "Heterogeneous graph neural networks (HGNNs) excel at capturing structural and\nsemantic information in heterogeneous graphs (HGs), while struggling to\ngeneralize across domains and tasks. Recently, some researchers have turned to\nintegrating HGNNs with large language models (LLMs) for more generalizable\nheterogeneous graph learning. However, these approaches typically extract\nstructural information via HGNNs as HG tokens, and disparities in embedding\nspaces between HGNNs and LLMs have been shown to bias the LLM's comprehension\nof HGs. Moreover, as these HG tokens are often derived from node-level tasks,\nthe model's ability to generalize across tasks remains limited. To this end, we\npropose a simple yet effective Masked Language Modeling-based method, called\nMLM4HG. MLM4HG introduces metapath-based textual sequences instead of HG tokens\nto extract structural and semantic information inherent in HGs, and designs\ncustomized textual templates to unify different graph tasks into a coherent\ncloze-style \"mask\" token prediction paradigm. Specifically, MLM4HG first\nconverts HGs from various domains to texts based on metapaths, and subsequently\ncombines them with the unified task texts to form a HG-based corpus. Moreover,\nthe corpus is fed into a pretrained LM for fine-tuning with a constrained\ntarget vocabulary, enabling the fine-tuned LM to generalize to unseen target\nHGs. Extensive cross-domain and multi-task experiments on four real-world\ndatasets demonstrate the superior generalization performance of MLM4HG over\nstate-of-the-art methods in both few-shot and zero-shot scenarios. Our code is\navailable at https://github.com/BUPT-GAMMA/MLM4HG.",
      "url": "http://arxiv.org/abs/2506.06157v1",
      "published_time_eastern_timestamp": 1749223284.0
    },
    {
      "title": "A Novel Large-scale Crop Dataset and Dual-stream Transformer Method for\n  Fine-grained Hierarchical Crop Classification from Integrated Hyperspectral\n  EnMAP Data and Multispectral Sentinel-2 Time Series",
      "summary": "Fine-grained crop classification is crucial for precision agriculture and\nfood security monitoring. It requires simultaneous capture of both phenological\ndynamics (obtained from multi-temporal satellite data like Sentinel-2) and\nsubtle spectral variations (demanding nanometer-scale spectral resolution from\nhyperspectral imagery). Research combining these two modalities remains scarce\ncurrently due to challenges in hyperspectral data acquisition and crop types\nannotation costs. To address these issues, we construct a hierarchical\nhyperspectral crop dataset (H2Crop) by integrating 30m-resolution EnMAP\nhyperspectral data with Sentinel-2 time series. With over one million annotated\nfield parcels organized in a four-tier crop taxonomy, H2Crop establishes a\nvital benchmark for fine-grained agricultural crop classification and\nhyperspectral image processing. We propose a dual-stream Transformer\narchitecture that synergistically processes these modalities. It coordinates\ntwo specialized pathways: a spectral-spatial Transformer extracts fine-grained\nsignatures from hyperspectral EnMAP data, while a temporal Swin Transformer\nextracts crop growth patterns from Sentinel-2 time series. The designed\nhierarchy classification heads with hierarchical fusion then simultaneously\ndelivers multi-level classification across all taxonomic tiers. Experiments\ndemonstrate that adding hyperspectral EnMAP data to Sentinel-2 time series\nyields a 4.2% average F1-scores improvement (peaking at 6.3%). Extensive\ncomparisons also confirming our method's higher accuracy over existing deep\nlearning approaches for crop type classification and the consistent benefits of\nhyperspectral data across varying temporal windows and crop change scenarios.\nCodes and dataset will be available at https://github.com/flyakon/H2Crop and\nwww.glass.hku.hk\n  Keywords: Crop type classification, precision agriculture, remote sensing,\ndeep learning, hyperspectral data, Sentinel-2 time series, fine-grained crops",
      "url": "http://arxiv.org/abs/2506.06155v1",
      "published_time_eastern_timestamp": 1749223130.0
    },
    {
      "title": "Joint-GCG: Unified Gradient-Based Poisoning Attacks on\n  Retrieval-Augmented Generation Systems",
      "summary": "Retrieval-Augmented Generation (RAG) systems enhance Large Language Models\n(LLMs) by retrieving relevant documents from external corpora before generating\nresponses. This approach significantly expands LLM capabilities by leveraging\nvast, up-to-date external knowledge. However, this reliance on external\nknowledge makes RAG systems vulnerable to corpus poisoning attacks that\nmanipulate generated outputs via poisoned document injection. Existing\npoisoning attack strategies typically treat the retrieval and generation stages\nas disjointed, limiting their effectiveness. We propose Joint-GCG, the first\nframework to unify gradient-based attacks across both retriever and generator\nmodels through three innovations: (1) Cross-Vocabulary Projection for aligning\nembedding spaces, (2) Gradient Tokenization Alignment for synchronizing\ntoken-level gradient signals, and (3) Adaptive Weighted Fusion for dynamically\nbalancing attacking objectives. Evaluations demonstrate that Joint-GCG achieves\nat most 25% and an average of 5% higher attack success rate than previous\nmethods across multiple retrievers and generators. While optimized under a\nwhite-box assumption, the generated poisons show unprecedented transferability\nto unseen models. Joint-GCG's innovative unification of gradient-based attacks\nacross retrieval and generation stages fundamentally reshapes our understanding\nof vulnerabilities within RAG systems. Our code is available at\nhttps://github.com/NicerWang/Joint-GCG.",
      "url": "http://arxiv.org/abs/2506.06151v1",
      "published_time_eastern_timestamp": 1749222726.0
    },
    {
      "title": "CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval",
      "summary": "Online video web content is richly multimodal: a single video blends vision,\nspeech, ambient audio, and on-screen text. Retrieval systems typically treat\nthese modalities as independent retrieval sources, which can lead to noisy and\nsubpar retrieval. We explore multimodal video content retrieval, where\nrelevance can be scored from one particular modality or jointly across multiple\nmodalities simultaneously. Consequently, an effective retriever must\ndynamically choose which modality (or set of modalities) best addresses the\nquery. We introduce CLaMR, a multimodal, late-interaction retriever that\njointly indexes 4 modalities: video frames, transcribed speech, on-screen text,\nand metadata. CLaMR jointly encodes all modalities with a unified multimodal\nbackbone for improved contextualization and is trained to enhance dynamic\nmodality selection via two key innovations. First, given the lack of training\ndata for multimodal retrieval, we introduce MultiVENT 2.0++, a large-scale\nsynthetic training dataset built on MultiVENT 2.0 (event-centric videos in\nvarious languages paired with queries) with modality-targeted queries. Next, we\npropose a modality-aware loss that jointly trains according to a standard\ncontrastive objective alongside an objective for learning correct modality\nusage. On the test sets of MultiVENT 2.0++ and MSRVTT, conventional aggregation\nstrategies, such as averaging similarities for baseline retrievers, degrade\nperformance by introducing noise from irrelevant modalities. In contrast, CLaMR\nconsistently outperforms existing retrievers: on MultiVENT 2.0++, CLaMR\nimproves nDCG@10 by 25.6 over the best single-modality retriever and by 35.4\nover the best multi-modality retriever. We illustrate CLaMR's downstream\nutility on long-video QA, retrieving relevant frames and obtaining a 3.50%\nboost over LanguageBind on Video-MME and 1.42% over dense sampling on\nLongVideoBench.",
      "url": "http://arxiv.org/abs/2506.06144v1",
      "published_time_eastern_timestamp": 1749222150.0
    },
    {
      "title": "Impact of initial mass function on the chemical evolution of\n  high-redshift galaxies",
      "summary": "Star formation and metal enrichment in galaxies are regulated by supernova\n(SN) explosions and metal yields from massive stars, which are sensitive to the\nhigh-mass end of the initial mass function (IMF). Recent JWST observations have\nreached a consensus on an invariant relation between stellar mass, metallicity,\nand star formation rate up to $z\\sim 8$ and its breakdown at higher redshifts.\nIt is crucial to understand the underlying physics, especially the role played\nby the IMF. We explore the impact of IMF on the chemical evolution of\nhigh-redshift galaxies and the interplay between IMF and galactic outflows. The\nultimate goal is to constrain the high-mass end of the IMF by the cosmic star\nformation history and stellar mass-metallicity-star formation rate relation\n(MZSFR) inferred from observations at $z\\sim 4-10$. Using the semi-analytical\ngalaxy evolution code A-SLOTH, we follow galactic baryon cycles along merger\ntrees built from cosmological simulations. Stellar feedback is modeled with\nup-to-date stellar evolution tracks covering the full metallicity range ($Z\n\\sim 10^{-11} - 0.03$) and a broad stellar mass range ($m_\\star\\sim2 - 600\\ \\rm\nM_\\odot$) including the metal yields from stellar winds, core-collapse SNe,\n(pulsational) pair-instability SNe, and Type Ia SNe. Assuming that the IMF\nfollows a Kroupa-like shape with a varying upper mass limit $m_{\\max}$, we find\n$m_{\\max} \\gtrsim 200\\ \\rm M_\\odot$ is required to reproduce the observed\nMZSFR. Observational data at $z\\gtrsim 6$ favor a galactic outflow model where\nthe outflow mass is proportional to the ratio of supernova energy to halo\nbinding energy. We conclude that very massive ($\\gtrsim 200\\ \\rm M_\\odot$)\nstars can play important roles in the star formation and chemical enrichment\nhistories of high-$z$ galaxies. We also discuss their implications for\ntransient sources of both electromagnetic waves and gravitational waves.",
      "url": "http://arxiv.org/abs/2506.06139v1",
      "published_time_eastern_timestamp": 1749221784.0
    }
  ]
}
{
  "last_updated": "2025-05-13T17:11:05.862773-04:00",
  "papers": [
    {
      "title": "DanceGRPO: Unleashing GRPO on Visual Generation",
      "summary": "Recent breakthroughs in generative models-particularly diffusion models and\nrectified flows-have revolutionized visual content creation, yet aligning model\noutputs with human preferences remains a critical challenge. Existing\nreinforcement learning (RL)-based methods for visual generation face critical\nlimitations: incompatibility with modern Ordinary Differential Equations\n(ODEs)-based sampling paradigms, instability in large-scale training, and lack\nof validation for video generation. This paper introduces DanceGRPO, the first\nunified framework to adapt Group Relative Policy Optimization (GRPO) to visual\ngeneration paradigms, unleashing one unified RL algorithm across two generative\nparadigms (diffusion models and rectified flows), three tasks (text-to-image,\ntext-to-video, image-to-video), four foundation models (Stable Diffusion,\nHunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video\naesthetics, text-image alignment, video motion quality, and binary reward). To\nour knowledge, DanceGRPO is the first RL-based unified framework capable of\nseamless adaptation across diverse generative paradigms, tasks, foundational\nmodels, and reward models. DanceGRPO demonstrates consistent and substantial\nimprovements, which outperform baselines by up to 181% on benchmarks such as\nHPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can\nstabilize policy optimization for complex video generation, but also enables\ngenerative policy to better capture denoising trajectories for Best-of-N\ninference scaling and learn from sparse binary feedback. Our results establish\nDanceGRPO as a robust and versatile solution for scaling Reinforcement Learning\nfrom Human Feedback (RLHF) tasks in visual generation, offering new insights\ninto harmonizing reinforcement learning and visual synthesis. The code will be\nreleased.",
      "url": "http://arxiv.org/abs/2505.07818v1",
      "published_time_eastern_timestamp": 1747072774.0
    },
    {
      "title": "Continuous Visual Autoregressive Generation via Score Maximization",
      "summary": "Conventional wisdom suggests that autoregressive models are used to process\ndiscrete data. When applied to continuous modalities such as visual data,\nVisual AutoRegressive modeling (VAR) typically resorts to quantization-based\napproaches to cast the data into a discrete space, which can introduce\nsignificant information loss. To tackle this issue, we introduce a Continuous\nVAR framework that enables direct visual autoregressive generation without\nvector quantization. The underlying theoretical foundation is strictly proper\nscoring rules, which provide powerful statistical tools capable of evaluating\nhow well a generative model approximates the true distribution. Within this\nframework, all we need is to select a strictly proper score and set it as the\ntraining objective to optimize. We primarily explore a class of training\nobjectives based on the energy score, which is likelihood-free and thus\novercomes the difficulty of making probabilistic predictions in the continuous\nspace. Previous efforts on continuous autoregressive generation, such as GIVT\nand diffusion loss, can also be derived from our framework using other strictly\nproper scores. Source code: https://github.com/shaochenze/EAR.",
      "url": "http://arxiv.org/abs/2505.07812v1",
      "published_time_eastern_timestamp": 1747072694.0
    },
    {
      "title": "A Comparative Analysis of Static Word Embeddings for Hungarian",
      "summary": "This paper presents a comprehensive analysis of various static word\nembeddings for Hungarian, including traditional models such as Word2Vec,\nFastText, as well as static embeddings derived from BERT-based models using\ndifferent extraction methods. We evaluate these embeddings on both intrinsic\nand extrinsic tasks to provide a holistic view of their performance. For\nintrinsic evaluation, we employ a word analogy task, which assesses the\nembeddings ability to capture semantic and syntactic relationships. Our results\nindicate that traditional static embeddings, particularly FastText, excel in\nthis task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among\nthe BERT-based models, the X2Static method for extracting static embeddings\ndemonstrates superior performance compared to decontextualized and aggregate\nmethods, approaching the effectiveness of traditional static embeddings. For\nextrinsic evaluation, we utilize a bidirectional LSTM model to perform Named\nEntity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results\nreveal that embeddings derived from dynamic models, especially those extracted\nusing the X2Static method, outperform purely static embeddings. Notably, ELMo\nembeddings achieve the highest accuracy in both NER and POS tagging tasks,\nunderscoring the benefits of contextualized representations even when used in a\nstatic form. Our findings highlight the continued relevance of static word\nembeddings in NLP applications and the potential of advanced extraction methods\nto enhance the utility of BERT-based models. This piece of research contributes\nto the understanding of embedding performance in the Hungarian language and\nprovides valuable insights for future developments in the field. The training\nscripts, evaluation codes, restricted vocabulary, and extracted embeddings will\nbe made publicly available to support further research and reproducibility.",
      "url": "http://arxiv.org/abs/2505.07809v1",
      "published_time_eastern_timestamp": 1747072631.0
    },
    {
      "title": "Automatically Differentiable Model Updating (ADiMU): conventional,\n  hybrid, and neural network material model discovery including\n  history-dependency",
      "summary": "We introduce the first Automatically Differentiable Model Updating (ADiMU)\nframework that finds any history-dependent material model from full-field\ndisplacement and global force data (global, indirect discovery) or from\nstrain-stress data (local, direct discovery). We show that ADiMU can update\nconventional (physics-based), neural network (data-driven), and hybrid material\nmodels. Moreover, this framework requires no fine-tuning of hyperparameters or\nadditional quantities beyond those inherent to the user-selected material model\narchitecture and optimizer. The robustness and versatility of ADiMU is\nextensively exemplified by updating different models spanning tens to millions\nof parameters, in both local and global discovery settings. Relying on fully\ndifferentiable code, the algorithmic implementation leverages vectorizing maps\nthat enable history-dependent automatic differentiation via efficient batched\nexecution of shared computation graphs. This contribution also aims to\nfacilitate the integration, evaluation and application of future material model\narchitectures by openly supporting the research community. Therefore, ADiMU is\nreleased as an open-source computational tool, integrated into a carefully\ndesigned and documented software named HookeAI.",
      "url": "http://arxiv.org/abs/2505.07801v1",
      "published_time_eastern_timestamp": 1747072194.0
    },
    {
      "title": "Learning from Peers in Reasoning Models",
      "summary": "Large Reasoning Models (LRMs) have the ability to self-correct even when they\nmake mistakes in their reasoning paths. However, our study reveals that when\nthe reasoning process starts with a short but poor beginning, it becomes\ndifficult for the model to recover. We refer to this phenomenon as the \"Prefix\nDominance Trap\". Inspired by psychological findings that peer interaction can\npromote self-correction without negatively impacting already accurate\nindividuals, we propose **Learning from Peers** (LeaP) to address this\nphenomenon. Specifically, every tokens, each reasoning path summarizes its\nintermediate reasoning and shares it with others through a routing mechanism,\nenabling paths to incorporate peer insights during inference. However, we\nobserve that smaller models sometimes fail to follow summarization and\nreflection instructions effectively. To address this, we fine-tune them into\nour **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025,\nand GPQA Diamond show that LeaP provides substantial improvements. For\ninstance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the\nbaseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks\nwith an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches\nthe performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis\nreveals LeaP's robust error correction by timely peer insights, showing strong\nerror tolerance and handling varied task difficulty. LeaP marks a milestone by\nenabling LRMs to collaborate during reasoning. Our code, datasets, and models\nare available at https://learning-from-peers.github.io/ .",
      "url": "http://arxiv.org/abs/2505.07787v1",
      "published_time_eastern_timestamp": 1747071596.0
    },
    {
      "title": "MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine\n  Learning Engineering",
      "summary": "We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement\nlearning, evaluating, and improving autonomous large language model (LLM)\nagents in iterative machine learning engineering (MLE) workflows. Unlike\nexisting benchmarks that primarily rely on static datasets or single-attempt\nevaluations, MLE-Dojo provides an interactive environment enabling agents to\niteratively experiment, debug, and refine solutions through structured feedback\nloops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse,\nopen-ended MLE tasks carefully curated to reflect realistic engineering\nscenarios such as data processing, architecture search, hyperparameter tuning,\nand code debugging. Its fully executable environment supports comprehensive\nagent training via both supervised fine-tuning and reinforcement learning,\nfacilitating iterative experimentation, realistic data sampling, and real-time\noutcome verification. Extensive evaluations of eight frontier LLMs reveal that\nwhile current models achieve meaningful iterative improvements, they still\nexhibit significant limitations in autonomously generating long-horizon\nsolutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's\nflexible and extensible architecture seamlessly integrates diverse data\nsources, tools, and evaluation protocols, uniquely enabling model-based agent\ntuning and promoting interoperability, scalability, and reproducibility. We\nopen-source our framework and benchmarks to foster community-driven innovation\ntowards next-generation MLE agents.",
      "url": "http://arxiv.org/abs/2505.07782v1",
      "published_time_eastern_timestamp": 1747071343.0
    },
    {
      "title": "Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for\n  Mathematical Problem Solving",
      "summary": "Large Language Models (LLMs) often struggle with mathematical reasoning tasks\nrequiring precise, verifiable computation. While Reinforcement Learning (RL)\nfrom outcome-based rewards enhances text-based reasoning, understanding how\nagents autonomously learn to leverage external tools like code execution\nremains crucial. We investigate RL from outcome-based rewards for\nTool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously\ngenerate and execute Python code for mathematical problems without supervised\ntool-use examples. Our central contribution is we demonstrate that as RL\ntraining progresses, key metrics scale predictably. Specifically, we observe\nstrong positive correlations where increased training steps lead to increases\nin the spontaneous code execution frequency, the average response length, and,\ncritically, the final task accuracy. This suggests a quantifiable relationship\nbetween computational effort invested in training and the emergence of\neffective, tool-augmented reasoning strategies. We implement a robust framework\nfeaturing a decoupled code execution environment and validate our findings\nacross standard RL algorithms and frameworks. Experiments show ZeroTIR\nsignificantly surpasses non-tool ZeroRL baselines on challenging math\nbenchmarks. Our findings provide a foundational understanding of how autonomous\ntool use is acquired and scales within Agent RL, offering a reproducible\nbenchmark for future studies. Code is released at\n\\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.",
      "url": "http://arxiv.org/abs/2505.07773v1",
      "published_time_eastern_timestamp": 1747070614.0
    },
    {
      "title": "Enhancing Code Generation via Bidirectional Comment-Level Mutual\n  Grounding",
      "summary": "Large Language Models (LLMs) have demonstrated unprecedented capability in\ncode generation. However, LLM-generated code is still plagued with a wide range\nof functional errors, especially for complex programming tasks that LLMs have\nnot seen before. Recent studies have shown that developers often struggle with\ninspecting and fixing incorrect code generated by LLMs, diminishing their\nproductivity and trust in LLM-based code generation. Inspired by the mutual\ngrounding theory in communication, we propose an interactive approach that\nleverages code comments as a medium for developers and LLMs to establish a\nshared understanding. Our approach facilitates iterative grounding by\ninterleaving code generation, inline comment generation, and contextualized\nuser feedback through editable comments to align generated code with developer\nintent. We evaluated our approach on two popular benchmarks and demonstrated\nthat our approach significantly improved multiple state-of-the-art LLMs, e.g.,\n17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we\nconducted a user study with 12 participants in comparison to two baselines: (1)\ninteracting with GitHub Copilot, and (2) interacting with a multi-step code\ngeneration paradigm called Multi-Turn Program Synthesis. Participants completed\nthe given programming tasks 16.7% faster and with 10.5% improvement in task\nsuccess rate when using our approach. Both results show that interactively\nrefining code comments enables the collaborative establishment of mutual\ngrounding, leading to more accurate code generation and higher developer\nconfidence.",
      "url": "http://arxiv.org/abs/2505.07768v1",
      "published_time_eastern_timestamp": 1747070430.0
    },
    {
      "title": "Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured\n  3D Assets",
      "summary": "While generative artificial intelligence has advanced significantly across\ntext, image, audio, and video domains, 3D generation remains comparatively\nunderdeveloped due to fundamental challenges such as data scarcity, algorithmic\nlimitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an\nopen framework addressing these challenges through: (1) a rigorous data\ncuration pipeline processing >5M assets to create a 2M high-quality dataset\nwith standardized geometric and textural properties; (2) a two-stage 3D-native\narchitecture combining a hybrid VAE-DiT geometry generator with an\ndiffusion-based texture synthesis module; and (3) the full open-source release\nof models, training code, and adaptation modules. For geometry generation, the\nhybrid VAE-DiT component produces TSDF representations by employing\nperceiver-based latent encoding with sharp edge sampling for detail\npreservation. The diffusion-based texture synthesis module then ensures\ncross-view consistency through geometric conditioning and latent-space\nsynchronization. Benchmark results demonstrate state-of-the-art performance\nthat exceeds existing open-source methods, while also achieving competitive\nquality with proprietary solutions. Notably, the framework uniquely bridges the\n2D and 3D generation paradigms by supporting direct transfer of 2D control\ntechniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data\nquality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish\nnew standards for open research in controllable 3D asset generation.",
      "url": "http://arxiv.org/abs/2505.07747v1",
      "published_time_eastern_timestamp": 1747068990.0
    },
    {
      "title": "The Effect of Luminosity Outbursts on the Abundance of Pebbles and Their\n  Ice Mantles in Protoplanetary Disks",
      "summary": "Centimeter-sized dust grains-pebbles-are necessary for planetesimal formation\nvia the streaming instability, they play an important role in forming\nprotoplanetary cores and giant planets, as well as enriching their atmospheres\nwith chemical elements. This work investigates the effect of luminosity\noutbursts on the abundance of pebbles and their ice mantles in protoplanetary\ndisks. We perform global simulations of formation and evolution of a\nself-gravitating viscous protoplanetary disk using the 2D hydrodynamic\nthin-disk FEOSAD code, which self-consistently reproduces luminosity outbursts.\nThe model includes thermal balance, dust evolution and its interaction with\ngas, development of magnetorotational instability, adsorption and desorption of\nfour volatile compounds (H$_2$O, CO$_2$, CH$_4$ and CO), and the feedback of\nice mantles on dust fragmentation properties. We show that luminosity outbursts\nhave a stronger effect on the positions of CO$_2$, CH$_4$ and CO snowlines\ncompared to the water snowline. This is because the H$_2$O snowline falls\nwithin the viscous heating dominated region during early disk evolution stages,\nwhile snowlines of other molecules are located in regions dominated by stellar\nirradiation heating and are thus more sensitive to temperature changes during\noutbursts. Nevertheless, luminosity outbursts reduce the total amount of\npebbles in the disk by half due to destruction of dust aggregates into monomers\nfollowing the loss of water ice that binds the aggregates together. Pebble\nrecovery occurs over several thousand years after the outburst ends due to\ncollisional coagulation, with recovery timescales significantly exceeding water\nfreeze-out times. Ice mantle desorption occurs in a complex non-axisymmetric 2D\nregion of the disk, associated with spiral substructure formation during early\nevolution of gravitationally unstable disks.",
      "url": "http://arxiv.org/abs/2505.07718v1",
      "published_time_eastern_timestamp": 1747067068.0
    },
    {
      "title": "Circuit Partitioning Using Large Language Models for Quantum Compilation\n  and Simulations",
      "summary": "We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where\nquantum computers are limited by noisy gates, some of which are more\nerror-prone than others and can render the final computation incomprehensible.\nQuantum circuit compilation algorithms attempt to minimize these noisy gates\nwhen mapping quantum algorithms onto quantum hardware but face computational\nchallenges that restrict their application to circuits with no more than 5-6\nqubits, necessitating the need to partition large circuits before the\napplication of noisy quantum gate minimization algorithms. The existing\ngeneration of these algorithms is heuristic in nature and does not account for\ndownstream gate minimization tasks. Large language models (LLMs) have the\npotential to change this and help improve quantum circuit partitions. This\npaper investigates the use of LLMs, such as Llama and Mistral, for partitioning\nquantum circuits by capitalizing on their abilities to understand and generate\ncode, including QASM. Specifically, we teach LLMs to partition circuits using\nthe quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through\nexperimental evaluations, we show that careful fine-tuning of open source LLMs\nenables us to obtain an accuracy of 53.4% for the partition task while\nover-the-shelf LLMs are unable to correctly partition circuits, using standard\n1-shot and few-shot training approaches.",
      "url": "http://arxiv.org/abs/2505.07711v1",
      "published_time_eastern_timestamp": 1747066728.0
    },
    {
      "title": "PatchTrack: A Comprehensive Analysis of ChatGPT's Influence on Pull\n  Request Outcomes",
      "summary": "The rapid adoption of large language models (LLMs) like ChatGPT in software\ndevelopment has introduced new ways for developers to interact with AI,\nparticularly in pull request workflows. While prior research has examined\nAI-generated code quality, there is limited understanding of how ChatGPT is\nutilized in real-world pull request decision-making and how its suggestions\ninfluence patch integration and rejection. To explore these aspects, we analyze\nself-admitted ChatGPT usage (SACU), where developers explicitly disclose their\nreliance on ChatGPT within pull request discussions. Our study examines 338\npull requests (285 merged, 53 closed) across 255 GitHub repositories,\ncontaining 645 ChatGPT-generated code snippets and 3,486 patches. We introduce\nPatchTrack, a classification tool that determines whether ChatGPT-generated\npatches were applied (PA, 115 cases), not applied (PN, 64 cases), or not\nsuggested (NE, 106 cases). Our findings reveal that full adoption of\nChatGPT-generated code is rare, developers frequently modify or selectively\nintegrate AI-generated patches to align with project constraints, with a median\nintegration rate of 25%. Through qualitative analysis, we identify key factors\ninfluencing patch integration and pull request rejection, including scope\nmisalignment, maintainability concerns, redundant solutions, and procedural\nbarriers such as incomplete documentation or administrative policies. By\nproviding empirical insights into ChatGPT's role in pull request workflows,\nthis study informs developers, maintainers, and educators on the evolving use\nof generative AI in collaborative software development. It also lays the\ngroundwork for future research on optimizing AI-assisted development, improving\ntransparency in AI adoption, and enhancing patch integration workflows.",
      "url": "http://arxiv.org/abs/2505.07700v1",
      "published_time_eastern_timestamp": 1747066173.0
    },
    {
      "title": "Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter\n  for Continual Learning of Vision-Language Models",
      "summary": "This study aims to address the problem of multi-domain task incremental\nlearning~(MTIL), which requires that vision-language models~(VLMs) continuously\nacquire new knowledge while maintaining their inherent zero-shot recognition\ncapability. Existing paradigms delegate the testing of unseen-domain samples to\nthe original CLIP, which only prevents the degradation of the model's zero-shot\ncapability but fails to enhance the generalization of the VLM further. To this\nend, we propose a novel MTIL framework, named AFA, which comprises two core\nmodules: (1) an against forward-forgetting adapter that learns task-invariant\ninformation for each dataset in the incremental tasks to enhance the zero-shot\nrecognition ability of VLMs; (2) an against backward-forgetting adapter that\nstrengthens the few-shot learning capability of VLMs while supporting\nincremental learning. Extensive experiments demonstrate that the AFA method\nsignificantly outperforms existing state-of-the-art approaches, especially in\nfew-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP\nin terms of transferability. The code is provided in the Supplementary\nMaterial.",
      "url": "http://arxiv.org/abs/2505.07690v1",
      "published_time_eastern_timestamp": 1747065383.0
    },
    {
      "title": "Anatomical Attention Alignment representation for Radiology Report\n  Generation",
      "summary": "Automated Radiology report generation (RRG) aims at producing detailed\ndescriptions of medical images, reducing radiologists' workload and improving\naccess to high-quality diagnostic services. Existing encoder-decoder models\nonly rely on visual features extracted from raw input images, which can limit\nthe understanding of spatial structures and semantic relationships, often\nresulting in suboptimal text generation. To address this, we propose Anatomical\nAttention Alignment Network (A3Net), a framework that enhance visual-textual\nunderstanding by constructing hyper-visual representations. Our approach\nintegrates a knowledge dictionary of anatomical structures with patch-level\nvisual features, enabling the model to effectively associate image regions with\ntheir corresponding anatomical entities. This structured representation\nimproves semantic reasoning, interpretability, and cross-modal alignment,\nultimately enhancing the accuracy and clinical relevance of generated reports.\nExperimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net\nsignificantly improves both visual perception and text generation quality. Our\ncode is available at \\href{https://github.com/Vinh-AI/A3Net}{GitHub}.",
      "url": "http://arxiv.org/abs/2505.07689v1",
      "published_time_eastern_timestamp": 1747065290.0
    },
    {
      "title": "ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical\n  Image Translation",
      "summary": "Accurate multi-modal medical image translation requires ha-rmonizing global\nanatomical semantics and local structural fidelity, a challenge complicated by\nintermodality information loss and structural distortion. We propose ABS-Mamba,\na novel architecture integrating the Segment Anything Model 2 (SAM2) for\norgan-aware semantic representation, specialized convolutional neural networks\n(CNNs) for preserving modality-specific edge and texture details, and Mamba's\nselective state-space modeling for efficient long- and short-range feature\ndependencies. Structurally, our dual-resolution framework leverages SAM2's\nimage encoder to capture organ-scale semantics from high-resolution inputs,\nwhile a parallel CNNs branch extracts fine-grained local features. The Robust\nFeature Fusion Network (RFFN) integrates these epresentations, and the\nBidirectional Mamba Residual Network (BMRN) models spatial dependencies using\nspiral scanning and bidirectional state-space dynamics. A three-stage skip\nfusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank\nAdaptation (LoRA+) fine-tuning to enable precise domain specialization while\nmaintaining the foundational capabilities of the pre-trained components.\nExtensive experimental validation on the SynthRAD2023 and BraTS2019 datasets\ndemonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering\nhigh-fidelity cross-modal synthesis that preserves anatomical semantics and\nstructural details to enhance diagnostic accuracy in clinical applications. The\ncode is available at https://github.com/gatina-yone/ABS-Mamba",
      "url": "http://arxiv.org/abs/2505.07687v1",
      "published_time_eastern_timestamp": 1747065075.0
    },
    {
      "title": "Energetic consistency and heat transport in Fourier-Galerkin truncations\n  of free slip 3D rotating convection",
      "summary": "This paper examines the effects of energetic consistency in Fourier truncated\nmodels of the 3D Boussinesq-Coriolis (BC) equations as a case-study towards\nimproving the realism of convective processes in climate models. As a benchmark\nwe consider the Nusselt number, defined as the average vertical heat transport\nof a convective flow. A set of formulae are derived which give the ODE\nprojection of the BC model onto any finite selection of modes. It is proven\nthat projected ODE models obey energy relations consistent with the PDE if and\nonly if a mode selection Criterion regarding the vertical resolution is\nsatisfied. It is also proven that the energy relations imply the existence of a\ncompact attractor for these ODE's, which then implies bounds on the Nusselt\nnumber. By contrast, it is proven that a broad class of energetically\ninconsistent models admit solutions with unbounded, exponential growth,\nprecluding the existence of a compact attractor and giving an infinite Nusselt\nnumber. On the other hand, certain energetically inconsistent models can admit\ncompact attractors as shown via a simple model. The above formulas are\nimplemented in MATLAB, enabling a user to study any desired Fourier truncated\nmodel by selecting a desired finite set of Fourier modes. All code is made\navailable on GitHub. Several numerical studies of the Nusselt number are\nconducted to assess the convergence of the Nusselt number with respect to\nincreasing spatial resolution for consistent models and measure the distorting\neffects of inconsistency for more general solutions.",
      "url": "http://arxiv.org/abs/2505.07678v1",
      "published_time_eastern_timestamp": 1747064671.0
    },
    {
      "title": "OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit",
      "summary": "We present OnPrem.LLM, a Python-based toolkit for applying large language\nmodels (LLMs) to sensitive, non-public data in offline or restricted\nenvironments. The system is designed for privacy-preserving use cases and\nprovides prebuilt pipelines for document processing and storage,\nretrieval-augmented generation (RAG), information extraction, summarization,\nclassification, and prompt/output processing with minimal configuration.\nOnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM,\nand Hugging Face Transformers -- with quantized model support, GPU\nacceleration, and seamless backend switching. Although designed for fully local\nexecution, OnPrem.LLM also supports integration with a wide range of cloud LLM\nproviders when permitted, enabling hybrid deployments that balance performance\nwith data control. A no-code web interface extends accessibility to\nnon-technical users.",
      "url": "http://arxiv.org/abs/2505.07672v1",
      "published_time_eastern_timestamp": 1747064187.0
    },
    {
      "title": "Benchmarking Retrieval-Augmented Generation for Chemistry",
      "summary": "Retrieval-augmented generation (RAG) has emerged as a powerful framework for\nenhancing large language models (LLMs) with external knowledge, particularly in\nscientific domains that demand specialized and dynamic information. Despite its\npromise, the application of RAG in the chemistry domain remains underexplored,\nprimarily due to the lack of high-quality, domain-specific corpora and\nwell-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a\ncomprehensive benchmark designed to systematically assess the effectiveness of\nRAG across a diverse set of chemistry-related tasks. The accompanying chemistry\ncorpus integrates heterogeneous knowledge sources, including scientific\nliterature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia\nentries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG\ntoolkit that supports five retrieval algorithms and eight LLMs. Using\nChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain\n-- achieving an average relative improvement of 17.4% over direct inference\nmethods. We further conduct in-depth analyses on retriever architectures,\ncorpus selection, and the number of retrieved passages, culminating in\npractical recommendations to guide future research and deployment of RAG\nsystems in the chemistry domain. The code and data is available at\nhttps://chemrag.github.io.",
      "url": "http://arxiv.org/abs/2505.07671v1",
      "published_time_eastern_timestamp": 1747064085.0
    },
    {
      "title": "A comparative study of Bitcoin and Ripple cryptocurrencies trading using\n  Deep Reinforcement Learning algorithms",
      "summary": "Artificial intelligence (AI) has demonstrated remarkable success across\nvarious applications. In light of this trend, the field of automated trading\nhas developed a keen interest in leveraging AI techniques to forecast the\nfuture prices of financial assets. This interest stems from the need to address\ntrading challenges posed by the inherent volatility and dynamic nature of asset\nprices. However, crafting a flawless strategy becomes a formidable task when\ndealing with assets characterized by intricate and ever-changing price\ndynamics. To surmount these formidable challenges, this research employs an\ninnovative rule-based strategy approach to train Deep Reinforcement Learning\n(DRL). This application is carried out specifically in the context of trading\nBitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration\nof Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks,\nalongside the Advantage Actor-Critic algorithms. Each of them aims to yield an\noptimal policy for our application. To evaluate the effectiveness of our Deep\nReinforcement Learning (DRL) approach, we rely on portfolio wealth and the\ntrade signal as performance metrics. The experimental outcomes highlight that\nDuelling and Double Deep Q-Network outperformed when using XRP with the\nincreasing of the portfolio wealth. All codes are available in this\n\\href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\\color{blue}Github\nlink}.",
      "url": "http://arxiv.org/abs/2505.07660v1",
      "published_time_eastern_timestamp": 1747063656.0
    },
    {
      "title": "Dynamical codes for hardware with noisy readouts",
      "summary": "Dynamical stabilizer codes may offer a practical route to large-scale quantum\ncomputation. Such codes are defined by a schedule of error-detecting\nmeasurements, which allows for flexibility in their construction. In this work,\nwe ask how best to optimise the measurement schedule of dynamically condensed\ncolour codes in various limits of noise bias. We take a particular focus on the\nsetting where measurements introduce more noise than unitary and idling\noperations - a noise model relevant to some hardware proposals. For\nmeasurement-biased noise models, we improve code performance by strategically\nrepeating measurements within the schedule. For unbiased or $Z$-biased noise\nmodels, we find repeating measurements offers little improvement - somewhat\ncontrary to our expectations - and investigate why this is. To perform this\nanalysis, we generalise a metric called the teraquop footprint to the teraquop\nvolume. This is the product of the number of qubits and number of rounds of\nmeasurements required such that the probability of a spacelike or timelike\nlogical error occurring is less than $10^{-12}$. In most cases, we find\ndifferences in performance are primarily due to the number of rounds of\nmeasurements required, rather than the number of qubits - emphasising the\nimportance of using the teraquop volume in the analysis. Additionally, our\nresults provide another example of the importance of making use of correlated\nerrors when decoding, in that using belief matching rather than minimum-weight\nperfect matching can turn a worst-performing code under a given noise model\ninto a best-performing code.",
      "url": "http://arxiv.org/abs/2505.07658v1",
      "published_time_eastern_timestamp": 1747063490.0
    }
  ]
}
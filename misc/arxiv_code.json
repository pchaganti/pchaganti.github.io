{
  "last_updated": "2025-11-15T07:21:03.009042-05:00",
  "papers": [
    {
      "title": "Impacts of Decoder Latency on Utility-Scale Quantum Computer Architectures",
      "summary": "The speed of a fault-tolerant quantum computer is dictated by the reaction time of its classical electronics, that is, the total time required by decoders and controllers to determine the outcome of a logical measurement and execute subsequent conditional logical operations. Despite its importance, the reaction time and its impact on the design of the logical microarchitecture of a quantum computer are not well understood. In this work, we build, for a surface code based architecture, a model for the reaction time in which the decoder latency is based on parallel space- and time-window decoding methods, and communication latencies are drawn from our envisioned quantum execution environment comprising a high-speed network of quantum processing units, controllers, decoders, and high-performance computing nodes. We use this model to estimate the increase in the logical error rate of magic state injections as a function of the reaction time. Next, we show how the logical microarchitecture can be optimized with respect to the reaction time, and then present detailed full-system quantum and classical resource estimates for executing utility-scale quantum circuits based on realistic hardware noise parameters and state-of-the-art decoding times. For circuits with $10^{6}$--$10^{11}$ $T$ gates involving 200--2000 logical qubits, under a $Λ=9.3$ hardware model representative of a realistic target for superconducting quantum processors operating at a 2.86 MHz stabilization frequency, we show that even decoding at a sub-microsecond per stabilization round speed introduces substantial resource overheads: approximately 100k--250k additional physical qubits for correction qubit storage in the magic state factory; 300k--1.75M extra physical qubits in the core processor due to the code distance increase of $d$ to $d+4$ for extra memory protection; and a longer runtime by roughly a factor of 100.",
      "url": "http://arxiv.org/abs/2511.10633v1",
      "published_time_eastern_timestamp": 1763060110.0
    },
    {
      "title": "One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models",
      "summary": "Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.",
      "url": "http://arxiv.org/abs/2511.10629v1",
      "published_time_eastern_timestamp": 1763060058.0
    },
    {
      "title": "SSR: Socratic Self-Refine for Large Language Model Reasoning",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.",
      "url": "http://arxiv.org/abs/2511.10621v1",
      "published_time_eastern_timestamp": 1763059627.0
    },
    {
      "title": "A Fast Earth-scattering Formalism for Light Dark Matter with Dark Photon Mediators",
      "summary": "While Dark Matter (DM) is typically assumed to interact only very weakly with the particles of the Standard Model, many direct detection experiments are currently exploring regions of parameter space where DM can have a large scattering cross section. In this scenario, DM may scatter in the atmosphere and Earth before reaching the detector, leading to a distortion of the DM flux and a daily modulation of the signal rate as the detector is shielded by more or less of the Earth at different times of day. This modulation is a distinctive signature of strongly-interacting DM and provides a powerful method of discriminating against time-independent backgrounds. However, the calculation of these Earth-scattering effects by Monte Carlo methods is computationally intensive, inhibiting a systematic exploration of the DM parameter space. Here, we present a semi-analytic formalism for calculating Earth-scattering effects, for models of MeV-mass DM which interacts via a dark photon mediator, and release the associated code Verne2. This formalism assumes that DM travels along straight-line trajectories until it scatters and is reflected back along its incoming path, along us to taking into account the affects of both attenuation and reflection in the Earth. We compare this formalism with the results of full Monte Carlo simulations for cross sections within reach of current and future DM-electron scattering searches. We find that Verne2 is accurate to better than 10-30%, making it suitable for performing signal modeling in the search for daily modulation, while reducing the computational cost by a factor of $\\sim10^4$ compared to full Monte Carlo simulations.",
      "url": "http://arxiv.org/abs/2511.10589v1",
      "published_time_eastern_timestamp": 1763058412.0
    },
    {
      "title": "Semi-Unified Sparse Dictionary Learning with Learnable Top-K LISTA and FISTA Encoders",
      "summary": "We present a semi-unified sparse dictionary learning framework that bridges the gap between classical sparse models and modern deep architectures. Specifically, the method integrates strict Top-$K$ LISTA and its convex FISTA-based variant (LISTAConv) into the discriminative LC-KSVD2 model, enabling co-evolution between the sparse encoder and the dictionary under supervised or unsupervised regimes. This unified design retains the interpretability of traditional sparse coding while benefiting from efficient, differentiable training.\n  We further establish a PALM-style convergence analysis for the convex variant, ensuring theoretical stability under block alternation. Experimentally, our method achieves 95.6\\% on CIFAR-10, 86.3\\% on CIFAR-100, and 88.5\\% on TinyImageNet with faster convergence and lower memory cost ($<$4GB GPU). The results confirm that the proposed LC-KSVD2 + LISTA/LISTAConv pipeline offers an interpretable and computationally efficient alternative for modern deep architectures.",
      "url": "http://arxiv.org/abs/2511.10575v1",
      "published_time_eastern_timestamp": 1763057554.0
    },
    {
      "title": "zkStruDul: Programming zkSNARKs with Structural Duality",
      "summary": "Non-Interactive Zero Knowledge (NIZK) proofs, such as zkSNARKS, let one prove knowledge of private data without revealing it or interacting with a verifier. While existing tooling focuses on specifying the predicate to be proven, real-world applications optimize predicate definitions to minimize proof generation overhead, but must correspondingly transform predicate inputs. Implementing these two steps separately duplicates logic that must precisely match to avoid catastrophic security flaws. We address this shortcoming with zkStruDul, a language that unifies input transformations and predicate definitions into a single combined abstraction from which a compiler can project both procedures, eliminating duplicate code and problematic mismatches. zkStruDul provides a high-level abstraction to layer on top of existing NIZK technology and supports important features like recursive proofs. We provide a source-level semantics and prove its behavior is identical to the projected semantics, allowing straightforward standard reasoning.",
      "url": "http://arxiv.org/abs/2511.10565v1",
      "published_time_eastern_timestamp": 1763057181.0
    },
    {
      "title": "Maximizing Efficiency of Dataset Compression for Machine Learning Potentials With Information Theory",
      "summary": "Machine learning interatomic potentials (MLIPs) balance high accuracy and lower costs compared to density functional theory calculations, but their performance often depends on the size and diversity of training datasets. Large datasets improve model accuracy and generalization but are computationally expensive to produce and train on, while smaller datasets risk discarding rare but important atomic environments and compromising MLIP accuracy/reliability. Here, we develop an information-theoretical framework to quantify the efficiency of dataset compression methods and propose an algorithm that maximizes this efficiency. By framing atomistic dataset compression as an instance of the minimum set cover (MSC) problem over atom-centered environments, our method identifies the smallest subset of structures that contains as much information as possible from the original dataset while pruning redundant information. The approach is extensively demonstrated on the GAP-20 and TM23 datasets, and validated on 64 varied datasets from the ColabFit repository. Across all cases, MSC consistently retains outliers, preserves dataset diversity, and reproduces the long-tail distributions of forces even at high compression rates, outperforming other subsampling methods. Furthermore, MLIPs trained on MSC-compressed datasets exhibit reduced error for out-of-distribution data even in low-data regimes. We explain these results using an outlier analysis and show that such quantitative conclusions could not be achieved with conventional dimensionality reduction methods. The algorithm is implemented in the open-source QUESTS package and can be used for several tasks in atomistic modeling, from data subsampling, outlier detection, and training improved MLIPs at a lower cost.",
      "url": "http://arxiv.org/abs/2511.10561v1",
      "published_time_eastern_timestamp": 1763056821.0
    },
    {
      "title": "Carbox: an end-to-end differentiable astrochemical simulation framework",
      "summary": "Since the first observations of interstellar molecules, astrochemical simulations have been employed to model and understand its formation and destruction path- ways. With the advent of high-resolution telescopes such as JWST and ALMA, the number of detected molecules has increased significantly, thereby creating a need for increasingly complex chemical reaction networks. To model such complex systems, we have developed Carbox, a new astrochemical simulation code that leverages the modern high-performance transformation framework Jax. With Jax enabling computational efficiency and differentiability, Carbox can easily utilize GPU acceleration, be used to study sensitivity and uncertainty, and interface with advances in Scientific Machine Learning. All of these features are crucial for modeling the molecules observed by current and next-generation telescopes.",
      "url": "http://arxiv.org/abs/2511.10558v1",
      "published_time_eastern_timestamp": 1763056737.0
    },
    {
      "title": "A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space",
      "summary": "Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.",
      "url": "http://arxiv.org/abs/2511.10555v1",
      "published_time_eastern_timestamp": 1763056570.0
    },
    {
      "title": "URaG: Unified Retrieval and Generation in Multimodal LLMs for Efficient Long Document Understanding",
      "summary": "Recent multimodal large language models (MLLMs) still struggle with long document understanding due to two fundamental challenges: information interference from abundant irrelevant content, and the quadratic computational cost of Transformer-based architectures. Existing approaches primarily fall into two categories: token compression, which sacrifices fine-grained details; and introducing external retrievers, which increase system complexity and prevent end-to-end optimization. To address these issues, we conduct an in-depth analysis and observe that MLLMs exhibit a human-like coarse-to-fine reasoning pattern: early Transformer layers attend broadly across the document, while deeper layers focus on relevant evidence pages. Motivated by this insight, we posit that the inherent evidence localization capabilities of MLLMs can be explicitly leveraged to perform retrieval during the reasoning process, facilitating efficient long document understanding. To this end, we propose URaG, a simple-yet-effective framework that Unifies Retrieval and Generation within a single MLLM. URaG introduces a lightweight cross-modal retrieval module that converts the early Transformer layers into an efficient evidence selector, identifying and preserving the most relevant pages while discarding irrelevant content. This design enables the deeper layers to concentrate computational resources on pertinent information, improving both accuracy and efficiency. Extensive experiments demonstrate that URaG achieves state-of-the-art performance while reducing computational overhead by 44-56%. The code is available at https://github.com/shi-yx/URaG.",
      "url": "http://arxiv.org/abs/2511.10552v1",
      "published_time_eastern_timestamp": 1763056449.0
    },
    {
      "title": "Non-Resonant Alpha-Induced Neutron-Emission: A Multi- Method Comparison Of Nuclear Reaction Rates",
      "summary": "The precise calculation of alpha-induced neutron-emission ($α$,n) reaction rates is fundamental to understanding nucleosynthesis in diverse stellar environments. This study investigates the nuclear reaction rates for various non-resonant alpha-induced neutron-emission reactions by employing analytical, theoretical, and computational methods. We have adopted analytical expressions from existing literature, and for the computational approach, we have implemented the Monte Carlo method. To better understand the functionality and validate the established open-source Monte Carlo scripts, we also developed our own computational code and used it to test various aspects of the method. A comprehensive comparison of the reaction rates from each method is presented by plotting their dependency on temperature.",
      "url": "http://arxiv.org/abs/2511.10536v1",
      "published_time_eastern_timestamp": 1763055291.0
    },
    {
      "title": "Parallel and GPU accelerated code for phase-field and reaction-diffusion simulations",
      "summary": "We present SymPhas 2.0, a major update of the compile-time symbolic algebra simulation framework SymPhas for phase-field and reaction-diffusion models. This release introduces significant expansions and enhancements that enable the definition of a phase-field model directly from the free-energy functional via compile-time evaluated functional differentiation. It also introduces directional derivatives, symbolic summation, tensor-valued expressions, and compile-time derived finite difference stencils of arbitrary order and accuracy. Furthermore, the code has been parallelized for CPUs with MPI, and GPU computing has been added using CUDA (Compute Unified Device Architecture). For the latter, symbolic expressions are compiled into optimized CUDA kernels, allowing large-scale simulations to execute entirely on the GPU. For large systems ($32,768^2$ in 2D and $1,024^3$ in 3D with double precision), speedups up to $\\sim \\!\\!1,000 \\times$ were obtained compared to the first version of SymPhas using multi-threaded CPU execution on a single system. These developments establish SymPhas 2.0 as a flexible and scalable framework for efficient implementation of phase-field and reaction-diffusion models on GPU-based high-performance computing platforms.",
      "url": "http://arxiv.org/abs/2511.10508v1",
      "published_time_eastern_timestamp": 1763054069.0
    },
    {
      "title": "Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising",
      "summary": "Although Total Variation (TV) performs well in noise reduction and edge preservation on images, its dependence on the lambda parameter limits its efficiency and makes it difficult to use effectively. In this study, we present a Learnable Total Variation (LTV) framework that couples an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet) predicting a per-pixel regularization map. The pipeline is trained end-to-end so that reconstruction and regularization are optimized jointly, yielding spatially adaptive smoothing: strong in homogeneous regions, relaxed near anatomical boundaries. Experiments on the DeepLesion dataset, using a realistic noise model adapted from the LoDoPaB-CT methodology, show consistent gains over classical TV and FBP+U-Net: +2.9 dB PSNR and +6% SSIM on average. LTV provides an interpretable alternative to black-box CNNs and a basis for 3D and data-consistency-driven reconstruction. Our codes are available at: https://github.com/itu-biai/deep_tv_for_ldct",
      "url": "http://arxiv.org/abs/2511.10500v1",
      "published_time_eastern_timestamp": 1763053536.0
    },
    {
      "title": "Odd-Ramsey numbers of Hamilton cycles",
      "summary": "The odd-Ramsey number $r_{\\text odd}(n,H)$ of a graph $H$, as introduced by Alon in his work on graph-codes, is the minimum number of colours needed to edge-colour $K_n$ so that every copy of $H$ intersects some colour class in an odd number of edges. In this paper, we determine the odd-Ramsey number of Hamilton cycles up to a small multiplicative factor, proving that $r_{\\text odd}(n,C_n) = Θ(\\sqrt{n})$. Our upper bound follows from an explicit finite-field construction, while the matching lower bound uses a combinatorial framework based on parity switches. We also initiate the study of odd-Ramsey numbers of Hamilton cycles in Dirac graphs, demonstrating that a small increase in the minimum degree beyond $n/2$ forces nontrivial odd-Ramsey numbers.",
      "url": "http://arxiv.org/abs/2511.10497v1",
      "published_time_eastern_timestamp": 1763053336.0
    },
    {
      "title": "SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers",
      "summary": "While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .",
      "url": "http://arxiv.org/abs/2511.10488v1",
      "published_time_eastern_timestamp": 1763052984.0
    },
    {
      "title": "Panda: Test-Time Adaptation with Negative Data Augmentation",
      "summary": "Pretrained VLMs exhibit strong zero-shot classification capabilities, but their predictions degrade significantly under common image corruptions. To improve robustness, many test-time adaptation (TTA) methods adopt positive data augmentation (PDA), which generates multiple views of each test sample to reduce prediction variance. However, these methods suffer from two key limitations. First, it introduces considerable computational overhead due to the large number of augmentations required per image. Second, it fails to mitigate prediction bias, where the model tends to predict certain classes disproportionately under corruption, as PDA operates on corrupted inputs and typically does not remove the corruption itself. To address these challenges, we propose Panda, a novel TTA method based on negative data augmentation (NDA). Unlike positive augmentations that preserve object semantics, Panda generates negative augmentations by disrupting semantic content. It divides images into patches and randomly assembles them from a shared patch pool. These negatively augmented images retain corruption-specific features while discarding object-relevant signals. We then subtract the mean feature of these negative samples from the original image feature, effectively suppressing corruption-related components while preserving class-relevant information. This mitigates prediction bias under distribution shifts. Panda allows augmentation to be shared across samples within a batch, resulting in minimal computational overhead. Panda can be seamlessly integrated into existing test-time adaptation frameworks and substantially improve their robustness. Our experiments indicate that Panda delivers superior performance compared to PDA methods, and a wide range of TTA methods exhibit significantly enhanced performance when integrated with Panda. Our code is available at https://github.com/ruxideng/Panda .",
      "url": "http://arxiv.org/abs/2511.10481v1",
      "published_time_eastern_timestamp": 1763052360.0
    },
    {
      "title": "Quantum Design Automation: Foundations, Challenges, and the Road Ahead",
      "summary": "Quantum computing is transitioning from laboratory research to industrial deployment, yet significant challenges persist: system scalability and performance, fabrication yields, and the advancement of algorithms and applications. We emphasize that in building quantum computers -- spanning quantum chips, system integration, instruction sets, algorithms, and middleware such as quantum error correction schemes -- design is everywhere. In this paper, we advocate for a holistic design perspective in quantum computing, a perspective we argue is pivotal to unlocking innovative co-design opportunities and addressing the aforementioned key challenges. To equip readers with sufficient background for exploring co-optimization opportunities, we detail how interconnected computational methods and tools collaborate to enable end-to-end quantum computer design. This coverage encompasses critical stages -- such as chip layout design automation, high-fidelity system-level simulation, Hamiltonian derivation for quantum system modeling, control pulse simulation, decoherence analysis, and physical verification and testing -- followed by quantum instruction set design. We then proceed to quantum system and software development, including quantum circuit synthesis, quantum error correction and fault tolerance, and logic verification and testing. Through these discussions, we illustrate with concrete examples -- including co-optimizing quantum instruction sets with algorithmic considerations, customizing error correction circuits to hardware-specific constraints, and streamlining quantum chip design through tailored code design, among others. We hope that the detailed end-to-end design workflow as well as these examples will foster dialogue between the hardware and software communities, ultimately facilitating the translation of meaningful research findings into future quantum hardware implementations.",
      "url": "http://arxiv.org/abs/2511.10479v1",
      "published_time_eastern_timestamp": 1763052276.0
    },
    {
      "title": "Intrinsic Dimensionality as a Model-Free Measure of Class Imbalance",
      "summary": "Imbalance in classification tasks is commonly quantified by the cardinalities of examples across classes. This, however, disregards the presence of redundant examples and inherent differences in the learning difficulties of classes. Alternatively, one can use complex measures such as training loss and uncertainty, which, however, depend on training a machine learning model. Our paper proposes using data Intrinsic Dimensionality (ID) as an easy-to-compute, model-free measure of imbalance that can be seamlessly incorporated into various imbalance mitigation methods. Our results across five different datasets with a diverse range of imbalance ratios show that ID consistently outperforms cardinality-based re-weighting and re-sampling techniques used in the literature. Moreover, we show that combining ID with cardinality can further improve performance. Code: https://github.com/cagries/IDIM.",
      "url": "http://arxiv.org/abs/2511.10475v1",
      "published_time_eastern_timestamp": 1763052097.0
    },
    {
      "title": "Beyond Elicitation: Provision-based Prompt Optimization for Knowledge-Intensive Tasks",
      "summary": "While prompt optimization has emerged as a critical technique for enhancing language model performance, existing approaches primarily focus on elicitation-based strategies that search for optimal prompts to activate models' capabilities. These methods exhibit fundamental limitations when addressing knowledge-intensive tasks, as they operate within fixed parametric boundaries rather than providing the factual knowledge, terminology precision, and reasoning patterns required in specialized domains. To address these limitations, we propose Knowledge-Provision-based Prompt Optimization (KPPO), a framework that reformulates prompt optimization as systematic knowledge integration rather than potential elicitation. KPPO introduces three key innovations: 1) a knowledge gap filling mechanism for knowledge gap identification and targeted remediation; 2) a batch-wise candidate evaluation approach that considers both performance improvement and distributional stability; 3) an adaptive knowledge pruning strategy that balances performance and token efficiency, reducing up to 29% token usage. Extensive evaluation on 15 knowledge-intensive benchmarks from various domains demonstrates KPPO's superiority over elicitation-based methods, with an average performance improvement of ~6% over the strongest baseline while achieving comparable or lower token consumption. Code at: https://github.com/xyz9911/KPPO.",
      "url": "http://arxiv.org/abs/2511.10465v1",
      "published_time_eastern_timestamp": 1763051598.0
    },
    {
      "title": "OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data",
      "summary": "We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.",
      "url": "http://arxiv.org/abs/2511.10461v1",
      "published_time_eastern_timestamp": 1763051315.0
    }
  ]
}
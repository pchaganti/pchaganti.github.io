{
  "last_updated": "2025-10-23T23:28:53.045140-04:00",
  "papers": [
    {
      "title": "HoloCine: Holistic Generation of Cinematic Multi-Shot Long Video\n  Narratives",
      "summary": "State-of-the-art text-to-video models excel at generating isolated clips but\nfall short of creating the coherent, multi-shot narratives, which are the\nessence of storytelling. We bridge this \"narrative gap\" with HoloCine, a model\nthat generates entire scenes holistically to ensure global consistency from the\nfirst shot to the last. Our architecture achieves precise directorial control\nthrough a Window Cross-Attention mechanism that localizes text prompts to\nspecific shots, while a Sparse Inter-Shot Self-Attention pattern (dense within\nshots but sparse between them) ensures the efficiency required for minute-scale\ngeneration. Beyond setting a new state-of-the-art in narrative coherence,\nHoloCine develops remarkable emergent abilities: a persistent memory for\ncharacters and scenes, and an intuitive grasp of cinematic techniques. Our work\nmarks a pivotal shift from clip synthesis towards automated filmmaking, making\nend-to-end cinematic creation a tangible future. Our code is available at:\nhttps://holo-cine.github.io/.",
      "url": "http://arxiv.org/abs/2510.20822v1",
      "published_time_eastern_timestamp": 1761242399.0
    },
    {
      "title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via\n  Speculation",
      "summary": "Large Vision-Language Models (VLMs) have achieved remarkable progress in\nmultimodal understanding, yet they struggle when reasoning over\ninformation-intensive images that densely interleave textual annotations with\nfine-grained graphical elements. The main challenges lie in precisely\nlocalizing critical cues in dense layouts and multi-hop reasoning to integrate\ndispersed evidence. We propose Speculative Verdict (SV), a training-free\nframework inspired by speculative decoding that combines multiple lightweight\ndraft experts with a large verdict model. In the draft stage, small VLMs act as\ndraft experts to generate reasoning paths that provide diverse localization\ncandidates; in the verdict stage, a strong VLM synthesizes these paths to\nproduce the final answer, minimizing computational cost while recovering\ncorrect answers. To further improve efficiency and accuracy, SV introduces a\nconsensus expert selection mechanism that forwards only high-agreement\nreasoning paths to the verdict. Empirically, SV achieves consistent gains on\nchallenging information-intensive and high-resolution visual question answering\nbenchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.\nBy synthesizing correct insights from multiple partially accurate reasoning\npaths, SV achieves both error correction and cost-efficiency compared to large\nproprietary models or training pipelines. Code is available at\nhttps://github.com/Tinaliu0123/speculative-verdict",
      "url": "http://arxiv.org/abs/2510.20812v1",
      "published_time_eastern_timestamp": 1761242361.0
    },
    {
      "title": "Testing the AGN Paradigm, Part I: a generic SED for Seyfert 1 galaxies",
      "summary": "This article presents the first part of a study aimed at testing the\nunification paradigm for AGN (UPAGN) using the SED reconstruction code\nX-CIGALE. Our method consists in obtaining a generic SED for a large sample of\nSeyfert 1 (Sy1; part 1), then applying this SED to Seyfert 2 (Sy2; Part~II),\nexpecting that the only difference will be the line-of-sight (LOS) angle, $i$,\nrelative to the polar axis of the torus of gas and dust obscuring the broad\nline regions (BLRs). Our sample is composed of 3,896 Type 1, Sy1 at low\nredshifts, $ z<0.4$, separated into four spectral subgroups depending on the\npresence or absence in their spectra of narrow emission lines, Sy1N/Sy1B, and\nAGN wind, Sy1Bw and Sy1Nw. The generic SED produced by X-CIGALE applies to 90\\%\nof the Sy1 in our sample. It includes a clumpy torus with an AGN engine seen\nface-on ($i \\sim 10^\\circ \\pm 5^\\circ$). Our analysis not only supports the\nexistence of a torus in Sy1, in good agreement with UPAGN, but also reveals new\nfacts about the accretion of matter and AGN wind: 1- a sudden accretion of\nmatter from the BLR to the accretion disk triggered the wind, 2- matter from\nthe wind replenishes the torus, consistent with a gradual formation of this\nstructure by recurrent AGN winds, and 3- Sy1Bw and Sy1Nw eventually evolve as\nAGN without wind, leaving behind a torus as evidence of a higher AGN activity\nin their past.",
      "url": "http://arxiv.org/abs/2510.20806v1",
      "published_time_eastern_timestamp": 1761242324.0
    },
    {
      "title": "Simple Context Compression: Mean-Pooling and Multi-Ratio Training",
      "summary": "A common strategy to reduce the computational costs of using long contexts in\nretrieval-augmented generation (RAG) with large language models (LLMs) is soft\ncontext compression, where the input sequence is transformed into a shorter\ncontinuous representation. We develop a lightweight and simple mean-pooling\napproach that consistently outperforms the widely used compression-tokens\narchitecture, and study training the same compressor to output multiple\ncompression ratios. We conduct extensive experiments across in-domain and\nout-of-domain QA datasets, as well as across model families, scales, and\ncompression ratios. Overall, our simple mean-pooling approach achieves the\nstrongest performance, with a relatively small drop when training for multiple\ncompression ratios. More broadly though, across architectures and training\nregimes the trade-offs are more nuanced, illustrating the complex landscape of\ncompression methods.",
      "url": "http://arxiv.org/abs/2510.20797v1",
      "published_time_eastern_timestamp": 1761242243.0
    },
    {
      "title": "Radar-Camera Fused Multi-Object Tracking: Online Calibration and Common\n  Feature",
      "summary": "This paper presents a Multi-Object Tracking (MOT) framework that fuses radar\nand camera data to enhance tracking efficiency while minimizing manual\ninterventions. Contrary to many studies that underutilize radar and assign it a\nsupplementary role--despite its capability to provide accurate range/depth\ninformation of targets in a world 3D coordinate system--our approach positions\nradar in a crucial role. Meanwhile, this paper utilizes common features to\nenable online calibration to autonomously associate detections from radar and\ncamera. The main contributions of this work include: (1) the development of a\nradar-camera fusion MOT framework that exploits online radar-camera calibration\nto simplify the integration of detection results from these two sensors, (2)\nthe utilization of common features between radar and camera data to accurately\nderive real-world positions of detected objects, and (3) the adoption of\nfeature matching and category-consistency checking to surpass the limitations\nof mere position matching in enhancing sensor association accuracy. To the best\nof our knowledge, we are the first to investigate the integration of\nradar-camera common features and their use in online calibration for achieving\nMOT. The efficacy of our framework is demonstrated by its ability to streamline\nthe radar-camera mapping process and improve tracking precision, as evidenced\nby real-world experiments conducted in both controlled environments and actual\ntraffic scenarios. Code is available at\nhttps://github.com/radar-lab/Radar_Camera_MOT",
      "url": "http://arxiv.org/abs/2510.20794v1",
      "published_time_eastern_timestamp": 1761242097.0
    },
    {
      "title": "Amplifying Prominent Representations in Multimodal Learning via\n  Variational Dirichlet Process",
      "summary": "Developing effective multimodal fusion approaches has become increasingly\nessential in many real-world scenarios, such as health care and finance. The\nkey challenge is how to preserve the feature expressiveness in each modality\nwhile learning cross-modal interactions. Previous approaches primarily focus on\nthe cross-modal alignment, while over-emphasis on the alignment of marginal\ndistributions of modalities may impose excess regularization and obstruct\nmeaningful representations within each modality. The Dirichlet process (DP)\nmixture model is a powerful Bayesian non-parametric method that can amplify the\nmost prominent features by its richer-gets-richer property, which allocates\nincreasing weights to them. Inspired by this unique characteristic of DP, we\npropose a new DP-driven multimodal learning framework that automatically\nachieves an optimal balance between prominent intra-modal representation\nlearning and cross-modal alignment. Specifically, we assume that each modality\nfollows a mixture of multivariate Gaussian distributions and further adopt DP\nto calculate the mixture weights for all the components. This paradigm allows\nDP to dynamically allocate the contributions of features and select the most\nprominent ones, leveraging its richer-gets-richer property, thus facilitating\nmultimodal feature fusion. Extensive experiments on several multimodal datasets\ndemonstrate the superior performance of our model over other competitors.\nAblation analysis further validates the effectiveness of DP in aligning\nmodality distributions and its robustness to changes in key hyperparameters.\nCode is anonymously available at https://github.com/HKU-MedAI/DPMM.git",
      "url": "http://arxiv.org/abs/2510.20736v1",
      "published_time_eastern_timestamp": 1761238404.0
    },
    {
      "title": "Co-Designing Quantum Codes with Transversal Diagonal Gates via\n  Multi-Agent Systems",
      "summary": "We present a multi-agent, human-in-the-loop workflow that co-designs quantum\ncodes with prescribed transversal diagonal gates. It builds on the Subset-Sum\nLinear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis\nstrings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL)\nequalities via small LPs. The workflow is powered by GPT-5 and implemented\nwithin TeXRA (https://texra.ai)-a multi-agent research assistant platform that\nsupports an iterative tool-use loop agent and a derivation-then-edit workflow\nreasoning agent. We work in a LaTeX-Python environment where agents reason,\nedit documents, execute code, and synchronize their work to Git/Overleaf.\nWithin this workspace, three roles collaborate: a Synthesis Agent formulates\nthe problem; a Search Agent sweeps/screens candidates and exactifies numerics\ninto rationals; and an Audit Agent independently checks all KL equalities and\nthe induced logical action. As a first step we focus on distance $d=2$ with\nnondegenerate residues. For code dimension $K\\in\\{2,3,4\\}$ and $n\\le6$ qubits,\nsystematic sweeps yield certificate-backed tables cataloging attainable cyclic\nlogical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$\nat $n=6$. From verified instances, Synthesis Agent abstracts recurring\nstructures into closed-form families and proves they satisfy the KL equalities\nfor all parameters. It further demonstrates that SSLP accommodates residue\ndegeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal\ncontrolled-phase $diag(1,1,1,i)$. Overall, the workflow recasts\ndiagonal-transversal feasibility as an analytical pipeline executed at scale,\ncombining systematic enumeration with exact analytical reconstruction. It\nyields reproducible code constructions, supports targeted extensions to larger\n$K$ and higher distances, and leads toward data-driven classification.",
      "url": "http://arxiv.org/abs/2510.20728v1",
      "published_time_eastern_timestamp": 1761237939.0
    },
    {
      "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
      "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
      "url": "http://arxiv.org/abs/2510.20707v1",
      "published_time_eastern_timestamp": 1761236267.0
    },
    {
      "title": "Trust, But Verify: An Empirical Evaluation of AI-Generated Code for SDN\n  Controllers",
      "summary": "Generative Artificial Intelligence (AI) tools have been used to generate\nhuman-like content across multiple domains (e.g., sound, image, text, and\nprogramming). However, their reliability in terms of correctness and\nfunctionality in novel contexts such as programmable networks remains unclear.\nHence, this paper presents an empirical evaluation of the source code of a POX\ncontroller generated by different AI tools, namely ChatGPT, Copilot, DeepSeek,\nand BlackBox.ai. To evaluate such a code, three networking tasks of increasing\ncomplexity were defined and for each task, zero-shot and few-shot prompting\ntechniques were input to the tools. Next, the output code was tested in\nemulated network topologies with Mininet and analyzed according to\nfunctionality, correctness, and the need for manual fixes. Results show that\nall evaluated models can produce functional controllers. However, ChatGPT and\nDeepSeek exhibited higher consistency and code quality, while Copilot and\nBlackBox.ai required more adjustments.",
      "url": "http://arxiv.org/abs/2510.20703v1",
      "published_time_eastern_timestamp": 1761236210.0
    },
    {
      "title": "Exploring Large Language Models for Access Control Policy Synthesis and\n  Summarization",
      "summary": "Cloud computing is ubiquitous, with a growing number of services being hosted\non the cloud every day. Typical cloud compute systems allow administrators to\nwrite policies implementing access control rules which specify how access to\nprivate data is governed. These policies must be manually written, and due to\ntheir complexity can often be error prone. Moreover, existing policies often\nimplement complex access control specifications and thus can be difficult to\nprecisely analyze in determining their behavior works exactly as intended.\nRecently, Large Language Models (LLMs) have shown great success in automated\ncode synthesis and summarization. Given this success, they could potentially be\nused for automatically generating access control policies or aid in\nunderstanding existing policies. In this paper, we explore the effectiveness of\nLLMs for access control policy synthesis and summarization. Specifically, we\nfirst investigate diverse LLMs for access control policy synthesis, finding\nthat: although LLMs can effectively generate syntactically correct policies,\nthey have permissiveness issues, generating policies equivalent to the given\nspecification 45.8% of the time for non-reasoning LLMs, and 93.7% of the time\nfor reasoning LLMs. We then investigate how LLMs can be used to analyze\npolicies by introducing a novel semantic-based request summarization approach\nwhich leverages LLMs to generate a precise characterization of the requests\nallowed by a policy. Our results show that while there are significant hurdles\nin leveraging LLMs for automated policy generation, LLMs show promising results\nwhen combined with symbolic approaches in analyzing existing policies.",
      "url": "http://arxiv.org/abs/2510.20692v1",
      "published_time_eastern_timestamp": 1761235575.0
    },
    {
      "title": "SafeFFI: Efficient Sanitization at the Boundary Between Safe and Unsafe\n  Code in Rust and Mixed-Language Applications",
      "summary": "Unsafe Rust code is necessary for interoperability with C/C++ libraries and\nimplementing low-level data structures, but it can cause memory safety\nviolations in otherwise memory-safe Rust programs. Sanitizers can catch such\nmemory errors at runtime, but introduce many unnecessary checks even for memory\naccesses guaranteed safe by the Rust type system. We introduce SafeFFI, a\nsystem for optimizing memory safety instrumentation in Rust binaries such that\nchecks occur at the boundary between unsafe and safe code, handing over the\nenforcement of memory safety from the sanitizer to the Rust type system. Unlike\nprevious approaches, our design avoids expensive whole-program analysis and\nadds much less compile-time overhead (2.64x compared to over 8.83x). On a\ncollection of popular Rust crates and known vulnerable Rust code, SafeFFI\nachieves superior performance compared to state-of-the-art systems, reducing\nsanitizer checks by up to 98%, while maintaining correctness and flagging all\nspatial and temporal memory safety violations.",
      "url": "http://arxiv.org/abs/2510.20688v1",
      "published_time_eastern_timestamp": 1761235365.0
    },
    {
      "title": "C-NAV: Towards Self-Evolving Continual Object Navigation in Open World",
      "summary": "Embodied agents are expected to perform object navigation in dynamic,\nopen-world environments. However, existing approaches typically rely on static\ntrajectories and a fixed set of object categories during training, overlooking\nthe real-world requirement for continual adaptation to evolving scenarios. To\nfacilitate related studies, we introduce the continual object navigation\nbenchmark, which requires agents to acquire navigation skills for new object\ncategories while avoiding catastrophic forgetting of previously learned\nknowledge. To tackle this challenge, we propose C-Nav, a continual visual\nnavigation framework that integrates two key innovations: (1) A dual-path\nanti-forgetting mechanism, which comprises feature distillation that aligns\nmulti-modal inputs into a consistent representation space to ensure\nrepresentation consistency, and feature replay that retains temporal features\nwithin the action decoder to ensure policy consistency. (2) An adaptive\nsampling strategy that selects diverse and informative experiences, thereby\nreducing redundancy and minimizing memory overhead. Extensive experiments\nacross multiple model architectures demonstrate that C-Nav consistently\noutperforms existing approaches, achieving superior performance even compared\nto baselines with full trajectory retention, while significantly lowering\nmemory requirements. The code will be publicly available at\nhttps://bigtree765.github.io/C-Nav-project.",
      "url": "http://arxiv.org/abs/2510.20685v1",
      "published_time_eastern_timestamp": 1761235063.0
    },
    {
      "title": "A Soundness and Precision Benchmark for Java Debloating Tools",
      "summary": "Modern software development reuses code by importing libraries as\ndependencies. Software projects typically include an average of 36\ndependencies, with 80% being transitive, meaning they are dependencies of\ndependencies. Recent research indicates that only 24.9% of these dependencies\nare required at runtime, and even within those, many program constructs remain\nunused, adding unnecessary code to the project. This has led to the development\nof debloating tools that remove unnecessary dependencies and program constructs\nwhile balancing precision by eliminating unused constructs and soundness by\npreserving all required constructs. To systematically evaluate this trade-off,\nwe developed Deblometer, a micro-benchmark consisting of 59 test cases designed\nto assess support for various Java language features in debloating tools. Each\ntest case includes a manually curated ground truth specifying necessary and\nbloated classes, methods, and fields, enabling precise measurement of soundness\nand precision. Using Deblometer, we evaluated three popular Java debloating\ntools: Deptrim, JShrink, and ProGuard. Our evaluation reveals that all tools\nremove required program constructs, which results in changed semantics or\nexecution crashes. In particular, the dynamic class loading feature introduces\nunsoundness in all evaluated tools. Our comparison shows that Deptrim retains\nmore bloated constructs, while ProGuard removes more required constructs.\nJShrink's soundness is significantly affected by limited support for\nannotations, which leads to corrupted debloated artifacts. These soundness\nissues highlight the need to improve debloating tools to ensure stable and\nreliable debloated software.",
      "url": "http://arxiv.org/abs/2510.20679v1",
      "published_time_eastern_timestamp": 1761234740.0
    },
    {
      "title": "Efficient Multi-bit Quantization Network Training via Weight Bias\n  Correction and Bit-wise Coreset Sampling",
      "summary": "Multi-bit quantization networks enable flexible deployment of deep neural\nnetworks by supporting multiple precision levels within a single model.\nHowever, existing approaches suffer from significant training overhead as\nfull-dataset updates are repeated for each supported bit-width, resulting in a\ncost that scales linearly with the number of precisions. Additionally, extra\nfine-tuning stages are often required to support additional or intermediate\nprecision options, further compounding the overall training burden. To address\nthis issue, we propose two techniques that greatly reduce the training overhead\nwithout compromising model utility: (i) Weight bias correction enables shared\nbatch normalization and eliminates the need for fine-tuning by neutralizing\nquantization-induced bias across bit-widths and aligning activation\ndistributions; and (ii) Bit-wise coreset sampling strategy allows each child\nmodel to train on a compact, informative subset selected via gradient-based\nimportance scores by exploiting the implicit knowledge transfer phenomenon.\nExperiments on CIFAR-10/100, TinyImageNet, and ImageNet-1K with both ResNet and\nViT architectures demonstrate that our method achieves competitive or superior\naccuracy while reducing training time up to 7.88x. Our code is released at\nhttps://github.com/a2jinhee/EMQNet_jk.",
      "url": "http://arxiv.org/abs/2510.20673v1",
      "published_time_eastern_timestamp": 1761234542.0
    },
    {
      "title": "GRACE: GRaph-based Addiction Care prEdiction",
      "summary": "Determining the appropriate locus of care for addiction patients is one of\nthe most critical clinical decisions that affects patient treatment outcomes\nand effective use of resources. With a lack of sufficient specialized treatment\nresources, such as inpatient beds or staff, there is an unmet need to develop\nan automated framework for the same. Current decision-making approaches suffer\nfrom severe class imbalances in addiction datasets. To address this limitation,\nwe propose a novel graph neural network (GRACE) framework that formalizes locus\nof care prediction as a structured learning problem. Further, we perform\nextensive feature engineering and propose a new approach of obtaining an\nunbiased meta-graph to train a GNN to overcome the class imbalance problem.\nExperimental results in real-world data show an improvement of 11-35% in terms\nof the F1 score of the minority class over competitive baselines. The codes and\nnote embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.",
      "url": "http://arxiv.org/abs/2510.20671v1",
      "published_time_eastern_timestamp": 1761234481.0
    },
    {
      "title": "\\textsc{CantoNLU}: A benchmark for Cantonese natural language\n  understanding",
      "summary": "Cantonese, although spoken by millions, remains under-resourced due to policy\nand diglossia. To address this scarcity of evaluation frameworks for Cantonese,\nwe introduce \\textsc{\\textbf{CantoNLU}}, a benchmark for Cantonese natural\nlanguage understanding (NLU). This novel benchmark spans seven tasks covering\nsyntax and semantics, including word sense disambiguation, linguistic\nacceptability judgment, language detection, natural language inference,\nsentiment analysis, part-of-speech tagging, and dependency parsing. In addition\nto the benchmark, we provide model baseline performance across a set of models:\na Mandarin model without Cantonese training, two Cantonese-adapted models\nobtained by continual pre-training a Mandarin model on Cantonese text, and a\nmonolingual Cantonese model trained from scratch. Results show that\nCantonese-adapted models perform best overall, while monolingual models perform\nbetter on syntactic tasks. Mandarin models remain competitive in certain\nsettings, indicating that direct transfer may be sufficient when Cantonese\ndomain data is scarce. We release all datasets, code, and model weights to\nfacilitate future research in Cantonese NLP.",
      "url": "http://arxiv.org/abs/2510.20670v1",
      "published_time_eastern_timestamp": 1761234447.0
    },
    {
      "title": "Niebla: an open-source code for modelling the extragalactic background\n  light",
      "summary": "Extragalactic very high-energy (VHE; $E>100\\,$GeV) gamma rays suffer\nabsorption in interactions with photons of the Extragalactic Background Light\n(EBL). The EBL is an isotropic diffuse photon field from optical to infrared\nwavelengths, which is difficult to measure directly due to strong foreground\nemission. We present niebla, the first open-source code to compute the EBL\nusing a forward-folding approach that accepts fully customizable inputs. This\nsoftware enables a detailed modelling of the influence of EBL opacities on VHE\nobservations and facilitates the distinction between different dust reemission\nmodels. The code models the optical background primarily from stellar emission,\nby evolving the spectrum of a single stellar population as a function of\nredshift, considering mean metallicity evolution and star formation rate\ndensity. Additional sources to the EBL can be provided by the user. The code\nalready includes optional contributions from, e.g., stripped stars, intra-halo\nlight, or the decay of axion dark matter. The optical emissivity is then\nabsorbed by interstellar dust and reemitted in the infrared regime. We provide\nmultiple prescriptions to model this process, using spectral dust templates or\na combination of blackbodies. We provide three EBL models calculated with\ndifferent dust reemission prescriptions, which have been fitted to various\nobservational data sets. In addition, we showcase the versatility of our model\nthrough a simulated observation of the blazar Markarian 501 in a high-flux\nstate with the Large High Altitude Air Shower Observatory array. We find that\nthe simulated VHE spectrum is highly sensitive to the EBL opacity coming from\nthe infrared. Our model will therefore allow the community to distinguish\nbetween different dust reemission models and constrain EBL parameters with\nfuture observations.",
      "url": "http://arxiv.org/abs/2510.20664v1",
      "published_time_eastern_timestamp": 1761233941.0
    },
    {
      "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale\n  High-Quality Dataset",
      "summary": "Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable\nprogress. However, two key challenges remain : 1) the absence of a large-scale\nhigh-quality UHR T2I dataset, and (2) the neglect of tailored training\nstrategies for fine-grained detail synthesis in UHR scenarios. To tackle the\nfirst challenge, we introduce \\textbf{UltraHR-100K}, a high-quality dataset of\n100K UHR images with rich captions, offering diverse content and strong visual\nfidelity. Each image exceeds 3K resolution and is rigorously curated based on\ndetail richness, content complexity, and aesthetic quality. To tackle the\nsecond challenge, we propose a frequency-aware post-training method that\nenhances fine-detail generation in T2I diffusion models. Specifically, we\ndesign (i) \\textit{Detail-Oriented Timestep Sampling (DOTS)} to focus learning\non detail-critical denoising steps, and (ii) \\textit{Soft-Weighting Frequency\nRegularization (SWFR)}, which leverages Discrete Fourier Transform (DFT) to\nsoftly constrain frequency components, encouraging high-frequency detail\npreservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks\ndemonstrate that our approach significantly improves the fine-grained detail\nquality and overall fidelity of UHR image generation. The code is available at\n\\href{https://github.com/NJU-PCALab/UltraHR-100k}{here}.",
      "url": "http://arxiv.org/abs/2510.20661v1",
      "published_time_eastern_timestamp": 1761233693.0
    },
    {
      "title": "Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound\n  for Representation Learning",
      "summary": "Mutual Information (MI) is a fundamental measure of statistical dependence\nwidely used in representation learning. While direct optimization of MI via its\ndefinition as a Kullback-Leibler divergence (KLD) is often intractable, many\nrecent methods have instead maximized alternative dependence measures, most\nnotably, the Jensen-Shannon divergence (JSD) between joint and product of\nmarginal distributions via discriminative losses. However, the connection\nbetween these surrogate objectives and MI remains poorly understood. In this\nwork, we bridge this gap by deriving a new, tight, and tractable lower bound on\nKLD as a function of JSD in the general case. By specializing this bound to\njoint and marginal distributions, we demonstrate that maximizing the JSD-based\ninformation increases a guaranteed lower bound on mutual information.\nFurthermore, we revisit the practical implementation of JSD-based objectives\nand observe that minimizing the cross-entropy loss of a binary classifier\ntrained to distinguish joint from marginal pairs recovers a known variational\nlower bound on the JSD. Extensive experiments demonstrate that our lower bound\nis tight when applied to MI estimation. We compared our lower bound to\nstate-of-the-art neural estimators of variational lower bound across a range of\nestablished reference scenarios. Our lower bound estimator consistently\nprovides a stable, low-variance estimate of a tight lower bound on MI. We also\ndemonstrate its practical usefulness in the context of the Information\nBottleneck framework. Taken together, our results provide new theoretical\njustifications and strong empirical evidence for using discriminative learning\nin MI-based representation learning.",
      "url": "http://arxiv.org/abs/2510.20644v1",
      "published_time_eastern_timestamp": 1761232692.0
    },
    {
      "title": "H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition",
      "summary": "We introduce H-SPLID, a novel algorithm for learning salient feature\nrepresentations through the explicit decomposition of salient and non-salient\nfeatures into separate spaces. We show that H-SPLID promotes learning\nlow-dimensional, task-relevant features. We prove that the expected prediction\ndeviation under input perturbations is upper-bounded by the dimension of the\nsalient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between\ninputs and representations. This establishes a link between robustness and\nlatent representation compression in terms of the dimensionality and\ninformation preserved. Empirical evaluations on image classification tasks show\nthat models trained with H-SPLID primarily rely on salient input components, as\nindicated by reduced sensitivity to perturbations affecting non-salient\nfeatures, such as image backgrounds. Our code is available at\nhttps://github.com/neu-spiral/H-SPLID.",
      "url": "http://arxiv.org/abs/2510.20627v1",
      "published_time_eastern_timestamp": 1761231727.0
    }
  ]
}
{
  "last_updated": "2025-05-15T17:11:16.124943-04:00",
  "papers": [
    {
      "title": "Customizing a Large Language Model for VHDL Design of High-Performance\n  Microprocessors",
      "summary": "The use of Large Language Models (LLMs) in hardware design has taken off in\nrecent years, principally through its incorporation in tools that increase chip\ndesigner productivity. There has been considerable discussion about the use of\nLLMs in RTL specifications of chip designs, for which the two most popular\nlanguages are Verilog and VHDL. LLMs and their use in Verilog design has\nreceived significant attention due to the higher popularity of the language,\nbut little attention so far has been given to VHDL despite its continued\npopularity in the industry. There has also been little discussion about the\nunique needs of organizations that engage in high-performance processor design,\nand techniques to deploy AI solutions in these settings. In this paper, we\ndescribe our journey in developing a Large Language Model (LLM) specifically\nfor the purpose of explaining VHDL code, a task that has particular importance\nin an organization with decades of experience and assets in high-performance\nprocessor design. We show how we developed test sets specific to our needs and\nused them for evaluating models as we performed extended pretraining (EPT) of a\nbase LLM. Expert evaluation of the code explanations produced by the EPT model\nincreased to 69% compared to a base model rating of 43%. We further show how we\ndeveloped an LLM-as-a-judge to gauge models similar to expert evaluators. This\nled us to deriving and evaluating a host of new models, including an\ninstruction-tuned version of the EPT model with an expected expert evaluator\nrating of 71%. Our experiments also indicate that with the potential use of\nnewer base models, this rating can be pushed to 85% and beyond. We conclude\nwith a discussion on further improving the quality of hardware design LLMs\nusing exciting new developments in the Generative AI world.",
      "url": "http://arxiv.org/abs/2505.09610v1",
      "published_time_eastern_timestamp": 1747245520.0
    },
    {
      "title": "Don't Forget your Inverse DDIM for Image Editing",
      "summary": "The field of text-to-image generation has undergone significant advancements\nwith the introduction of diffusion models. Nevertheless, the challenge of\nediting real images persists, as most methods are either computationally\nintensive or produce poor reconstructions. This paper introduces SAGE\n(Self-Attention Guidance for image Editing) - a novel technique leveraging\npre-trained diffusion models for image editing. SAGE builds upon the DDIM\nalgorithm and incorporates a novel guidance mechanism utilizing the\nself-attention layers of the diffusion U-Net. This mechanism computes a\nreconstruction objective based on attention maps generated during the inverse\nDDIM process, enabling efficient reconstruction of unedited regions without the\nneed to precisely reconstruct the entire input image. Thus, SAGE directly\naddresses the key challenges in image editing. The superiority of SAGE over\nother methods is demonstrated through quantitative and qualitative evaluations\nand confirmed by a statistically validated comprehensive user study, in which\nall 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE\nranks as the top-performing method in seven out of 10 quantitative analyses and\nsecures second and third places in the remaining three.",
      "url": "http://arxiv.org/abs/2505.09571v1",
      "published_time_eastern_timestamp": 1747242903.0
    },
    {
      "title": "MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8",
      "summary": "With the rapid advancement of powerful large language models (LLMs) in recent\nyears, a wide range of software engineering tasks can now be addressed using\nLLMs, significantly enhancing productivity and scalability. Numerous benchmark\ndatasets have been developed to evaluate the coding capabilities of these\nmodels, while they primarily focus on problem-solving and issue-resolution\ntasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a\ndistinct focus: code migration. MIGRATION-BENCH aims to serve as a\ncomprehensive benchmark for migration from Java 8 to the latest long-term\nsupport (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset\nand its subset selected with $5,102$ and $300$ repositories respectively.\nSelected is a representative subset curated for complexity and difficulty,\noffering a versatile resource to support research in the field of code\nmigration. Additionally, we provide a comprehensive evaluation framework to\nfacilitate rigorous and standardized assessment of LLMs on this challenging\ntask. We further propose SD-Feedback and demonstrate that LLMs can effectively\ntackle repository-level code migration to Java 17. For the selected subset with\nClaude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate\n(pass@1) for minimal and maximal migration respectively. The benchmark dataset\nand source code are available at:\nhttps://huggingface.co/collections/AmazonScience and\nhttps://github.com/amazon-science/self_debug respectively.",
      "url": "http://arxiv.org/abs/2505.09569v1",
      "published_time_eastern_timestamp": 1747242683.0
    },
    {
      "title": "BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture,\n  Training and Dataset",
      "summary": "Unifying image understanding and generation has gained growing attention in\nrecent research on multimodal models. Although design choices for image\nunderstanding have been extensively studied, the optimal model architecture and\ntraining recipe for a unified framework with image generation remain\nunderexplored. Motivated by the strong potential of autoregressive and\ndiffusion models for high-quality generation and scalability, we conduct a\ncomprehensive study of their use in unified multimodal settings, with emphasis\non image representations, modeling objectives, and training strategies.\nGrounded in these investigations, we introduce a novel approach that employs a\ndiffusion transformer to generate semantically rich CLIP image features, in\ncontrast to conventional VAE-based representations. This design yields both\nhigher training efficiency and improved generative quality. Furthermore, we\ndemonstrate that a sequential pretraining strategy for unified models-first\ntraining on image understanding and subsequently on image generation-offers\npractical advantages by preserving image understanding capability while\ndeveloping strong image generation ability. Finally, we carefully curate a\nhigh-quality instruction-tuning dataset BLIP3o-60k for image generation by\nprompting GPT-4o with a diverse set of captions covering various scenes,\nobjects, human gestures, and more. Building on our innovative model design,\ntraining recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art\nunified multimodal models. BLIP3-o achieves superior performance across most of\nthe popular benchmarks spanning both image understanding and generation tasks.\nTo facilitate future research, we fully open-source our models, including code,\nmodel weights, training scripts, and pretraining and instruction tuning\ndatasets.",
      "url": "http://arxiv.org/abs/2505.09568v1",
      "published_time_eastern_timestamp": 1747242667.0
    },
    {
      "title": "Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through\n  Differentiable Object Shapes",
      "summary": "Autonomous vehicles need a complete map of their surroundings to plan and\nact. This has sparked research into the tasks of 3D occupancy prediction, 3D\nscene completion, and 3D panoptic scene completion, which predict a dense map\nof the ego vehicle's surroundings as a voxel grid. Scene completion extends\noccupancy prediction by predicting occluded regions of the voxel grid, and\npanoptic scene completion further extends this task by also distinguishing\nobject instances within the same class; both aspects are crucial for path\nplanning and decision-making. However, 3D panoptic scene completion is\ncurrently underexplored. This work introduces a novel framework for 3D panoptic\nscene completion that extends existing 3D semantic scene completion models. We\npropose an Object Module and Panoptic Module that can easily be integrated with\n3D occupancy and scene completion methods presented in the literature. Our\napproach leverages the available annotations in occupancy benchmarks, allowing\nindividual object shapes to be learned as a differentiable problem. The code is\navailable at https://github.com/nicolamarinello/OffsetOcc .",
      "url": "http://arxiv.org/abs/2505.09562v1",
      "published_time_eastern_timestamp": 1747242312.0
    },
    {
      "title": "WavReward: Spoken Dialogue Models With Generalist Reward Evaluators",
      "summary": "End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered\nsignificant attention in the speech domain. However, the evaluation of spoken\ndialogue models' conversational performance has largely been overlooked. This\nis primarily due to the intelligent chatbots convey a wealth of non-textual\ninformation which cannot be easily measured using text-based language models\nlike ChatGPT. To address this gap, we propose WavReward, a reward feedback\nmodel based on audio language models that can evaluate both the IQ and EQ of\nspoken dialogue systems with speech input. Specifically, 1) based on audio\nlanguage models, WavReward incorporates the deep reasoning process and the\nnonlinear reward mechanism for post-training. By utilizing multi-sample\nfeedback via the reinforcement learning algorithm, we construct a specialized\nevaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a\npreference dataset used to train WavReward. ChatReward-30K includes both\ncomprehension and generation aspects of spoken dialogue models. These scenarios\nspan various tasks, such as text-based chats, nine acoustic attributes of\ninstruction chats, and implicit chats. WavReward outperforms previous\nstate-of-the-art evaluation models across multiple spoken dialogue scenarios,\nachieving a substantial improvement about Qwen2.5-Omni in objective accuracy\nfrom 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a\nmargin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each\ncomponent of WavReward. All data and code will be publicly at\nhttps://github.com/jishengpeng/WavReward after the paper is accepted.",
      "url": "http://arxiv.org/abs/2505.09558v1",
      "published_time_eastern_timestamp": 1747241655.0
    },
    {
      "title": "Conformal Bounds on Full-Reference Image Quality for Imaging Inverse\n  Problems",
      "summary": "In imaging inverse problems, we would like to know how close the recovered\nimage is to the true image in terms of full-reference image quality (FRIQ)\nmetrics like PSNR, SSIM, LPIPS, etc. This is especially important in\nsafety-critical applications like medical imaging, where knowing that, say, the\nSSIM was poor could potentially avoid a costly misdiagnosis. But since we don't\nknow the true image, computing FRIQ is non-trivial. In this work, we combine\nconformal prediction with approximate posterior sampling to construct bounds on\nFRIQ that are guaranteed to hold up to a user-specified error probability. We\ndemonstrate our approach on image denoising and accelerated magnetic resonance\nimaging (MRI) problems. Code is available at\nhttps://github.com/jwen307/quality_uq.",
      "url": "http://arxiv.org/abs/2505.09528v1",
      "published_time_eastern_timestamp": 1747239806.0
    },
    {
      "title": "Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI\n  Reconstruction based on Multi-directional Time-Frequency Convolutional\n  Attention Encoder and Vision-Mamba U-Net",
      "summary": "High-resolution functional magnetic resonance imaging (fMRI) is essential for\nmapping human brain activity; however, it remains costly and logistically\nchallenging. If comparable volumes could be generated directly from widely\navailable scalp electroencephalography (EEG), advanced neuroimaging would\nbecome significantly more accessible. Existing EEG-to-fMRI generators rely on\nplain CNNs that fail to capture cross-channel time-frequency cues or on heavy\ntransformer/GAN decoders that strain memory and stability. We propose\nSpec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts\nthese issues via a Multi-directional Time-Frequency Convolutional Attention\nEncoder, stacking temporal, spectral and joint convolutions with\nself-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space\nblocks enable efficient long-range spatial modelling. Trained end-to-end with a\nhybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on\nthree public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball\nand 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9%\nrespectively over previous best SSIM scores. Furthermore, it achieves\ncompetitive PSNR scores, particularly excelling on the CN-EPFL dataset with a\n4.6% improvement over the previous best PSNR, thus striking a better balance in\nreconstruction quality. The proposed model is lightweight and efficient, making\nit suitable for real-time applications in clinical and research settings. The\ncode is available at https://github.com/hdy6438/Spec2VolCAMU-Net.",
      "url": "http://arxiv.org/abs/2505.09521v1",
      "published_time_eastern_timestamp": 1747239501.0
    },
    {
      "title": "Towards Fair In-Context Learning with Tabular Foundation Models",
      "summary": "Tabular foundational models have exhibited strong in-context learning (ICL)\ncapabilities on structured data, allowing them to make accurate predictions on\ntest sets without parameter updates, using training examples as context. This\nemerging approach positions itself as a competitive alternative to traditional\ngradient-boosted tree methods. However, while biases in conventional machine\nlearning models are well documented, it remains unclear how these biases\nmanifest in tabular ICL. The paper investigates the fairness implications of\ntabular ICL and explores three preprocessing strategies--correlation removal,\ngroup-balanced demonstration selection, and uncertainty-based demonstration\nselection--to address bias. Comprehensive experiments indicate that\nuncertainty-based demonstration selection consistently enhances group fairness\nof in-context predictions. The source code for reproducing the results of this\nwork can be found at https://github.com/patrikken/Fair-TabICL.",
      "url": "http://arxiv.org/abs/2505.09503v1",
      "published_time_eastern_timestamp": 1747237994.0
    },
    {
      "title": "Radon Exposure Dataset",
      "summary": "Exposure to elevated radon levels in the home is one of the leading causes of\nlung cancer in the world. The following study describes the creation of a\ncomprehensive, state-level dataset designed to enable the modeling and\nprediction of household radon concentrations at Zip Code Tabulation Area (ZCTA)\nand sub-kilometer scales. Details include the data collection and processing\ninvolved in compiling physical and demographic factors for Pennsylvania and\nUtah. Attempting to mitigate this risk requires identifying the underlying\ngeological causes and the populations that might be at risk. This work focuses\non identifying at-risk populations throughout Pennsylvania and Utah, where\nradon levels are some of the highest in the country. The resulting dataset\nharmonizes geological and demographic factors from various sources and spatial\nresolutions, including temperature, geochemistry, and soil characteristics.\nDemographic variables such as the household heating fuel used, the age of\nbuilding, and the housing type provide further insight into which populations\ncould be most susceptible in areas with potentially high radon levels. This\ndataset also serves as a foundational resource for two other studies conducted\nby the authors. The resolution of the data provides a novel approach to\npredicting potential radon exposure, and the data processing conducted for\nthese states can be scaled up to larger spatial resolutions (e.g., the\nContiguous United States [CONUS]) and allow for a broad reclassification of\nradon exposure potential in the United States.",
      "url": "http://arxiv.org/abs/2505.09489v1",
      "published_time_eastern_timestamp": 1747237240.0
    },
    {
      "title": "Denoising and Alignment: Rethinking Domain Generalization for Multimodal\n  Face Anti-Spoofing",
      "summary": "Face Anti-Spoofing (FAS) is essential for the security of facial recognition\nsystems in diverse scenarios such as payment processing and surveillance.\nCurrent multimodal FAS methods often struggle with effective generalization,\nmainly due to modality-specific biases and domain shifts. To address these\nchallenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising\nand \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot\ngeneralization capability of CLIP, the MMDA framework effectively suppresses\nnoise in multimodal data through denoising and alignment mechanisms, thereby\nsignificantly enhancing the generalization performance of cross-modal\nalignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential\n\\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the\nimpacts of domain and modality noise by refining the attention mechanism based\non extracted common noise features. Furthermore, the \\textbf{R}epresentation\n\\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the\npre-trained CLIP model to align multi-domain multimodal data into a generalized\nrepresentation space in a flexible manner, preserving intricate representations\nand enhancing the model's adaptability to various unseen conditions. We also\ndesign a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation\n(\\textbf{U-DSA}) module to enhance the adaptability of representations while\nmaintaining generalization performance. These improvements not only enhance the\nframework's generalization capabilities but also boost its ability to represent\ncomplex representations. Our experimental results on four benchmark datasets\nunder different evaluation protocols demonstrate that the MMDA framework\noutperforms existing state-of-the-art methods in terms of cross-domain\ngeneralization and multimodal detection accuracy. The code will be released\nsoon.",
      "url": "http://arxiv.org/abs/2505.09484v1",
      "published_time_eastern_timestamp": 1747237004.0
    },
    {
      "title": "Function-Correcting $b$-symbol Codes for Locally $(λ,\n  ρ,b)$-Functions",
      "summary": "The family of functions plays a central role in the design and effectiveness\nof function-correcting codes. By focusing on a well-defined family of\nfunctions, function-correcting codes can be constructed with minimal length\nwhile still ensuring full error detection or correction within that family. In\nthis paper, we explore locally ($\\lambda,\\rho$)-functions and develop\nfunction-correcting codes using these functions for $b$-symbol read channels.\nWe establish the recurrence relation between the optimal redundancy of $(f,t)$\n-function-correcting codes for the $(b+1)$-read and $b$-read channels. We\nestablish an upper bound on the redundancy of general locally ($\\lambda,\\rho$,\n$b$)-function-correcting codes by linking it to the minimum achievable length\nof $b$-symbol error-correcting codes and traditional Hamming-metric codes,\ngiven a fixed number of codewords and a specified minimum distance.\nSpecifically, we present explicit upper bounds for the classes of\n($4,2t,b$)-local functions and ($2^b,2t,b$)-local functions. Additionally, for\nthe case where $b=1$, we show that a ($3,2t,1$)-local function achieves the\noptimal redundancy of $3t$ under certain conditions. Moreover, we explicitly\ninvestigate locality and redundancy for the weight distribution function.",
      "url": "http://arxiv.org/abs/2505.09473v1",
      "published_time_eastern_timestamp": 1747236162.0
    },
    {
      "title": "The Parity Flow Formalism: Tracking Quantum Information Throughout\n  Computation",
      "summary": "We propose the Parity Flow formalism, a method for tracking the information\nflow in quantum circuits. This method adds labels to quantum circuit diagrams\nsuch that the action of Clifford gates can be understood as a recoding of\nquantum information. The action of non-Clifford gates in the encoded space can\nbe directly deduced from those labels without backtracking. An application of\nflow tracking is to design resource-efficient quantum circuits by changing any\npresent encoding via a simple set of rules. Finally, the Parity Flow formalism\ncan be used in combination with stabilizer codes to further reduce quantum\ncircuit depth and to reveal additional operations that can be implemented in\nparallel.",
      "url": "http://arxiv.org/abs/2505.09468v1",
      "published_time_eastern_timestamp": 1747236005.0
    },
    {
      "title": "The asymmetry of white dwarf double detonations and the observed scatter\n  around the Phillips relation",
      "summary": "Recent Type Ia supernova (SN Ia) simulations featuring a double detonation\nscenario have managed to reproduce the overall trend of the Phillips relation\nreasonably well. However, most, if not all, multidimensional simulations\nstruggle to reproduce the scatter of observed SNe around this relation,\nexceeding it substantially. In this study, we investigate whether the excessive\nscatter around the Phillips relation can be caused by an off-center ignition of\nthe carbon-oxygen (CO) core in the double detonation scenario and if this can\nhelp constrain possible SN Ia explosion channels. We simulated the detonation\nof three different initial CO white dwarfs of $0.9$, $1.0$, and $1.1\\,M_\\odot$,\nartificially ignited at systematically offset locations using the Arepo code.\nAfter nucleosynthetic postprocessing, we generated synthetic observables using\nthe Artis code and compared these results against observational data and models\nof other works. We find that our simulations produce synthetic observables well\nwithin the range of the observed data in terms of viewing angle scatter. The\nmajority of the viewing angle variability seems to be caused by line blanketing\nin the blue wavelengths of intermediate-mass elements and lighter iron-group\nelements, which are asymmetrically distributed in the outer layers of the\nashes. Our results suggest that although the off-center ignition of the CO\nintroduces substantial line of sight effects, it is not responsible for the\nexcessive viewing angle scatter observed in other models. Instead, this effect\nseems to be caused by the detonation ashes from the rather massive helium (He)\nshells in current state-of-the-art models. Further reducing the He-shell masses\nof double detonation progenitors may be able to alleviate this issue and yield\nobservables that reproduce the Phillips relation.",
      "url": "http://arxiv.org/abs/2505.09445v1",
      "published_time_eastern_timestamp": 1747234464.0
    },
    {
      "title": "Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One\n  GPU",
      "summary": "We present a method for training multi-task vision-language robotic diffusion\npolicies that reduces training time and memory usage by an order of magnitude.\nThis improvement arises from a previously underexplored distinction between\naction diffusion and the image diffusion techniques that inspired it: image\ngeneration targets are high-dimensional, while robot actions lie in a much\nlower-dimensional space. Meanwhile, the vision-language conditions for action\ngeneration remain high-dimensional. Our approach, Mini-Diffuser, exploits this\nasymmetry by introducing Level-2 minibatching, which pairs multiple noised\naction samples with each vision-language condition, instead of the conventional\none-to-one sampling strategy. To support this batching scheme, we introduce\narchitectural adaptations to the diffusion transformer that prevent information\nleakage across samples while maintaining full conditioning access. In RLBench\nsimulations, Mini-Diffuser achieves 95\\% of the performance of state-of-the-art\nmulti-task diffusion policies, while using only 5\\% of the training time and\n7\\% of the memory. Real-world experiments further validate that Mini-Diffuser\npreserves the key strengths of diffusion-based policies, including the ability\nto model multimodal action distributions and produce behavior conditioned on\ndiverse perceptual inputs. Code available at\ngithub.com/utomm/mini-diffuse-actor.",
      "url": "http://arxiv.org/abs/2505.09430v1",
      "published_time_eastern_timestamp": 1747233280.0
    },
    {
      "title": "Unraveling spin entanglement using quantum gates with scanning tunneling\n  microscopy-driven electron spin resonance",
      "summary": "Quantum entanglement is a fundamental resource for quantum information\nprocessing, and its controlled generation and detection remain key challenges\nin scalable quantum architectures. Here, we numerically demonstrate the\ndeterministic generation of entangled spin states in a solid-state platform by\nimplementing quantum gates via electron spin resonance combined with scanning\ntunneling microscopy (ESR-STM). Using two titanium atoms on a MgO/Ag(100)\nsubstrate as a model, we construct a two-qubit system whose dynamics are\ncoherently manipulated through tailored microwave pulse sequences. We generate\nBell states by implementing a Hadamard gate followed by a controlled-NOT gate,\nand evaluate its fidelity and concurrence using the quantum-master\nequation-based code TimeESR. Our results demonstrate that ESR-STM can create\nentangled states with significant fidelity. This study paves the way for the\nrealization of atom-based quantum circuits and highlights ESR-STM as a powerful\ntool for probing and engineering entangled states on surfaces.",
      "url": "http://arxiv.org/abs/2505.09428v1",
      "published_time_eastern_timestamp": 1747232988.0
    },
    {
      "title": "Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion",
      "summary": "Recent studies have proved that imitation learning shows strong potential in\nthe field of robotic manipulation. However, existing methods still struggle\nwith precision manipulation task and rely on inefficient image/point cloud\nobservations. In this paper, we explore to introduce SE(3) object pose into\nimitation learning and propose the pose-guided efficient imitation learning\nmethods for robotic precise insertion task. First, we propose a precise\ninsertion diffusion policy which utilizes the relative SE(3) pose as the\nobservation-action pair. The policy models the source object SE(3) pose\ntrajectory relative to the target object. Second, we explore to introduce the\nRGBD data to the pose-guided diffusion policy. Specifically, we design a\ngoal-conditioned RGBD encoder to capture the discrepancy between the current\nstate and the goal state. In addition, a pose-guided residual gated fusion\nmethod is proposed, which takes pose features as the backbone, and the RGBD\nfeatures selectively compensate for pose feature deficiencies through an\nadaptive gating mechanism. Our methods are evaluated on 6 robotic precise\ninsertion tasks, demonstrating competitive performance with only 7-10\ndemonstrations. Experiments demonstrate that the proposed methods can\nsuccessfully complete precision insertion tasks with a clearance of about 0.01\nmm. Experimental results highlight its superior efficiency and generalization\ncapability compared to existing baselines. Code will be available at\nhttps://github.com/sunhan1997/PoseInsert.",
      "url": "http://arxiv.org/abs/2505.09424v1",
      "published_time_eastern_timestamp": 1747232732.0
    },
    {
      "title": "FaceShield: Explainable Face Anti-Spoofing with Multimodal Large\n  Language Models",
      "summary": "Face anti-spoofing (FAS) is crucial for protecting facial recognition systems\nfrom presentation attacks. Previous methods approached this task as a\nclassification problem, lacking interpretability and reasoning behind the\npredicted results. Recently, multimodal large language models (MLLMs) have\nshown strong capabilities in perception, reasoning, and decision-making in\nvisual tasks. However, there is currently no universal and comprehensive MLLM\nand dataset specifically designed for FAS task. To address this gap, we propose\nFaceShield, a MLLM for FAS, along with the corresponding pre-training and\nsupervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K.\nFaceShield is capable of determining the authenticity of faces, identifying\ntypes of spoofing attacks, providing reasoning for its judgments, and detecting\nattack areas. Specifically, we employ spoof-aware vision perception (SAVP) that\nincorporates both the original image and auxiliary information based on prior\nknowledge. We then use an prompt-guided vision token masking (PVTM) strategy to\nrandom mask vision tokens, thereby improving the model's generalization\nability. We conducted extensive experiments on three benchmark datasets,\ndemonstrating that FaceShield significantly outperforms previous deep learning\nmodels and general MLLMs on four FAS tasks, i.e., coarse-grained\nclassification, fine-grained classification, reasoning, and attack\nlocalization. Our instruction datasets, protocols, and codes will be released\nsoon.",
      "url": "http://arxiv.org/abs/2505.09415v1",
      "published_time_eastern_timestamp": 1747231843.0
    },
    {
      "title": "FACTors: A New Dataset for Studying the Fact-checking Ecosystem",
      "summary": "Our fight against false information is spearheaded by fact-checkers. They\ninvestigate the veracity of claims and document their findings as fact-checking\nreports. With the rapid increase in the amount of false information circulating\nonline, the use of automation in fact-checking processes aims to strengthen\nthis ecosystem by enhancing scalability. Datasets containing fact-checked\nclaims play a key role in developing such automated solutions. However, to the\nbest of our knowledge, there is no fact-checking dataset at the ecosystem\nlevel, covering claims from a sufficiently long period of time and sourced from\na wide range of actors reflecting the entire ecosystem that admittedly follows\nwidely-accepted codes and principles of fact-checking. We present a new dataset\nFACTors, the first to fill this gap by presenting ecosystem-level data on\nfact-checking. It contains 118,112 claims from 117,993 fact-checking reports in\nEnglish (co-)authored by 1,953 individuals and published during the period of\n1995-2025 by 39 fact-checking organisations that are active signatories of the\nIFCN (International Fact-Checking Network) and/or EFCSN (European Fact-Checking\nStandards Network). It contains 7,327 overlapping claims investigated by\nmultiple fact-checking organisations, corresponding to 2,977 unique claims. It\nallows to conduct new ecosystem-level studies of the fact-checkers\n(organisations and individuals). To demonstrate the usefulness of FACTors, we\npresent three example applications, including a first-of-its-kind statistical\nanalysis of the fact-checking ecosystem, examining the political inclinations\nof the fact-checking organisations, and attempting to assign a credibility\nscore to each organisation based on the findings of the statistical analysis\nand political leanings. Our methods for constructing FACTors are generic and\ncan be used to maintain a live dataset that can be updated dynamically.",
      "url": "http://arxiv.org/abs/2505.09414v1",
      "published_time_eastern_timestamp": 1747231822.0
    },
    {
      "title": "Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians",
      "summary": "Current learning-based methods predict NeRF or 3D Gaussians from point clouds\nto achieve photo-realistic rendering but still depend on categorical priors,\ndense point clouds, or additional refinements. Hence, we introduce a novel\npoint cloud rendering method by predicting 2D Gaussians from point clouds. Our\nmethod incorporates two identical modules with an entire-patch architecture\nenabling the network to be generalized to multiple datasets. The module\nnormalizes and initializes the Gaussians utilizing the point cloud information\nincluding normals, colors and distances. Then, splitting decoders are employed\nto refine the initial Gaussians by duplicating them and predicting more\naccurate results, making our methodology effectively accommodate sparse point\nclouds as well. Once trained, our approach exhibits direct generalization to\npoint clouds across different categories. The predicted Gaussians are employed\ndirectly for rendering without additional refinement on the rendered images,\nretaining the benefits of 2D Gaussians. We conduct extensive experiments on\nvarious datasets, and the results demonstrate the superiority and\ngeneralization of our method, which achieves SOTA performance. The code is\navailable at\nhttps://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.",
      "url": "http://arxiv.org/abs/2505.09413v1",
      "published_time_eastern_timestamp": 1747231809.0
    }
  ]
}